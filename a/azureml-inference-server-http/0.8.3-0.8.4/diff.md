# Comparing `tmp/azureml_inference_server_http-0.8.3-py3-none-any.whl.zip` & `tmp/azureml_inference_server_http-0.8.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,58 +1,58 @@
-Zip file size: 58069 bytes, number of entries: 56
--rw-rw-r--  2.0 unx      332 b- defN 23-Mar-23 23:05 azureml_inference_server_http/__init__.py
--rw-rw-r--  2.0 unx       35 b- defN 23-Mar-23 23:05 azureml_inference_server_http/__main__.py
--rw-rw-r--  2.0 unx       22 b- defN 23-Mar-23 23:05 azureml_inference_server_http/_version.py
--rw-rw-r--  2.0 unx      405 b- defN 23-Mar-23 23:05 azureml_inference_server_http/aml_prepost_server.py
--rw-rw-r--  2.0 unx     7252 b- defN 23-Mar-23 23:05 azureml_inference_server_http/amlserver.py
--rw-rw-r--  2.0 unx     1585 b- defN 23-Mar-23 23:05 azureml_inference_server_http/amlserver_linux.py
--rw-rw-r--  2.0 unx      610 b- defN 23-Mar-23 23:05 azureml_inference_server_http/amlserver_win.py
--rw-rw-r--  2.0 unx     2931 b- defN 23-Mar-23 23:05 azureml_inference_server_http/args.py
--rw-rw-r--  2.0 unx     1080 b- defN 23-Mar-23 23:05 azureml_inference_server_http/constants.py
--rw-rw-r--  2.0 unx     1186 b- defN 23-Mar-23 23:05 azureml_inference_server_http/log_config.py
--rw-rw-r--  2.0 unx     1376 b- defN 23-Mar-23 23:05 azureml_inference_server_http/logging.json
--rw-rw-r--  2.0 unx      944 b- defN 23-Mar-23 23:05 azureml_inference_server_http/print_log_hook.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Mar-23 23:05 azureml_inference_server_http/api/__init__.py
--rw-rw-r--  2.0 unx     1354 b- defN 23-Mar-23 23:05 azureml_inference_server_http/api/aml_request.py
--rw-rw-r--  2.0 unx     1552 b- defN 23-Mar-23 23:05 azureml_inference_server_http/api/aml_response.py
--rw-rw-r--  2.0 unx     2643 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/README.md
--rw-rw-r--  2.0 unx     1917 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/__init__.py
--rw-rw-r--  2.0 unx     4724 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/aml_blueprint.py
--rw-rw-r--  2.0 unx     3223 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/config_manager.py
--rw-rw-r--  2.0 unx     1155 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/constants.py
--rw-rw-r--  2.0 unx      806 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/error_handlers.py
--rw-rw-r--  2.0 unx     3151 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/log_settings.py
--rw-rw-r--  2.0 unx     2600 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/middleware.py
--rw-rw-r--  2.0 unx      400 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/run.py
--rw-rw-r--  2.0 unx     3020 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/script_loader.py
--rw-rw-r--  2.0 unx      776 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/utils.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/clients/__init__.py
--rw-rw-r--  2.0 unx      442 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/clients/client_factory.py
--rw-rw-r--  2.0 unx      366 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/clients/model_server_client.py
--rw-rw-r--  2.0 unx     2416 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/clients/triton_grpc_client.py
--rw-rw-r--  2.0 unx     1145 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/clients/triton_rest_client.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/user/__init__.py
--rw-rw-r--  2.0 unx     1070 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/user/exceptions.py
--rw-rw-r--  2.0 unx      965 b- defN 23-Mar-23 23:05 azureml_inference_server_http/prepost_server/user/model_handler_base.py
--rw-rw-r--  2.0 unx     1273 b- defN 23-Mar-23 23:05 azureml_inference_server_http/server/__init__.py
--rw-rw-r--  2.0 unx     5590 b- defN 23-Mar-23 23:05 azureml_inference_server_http/server/aml_blueprint.py
--rw-rw-r--  2.0 unx     7927 b- defN 23-Mar-23 23:05 azureml_inference_server_http/server/appinsights_client.py
--rw-rw-r--  2.0 unx     7233 b- defN 23-Mar-23 23:05 azureml_inference_server_http/server/config.py
--rw-rw-r--  2.0 unx     3034 b- defN 23-Mar-23 23:05 azureml_inference_server_http/server/create_app.py
--rw-rw-r--  2.0 unx      659 b- defN 23-Mar-23 23:05 azureml_inference_server_http/server/entry.py
--rw-rw-r--  2.0 unx      455 b- defN 23-Mar-23 23:05 azureml_inference_server_http/server/exceptions.py
--rw-rw-r--  2.0 unx     5412 b- defN 23-Mar-23 23:05 azureml_inference_server_http/server/input_parsers.py
--rw-rw-r--  2.0 unx     9887 b- defN 23-Mar-23 23:05 azureml_inference_server_http/server/routes.py
--rw-rw-r--  2.0 unx     8438 b- defN 23-Mar-23 23:05 azureml_inference_server_http/server/swagger.py
--rw-rw-r--  2.0 unx     3622 b- defN 23-Mar-23 23:05 azureml_inference_server_http/server/swagger2_template.json
--rw-rw-r--  2.0 unx     4279 b- defN 23-Mar-23 23:05 azureml_inference_server_http/server/swagger3_template.json
--rw-rw-r--  2.0 unx     8350 b- defN 23-Mar-23 23:05 azureml_inference_server_http/server/user_script.py
--rw-rw-r--  2.0 unx     2137 b- defN 23-Mar-23 23:05 azureml_inference_server_http/server/utils.py
--rw-rw-r--  2.0 unx      275 b- defN 23-Mar-23 23:05 azureml_inference_server_http/server/azureml/contrib/services/aml_request.py
--rw-rw-r--  2.0 unx      275 b- defN 23-Mar-23 23:05 azureml_inference_server_http/server/azureml/contrib/services/aml_response.py
--rw-rw-r--  2.0 unx     1068 b- defN 23-Mar-23 23:05 azureml_inference_server_http-0.8.3.dist-info/LICENSE.txt
--rw-rw-r--  2.0 unx    12224 b- defN 23-Mar-23 23:05 azureml_inference_server_http-0.8.3.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Mar-23 23:05 azureml_inference_server_http-0.8.3.dist-info/WHEEL
--rw-rw-r--  2.0 unx       76 b- defN 23-Mar-23 23:05 azureml_inference_server_http-0.8.3.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx       30 b- defN 23-Mar-23 23:05 azureml_inference_server_http-0.8.3.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     6154 b- defN 23-Mar-23 23:05 azureml_inference_server_http-0.8.3.dist-info/RECORD
-56 files, 139973 bytes uncompressed, 47695 bytes compressed:  65.9%
+Zip file size: 58443 bytes, number of entries: 56
+-rw-rw-r--  2.0 unx      332 b- defN 23-Apr-20 14:30 azureml_inference_server_http/__init__.py
+-rw-rw-r--  2.0 unx       35 b- defN 23-Apr-20 14:30 azureml_inference_server_http/__main__.py
+-rw-rw-r--  2.0 unx       22 b- defN 23-Apr-20 14:30 azureml_inference_server_http/_version.py
+-rw-rw-r--  2.0 unx      405 b- defN 23-Apr-20 14:30 azureml_inference_server_http/aml_prepost_server.py
+-rw-rw-r--  2.0 unx     8444 b- defN 23-Apr-20 14:30 azureml_inference_server_http/amlserver.py
+-rw-rw-r--  2.0 unx     1755 b- defN 23-Apr-20 14:30 azureml_inference_server_http/amlserver_linux.py
+-rw-rw-r--  2.0 unx      610 b- defN 23-Apr-20 14:30 azureml_inference_server_http/amlserver_win.py
+-rw-rw-r--  2.0 unx     3078 b- defN 23-Apr-20 14:30 azureml_inference_server_http/args.py
+-rw-rw-r--  2.0 unx     1197 b- defN 23-Apr-20 14:30 azureml_inference_server_http/constants.py
+-rw-rw-r--  2.0 unx     1186 b- defN 23-Apr-20 14:30 azureml_inference_server_http/log_config.py
+-rw-rw-r--  2.0 unx     1376 b- defN 23-Apr-20 14:30 azureml_inference_server_http/logging.json
+-rw-rw-r--  2.0 unx      944 b- defN 23-Apr-20 14:30 azureml_inference_server_http/print_log_hook.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Apr-20 14:30 azureml_inference_server_http/api/__init__.py
+-rw-rw-r--  2.0 unx     1354 b- defN 23-Apr-20 14:30 azureml_inference_server_http/api/aml_request.py
+-rw-rw-r--  2.0 unx     1553 b- defN 23-Apr-20 14:30 azureml_inference_server_http/api/aml_response.py
+-rw-rw-r--  2.0 unx     2643 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/README.md
+-rw-rw-r--  2.0 unx     1917 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/__init__.py
+-rw-rw-r--  2.0 unx     4724 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/aml_blueprint.py
+-rw-rw-r--  2.0 unx     3223 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/config_manager.py
+-rw-rw-r--  2.0 unx     1155 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/constants.py
+-rw-rw-r--  2.0 unx      806 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/error_handlers.py
+-rw-rw-r--  2.0 unx     3151 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/log_settings.py
+-rw-rw-r--  2.0 unx     2600 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/middleware.py
+-rw-rw-r--  2.0 unx      400 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/run.py
+-rw-rw-r--  2.0 unx     3020 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/script_loader.py
+-rw-rw-r--  2.0 unx      776 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/utils.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/clients/__init__.py
+-rw-rw-r--  2.0 unx      442 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/clients/client_factory.py
+-rw-rw-r--  2.0 unx      366 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/clients/model_server_client.py
+-rw-rw-r--  2.0 unx     2416 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/clients/triton_grpc_client.py
+-rw-rw-r--  2.0 unx     1145 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/clients/triton_rest_client.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/user/__init__.py
+-rw-rw-r--  2.0 unx     1070 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/user/exceptions.py
+-rw-rw-r--  2.0 unx      965 b- defN 23-Apr-20 14:30 azureml_inference_server_http/prepost_server/user/model_handler_base.py
+-rw-rw-r--  2.0 unx     1273 b- defN 23-Apr-20 14:30 azureml_inference_server_http/server/__init__.py
+-rw-rw-r--  2.0 unx     5366 b- defN 23-Apr-20 14:30 azureml_inference_server_http/server/aml_blueprint.py
+-rw-rw-r--  2.0 unx     7927 b- defN 23-Apr-20 14:30 azureml_inference_server_http/server/appinsights_client.py
+-rw-rw-r--  2.0 unx     7233 b- defN 23-Apr-20 14:30 azureml_inference_server_http/server/config.py
+-rw-rw-r--  2.0 unx     3034 b- defN 23-Apr-20 14:30 azureml_inference_server_http/server/create_app.py
+-rw-rw-r--  2.0 unx      659 b- defN 23-Apr-20 14:30 azureml_inference_server_http/server/entry.py
+-rw-rw-r--  2.0 unx      455 b- defN 23-Apr-20 14:30 azureml_inference_server_http/server/exceptions.py
+-rw-rw-r--  2.0 unx     5412 b- defN 23-Apr-20 14:30 azureml_inference_server_http/server/input_parsers.py
+-rw-rw-r--  2.0 unx     9974 b- defN 23-Apr-20 14:30 azureml_inference_server_http/server/routes.py
+-rw-rw-r--  2.0 unx     8438 b- defN 23-Apr-20 14:30 azureml_inference_server_http/server/swagger.py
+-rw-rw-r--  2.0 unx     3622 b- defN 23-Apr-20 14:30 azureml_inference_server_http/server/swagger2_template.json
+-rw-rw-r--  2.0 unx     4279 b- defN 23-Apr-20 14:30 azureml_inference_server_http/server/swagger3_template.json
+-rw-rw-r--  2.0 unx     8350 b- defN 23-Apr-20 14:30 azureml_inference_server_http/server/user_script.py
+-rw-rw-r--  2.0 unx     2137 b- defN 23-Apr-20 14:30 azureml_inference_server_http/server/utils.py
+-rw-rw-r--  2.0 unx      275 b- defN 23-Apr-20 14:30 azureml_inference_server_http/server/azureml/contrib/services/aml_request.py
+-rw-rw-r--  2.0 unx      275 b- defN 23-Apr-20 14:30 azureml_inference_server_http/server/azureml/contrib/services/aml_response.py
+-rw-rw-r--  2.0 unx     1068 b- defN 23-Apr-20 14:31 azureml_inference_server_http-0.8.4.dist-info/LICENSE.txt
+-rw-rw-r--  2.0 unx    12471 b- defN 23-Apr-20 14:31 azureml_inference_server_http-0.8.4.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Apr-20 14:31 azureml_inference_server_http-0.8.4.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       76 b- defN 23-Apr-20 14:31 azureml_inference_server_http-0.8.4.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx       30 b- defN 23-Apr-20 14:31 azureml_inference_server_http-0.8.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6154 b- defN 23-Apr-20 14:31 azureml_inference_server_http-0.8.4.dist-info/RECORD
+56 files, 141710 bytes uncompressed, 48069 bytes compressed:  66.1%
```

## zipnote {}

```diff
@@ -144,26 +144,26 @@
 
 Filename: azureml_inference_server_http/server/azureml/contrib/services/aml_request.py
 Comment: 
 
 Filename: azureml_inference_server_http/server/azureml/contrib/services/aml_response.py
 Comment: 
 
-Filename: azureml_inference_server_http-0.8.3.dist-info/LICENSE.txt
+Filename: azureml_inference_server_http-0.8.4.dist-info/LICENSE.txt
 Comment: 
 
-Filename: azureml_inference_server_http-0.8.3.dist-info/METADATA
+Filename: azureml_inference_server_http-0.8.4.dist-info/METADATA
 Comment: 
 
-Filename: azureml_inference_server_http-0.8.3.dist-info/WHEEL
+Filename: azureml_inference_server_http-0.8.4.dist-info/WHEEL
 Comment: 
 
-Filename: azureml_inference_server_http-0.8.3.dist-info/entry_points.txt
+Filename: azureml_inference_server_http-0.8.4.dist-info/entry_points.txt
 Comment: 
 
-Filename: azureml_inference_server_http-0.8.3.dist-info/top_level.txt
+Filename: azureml_inference_server_http-0.8.4.dist-info/top_level.txt
 Comment: 
 
-Filename: azureml_inference_server_http-0.8.3.dist-info/RECORD
+Filename: azureml_inference_server_http-0.8.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## azureml_inference_server_http/_version.py

```diff
@@ -1 +1 @@
-__version__ = "0.8.3"
+__version__ = "0.8.4"
```

## azureml_inference_server_http/amlserver.py

```diff
@@ -35,18 +35,20 @@
     server_env_vars_descriptions = {
         ENV_AZUREML_ENTRY_SCRIPT: "Entry Script Name",
         ENV_AZUREML_MODEL_DIR: "Model Directory",
         ENV_AZUREML_CONFIG_FILE: "Config File",
         ENV_WORKER_COUNT: "Worker Count",
         ENV_WORKER_TIMEOUT: "Worker Timeout (seconds)",
         ENV_PORT: "Server Port",
+        ENV_HEALTH_PORT: "Health Port",
         ENV_AML_APP_INSIGHTS_ENABLED: "Application Insights Enabled",
         ENV_AML_APP_INSIGHTS_KEY: "Application Insights Key",
         ENV_AZUREML_SERVER_VERSION: "Inferencing HTTP server version",
         ENV_AML_CORS_ORIGINS: "CORS for the specified origins",
+        ENV_SEPERATE_HEALTH_ENDPOINT: "Create dedicated endpoint for health",
     }
 
     print()
     print("Server Settings")
     print("---------------")
     for var in server_env_vars_descriptions:
         if var == ENV_AML_APP_INSIGHTS_KEY and ENV_AML_APP_INSIGHTS_KEY in os.environ:
@@ -64,15 +66,15 @@
     print()
 
 
 def print_routes():
     print()
     print("Server Routes")
     print("---------------")
-    print(f"Liveness Probe: GET   127.0.0.1:{os.environ[ENV_PORT]}/")
+    print(f"Liveness Probe: GET   127.0.0.1:{os.environ[ENV_HEALTH_PORT]}/")
     print(f"Score:          POST  127.0.0.1:{os.environ[ENV_PORT]}/score")
     print()
 
 
 def set_environment_variables(arg_val, env_var_name, default_val=None):
     if arg_val is not None:
         os.environ[env_var_name] = str(arg_val)
@@ -144,14 +146,22 @@
     ):
         set_environment_variables(f"azmlinfsrv/{__version__}", ENV_AZUREML_SERVER_VERSION)
 
     # If the port environment variable is set outside of cli, we overwrite it
     os.environ[ENV_PORT] = str(DEFAULT_PORT)
     set_environment_variables(args.port, ENV_PORT, default_val=DEFAULT_PORT)
 
+    # If seperate health endpoint env is set, overwrite env of health_port w/ args.health_port or default health port
+    # If not, then overwrite env of health_port with the normal default port (no seperate listener created)
+    if os.getenv(ENV_SEPERATE_HEALTH_ENDPOINT) == "true":
+        os.environ[ENV_HEALTH_PORT] = str(DEFAULT_HEALTH_PORT)
+        set_environment_variables(args.health_port, ENV_HEALTH_PORT, default_val=DEFAULT_HEALTH_PORT)
+    else:
+        set_environment_variables(args.port, ENV_HEALTH_PORT, default_val=DEFAULT_PORT)
+
     # We assume if the instrumentation key is set via cli, then appinsights is enabled
     # Validation takes place later and will disable this if not a valid key
     if args.appinsights_instrumentation_key is not None:
         os.environ[ENV_AML_APP_INSIGHTS_ENABLED] = "true"
     set_environment_variables(args.appinsights_instrumentation_key, ENV_AML_APP_INSIGHTS_KEY)
 
     if args.access_control_allow_origins is not None:
@@ -178,18 +188,30 @@
     print_server_info()
     print_server_settings()
     print_routes()
     print_python_path()
 
     if sys.platform == "win32":
         from azureml_inference_server_http import amlserver_win as srv
+
+        srv.run(DEFAULT_HOST, int(os.environ[ENV_PORT]), int(os.environ[ENV_WORKER_COUNT]))
     else:
         if os.environ[ENV_PREPOST] == "True":
             from azureml_inference_server_http import aml_prepost_server as srv
+
+            srv.run(DEFAULT_HOST, int(os.environ[ENV_PORT]), int(os.environ[ENV_WORKER_COUNT]))
         else:
             from azureml_inference_server_http import amlserver_linux as srv
 
-    srv.run(DEFAULT_HOST, int(os.environ[ENV_PORT]), int(os.environ[ENV_WORKER_COUNT]))
+            if os.getenv(ENV_SEPERATE_HEALTH_ENDPOINT) == "true":
+                srv.run(
+                    DEFAULT_HOST,
+                    int(os.environ[ENV_PORT]),
+                    int(os.environ[ENV_WORKER_COUNT]),
+                    int(os.environ[ENV_HEALTH_PORT]),
+                )
+            else:
+                srv.run(DEFAULT_HOST, int(os.environ[ENV_PORT]), int(os.environ[ENV_WORKER_COUNT]))
 
 
 if __name__ == "__main__":
     run()
```

## azureml_inference_server_http/amlserver_linux.py

```diff
@@ -1,24 +1,25 @@
 import os
 import sys
 
 import gunicorn.app.wsgiapp
 
 from .constants import (
+    DEFAULT_HEALTH_PORT,
     DEFAULT_HOST,
     DEFAULT_PORT,
     DEFAULT_WORKER_COUNT,
     DEFAULT_WORKER_PRELOAD,
     DEFAULT_WORKER_TIMEOUT_SECONDS,
     ENV_WORKER_PRELOAD,
     ENV_WORKER_TIMEOUT,
 )
 
 
-def run(host, port, worker_count):
+def run(host, port, worker_count, health_port=None):
     #
     # Manipulate the sys.argv to apply settings to gunicorn.app.wsgiapp.
     #
     # Not all gunicorn settings can be applied using environment variables and
     # command arguments have higher authoritative than other settings.
     #
     # Configuration authoritative:
@@ -38,17 +39,21 @@
         "-",
         # Not sending error logs to /dev/null results in them being output
         # twice. Once by our log handler, once to the gunicorn log handler
         "--error-logfile",
         "/dev/null",
     ]
 
+    if health_port:
+        sys.argv.insert(1, "-b")
+        sys.argv.insert(2, f"{host}:{health_port}")
+
     if os.environ.get(ENV_WORKER_PRELOAD, DEFAULT_WORKER_PRELOAD).lower() == "true":
         sys.argv.append("--preload")
 
     sys.argv.append("azureml_inference_server_http.server.entry:app")
 
     gunicorn.app.wsgiapp.WSGIApplication("%(prog)s [OPTIONS] [APP_MODULE]").run()
 
 
 if __name__ == "__main__":
-    run(DEFAULT_HOST, DEFAULT_PORT, DEFAULT_WORKER_COUNT)
+    run(DEFAULT_HOST, DEFAULT_PORT, DEFAULT_WORKER_COUNT, DEFAULT_HEALTH_PORT)
```

## azureml_inference_server_http/args.py

```diff
@@ -45,14 +45,17 @@
         required=False,
         help="The instrumentation key to the application insights where the logs will be published.",
     )
     parser.add_argument(
         "--port", required=False, type=validate_port, help="The serving port of the server. Default is 5001."
     )
     parser.add_argument(
+        "--health_port", required=False, type=validate_port, help="The health port of the server. Default is 5000."
+    )
+    parser.add_argument(
         "--worker_count",
         required=False,
         type=validate_worker_count,
         help="The number of worker threads which will process concurrent requests. Default is 1.",
     )
     parser.add_argument(
         "--access_control_allow_origins",
```

## azureml_inference_server_http/constants.py

```diff
@@ -2,14 +2,15 @@
 import sys
 
 DEFAULT_APP_ROOT = "/var/azureml-app"
 PACKAGE_ROOT = os.path.dirname(os.path.realpath(sys.modules[__package__].__file__))
 SERVER_ROOT = os.path.join(PACKAGE_ROOT, "server")
 
 DEFAULT_PORT = 5001
+DEFAULT_HEALTH_PORT = 5000
 DEFAULT_HOST = "0.0.0.0"
 DEFAULT_WORKER_COUNT = 1
 DEFAULT_APPINSIGHTS_ENABLED = "false"
 DEFAULT_WORKER_TIMEOUT_SECONDS = "300"
 DEFAULT_WORKER_PRELOAD = "false"
 
 
@@ -20,13 +21,15 @@
 ENV_AZUREML_MODEL_DIR = "AZUREML_MODEL_DIR"
 ENV_AML_APP_INSIGHTS_ENABLED = "AML_APP_INSIGHTS_ENABLED"
 ENV_AML_APP_INSIGHTS_KEY = "AML_APP_INSIGHTS_KEY"
 ENV_WORKER_COUNT = "WORKER_COUNT"
 ENV_WORKER_TIMEOUT = "WORKER_TIMEOUT"
 ENV_WORKER_PRELOAD = "WORKER_PRELOAD"
 ENV_PORT = "SERVER_PORT"
+ENV_HEALTH_PORT = "HEALTH_PORT"
 ENV_PREPOST = "PREPOST_SERVER"
 ENV_BACKEND_TRANSPORT_PROTOCOL = "TRANSPORT_PROTOCOL"
 ENV_AZUREML_SERVER_VERSION = "HTTP_X_MS_SERVER_VERSION"
 ENV_AZUREML_SERVER_VERSION_ENABLED = "SERVER_VERSION_LOG_RESPONSE_ENABLED"
 ENV_AML_CORS_ORIGINS = "AML_CORS_ORIGINS"
 ENV_AZUREML_CONFIG_FILE = "AZUREML_CONFIG_FILE"
+ENV_SEPERATE_HEALTH_ENDPOINT = "SEPERATE_HEALTH_ENDPOINT"
```

## azureml_inference_server_http/api/aml_response.py

```diff
@@ -1,15 +1,16 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 """AMLResponse class used by score.py that needs raw HTTP access"""
 
-from flask import Response
 import json
 
+from flask import Response
+
 
 class AMLResponse(Response):
     """AMLResponse class used by score.py that needs raw HTTP access"""
 
     def __init__(self, message, status_code, response_headers={}, json_str=False, run_function_failed=False):
         """Create new instance"""
         if message is not None:
```

## azureml_inference_server_http/server/aml_blueprint.py

```diff
@@ -92,21 +92,16 @@
                 logger.error("No score script found. Expected score script main.py.")
                 logger.error(f"Expected script to be found in PYTHONPATH: {sys.path}")
                 if os.path.isdir(config.app_root):
                     logger.error(f"Current contents of AML_APP_ROOT: {os.listdir(config.app_root)}")
                 else:
                     logger.error(f"The directory {config.app_root} not an accessible directory in the container.")
             elif "FileNotFoundError" in traceback.format_exc():
-                logger.error("Scoring script not found. No such file or directory.")
-                logger.error(
-                    (
-                        "Expected script to be found in: "
-                        f"{os.path.join(config.app_root,config.entry_script.replace('/', os.sep))}"
-                    )
-                )
+                logger.error("No such file or directory.")
+                logger.error(traceback.format_exc())
             else:
                 logger.error(traceback.format_exc())
             sys.exit(3)
 
         try:
             self.user_script.invoke_init()
         except UserScriptError:
```

## azureml_inference_server_http/server/routes.py

```diff
@@ -2,18 +2,18 @@
 import faulthandler
 import logging
 import os
 import time
 import traceback
 import uuid
 
-from azureml_inference_server_http.api.aml_response import AMLResponse
 from flask import g, request, Response
 from werkzeug.exceptions import HTTPException
 
+from azureml_inference_server_http.api.aml_response import AMLResponse
 from .aml_blueprint import AMLInferenceBlueprint
 from .config import config
 from .input_parsers import (
     BadInput,
     JsonStringInput,
     RawRequestInput,
     UnsupportedHTTPMethod,
@@ -162,15 +162,18 @@
     duration_ms = (time.perf_counter() - g.starting_perf_counter) * 1e3  # second to millisecond
     if g.api_name:
         path = request.path
         # No query, don't bother
         if request.query_string:
             path += f"?{request.query_string.decode()}"
 
-        response_length = response.calculate_content_length()
+        if response.is_streamed:
+            response_length = "N/A"
+        else:
+            response_length = response.calculate_content_length()
 
         response_props = (
             request.method,
             path,
             response.status_code,
             # Always show 3 decimal places of precision
             "{:0.3f}ms".format(round(duration_ms, 3)),
```

## Comparing `azureml_inference_server_http-0.8.3.dist-info/LICENSE.txt` & `azureml_inference_server_http-0.8.4.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `azureml_inference_server_http-0.8.3.dist-info/METADATA` & `azureml_inference_server_http-0.8.4.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: azureml-inference-server-http
-Version: 0.8.3
+Version: 0.8.4
 Summary: Azure Machine Learning inferencing server.
 Home-page: UNKNOWN
 Author: Microsoft Corp
 Author-email: amlInferenceImages@microsoft.com
 License: https://aka.ms/azureml-sdk-license
 Platform: UNKNOWN
 Classifier: Programming Language :: Python
@@ -149,14 +149,29 @@
 | "AML_APP_INSIGHTS_ENABLED": true
 | }
 
 
 Changelog
 =========
 
+0.8.4 (2023-04-19)
+~~~~~~~~~~~~~~~~~~
+
+Features
+--------
+
+- Added ability to configure seperate dedicated health check port.
+
+Fixes
+-----
+
+- Restored streaming for score response.
+- Updated the error message related to scoring script not found.
+
+
 0.8.3 (2023-03-23)
 ~~~~~~~~~~~~~~~~~~
 
 Fixes
 -----
 
 - Fixed the issue related to compatibility with flask1.
```

## Comparing `azureml_inference_server_http-0.8.3.dist-info/RECORD` & `azureml_inference_server_http-0.8.4.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 azureml_inference_server_http/__init__.py,sha256=u1dzO0E13FCgtv0XuFeOj3NvVDgckVNuL8mC8MUYhPg,332
 azureml_inference_server_http/__main__.py,sha256=Eewn-B_qD_ZbrFO2Wp8SDvxAwNBaR3syjbt5rpKTvuU,35
-azureml_inference_server_http/_version.py,sha256=otnwfmvJLUamPajTPUaIekiO9mA-2HPi-_h_E0N-uOQ,22
+azureml_inference_server_http/_version.py,sha256=jhHEJFZWhkQDemoZMomBYq-RNrKXknYzUaeIU9A6XsI,22
 azureml_inference_server_http/aml_prepost_server.py,sha256=TjlMrnR_6G8r-flKdDEwJ-NyEzN4xaNMS3XaNubxMj4,405
-azureml_inference_server_http/amlserver.py,sha256=ZC6dF1u8_xRDiOz2nTyqhIz23R5QZGrN9KEbzolGUww,7252
-azureml_inference_server_http/amlserver_linux.py,sha256=pa0oLblyr8bEwD3Y_B1LGMbHX2yNWHLXYntW7pzwkZg,1585
+azureml_inference_server_http/amlserver.py,sha256=gf_49s-JeUmYbVMhT_9mO-qqw9E1LC9POSnfOb_BHGY,8444
+azureml_inference_server_http/amlserver_linux.py,sha256=NfDrNnPfBtR3MlWCLZ756lRypSpT57zGV8JmqoTGyvg,1755
 azureml_inference_server_http/amlserver_win.py,sha256=DTNG6kkFEOqlzzm0T-KIuM1wkuR6BOAaCFVe5yXgveU,610
-azureml_inference_server_http/args.py,sha256=ruWjCJDe69aIEbA_jElxBu4ojk3xngv8s4aH4t2a_V4,2931
-azureml_inference_server_http/constants.py,sha256=9DIaF_fAokTtdyUhP1T6hYEHbCM4VmTWLAwp_qRBtcE,1080
+azureml_inference_server_http/args.py,sha256=fR6G8l6kXeBRCB711en2BDfDmS-rQRAVXCtgw9GkDhg,3078
+azureml_inference_server_http/constants.py,sha256=6LDDtgBzh9O7QNYsJ9BkroALCwUbbH5iVBn9cSO-4fI,1197
 azureml_inference_server_http/log_config.py,sha256=wLuf3rG2xXG7bbl31pi1Boq7bd1UrcF4rZy2GdakZAE,1186
 azureml_inference_server_http/logging.json,sha256=FFTzvXByC_u1xlGukY_RrRVR5Cjt7m9yOqn3wBvGYpE,1376
 azureml_inference_server_http/print_log_hook.py,sha256=h-q9HuD5qR4F_585lc81CuA3lGTgAokHVRMrHV45iR4,944
 azureml_inference_server_http/api/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 azureml_inference_server_http/api/aml_request.py,sha256=fj4t0u_fVMAY_FACM7TlIjSf8pDaS-eQhC0aSTKkKV0,1354
-azureml_inference_server_http/api/aml_response.py,sha256=JKrqzcbb2XUr_CI3ipfirBpF3doUcK1kNPLAfA2QvLo,1552
+azureml_inference_server_http/api/aml_response.py,sha256=kVDufgr-nwpdzjfcq2JNW6BjTfEviJO_t6JPOJoFYpI,1553
 azureml_inference_server_http/prepost_server/README.md,sha256=wKcAurTJujA2GyCQQFxnHjdBVhq9NyddXi8rQ6zgStE,2643
 azureml_inference_server_http/prepost_server/__init__.py,sha256=tCBzKrB2mdhUHXD7WHpWHl94HzQIFsrGKkJxWnOTXMw,1917
 azureml_inference_server_http/prepost_server/aml_blueprint.py,sha256=orgRiaw-cN9BsQraUX9W5HMx1kay1HKawpvQWR07_Ck,4724
 azureml_inference_server_http/prepost_server/config_manager.py,sha256=UxZwsdI-lkwermqcNAn5XDOd6gLo-mWRzacM_K98UNA,3223
 azureml_inference_server_http/prepost_server/constants.py,sha256=lEbMFlHTIV4D4pb8lgNWY9iAS0aqsgBhPCZLHz7lAJg,1155
 azureml_inference_server_http/prepost_server/error_handlers.py,sha256=s0lGJcBs1OPTi5Xv_fwDGIwuZIJ-bX8D3CkruDPZJAQ,806
 azureml_inference_server_http/prepost_server/log_settings.py,sha256=S8EHCfS-Miw719YiZrvN0Fvp43aItIfEG1RrWccNS9w,3151
@@ -29,28 +29,28 @@
 azureml_inference_server_http/prepost_server/clients/model_server_client.py,sha256=MGnMxgXtf5fOLUVSt-a1uthJON7fr0Yjnoi_TYHPu7o,366
 azureml_inference_server_http/prepost_server/clients/triton_grpc_client.py,sha256=q1_bwTEtYlZ0xRMIoToYqBgxMBSeU2rdJPK3xSnGQIU,2416
 azureml_inference_server_http/prepost_server/clients/triton_rest_client.py,sha256=hT54n8GMPtqGG2SXLIa6AQuFnujTiibRaPGbSKJ6wz4,1145
 azureml_inference_server_http/prepost_server/user/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 azureml_inference_server_http/prepost_server/user/exceptions.py,sha256=eI5dPRax0IZcwtKnWUXs2usMo_teDR_ljJ1gCNiByqc,1070
 azureml_inference_server_http/prepost_server/user/model_handler_base.py,sha256=-Vxa9z8tO0ePjL6zXihuKHgWpsNcn8o_6N-e7l8Ndg0,965
 azureml_inference_server_http/server/__init__.py,sha256=xb9_G1Cri77lwxDgiu5xYE-lxhLJj_9_2kdEz1If2oc,1273
-azureml_inference_server_http/server/aml_blueprint.py,sha256=qiQdpIU7xOnt9U1Vol4cjoJnXmmaHToJUNBxzMKfTfw,5590
+azureml_inference_server_http/server/aml_blueprint.py,sha256=pA9r7M4JITbyQqmqA9AdexYhR7XcRhxzZTVFYymdGj4,5366
 azureml_inference_server_http/server/appinsights_client.py,sha256=nUNrP_zIHX712kTqSQzs0d8UKNuPoRaA6t-nYqwJ6iM,7927
 azureml_inference_server_http/server/config.py,sha256=VmswdGZqLKkuulcaBCOKAsufKMnlL2I9q99gAhQkLqs,7233
 azureml_inference_server_http/server/create_app.py,sha256=Wd95umrKiMz--htxUPIrX-E1952k980gG1ik0BfYh30,3034
 azureml_inference_server_http/server/entry.py,sha256=Ud7M5jiB00OYsmbp8YfBd9a0Em-d-61nDNlIVcIwjoE,659
 azureml_inference_server_http/server/exceptions.py,sha256=H7FHV93FjZYTqpSvuP7EqrdSBSTz8e5ntPK_WtkWt4c,455
 azureml_inference_server_http/server/input_parsers.py,sha256=oQuR8ZUMFCKm3DPMfio2HBscdHeQMg8YzPB8pQp8Zd0,5412
-azureml_inference_server_http/server/routes.py,sha256=ddlQEWIlp5DGLhO3Mr201yia2S_C5cTy4uX6gagZl5o,9887
+azureml_inference_server_http/server/routes.py,sha256=XFdFFSz_egdMh2kFG4rKXZP_lsRBoS02ZrnySX3vRds,9974
 azureml_inference_server_http/server/swagger.py,sha256=P6GohwcSK-ekExL0r008D9QtXUalKd6pAgGCE3F3z0c,8438
 azureml_inference_server_http/server/swagger2_template.json,sha256=Y865cXAdv-RSPCwROcaQy-_1MPfCFMwjkyT_Wkxd-DU,3622
 azureml_inference_server_http/server/swagger3_template.json,sha256=w_dwUIQgiYAmId6uTTZzd5S5vQiuvJwUdJVHMN40QB0,4279
 azureml_inference_server_http/server/user_script.py,sha256=kaklJtoDTZ7ewzowwU0N7dr27XwgLtQDwi8oUjuetws,8350
 azureml_inference_server_http/server/utils.py,sha256=T1x8duJD2vnoXp_m9c6W_TR7BaRZyOLzJ_RxB8oragM,2137
 azureml_inference_server_http/server/azureml/contrib/services/aml_request.py,sha256=JTTq4IOs1MXyb-553LVOp3xPkiC8esq_BmGovdCo9HQ,275
 azureml_inference_server_http/server/azureml/contrib/services/aml_response.py,sha256=JTTq4IOs1MXyb-553LVOp3xPkiC8esq_BmGovdCo9HQ,275
-azureml_inference_server_http-0.8.3.dist-info/LICENSE.txt,sha256=d7LbQoNLxssK726XHp1FH9Sy7YchcVM-1rya_q8FIQI,1068
-azureml_inference_server_http-0.8.3.dist-info/METADATA,sha256=PKDzHkGeo7b3mWudJneFK-QNMxK0-atZuHIf3D4c09A,12224
-azureml_inference_server_http-0.8.3.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-azureml_inference_server_http-0.8.3.dist-info/entry_points.txt,sha256=QJIfw5WH5Xq4kiO00No9VfUIxiw4LDcTvgNNyesz56U,76
-azureml_inference_server_http-0.8.3.dist-info/top_level.txt,sha256=nwTGwbXm569vU_wbQtbjUk-5v1R6ukmMZHP2syPv3L4,30
-azureml_inference_server_http-0.8.3.dist-info/RECORD,,
+azureml_inference_server_http-0.8.4.dist-info/LICENSE.txt,sha256=d7LbQoNLxssK726XHp1FH9Sy7YchcVM-1rya_q8FIQI,1068
+azureml_inference_server_http-0.8.4.dist-info/METADATA,sha256=mDBPnsc9wGQ_lVMt9-dyygJJqz2oA0z5r0SUiA43uBg,12471
+azureml_inference_server_http-0.8.4.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+azureml_inference_server_http-0.8.4.dist-info/entry_points.txt,sha256=QJIfw5WH5Xq4kiO00No9VfUIxiw4LDcTvgNNyesz56U,76
+azureml_inference_server_http-0.8.4.dist-info/top_level.txt,sha256=nwTGwbXm569vU_wbQtbjUk-5v1R6ukmMZHP2syPv3L4,30
+azureml_inference_server_http-0.8.4.dist-info/RECORD,,
```

