# Comparing `tmp/autoprof-0.6.1-py2.py3-none-any.whl.zip` & `tmp/autoprof-0.6.2-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,65 +1,66 @@
-Zip file size: 133877 bytes, number of entries: 78
+Zip file size: 138711 bytes, number of entries: 79
 -rw-rw-r--  2.0 unx     3171 b- defN 23-Mar-17 23:11 autoprof/AP_config.py
--rw-rw-r--  2.0 unx     5650 b- defN 23-Apr-01 17:13 autoprof/__init__.py
+-rw-rw-r--  2.0 unx     5650 b- defN 23-Apr-21 02:56 autoprof/__init__.py
 -rw-rw-r--  2.0 unx      281 b- defN 23-Mar-17 23:11 autoprof/__main__.py
 -rw-rw-r--  2.0 unx     1072 b- defN 23-Mar-17 23:11 autoprof/fit/__init__.py
--rw-rw-r--  2.0 unx     6689 b- defN 23-Mar-30 13:28 autoprof/fit/base.py
--rw-rw-r--  2.0 unx       30 b- defN 22-Dec-15 19:05 autoprof/fit/gp.py
--rw-rw-r--  2.0 unx     6704 b- defN 23-Mar-30 13:28 autoprof/fit/gradient.py
--rw-rw-r--  2.0 unx     7538 b- defN 23-Mar-30 13:28 autoprof/fit/hmc.py
--rw-rw-r--  2.0 unx    13206 b- defN 23-Apr-01 17:12 autoprof/fit/iterative.py
--rw-rw-r--  2.0 unx    29711 b- defN 23-Mar-31 20:27 autoprof/fit/lm.py
+-rw-rw-r--  2.0 unx     6689 b- defN 23-Apr-08 19:05 autoprof/fit/base.py
+-rw-rw-r--  2.0 unx       30 b- defN 23-Apr-08 19:05 autoprof/fit/gp.py
+-rw-rw-r--  2.0 unx     6704 b- defN 23-Apr-08 19:05 autoprof/fit/gradient.py
+-rw-rw-r--  2.0 unx     9341 b- defN 23-Apr-21 02:55 autoprof/fit/hmc.py
+-rw-rw-r--  2.0 unx    13206 b- defN 23-Apr-08 19:05 autoprof/fit/iterative.py
+-rw-rw-r--  2.0 unx    29711 b- defN 23-Apr-21 02:55 autoprof/fit/lm.py
 -rw-rw-r--  2.0 unx     4546 b- defN 23-Mar-30 13:28 autoprof/fit/mhmcmc.py
--rw-rw-r--  2.0 unx     1097 b- defN 23-Mar-17 23:11 autoprof/image/__init__.py
--rw-rw-r--  2.0 unx    24702 b- defN 23-Mar-30 13:28 autoprof/image/image_object.py
--rw-rw-r--  2.0 unx     5277 b- defN 23-Mar-30 13:28 autoprof/image/jacobian_image.py
--rw-rw-r--  2.0 unx     6627 b- defN 23-Mar-17 23:11 autoprof/image/model_image.py
--rw-rw-r--  2.0 unx    16047 b- defN 23-Mar-30 13:28 autoprof/image/target_image.py
--rw-rw-r--  2.0 unx    22851 b- defN 23-Mar-30 13:28 autoprof/image/window_object.py
--rw-rw-r--  2.0 unx      654 b- defN 23-Apr-01 17:12 autoprof/models/__init__.py
--rw-rw-r--  2.0 unx     4281 b- defN 23-Mar-17 23:11 autoprof/models/_model_methods.py
--rw-rw-r--  2.0 unx    20263 b- defN 23-Apr-01 17:12 autoprof/models/_shared_methods.py
--rw-rw-r--  2.0 unx    18946 b- defN 23-Mar-30 13:28 autoprof/models/core_model.py
--rw-rw-r--  2.0 unx     5774 b- defN 23-Mar-30 13:28 autoprof/models/edgeon_model.py
+-rw-rw-r--  2.0 unx     1125 b- defN 23-Apr-21 02:55 autoprof/image/__init__.py
+-rw-rw-r--  2.0 unx    11195 b- defN 23-Apr-21 02:55 autoprof/image/image_header.py
+-rw-rw-r--  2.0 unx    21644 b- defN 23-Apr-21 02:55 autoprof/image/image_object.py
+-rw-rw-r--  2.0 unx     5269 b- defN 23-Apr-21 02:55 autoprof/image/jacobian_image.py
+-rw-rw-r--  2.0 unx     6615 b- defN 23-Apr-21 02:55 autoprof/image/model_image.py
+-rw-rw-r--  2.0 unx    15842 b- defN 23-Apr-21 02:55 autoprof/image/target_image.py
+-rw-rw-r--  2.0 unx    22836 b- defN 23-Apr-21 02:55 autoprof/image/window_object.py
+-rw-rw-r--  2.0 unx      654 b- defN 23-Apr-01 17:55 autoprof/models/__init__.py
+-rw-rw-r--  2.0 unx     4277 b- defN 23-Apr-21 02:55 autoprof/models/_model_methods.py
+-rw-rw-r--  2.0 unx    20263 b- defN 23-Apr-01 17:55 autoprof/models/_shared_methods.py
+-rw-rw-r--  2.0 unx    18936 b- defN 23-Apr-21 02:55 autoprof/models/core_model.py
+-rw-rw-r--  2.0 unx     5837 b- defN 23-Apr-21 02:55 autoprof/models/edgeon_model.py
 -rw-rw-r--  2.0 unx    12642 b- defN 23-Mar-30 13:28 autoprof/models/exponential_model.py
--rw-rw-r--  2.0 unx     1824 b- defN 23-Mar-30 13:28 autoprof/models/flatsky_model.py
+-rw-rw-r--  2.0 unx     1945 b- defN 23-Apr-21 02:55 autoprof/models/flatsky_model.py
 -rw-rw-r--  2.0 unx     8266 b- defN 23-Mar-17 23:11 autoprof/models/foureirellipse_model.py
--rw-rw-r--  2.0 unx     4748 b- defN 23-Mar-21 15:07 autoprof/models/galaxy_model_object.py
+-rw-rw-r--  2.0 unx     4825 b- defN 23-Apr-21 02:55 autoprof/models/galaxy_model_object.py
 -rw-rw-r--  2.0 unx    11660 b- defN 23-Mar-30 13:28 autoprof/models/gaussian_model.py
--rw-rw-r--  2.0 unx    14768 b- defN 23-Apr-01 17:12 autoprof/models/group_model_object.py
--rw-rw-r--  2.0 unx    23773 b- defN 23-Apr-01 17:12 autoprof/models/model_object.py
+-rw-rw-r--  2.0 unx    14753 b- defN 23-Apr-21 02:55 autoprof/models/group_model_object.py
+-rw-rw-r--  2.0 unx    25819 b- defN 23-Apr-21 02:55 autoprof/models/model_object.py
 -rw-rw-r--  2.0 unx     3380 b- defN 23-Mar-17 23:11 autoprof/models/moffat_model.py
 -rw-rw-r--  2.0 unx    17123 b- defN 23-Mar-17 23:11 autoprof/models/nuker_model.py
--rw-rw-r--  2.0 unx    17262 b- defN 23-Mar-31 20:27 autoprof/models/parameter_object.py
--rw-rw-r--  2.0 unx     2135 b- defN 23-Mar-30 13:28 autoprof/models/planesky_model.py
--rw-rw-r--  2.0 unx     3342 b- defN 23-Mar-30 13:28 autoprof/models/psf_model.py
--rw-rw-r--  2.0 unx     4523 b- defN 23-Mar-17 23:11 autoprof/models/ray_model.py
+-rw-rw-r--  2.0 unx    17262 b- defN 23-Apr-01 17:55 autoprof/models/parameter_object.py
+-rw-rw-r--  2.0 unx     2212 b- defN 23-Apr-21 02:55 autoprof/models/planesky_model.py
+-rw-rw-r--  2.0 unx     3439 b- defN 23-Apr-21 02:55 autoprof/models/psf_model.py
+-rw-rw-r--  2.0 unx     4600 b- defN 23-Apr-21 02:55 autoprof/models/ray_model.py
 -rw-rw-r--  2.0 unx    14516 b- defN 23-Mar-17 23:11 autoprof/models/sersic_model.py
 -rw-rw-r--  2.0 unx      875 b- defN 23-Mar-17 23:11 autoprof/models/sky_model_object.py
--rw-rw-r--  2.0 unx    10193 b- defN 23-Apr-01 17:12 autoprof/models/spline_model.py
+-rw-rw-r--  2.0 unx    10193 b- defN 23-Apr-01 17:55 autoprof/models/spline_model.py
 -rw-rw-r--  2.0 unx     1125 b- defN 23-Mar-17 23:11 autoprof/models/star_model_object.py
 -rw-rw-r--  2.0 unx     3073 b- defN 23-Mar-17 23:11 autoprof/models/superellipse_model.py
 -rw-rw-r--  2.0 unx     4332 b- defN 23-Mar-17 23:11 autoprof/models/warp_model.py
--rw-rw-r--  2.0 unx     3469 b- defN 23-Mar-17 23:11 autoprof/models/wedge_model.py
+-rw-rw-r--  2.0 unx     3546 b- defN 23-Apr-21 02:55 autoprof/models/wedge_model.py
 -rw-rw-r--  2.0 unx       57 b- defN 23-Feb-16 02:48 autoprof/parse_config/__init__.py
 -rw-rw-r--  2.0 unx     4147 b- defN 23-Mar-17 23:11 autoprof/parse_config/basic_config.py
 -rw-rw-r--  2.0 unx     4590 b- defN 23-Mar-17 23:11 autoprof/parse_config/galfit_config.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Feb-16 02:48 autoprof/parse_config/shared_methods.py
 -rw-rw-r--  2.0 unx       67 b- defN 22-Oct-27 02:37 autoprof/plots/__init__.py
--rw-rw-r--  2.0 unx     6449 b- defN 23-Mar-31 20:27 autoprof/plots/image.py
--rw-rw-r--  2.0 unx     7559 b- defN 23-Apr-01 17:12 autoprof/plots/profile.py
+-rw-rw-r--  2.0 unx     6449 b- defN 23-Apr-01 17:55 autoprof/plots/image.py
+-rw-rw-r--  2.0 unx     7559 b- defN 23-Apr-01 17:55 autoprof/plots/profile.py
 -rw-rw-r--  2.0 unx     3048 b- defN 23-Mar-17 23:11 autoprof/plots/shared_elements.py
 -rw-rw-r--  2.0 unx     3527 b- defN 23-Mar-17 23:11 autoprof/plots/visuals.py
 -rw-rw-r--  2.0 unx        0 b- defN 22-Sep-15 14:58 autoprof/utils/__init__.py
 -rw-rw-r--  2.0 unx      800 b- defN 23-Mar-30 13:28 autoprof/utils/angle_operations.py
 -rw-rw-r--  2.0 unx     8609 b- defN 23-Mar-17 23:11 autoprof/utils/interpolate.py
--rw-rw-r--  2.0 unx     1977 b- defN 23-Mar-17 23:11 autoprof/utils/operations.py
+-rw-rw-r--  2.0 unx     6369 b- defN 23-Apr-21 02:55 autoprof/utils/operations.py
 -rw-rw-r--  2.0 unx      963 b- defN 23-Mar-17 23:11 autoprof/utils/optimization.py
--rw-rw-r--  2.0 unx     5990 b- defN 23-Apr-01 17:12 autoprof/utils/parametric_profiles.py
+-rw-rw-r--  2.0 unx     5990 b- defN 23-Apr-01 17:55 autoprof/utils/parametric_profiles.py
 -rw-rw-r--  2.0 unx        0 b- defN 22-Dec-16 16:54 autoprof/utils/conversions/__init__.py
 -rw-rw-r--  2.0 unx     2317 b- defN 23-Mar-17 23:11 autoprof/utils/conversions/coordinates.py
 -rw-rw-r--  2.0 unx     1095 b- defN 23-Mar-21 14:13 autoprof/utils/conversions/dict_to_hdf5.py
 -rw-rw-r--  2.0 unx     1829 b- defN 23-Mar-30 13:28 autoprof/utils/conversions/functions.py
 -rw-rw-r--  2.0 unx     2728 b- defN 23-Mar-30 13:28 autoprof/utils/conversions/optimization.py
 -rw-rw-r--  2.0 unx     2536 b- defN 23-Mar-30 13:28 autoprof/utils/conversions/units.py
 -rw-rw-r--  2.0 unx      269 b- defN 23-Mar-30 13:28 autoprof/utils/initialize/__init__.py
@@ -67,14 +68,14 @@
 -rw-rw-r--  2.0 unx     3296 b- defN 23-Mar-30 13:28 autoprof/utils/initialize/construct_psf.py
 -rw-rw-r--  2.0 unx     3921 b- defN 23-Mar-17 23:11 autoprof/utils/initialize/initialize.py
 -rw-rw-r--  2.0 unx     7036 b- defN 23-Mar-30 13:28 autoprof/utils/initialize/segmentation_map.py
 -rw-rw-r--  2.0 unx        0 b- defN 22-Dec-16 16:54 autoprof/utils/isophote/__init__.py
 -rw-rw-r--  2.0 unx     1085 b- defN 23-Mar-17 23:11 autoprof/utils/isophote/ellipse.py
 -rw-rw-r--  2.0 unx     8531 b- defN 23-Mar-30 13:28 autoprof/utils/isophote/extract.py
 -rw-rw-r--  2.0 unx     7012 b- defN 23-Mar-17 23:11 autoprof/utils/isophote/integrate.py
--rw-rw-r--  2.0 unx    35149 b- defN 23-Apr-01 17:14 autoprof-0.6.1.dist-info/LICENSE
--rw-rw-r--  2.0 unx     3858 b- defN 23-Apr-01 17:14 autoprof-0.6.1.dist-info/METADATA
--rw-rw-r--  2.0 unx      110 b- defN 23-Apr-01 17:14 autoprof-0.6.1.dist-info/WHEEL
--rw-rw-r--  2.0 unx       57 b- defN 23-Apr-01 17:14 autoprof-0.6.1.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx        9 b- defN 23-Apr-01 17:14 autoprof-0.6.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     6843 b- defN 23-Apr-01 17:14 autoprof-0.6.1.dist-info/RECORD
-78 files, 506713 bytes uncompressed, 122997 bytes compressed:  75.7%
+-rw-rw-r--  2.0 unx    35149 b- defN 23-Apr-21 02:57 autoprof-0.6.2.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     3788 b- defN 23-Apr-21 02:57 autoprof-0.6.2.dist-info/METADATA
+-rw-rw-r--  2.0 unx      110 b- defN 23-Apr-21 02:57 autoprof-0.6.2.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       57 b- defN 23-Apr-21 02:57 autoprof-0.6.2.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx        9 b- defN 23-Apr-21 02:57 autoprof-0.6.2.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     6931 b- defN 23-Apr-21 02:57 autoprof-0.6.2.dist-info/RECORD
+79 files, 523457 bytes uncompressed, 127695 bytes compressed:  75.6%
```

## zipnote {}

```diff
@@ -30,14 +30,17 @@
 
 Filename: autoprof/fit/mhmcmc.py
 Comment: 
 
 Filename: autoprof/image/__init__.py
 Comment: 
 
+Filename: autoprof/image/image_header.py
+Comment: 
+
 Filename: autoprof/image/image_object.py
 Comment: 
 
 Filename: autoprof/image/jacobian_image.py
 Comment: 
 
 Filename: autoprof/image/model_image.py
@@ -210,26 +213,26 @@
 
 Filename: autoprof/utils/isophote/extract.py
 Comment: 
 
 Filename: autoprof/utils/isophote/integrate.py
 Comment: 
 
-Filename: autoprof-0.6.1.dist-info/LICENSE
+Filename: autoprof-0.6.2.dist-info/LICENSE
 Comment: 
 
-Filename: autoprof-0.6.1.dist-info/METADATA
+Filename: autoprof-0.6.2.dist-info/METADATA
 Comment: 
 
-Filename: autoprof-0.6.1.dist-info/WHEEL
+Filename: autoprof-0.6.2.dist-info/WHEEL
 Comment: 
 
-Filename: autoprof-0.6.1.dist-info/entry_points.txt
+Filename: autoprof-0.6.2.dist-info/entry_points.txt
 Comment: 
 
-Filename: autoprof-0.6.1.dist-info/top_level.txt
+Filename: autoprof-0.6.2.dist-info/top_level.txt
 Comment: 
 
-Filename: autoprof-0.6.1.dist-info/RECORD
+Filename: autoprof-0.6.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## autoprof/__init__.py

```diff
@@ -1,15 +1,15 @@
 import sys
 import argparse
 import requests
 from .parse_config import galfit_config, basic_config
 from . import models, image, plots, utils, fit, AP_config
 
 # meta data
-__version__ = "0.6.1"
+__version__ = "0.6.2"
 __author__ = "Connor Stone"
 __email__ = "connorstone628@gmail.com"
 
 
 def run_from_terminal() -> None:
     """
     Execute AutoProf from the command line with various options.
```

## autoprof/fit/hmc.py

```diff
@@ -35,34 +35,37 @@
         model: "AutoProf_Model",
         initial_state: Optional[Sequence] = None,
         max_iter: int = 1000,
         **kwargs
     ):
         super().__init__(model, initial_state, max_iter=max_iter, **kwargs)
 
-        self.epsilon = kwargs.get("epsilon", 1e-2)
+        self.epsilon = kwargs.get("epsilon", 1e-5)
         self.leapfrog_steps = kwargs.get("leapfrog_steps", 20)
-        self.mass = torch.tensor(kwargs.get("mass", 1.0))
-        self.temperature = torch.tensor(kwargs.get("temperature", 1.0))
-        self.temper = torch.tensor(kwargs.get("temper", 1.0))
+        self.mass = kwargs.get("mass", None)
+        self.temperature = torch.tensor(kwargs.get("temperature", 1.0), dtype = AP_config.ap_dtype, device = AP_config.ap_device)
+        self.temper = torch.tensor(kwargs.get("temper", 1.0), dtype = AP_config.ap_dtype, device = AP_config.ap_device)
+        self.progress_bar = kwargs.get("progress_bar", True)
+        self.min_accept = kwargs.get("min_accept", 0.1)
 
         self.Y = self.model.target[self.model.window].flatten("data")
         #        1 / sigma^2
         self.W = (
             1.0 / self.model.target[self.model.window].flatten("variance")
             if model.target.has_variance
             else 1.0
         )
-        #          # pixels      # parameters
-        self.ndf = len(self.Y) - len(self.current_state)
 
+        self.reset_chain()
+
+    def reset_chain(self):
         self.chain = []
         self._accepted = 0
         self._sampled = 0
-
+        
     def fit(
         self,
         state: Optional[torch.Tensor] = None,
         nsamples: Optional[int] = None,
         restart_chain: bool = True,
     ):
         """
@@ -73,27 +76,29 @@
             nsamples = self.max_iter
 
         if state is None:
             state = self.current_state
         score, chi2 = self.score_fn(state)
 
         if restart_chain:
-            self.chain = []
+            self.reset_chain()
         else:
             self.chain = list(self.chain)
-        for _ in tqdm(range(nsamples)):
+        for _ in self.iter_generator(nsamples):
             while (
                 True
             ):  # rerun step function if it encounters a numerical error. Note that many such re-runs will bias the final posterior
                 try:
                     state, score, chi2 = self.step(state, score, chi2)
                     break
-                except RuntimeError:
+                except RuntimeError as e:
+                    print("Error encountered. Reducing step size epsilon by factor 10")
+                    self.epsilon /= 10.
                     warnings.warn(
-                        "HMC numerical integration error, infinite momentum, consider smaller step size epsilon",
+                        "HMC numerical integration error. Perhaps rerun with smaller step size.",
                         RuntimeWarning,
                     )
 
             self.append_chain(state)
         self.current_state = state
         self.chain = np.stack(self.chain)
         return self
@@ -115,20 +120,21 @@
         gradstate.requires_grad = True
 
         # Sample model
         Y = self.model(parameters=gradstate, as_representation=True).flatten("data")
 
         # Compute Chi^2
         if self.model.target.has_mask:
-            loss = (
-                torch.sum(((self.Y - Y) ** 2 * self.W)[torch.logical_not(self.mask)])
-                / self.ndf
-            )
+            loss = torch.sum(
+                ((self.Y - Y) ** 2 * self.W)[torch.logical_not(self.mask)]
+            ) / 2.
         else:
-            loss = torch.sum((self.Y - Y) ** 2 * self.W) / self.ndf
+            loss = torch.sum(
+                (self.Y - Y) ** 2 * self.W
+            ) / 2.
 
         # Compute score
         loss.backward()
 
         return -gradstate.grad, loss.detach()
 
     @staticmethod
@@ -140,27 +146,28 @@
 
     def step(
         self, state: torch.Tensor, score: torch.Tensor, chi2: torch.Tensor
     ) -> torch.Tensor:
         """
         Takes one step of the HMC sampler by integrating along a path initiated with a random momentum.
         """
-        momentum_0 = torch.normal(
-            mean=torch.zeros_like(state), std=self.temperature * self.mass
-        )
+        momentum_0 = torch.distributions.MultivariateNormal(
+            loc = torch.zeros_like(state),
+            covariance_matrix = self.mass
+        ).sample()
         momentum_t = torch.clone(momentum_0)
         x_t = torch.clone(state)
         score_t = torch.clone(score)
         temper = torch.sqrt(self.temper)
         for leap in range(self.leapfrog_steps):
             # Update step
             momentum_tp = (
                 temper if leap < self.leapfrog_steps / 2 else (1 / temper)
             ) * (momentum_t + self.epsilon * score_t / 2)
-            x_tp1 = x_t + self.epsilon * momentum_tp / self.mass
+            x_tp1 = x_t + self.epsilon * (self._inv_mass @ momentum_tp)
             score_tp1, chi2_tp1 = self.score_fn(x_tp1)
             momentum_tp1 = (
                 temper if leap < self.leapfrog_steps // 2 else (1 / temper)
             ) * (momentum_tp + self.epsilon * score_tp1 / 2)
 
             # set for next step
             x_t = torch.clone(x_tp1)
@@ -168,40 +175,80 @@
             score_t = torch.clone(score_tp1)
 
             # Test for failure case
             if torch.any(torch.logical_not(torch.isfinite(momentum_t))):
                 raise RuntimeError(
                     "HMC numerical integration error, infinite momentum, consider smaller step size epsilon"
                 )
+            
 
         # Set the proposed values as the end of the leapfrog integration
         proposal_state = x_t
         proposal_chi2 = chi2_tp1
         proposal_score = score_tp1
 
         # Evaluate the Hamiltonian likelihood
         DU = chi2 - proposal_chi2
-        DP = (
-            0.5
-            * (torch.dot(momentum_0, momentum_0) - torch.dot(momentum_t, momentum_t))
-            / self.mass
+        DP = 0.5 * (
+            (momentum_0 @ self._inv_mass @ momentum_0) - (momentum_t @ self._inv_mass @ momentum_t)
         )
         log_alpha = (DU + DP) / self.temperature
 
         # Determine if proposal is accepted
         accept = self.accept(log_alpha)
 
         # Record result
         self._accepted += accept
         self._sampled += 1
+
+        if len(self.chain) > 100 and self.acceptance() < self.min_accept:
+            raise RuntimeError("HMC acceptance too low, consider smaller step size.")
+        
         return (
             (proposal_state, proposal_score, proposal_chi2)
             if accept
             else (state, score, chi2)
         )
 
     @property
     def acceptance(self):
         """
         Returns the ratio of accepted states to total states sampled.
         """
         return self._accepted / self._sampled
+
+    @property
+    def mass(self):
+        return self._mass
+    @mass.setter
+    def mass(self, value):
+        """Set the mass matrix for the HMC sampler
+
+        A note when setting the mass matrix it is often a good idea to
+        set it to `mass / mean(mass)` to normalize the matrix.
+        Otherise it is possible for the numerical stability to be off
+        if there is a huge discrepancy between the parameters and the
+        momentum. This can show up as requring a very small epsilon
+        for the chain to run, which then leaves a high
+        autocorrelation.
+
+        """
+        if value is None:
+            value = torch.eye(
+                len(self.current_state),
+                dtype = AP_config.ap_dtype,
+                device = AP_config.ap_device
+            )
+        self._mass = torch.as_tensor(value, dtype = AP_config.ap_dtype, device = AP_config.ap_device)
+        self._inv_mass = torch.linalg.inv(self._mass)
+        self._det_mass = torch.linalg.det(self._mass)
+
+    def iter_generator(self, N):
+        if self.progress_bar:
+            return tqdm(range(N))
+        return range(N)
+        
+    def estimate_mass(self, chain = None):
+        if chain is None:
+            chain = self.chain
+
+        return np.cov(chain, rowvar = False)
```

## autoprof/fit/lm.py

```diff
@@ -410,15 +410,15 @@
         # set the uncertainty for each parameter
         if self.use_broyden:
             self.update_J_AD()
             self.update_hess()
         cov = self.covariance_matrix()
         if torch.all(torch.isfinite(cov)):
             self.model.set_uncertainty(
-                torch.sqrt(2 * torch.abs(torch.diag(cov))),
+                torch.sqrt(torch.abs(torch.diag(cov))),
                 as_representation=True,
                 parameters_identity=self.fit_parameters_identity,
             )
 
         return self
 
     @torch.no_grad()
@@ -613,23 +613,23 @@
             dtype=AP_config.ap_dtype,
             device=AP_config.ap_device,
         )
 
     @torch.no_grad()
     def covariance_matrix(self) -> torch.Tensor:
         try:
-            return torch.linalg.inv(self.hess)
+            return torch.linalg.inv(-self.hess)
         except:
             AP_config.ap_logger.warning(
                 "WARNING: Hessian is singular, likely at least one model is non-physical. Will massage Hessian to continue but results should be inspected."
             )
             self.hess += torch.eye(
                 len(self.grad), dtype=AP_config.ap_dtype, device=AP_config.ap_device
             ) * (torch.diag(self.hess) == 0)
-            return torch.linalg.inv(self.hess) * self.res_loss()
+            return torch.linalg.inv(-self.hess) # * self.res_loss()
 
     @torch.no_grad()
     def update_grad(self, Yph) -> None:
         """
         Update the gradient using the model evaluation on all pixels
         """
         self.grad = torch.matmul(self.J.T, self.W * (self.Y - Yph))
```

## autoprof/image/__init__.py

```diff
@@ -1,8 +1,9 @@
 from .image_object import *
+from .image_header import *
 from .target_image import *
 from .jacobian_image import *
 from .model_image import *
 from .window_object import *
 
 """
 Import all the classes from the module.
```

## autoprof/image/image_object.py

```diff
@@ -3,20 +3,20 @@
 
 import torch
 from torch.nn.functional import pad
 import numpy as np
 from astropy.io import fits
 
 from .window_object import Window, Window_List
+from .image_header import Image_Header
 from .. import AP_config
 
-__all__ = ["BaseImage", "Image_List"]
+__all__ = ["Image", "Image_List"]    
 
-
-class BaseImage(object):
+class Image(object):
     """Core class to represent images with pixel values, pixel scale,
        and a window defining the spatial coordinates on the sky.
        It supports arithmetic operations with other image objects while preserving logical image boundaries.
        It also provides methods for determining the coordinate locations of pixels
 
     Parameters:
         data: the matrix of pixel values for the image
@@ -27,14 +27,15 @@
         note: a note about this image if any
         origin: The origin of the image in the coordinate system.
     """
 
     def __init__(
         self,
         data: Optional[Union[torch.Tensor]] = None,
+        header: Optional[Image_Header] = None,
         pixelscale: Optional[Union[float, torch.Tensor]] = None,
         window: Optional[Window] = None,
         filename: Optional[str] = None,
         zeropoint: Optional[Union[float, torch.Tensor]] = None,
         note: Optional[str] = None,
         origin: Optional[Sequence] = None,
         center: Optional[Sequence] = None,
@@ -64,133 +65,97 @@
 
         Returns:
         --------
         None
         """
         self._data = None
 
-        # Record identity
-        if _identity is None:
-            self.identity = str(id(self))
+        if header is None:
+            self.header = Image_Header(
+                data_shape = None if data is None else data.shape,
+                pixelscale = pixelscale,
+                window = window,
+                filename = filename,
+                zeropoint = zeropoint,
+                note = note,
+                origin = origin,
+                center = center,
+                _identity = _identity,
+                **kwargs
+            )
         else:
-            self.identity = _identity
+            self.header = header
 
         if filename is not None:
             self.load(filename)
             return
 
-        assert not (pixelscale is None and window is None)
-
         # set the data
         self.data = data
 
-        # set Zeropoint
-        if zeropoint is None:
-            self.zeropoint = None
-        else:
-            self.zeropoint = torch.as_tensor(
-                zeropoint, dtype=AP_config.ap_dtype, device=AP_config.ap_device
-            )
-
-        # set a note for the image
-        self.note = note
-
-        # Set Window
-        if window is None:
-            # If window is not provided, create one based on pixelscale and data shape
-            assert (
-                pixelscale is not None
-            ), "pixelscale cannot be None if window is not provided"
-
-            self.pixelscale = torch.as_tensor(
-                pixelscale, dtype=AP_config.ap_dtype, device=AP_config.ap_device
-            )
-            shape = (
-                torch.flip(
-                    torch.tensor(
-                        data.shape, dtype=AP_config.ap_dtype, device=AP_config.ap_device
-                    ),
-                    (0,),
-                )
-                * self.pixelscale
-            )
-            if origin is None and center is None:
-                origin = torch.zeros(
-                    2, dtype=AP_config.ap_dtype, device=AP_config.ap_device
-                )
-            elif center is None:
-                origin = torch.as_tensor(
-                    origin, dtype=AP_config.ap_dtype, device=AP_config.ap_device
-                )
-            else:
-                origin = (
-                    torch.as_tensor(
-                        center, dtype=AP_config.ap_dtype, device=AP_config.ap_device
-                    )
-                    - shape / 2
-                )
-
-            self.window = Window(origin=origin, shape=shape)
-        else:
-            # When The Window object is provided
-            self.window = window
-            if pixelscale is None:
-                self.pixelscale = self.window.shape[0] / self.data.shape[1]
-            else:
-                self.pixelscale = torch.as_tensor(
-                    pixelscale, dtype=AP_config.ap_dtype, device=AP_config.ap_device
-                )
 
     @property
     def origin(self) -> torch.Tensor:
         """
         Returns the origin (bottom-left corner) of the image window.
 
         Returns:
             torch.Tensor: A 1D tensor of shape (2,) containing the (x, y) coordinates of the origin.
         """
-        return self.window.origin
+        return self.header.window.origin
 
     @property
     def shape(self) -> torch.Tensor:
         """
         Returns the shape (size) of the image window.
 
         Returns:
                 torch.Tensor: A 1D tensor of shape (2,) containing the (width, height) of the window in pixels.
         """
-        return self.window.shape
+        return self.header.window.shape
 
     @property
     def center(self) -> torch.Tensor:
         """
         Returns the center of the image window.
 
         Returns:
             torch.Tensor: A 1D tensor of shape (2,) containing the (x, y) coordinates of the center.
         """
-        return self.window.center
+        return self.header.window.center
+
+    @property
+    def window(self):
+        return self.header.window
+    @property
+    def pixelscale(self):
+        return self.header.pixelscale
+    @property
+    def zeropoint(self):
+        return self.header.zeropoint
+    @property
+    def note(self):
+        return self.header.note
+    @property
+    def identity(self):
+        return self.header.identity
 
     def center_alignment(self) -> torch.Tensor:
         """Determine if the center of the image is aligned at a pixel center (True)
         or if it is aligned at a pixel edge (False).
 
         """
-        return torch.isclose(
-            ((self.center - self.origin) / self.pixelscale) % 1,
-            torch.tensor(0.5, dtype=AP_config.ap_dtype, device=AP_config.ap_device),
-            atol=0.25,
-        )
+        return self.header.center_alignment()
 
     @torch.no_grad()
     def pixel_center_alignment(self) -> torch.Tensor:
         """
         Determine the relative position of the center of a pixel with respect to the origin (mod 1)
         """
-        return ((self.origin + 0.5 * self.pixelscale) / self.pixelscale) % 1
+        return self.header.pixel_center_alignment()
 
     @property
     def data(self) -> torch.Tensor:
         """
         Returns the image data.
         """
         return self._data
@@ -230,108 +195,84 @@
         """Produce a copy of this image with all of the same properties. This
         can be used when one wishes to make temporary modifications to
         an image and then will want the original again.
 
         """
         return self.__class__(
             data=torch.clone(self.data),
-            zeropoint=self.zeropoint,
-            origin=self.origin,
-            note=self.note,
-            window=self.window,
-            _identity=self.identity,
+            header = self.header.copy(**kwargs),
             **kwargs,
         )
 
     def blank_copy(self, **kwargs):
         """Produces a blank copy of the image which has the same properties
-        except that its data is not filled with zeros.
+        except that its data is now filled with zeros.
 
         """
         return self.__class__(
             data=torch.zeros_like(self.data),
-            zeropoint=self.zeropoint,
-            origin=self.origin,
-            note=self.note,
-            window=self.window,
-            _identity=self.identity,
+            header=self.header.copy(**kwargs),
             **kwargs,
         )
 
     def get_window(self, window, **kwargs):
         """Get a sub-region of the image as defined by a window on the sky."""
         return self.__class__(
             data=self.data[window.get_indices(self)],
-            pixelscale=self.pixelscale,
-            zeropoint=self.zeropoint,
-            note=self.note,
-            origin=(self.window & window).origin,
-            _identity=self.identity,
+            header=self.header.get_window(window, **kwargs),
             **kwargs,
         )
 
     def to(self, dtype=None, device=None):
         if dtype is None:
             dtype = AP_config.ap_dtype
         if device is None:
             device = AP_config.ap_device
         if self._data is not None:
             self._data = self._data.to(dtype=dtype, device=device)
-        self.window.to(dtype=dtype, device=device)
+        self.header.to(dtype=dtype, device=device)
         return self
 
     def crop(self, pixels):
         # does this show up?
         if len(pixels) == 1:  # same crop in all dimension
             self.set_data(
                 self.data[
                     pixels[0] : self.data.shape[0] - pixels[0],
                     pixels[0] : self.data.shape[1] - pixels[0],
                 ],
                 require_shape=False,
             )
-            self.window -= pixels[0] * self.pixelscale
         elif len(pixels) == 2:  # different crop in each dimension
             self.set_data(
                 self.data[
                     pixels[1] : self.data.shape[0] - pixels[1],
                     pixels[0] : self.data.shape[1] - pixels[0],
                 ],
                 require_shape=False,
             )
-            self.window -= (
-                torch.as_tensor(
-                    pixels, dtype=AP_config.ap_dtype, device=AP_config.ap_device
-                )
-                * self.pixelscale
-            )
         elif len(pixels) == 4:  # different crop on all sides
             self.set_data(
                 self.data[
                     pixels[2] : self.data.shape[0] - pixels[3],
                     pixels[0] : self.data.shape[1] - pixels[1],
                 ],
                 require_shape=False,
             )
-            self.window -= (
-                torch.as_tensor(
-                    pixels, dtype=AP_config.ap_dtype, device=AP_config.ap_device
-                )
-                * self.pixelscale
-            )
+        self.header = self.header.crop(pixels)
         return self
 
     def flatten(self, attribute: str = "data") -> np.ndarray:
         return getattr(self, attribute).reshape(-1)
 
     def get_coordinate_meshgrid_np(self, x: float = 0.0, y: float = 0.0) -> np.ndarray:
-        return self.window.get_coordinate_meshgrid_np(self.pixelscale, x, y)
+        return self.header.get_coordinate_meshgrid_np(x, y)
 
     def get_coordinate_meshgrid_torch(self, x=0.0, y=0.0):
-        return self.window.get_coordinate_meshgrid_torch(self.pixelscale, x, y)
+        return self.header.get_coordinate_meshgrid_torch(x, y)
 
     def reduce(self, scale: int, **kwargs):
         """This operation will downsample an image by the factor given. If
         scale = 2 then 2x2 blocks of pixels will be summed together to
         form individual larger pixels. A new image object will be
         returned with the appropriate pixelscale and data tensor. Note
         that the window does not change in this operation since the
@@ -348,42 +289,31 @@
 
         MS = self.data.shape[0] // scale
         NS = self.data.shape[1] // scale
         return self.__class__(
             data=self.data[: MS * scale, : NS * scale]
             .reshape(MS, scale, NS, scale)
             .sum(axis=(1, 3)),
-            pixelscale=self.pixelscale * scale,
-            zeropoint=self.zeropoint,
-            note=self.note,
-            window=self.window.make_copy(),
-            _identity=self.identity,
+            header=self.header.reduce(scale, **kwargs),
             **kwargs,
         )
 
     def expand(self, padding: Tuple[float]) -> None:
         """
         Args:
           padding tuple[float]: length 4 tuple with amounts to pad each dimension in physical units
         """
         padding = np.array(padding)
         assert np.all(padding >= 0), "negative padding not allowed in expand method"
         pad_boundaries = tuple(np.int64(np.round(np.array(padding) / self.pixelscale)))
         self.data = pad(self.data, pad=pad_boundaries, mode="constant", value=0)
-        self.window += tuple(padding)
+        self.header.expand(padding)
 
     def _save_image_list(self):
-        img_header = fits.Header()
-        img_header["IMAGE"] = "PRIMARY"
-        img_header["PXLSCALE"] = str(self.pixelscale.detach().cpu().item())
-        img_header["WINDOW"] = str(self.window.get_state())
-        if not self.zeropoint is None:
-            img_header["ZEROPNT"] = str(self.zeropoint.detach().cpu().item())
-        if not self.note is None:
-            img_header["NOTE"] = str(self.note)
+        img_header = self.header._save_image_list()
         image_list = [
             fits.PrimaryHDU(self._data.detach().cpu().numpy(), header=img_header)
         ]
         return image_list
 
     def save(self, filename=None, overwrite=True):
         image_list = self._save_image_list()
@@ -393,23 +323,20 @@
         return hdul
 
     def load(self, filename):
         hdul = fits.open(filename)
         for hdu in hdul:
             if "IMAGE" in hdu.header and hdu.header["IMAGE"] == "PRIMARY":
                 self.set_data(np.array(hdu.data, dtype=np.float64), require_shape=False)
-                self.pixelscale = eval(hdu.header.get("PXLSCALE"))
-                self.zeropoint = eval(hdu.header.get("ZEROPNT"))
-                self.note = hdu.header.get("NOTE")
-                self.window = Window(**eval(hdu.header.get("WINDOW")))
                 break
+        self.header.load(filename)
         return hdul
 
     def __sub__(self, other):
-        if isinstance(other, BaseImage):
+        if isinstance(other, Image):
             if not torch.isclose(self.pixelscale, other.pixelscale):
                 raise IndexError("Cannot subtract images with different pixelscale!")
             if torch.any(self.origin + self.shape < other.origin) or torch.any(
                 other.origin + other.shape < self.origin
             ):
                 raise IndexError("images have no overlap, cannot subtract!")
             new_img = self[other.window].copy()
@@ -417,15 +344,15 @@
             return new_img
         else:
             new_img = self[other.window.get_indices(self)].copy()
             new_img.data -= other
             return new_img
 
     def __add__(self, other):
-        if isinstance(other, BaseImage):
+        if isinstance(other, Image):
             if not torch.isclose(self.pixelscale, other.pixelscale):
                 raise IndexError("Cannot add images with different pixelscale!")
             if torch.any(self.origin + self.shape < other.origin) or torch.any(
                 other.origin + other.shape < self.origin
             ):
                 return self
             new_img = self[other.window].copy()
@@ -433,15 +360,15 @@
             return new_img
         else:
             new_img = self[other.window.get_indices(self)].copy()
             new_img.data += other
             return new_img
 
     def __sub__(self, other):
-        if isinstance(other, BaseImage):
+        if isinstance(other, Image):
             if not torch.isclose(self.pixelscale, other.pixelscale):
                 raise IndexError("Cannot subtract images with different pixelscale!")
             if torch.any(self.origin + self.shape < other.origin) or torch.any(
                 other.origin + other.shape < self.origin
             ):
                 raise IndexError("images have no overlap, cannot subtract!")
             new_img = self[other.window].copy()
@@ -449,15 +376,15 @@
             return new_img
         else:
             new_img = self[other.window.get_indices(self)].copy()
             new_img.data -= other
             return new_img
 
     def __add__(self, other):
-        if isinstance(other, BaseImage):
+        if isinstance(other, Image):
             if not torch.isclose(self.pixelscale, other.pixelscale):
                 raise IndexError("Cannot add images with different pixelscale!")
             if torch.any(self.origin + self.shape < other.origin) or torch.any(
                 other.origin + other.shape < self.origin
             ):
                 return self
             new_img = self[other.window].copy()
@@ -465,30 +392,30 @@
             return new_img
         else:
             new_img = self[other.window.get_indices(self)].copy()
             new_img.data += other
             return new_img
 
     def __iadd__(self, other):
-        if isinstance(other, BaseImage):
+        if isinstance(other, Image):
             if not torch.isclose(self.pixelscale, other.pixelscale):
                 raise IndexError("Cannot add images with different pixelscale!")
             if torch.any(self.origin + self.shape < other.origin) or torch.any(
                 other.origin + other.shape < self.origin
             ):
                 return self
             self.data[other.window.get_indices(self)] += other.data[
                 self.window.get_indices(other)
             ]
         else:
             self.data += other
         return self
 
     def __isub__(self, other):
-        if isinstance(other, BaseImage):
+        if isinstance(other, Image):
             if not torch.isclose(self.pixelscale, other.pixelscale):
                 raise IndexError("Cannot subtract images with different pixelscale!")
             if torch.any(self.origin + self.shape < other.origin) or torch.any(
                 other.origin + other.shape < self.origin
             ):
                 return self
             self.data[other.window.get_indices(self)] -= other.data[
@@ -497,23 +424,23 @@
         else:
             self.data -= other
         return self
 
     def __getitem__(self, *args):
         if len(args) == 1 and isinstance(args[0], Window):
             return self.get_window(args[0])
-        if len(args) == 1 and isinstance(args[0], BaseImage):
+        if len(args) == 1 and isinstance(args[0], Image):
             return self.get_window(args[0].window)
-        raise ValueError("Unrecognized BaseImage getitem request!")
+        raise ValueError("Unrecognized Image getitem request!")
 
     def __str__(self):
         return f"image pixelscale: {self.pixelscale} origin: {self.origin}\ndata: {self.data}"
 
 
-class Image_List(BaseImage):
+class Image_List(Image):
     def __init__(self, image_list):
         self.image_list = list(image_list)
 
     @property
     def window(self):
         return Window_List(list(image.window for image in self.image_list))
 
@@ -546,15 +473,15 @@
 
     def get_window(self, window):
         return self.__class__(
             tuple(image[win] for image, win in zip(self.image_list, window)),
         )
 
     def index(self, other):
-        if isinstance(other, BaseImage) and hasattr(other, "identity"):
+        if isinstance(other, Image) and hasattr(other, "identity"):
             for i, self_image in enumerate(self.image_list):
                 if other.identity == self_image.identity:
                     return i
         raise NotImplementedError(f"Image_List cannot get index for {type(other)}")
 
     def to(self, dtype=None, device=None):
         if dtype is not None:
@@ -637,15 +564,15 @@
 
     def load(self, filename):
         raise NotImplementedError("Save/load not yet available for image lists")
 
     def __getitem__(self, *args):
         if len(args) == 1 and isinstance(args[0], Window):
             return self.get_window(args[0])
-        if len(args) == 1 and isinstance(args[0], BaseImage):
+        if len(args) == 1 and isinstance(args[0], Image):
             return self.get_window(args[0].window)
         if all(isinstance(arg, (int, slice)) for arg in args):
             return self.image_list.__getitem__(*args)
         raise ValueError("Unrecognized Image_List getitem request!")
 
     def __str__(self):
         return f"image list of:\n" + "\n".join(
```

## autoprof/image/jacobian_image.py

```diff
@@ -1,21 +1,21 @@
 import warnings
 from typing import Optional, Union, List
 
 import torch
 from torch.nn.functional import pad
 
-from .image_object import BaseImage, Image_List
+from .image_object import Image, Image_List
 from .. import AP_config
 
 __all__ = ["Jacobian_Image", "Jacobian_Image_List"]
 
 
 ######################################################################
-class Jacobian_Image(BaseImage):
+class Jacobian_Image(Image):
     """Jacobian of a model evaluated in an image.
 
     Image object which represents the evaluation of a jacobian on an
     image. It takes the form of a 3D (Image x Nparameters)
     tensor. This object can be added other other Jacobian images to
     build up a full jacobian for a complex model.
```

## autoprof/image/model_image.py

```diff
@@ -1,20 +1,20 @@
 import torch
 import numpy as np
 
 from .. import AP_config
-from .image_object import BaseImage, Image_List
+from .image_object import Image, Image_List
 from .window_object import Window
 from ..utils.interpolate import shift_Lanczos_torch
 
 __all__ = ["Model_Image", "Model_Image_List"]
 
 
 ######################################################################
-class Model_Image(BaseImage):
+class Model_Image(Image):
     """Image object which represents the sampling of a model at the given
     coordinates of the image. Extra arithmetic operations are
     available which can update model values in the image. The whole
     model can be shifted by less than a pixel to account for sub-pixel
     accuracy.
 
     """
@@ -53,15 +53,15 @@
             window, target_identity=self.target_identity, **kwargs
         )
 
     def reduce(self, scale, **kwargs):
         return super().reduce(scale, target_identity=self.target_identity, **kwargs)
 
     def replace(self, other, data=None):
-        if isinstance(other, BaseImage):
+        if isinstance(other, Image):
             if not torch.isclose(self.pixelscale, other.pixelscale):
                 raise IndexError("Cannot add images with different pixelscale!")
             if torch.any((self.origin + self.shape) < other.origin) or torch.any(
                 (other.origin + other.shape) < self.origin
             ):
                 return
             other_indices = self.window.get_indices(other)
```

## autoprof/image/target_image.py

```diff
@@ -1,24 +1,23 @@
 from typing import List, Optional
 
 import torch
 import numpy as np
 from torch.nn.functional import avg_pool2d
 
-from .image_object import BaseImage, Image_List
+from .image_object import Image, Image_List
 from .jacobian_image import Jacobian_Image, Jacobian_Image_List
 from .model_image import Model_Image, Model_Image_List
 from astropy.io import fits
-from .image_object import BaseImage, Image_List
 from .. import AP_config
 
 __all__ = ["Target_Image", "Target_Image_List"]
 
 
-class Target_Image(BaseImage):
+class Target_Image(Image):
     """Image object which represents the data to be fit by a model. It can
     include a variance image, mask, and PSF as anciliary data which
     describes the target image.
 
     """
 
     image_count = 0
@@ -195,15 +194,15 @@
             mask=self._mask, psf=self._psf, psf_upscale=self.psf_upscale, **kwargs
         )
 
     def get_window(self, window, **kwargs):
         """Get a sub-region of the image as defined by a window on the sky."""
         indices = window.get_indices(self)
         return super().get_window(
-            window,
+            window=window,
             variance=self._variance[indices] if self.has_variance else None,
             mask=self._mask[indices] if self.has_mask else None,
             psf=self._psf,
             psf_upscale=self.psf_upscale,
             **kwargs,
         )
 
@@ -222,27 +221,23 @@
                 dtype=AP_config.ap_dtype,
                 device=AP_config.ap_device,
             )
         return Jacobian_Image(
             parameters=parameters,
             target_identity=self.identity,
             data=data,
-            pixelscale=self.pixelscale,
-            zeropoint=self.zeropoint,
-            window=self.window,
+            header=self.header,
             **kwargs,
         )
 
     def model_image(self, data: Optional[torch.Tensor] = None, **kwargs):
         return Model_Image(
             data=torch.zeros_like(self.data) if data is None else data,
-            pixelscale=self.pixelscale,
+            header=self.header,
             target_identity=self.identity,
-            zeropoint=self.zeropoint,
-            window=self.window,
             **kwargs,
         )
 
     def reduce(self, scale, **kwargs):
         MS = self.data.shape[0] // scale
         NS = self.data.shape[1] // scale
         if self.has_psf:
```

## autoprof/image/window_object.py

```diff
@@ -56,15 +56,15 @@
                 self.origin[0],
                 self.origin[0] + self.shape[0],
                 self.origin[1],
                 self.origin[1] + self.shape[1],
             )
         )
 
-    def make_copy(self):
+    def copy(self):
         return Window(origin=torch.clone(self.origin), shape=torch.clone(self.shape))
 
     def to(self, dtype=None, device=None):
         if dtype is None:
             dtype = AP_config.ap_dtype
         if device is None:
             device = AP_config.ap_device
@@ -485,16 +485,16 @@
         return torch.max(ends, dim=0)[0] - self.origin
 
     def shift_origin(self, shift):
         for window, sub_shift in zip(self, shift):
             window.shift_origin(sub_shift)
         return self
 
-    def make_copy(self):
-        return Window_List(list(w.make_copy() for w in self.window_list))
+    def copy(self):
+        return Window_List(list(w.copy() for w in self.window_list))
 
     def to(self, dtype=None, device=None):
         if dtype is None:
             dtype = AP_config.ap_dtype
         if device is None:
             device = AP_config.ap_device
         for window in self.window_list:
```

## autoprof/models/_model_methods.py

```diff
@@ -4,15 +4,15 @@
 from copy import deepcopy
 from .parameter_object import Parameter
 from ..utils.conversions.coordinates import coord_to_index, index_to_coord
 from ..image import Model_Image, Target_Image, Window
 from .. import AP_config
 
 
-def integrate_window(self, image: "BaseImage", fix_to: str = "center") -> Window:
+def integrate_window(self, image: "Image", fix_to: str = "center") -> Window:
     """
     The appropriately sized window in which to perform integration for
     this model, centered on the model center.
     Args:
     image (Model_Image): The image object to integrate.
 
     fix_to (str): Specifies whether to fix the window to the model center or the nearest pixel center.
```

## autoprof/models/core_model.py

```diff
@@ -230,22 +230,22 @@
     ):
         raise NotImplementedError("please use a subclass of AutoProf_Model")
 
     @property
     def window(self):
         try:
             if self._window is None:
-                return self.target.window.make_copy()
+                return self.target.window.copy()
             return self._window
         except AttributeError:
             if self.target is None:
                 raise ValueError(
                     "This model has no target or window, these must be provided by the user"
                 )
-            return self.target.window.make_copy()
+            return self.target.window.copy()
 
     def set_window(self, window):
         # If no window given, dont go any further
         if window is None:
             return
 
         # If the window is given in proper format, simply use as-is
```

## autoprof/models/edgeon_model.py

```diff
@@ -9,15 +9,14 @@
 from ..utils.conversions.coordinates import (
     Rotate_Cartesian,
     Axis_Ratio_Cartesian,
     coord_to_index,
     index_to_coord,
 )
 
-
 __all__ = ["Edgeon_Model"]
 
 
 class Edgeon_Model(Component_Model):
     """General Edge-On galaxy model to be subclassed for any specific
     representation such as radial light profile or the structure of
     the galaxy on the sky. Defines an edgeon galaxy as an object with
@@ -75,18 +74,19 @@
             % np.pi,
             override_locked=True,
         )
 
     def transform_coordinates(self, X, Y):
         return Rotate_Cartesian(-self["PA"].value, X, Y)
 
-    def evaluate_model(self, image):
-        X, Y = image.get_coordinate_meshgrid_torch(
-            self["center"].value[0], self["center"].value[1]
-        )
+    def evaluate_model(self, image, X = None, Y = None, **kwargs):
+        if X is None:
+            X, Y = image.get_coordinate_meshgrid_torch(
+                self["center"].value[0], self["center"].value[1]
+            )
         XX, YY = self.transform_coordinates(X, Y)
 
         return self.brightness_model(torch.abs(XX), torch.abs(YY), image)
 
 
 class Edgeon_Sech(Edgeon_Model):
     """An edgeon profile where the vertical distribution is a sech^2
```

## autoprof/models/flatsky_model.py

```diff
@@ -48,11 +48,16 @@
                     )
                     / np.sqrt(np.prod(self.window.shape.detach().cpu().numpy()))
                 )
                 / (10 ** self["sky"].value * np.log(10)),
                 override_locked=True,
             )
 
-    def evaluate_model(self, image):
-        return torch.ones_like(image.data) * (
+    def evaluate_model(self, image, X = None, Y = None, **kwargs):
+        if X is None:
+            ref = image.data
+        else:
+            ref = X
+        return torch.ones_like(ref) * (
             (10 ** self["sky"].value) * image.pixelscale ** 2
         )
+
```

## autoprof/models/galaxy_model_object.py

```diff
@@ -115,13 +115,14 @@
     def transform_coordinates(self, X, Y):
         X, Y = Rotate_Cartesian(-self["PA"].value, X, Y)
         return (
             X,
             Y / self["q"].value,
         )  # Axis_Ratio_Cartesian(self["q"].value, X, Y, self["PA"].value, inv_scale = True)
 
-    def evaluate_model(self, image):
-        X, Y = image.get_coordinate_meshgrid_torch(
-            self["center"].value[0], self["center"].value[1]
-        )
+    def evaluate_model(self, image, X = None, Y = None, **kwargs):
+        if X is None or Y is None:
+            X, Y = image.get_coordinate_meshgrid_torch(
+                self["center"].value[0], self["center"].value[1]
+            )
         XX, YY = self.transform_coordinates(X, Y)
         return self.radial_model(self.radius_metric(XX, YY), image)
```

## autoprof/models/group_model_object.py

```diff
@@ -117,35 +117,35 @@
             for model in self.model_list:
                 if model.locked and not include_locked:
                     continue
                 if isinstance(model.target, Image_List):
                     for target, window in zip(model.target, model.window):
                         index = self.target.index(target)
                         if new_window[index] is None:
-                            new_window[index] = window.make_copy()
+                            new_window[index] = window.copy()
                         else:
                             new_window[index] |= window
                 elif isinstance(model.target, Target_Image):
                     index = self.target.index(model.target)
                     if new_window[index] is None:
-                        new_window[index] = model.window.make_copy()
+                        new_window[index] = model.window.copy()
                     else:
                         new_window[index] |= model.window
                 else:
                     raise NotImplementedError(
                         "Group_Model cannot construct a window for itself using {type(model.target)} object. Must be a Target_Image"
                     )
             new_window = Window_List(new_window)
         else:
             new_window = None
             for model in self.model_list:
                 if model.locked and not include_locked:
                     continue
                 if new_window is None:
-                    new_window = model.window.make_copy()
+                    new_window = model.window.copy()
                 else:
                     new_window |= model.window
         self.window = new_window
 
     def parameter_tuples_order(self, override_locked: bool = True):
         """Constructs a list where each entry is a tuple with a unique name
         for the parameter and the parameter object itself.
```

## autoprof/models/model_object.py

```diff
@@ -6,15 +6,15 @@
 import numpy as np
 import torch
 
 from .core_model import AutoProf_Model
 from ..image import Model_Image, Window
 from .parameter_object import Parameter
 from ..utils.initialize import center_of_mass
-from ..utils.operations import fft_convolve_torch, fft_convolve_multi_torch
+from ..utils.operations import fft_convolve_torch, fft_convolve_multi_torch, selective_integrate
 from ..utils.interpolate import _shift_Lanczos_kernel_torch
 from ..utils.conversions.coordinates import coord_to_index, index_to_coord
 from ._shared_methods import select_target
 from .. import AP_config
 
 __all__ = ["Component_Model"]
 
@@ -57,24 +57,29 @@
     _parameter_order = ("center",)
 
     # Technique and scope for PSF convolution
     psf_mode = "none"  # none, window/full
     # size in pixels of the PSF convolution box
     psf_window_size = 50
     # Integration scope for model
-    integrate_mode = "window"  # none, window, full
-    # size of the window in which to perform integration
-    integrate_window_size = 10
-    # Factor by which to upscale each dimension when integrating
-    integrate_factor = 3  # number of pixels on one axis by which to supersample
-    integrate_recursion_factor = 2  # relative size of windows between recursion levels (2 means each window will be half the size of the previous one)
-    integrate_recursion_depth = (
-        2  # number of recursion cycles to apply when integrating
-    )
-    jacobian_chunksize = 10 # maximum size of parameter list before jacobian will be broken into smaller chunks, this is helpful for limiting the memory requirements to build a model, lower jacobian_chunksize is slower but uses less memory
+    integrate_mode = "threshold"  # none, window, threshold
+
+    # Size of the window in which to perform integration (window mode)
+    integrate_window_size = 10 
+    # Number of pixels on one axis by which to supersample (window mode)
+    integrate_factor = 3  
+    # Relative size of windows between recursion levels (2 means each window will be half the size of the previous one, window mode)
+    integrate_recursion_factor = 2  
+    # Number of recursion cycles to apply when integrating (window or threshold mode)
+    integrate_recursion_depth = 3  
+    # Threshold for triggering pixel integration (threshold mode)
+    integrate_threshold = 1e-2 
+
+    # Maximum size of parameter list before jacobian will be broken into smaller chunks, this is helpful for limiting the memory requirements to build a model, lower jacobian_chunksize is slower but uses less memory
+    jacobian_chunksize = 10 
 
     # Parameters which are treated specially by the model object and should not be updated directly when initializing
     special_kwargs = ["parameters", "filename", "model_type"]
     useable = False
 
     def __init__(self, name, *args, **kwargs):
         super().__init__(name, *args, **kwargs)
@@ -169,28 +174,32 @@
         # Convert center of mass indices to coordinates
         COM_center = index_to_coord(COM[0], COM[1], target_area)
         # Set the new coordinates as the model center
         self["center"].value = COM_center
 
     # Fit loop functions
     ######################################################################
-    def evaluate_model(self, image: "BaseImage"):
+    def evaluate_model(self, image: Union["Image", "Image_Header"], X: Optional[torch.Tensor] = None, Y: Optional[torch.Tensor] = None, **kwargs):
         """Evaluate the model on every pixel in the given image. The
         basemodel object simply returns zeros, this function should be
         overloaded by subclasses.
 
         Args:
-          image (BaseImage): The image defining the set of pixels on which to evaluate the model
+          image (Image): The image defining the set of pixels on which to evaluate the model
 
         """
-        return torch.zeros_like(image.data)  # do nothing in base model
+        if X is None or Y is None:
+            X, Y = image.get_coordinate_meshgrid_torch(
+                self["center"].value[0], self["center"].value[1]
+            )
+        return torch.zeros_like(X)  # do nothing in base model
 
     def sample(
         self,
-        image: Optional["BaseImage"] = None,
+        image: Optional["Image"] = None,
         window: Optional[Window] = None,
     ):
         """Evaluate the model on the space covered by an image object. This
         function properly calls integration methods and PSF
         convolution. This should not be overloaded except in special
         cases.
 
@@ -198,35 +207,35 @@
         image or within a specified window. It takes care of sub-pixel
         sampling, recursive integration for high curvature regions,
         PSF convolution, and proper alignment of the computed model
         with the original pixel grid. The final model is then added to
         the requested image.
 
         Args:
-          image (Optional[BaseImage]): An AutoProf Image object (likely a Model_Image) 
+          image (Optional[Image]): An AutoProf Image object (likely a Model_Image) 
                                      on which to evaluate the model values. If not 
                                      provided, a new Model_Image object will be created.
           window (Optional[Window]): A window within which to evaluate the model. 
                                    Should only be used if a subset of the full image 
                                    is needed. If not provided, the entire image will 
                                    be used.
 
         Returns:
-          BaseImage: The image with the computed model values.
+          Image: The image with the computed model values.
 
         """
         # Image on which to evaluate model
         if image is None:
             image = self.make_model_image(window=window)
 
         # Window within which to evaluate model
         if window is None:
-            working_window = image.window.make_copy()
+            working_window = image.window.copy()
         else:
-            working_window = window.make_copy() & image.window
+            working_window = window.copy() & image.window
 
         if "window" in self.psf_mode:
             raise NotImplementedError("PSF convolution in sub-window not available yet")
 
         if "full" in self.psf_mode:
             # Add border for psf convolution edge effects, will be cropped out later
             working_window += self.target.psf_border
@@ -244,21 +253,37 @@
             )
             working_window.shift_origin(center_shift)
             # Make the image object to which the samples will be tracked
             working_image = Model_Image(
                 pixelscale=working_pixelscale, window=working_window
             )
             # Evaluate the model at the current resolution
-            working_image.data += self.evaluate_model(working_image)
+            working_image.data += self.evaluate_model(image = working_image)
             # If needed, super-resolve the image in areas of high curvature so pixels are properly sampled
-            self.integrate_model(
-                working_image,
-                self.integrate_window(working_image, "center"),
-                self.integrate_recursion_depth,
-            )
+            if self.integrate_mode == "none":
+                pass
+            if self.integrate_mode == "threshold":
+                X, Y = working_image.get_coordinate_meshgrid_torch(self["center"].value[0], self["center"].value[1])
+                selective_integrate(
+                    X = X,
+                    Y = Y,
+                    data = working_image.data,
+                    image_header = working_image.header,
+                    eval_brightness = self.evaluate_model,
+                    max_depth = self.integrate_recursion_depth,
+                    integrate_threshold = self.integrate_threshold,
+                )
+            elif self.integrate_mode == "window":                
+                self.window_integrate(
+                    working_image,
+                    self.integrate_window(working_image, "center"),
+                    self.integrate_recursion_depth,
+                )
+            else:
+                raise ValueError(f"{self.name} has unknown integration mode: {self.integrate_mode}")
             # Convolve the PSF
             LL = _shift_Lanczos_kernel_torch(
                 -center_shift[0] / working_image.pixelscale,
                 -center_shift[1] / working_image.pixelscale,
                 3,
                 AP_config.ap_dtype,
                 AP_config.ap_device,
@@ -281,38 +306,54 @@
             # Create an image to store pixel samples
             working_image = Model_Image(
                 pixelscale=image.pixelscale, window=working_window
             )
             # Evaluate the model on the image
             working_image.data += self.evaluate_model(working_image)
             # Super-resolve and integrate where needed
-            self.integrate_model(
-                working_image,
-                self.integrate_window(working_image, "pixel"),
-                self.integrate_recursion_depth,
-            )
+            if self.integrate_mode == "none":
+                pass
+            elif self.integrate_mode == "threshold":
+                X, Y = working_image.get_coordinate_meshgrid_torch(self["center"].value[0], self["center"].value[1])
+                selective_integrate(
+                    X = X,
+                    Y = Y,
+                    data = working_image.data,
+                    image_header = working_image.header,
+                    eval_brightness = self.evaluate_model,
+                    max_depth = self.integrate_recursion_depth,
+                    integrate_threshold = self.integrate_threshold,
+                )
+            elif self.integrate_mode == "window":
+                self.window_integrate(
+                    working_image,
+                    self.integrate_window(working_image, "pixel"),
+                    self.integrate_recursion_depth,
+                )
+            else:
+                raise ValueError(f"{self.name} has unknown integration mode: {self.integrate_mode}")
             # Add the sampled/integrated pixels to the requested image
             image += working_image
 
         return image
 
-    def integrate_model(
-        self, working_image: "BaseImage", window: Window, depth: int = 2
+    def window_integrate(
+        self, working_image: "Image", window: Window, depth: int = 2
     ):
         """Sample the model at a higher resolution than the given image, then
         integrate the super resolution up to the image resolution.
 
         This method improves the accuracy of the model evaluation by
         evaluating it at a finer resolution and integrating the
         results back to the original resolution. It recursively
         evaluates smaller windows in regions of high curvature until
         the specified recursion depth is reached.
 
         Args:
-          working_image (BaseImage): The image on which to perform the model
+          working_image (Image): The image on which to perform the model
                                      integration. Pixels in this image will be
                                      replaced with the integrated values.
           window (Window): A Window object within which to perform the integration.
                            Specifies the region of interest for integration.
           depth (int, optional): Recursion depth tracker. When called with depth = n,
                                  this function will call itself again with depth = n-1
                                  until depth is 0, at which point it will exit without
```

## autoprof/models/planesky_model.py

```diff
@@ -50,16 +50,17 @@
                 / np.sqrt(np.prod(self.window.shape.detach().cpu().numpy())),
                 override_locked=True,
             )
         if self["delta"].value is None:
             self["delta"].set_value([0.0, 0.0], override_locked=True)
             self["delta"].set_uncertainty([0.1, 0.1], override_locked=True)
 
-    def evaluate_model(self, image):
-        X, Y = image.get_coordinate_meshgrid_torch(
-            self["center"].value[0], self["center"].value[1]
-        )
+    def evaluate_model(self, image, X = None, Y = None, **kwargs):
+        if X is None or Y is None:
+            X, Y = image.get_coordinate_meshgrid_torch(
+                self["center"].value[0], self["center"].value[1]
+            )
         return (
             (self["sky"].value * image.pixelscale ** 2)
             + X * self["delta"].value[0]
             + Y * self["delta"].value[1]
         )
```

## autoprof/models/psf_model.py

```diff
@@ -57,15 +57,15 @@
                 override_locked=True,
             )
         if self["flux"].uncertainty is None:
             self["flux"].set_uncertainty(
                 torch.abs(self["flux"].value) * 1e-2, override_locked=True
             )
 
-    def evaluate_model(self, image):
+    def evaluate_model(self, image, X = None, Y = None, **kwargs):
         new_origin = self["center"].value - self.psf_model.shape / 2
         pixel_origin = torch.round(new_origin / image.pixelscale) * image.pixelscale
         pixel_shift = (
             new_origin / image.pixelscale - pixel_origin / image.pixelscale
         ) * image.pixelscale
         LL = _shift_Lanczos_kernel_torch(
             -pixel_shift[0] / image.pixelscale,
@@ -82,10 +82,12 @@
                 ).view(1, 1, *self.psf_model.data.shape),
                 LL.view(1, 1, *LL.shape),
                 padding="same",
             )[0][0],
             origin=pixel_origin - pixel_shift,
             pixelscale=self.psf_model.pixelscale,
         )
+
+        # fixme pick nearest neighbor for each X, Y? interpolate?
         img = image.blank_copy()
         img += psf
         return img.data
```

## autoprof/models/ray_model.py

```diff
@@ -86,18 +86,19 @@
                     angles < (2 * np.pi / self.rays),
                     angles >= (np.pi * (2 - 1 / self.rays)),
                 )
                 weight = (torch.cos(angles[indices] * self.rays) + 1) / 2
                 model[indices] += weight * self.iradial_model(r, R[indices], image)
         return model
 
-    def evaluate_model(self, image):
-        X, Y = image.get_coordinate_meshgrid_torch(
-            self["center"].value[0], self["center"].value[1]
-        )
+    def evaluate_model(self, image, X = None, Y = None, **kwargs):
+        if X is None or Y is None:
+            X, Y = image.get_coordinate_meshgrid_torch(
+                self["center"].value[0], self["center"].value[1]
+            )
         XX, YY = self.transform_coordinates(X, Y)
 
         return self.polar_model(
             self.radius_metric(XX, YY), self.angular_metric(XX, YY), image
         )
```

## autoprof/models/wedge_model.py

```diff
@@ -67,16 +67,17 @@
                 indices = torch.logical_or(
                     angles < (np.pi / self.wedges),
                     angles >= (np.pi * (2 - 1 / self.wedges)),
                 )
                 model[indices] += self.iradial_model(w, R[indices], image)
         return model
 
-    def evaluate_model(self, image):
-        X, Y = image.get_coordinate_meshgrid_torch(
-            self["center"].value[0], self["center"].value[1]
-        )
+    def evaluate_model(self, image, X = None, Y = None, **kwargs):
+        if X is None or Y is None:
+            X, Y = image.get_coordinate_meshgrid_torch(
+                self["center"].value[0], self["center"].value[1]
+            )
         XX, YY = self.transform_coordinates(X, Y)
 
         return self.polar_model(
             self.radius_metric(XX, YY), self.angular_metric(XX, YY), image
         )
```

## autoprof/utils/operations.py

```diff
@@ -1,22 +1,24 @@
+from typing import Callable, Optional
+
 import torch
 import matplotlib.pyplot as plt
 import numpy as np
 from astropy.convolution import convolve, convolve_fft
-
+from scipy.fft import next_fast_len
 
 def fft_convolve_torch(img, psf, psf_fft=False, img_prepadded=False):
     # Ensure everything is tensor
     img = torch.as_tensor(img)
     psf = torch.as_tensor(psf)
 
     if img_prepadded:
         s = img.size()
     else:
-        s = list(int(d + (p + 1) / 2) for d, p in zip(img.size(), psf.size()))
+        s = tuple(next_fast_len(int(d+(p+1)/2), real = True) for d,p in zip(img.size(), psf.size())) #list(int(d + (p + 1) / 2) for d, p in zip(img.size(), psf.size()))
 
     img_f = torch.fft.rfft2(img, s=s)
 
     if not psf_fft:
         psf_f = torch.fft.rfft2(psf, s=s)
     else:
         psf_f = psf
@@ -64,7 +66,95 @@
         conv,
         shifts=(
             -int((sum(kernel.size()[0] for kernel in kernels) - 1) / 2),
             -int((sum(kernel.size()[1] for kernel in kernels) - 1) / 2),
         ),
         dims=(0, 1),
     )[: img.size()[0], : img.size()[1]]
+
+def displacement_spacing(N, dtype = torch.float64, device = "cpu"):
+    return torch.linspace(-(N - 1)/(2*N), (N - 1)/(2*N), N, dtype = dtype, device = device)
+    
+def displacement_grid(*N, pixelscale = 1., dtype = torch.float64, device = "cpu"):
+    return torch.meshgrid(*tuple(displacement_spacing(n, dtype = dtype, device = device)*pixelscale for n in N), indexing = "xy")
+    
+def selective_integrate(
+        X: torch.Tensor,
+        Y: torch.Tensor,
+        data: torch.Tensor,
+        image_header: "Image_Header",
+        eval_brightness: Callable,
+        max_depth: int = 3,
+        _depth: int = 1,
+        _reference_brightness: Optional[float] = None,
+        integrate_threshold: float = 1e-2,
+):
+    """Sample the model at higher resolution than the input image.
+    
+    This function selectively refines the integration of an input
+    image based on the local curvature of the image data.  It
+    recursively evaluates the model at higher resolutions in areas
+    where the curvature exceeds the specified threshold.  With
+    each level of recursion, the function refines the affected
+    areas using a 3x3 grid for super-resolution.
+
+    Args:
+      X (torch.tensor): A tensor representing the X coordinates of the input image.
+      Y (torch.tensor): A tensor representing the Y coordinates of the input image.
+      data (torch.tensor): A tensor containing the input image data.
+      image_header (Image_Header): An instance of the Image_Header class containing the image's header information.
+      eval_brightness (Callable): Function which evaluates the brightness at a given coordinate.
+      _depth (int, optional): The current recursion depth. Default is 1.
+      max_depth (int, optional): The maximum recursion depth allowed. Default is 3.
+      _reference_brightness (float or None, optional): The reference brightness value used to normalize the curvature
+                                                       values. If None, the maximum value of the input data divided by
+                                                       10 will be used. Default is None.
+
+    Returns:
+        None. The function updates the input data tensor in-place with the selectively integrated values.
+  
+    """
+    # check recursion depth, exit if too deep
+    if _depth > max_depth:
+        return
+        
+    with torch.no_grad():
+        if _reference_brightness is None:
+            _reference_brightness = torch.max(data)/10
+        curvature_kernel = torch.tensor([[0,1.,0],[1.,-4,1.],[0,1.,0]], device = data.device, dtype = data.dtype)
+        if _depth == 1:
+            curvature = torch.abs(fft_convolve_torch(data, curvature_kernel))
+            curvature[:,0] = 0
+            curvature[:,-1] = 0
+            curvature[0,:] = 0
+            curvature[-1,:] = 0
+            curvature /= _reference_brightness
+            select = curvature > integrate_threshold
+        else:
+            curvature = torch.sum(data * curvature_kernel, axis = (1,2)) / _reference_brightness
+            select = curvature > integrate_threshold
+            select = select.view(-1,1,1).repeat(1,3,3)
+        
+        # compute the subpixel coordinate shifts for even integration within a pixel 
+        shiftsx, shiftsy = displacement_grid(3, 3, pixelscale = image_header.pixelscale, device = data.device, dtype = data.dtype)
+                        
+    # Reshape coordinates to add two dimensions with the super-resolved coordiantes
+    Xs = X[select].view(-1,1,1).repeat(1,3,3) + shiftsx
+    Ys = Y[select].view(-1,1,1).repeat(1,3,3) + shiftsy
+    # evaluate the model on the new smaller coordinate grid in each pixel
+    res = eval_brightness(image = image_header.super_resolve(3), X = Xs, Y = Ys)
+    
+    # Apply recursion to integrate any further pixels as needed
+    selective_integrate(
+        X = Xs,
+        Y = Ys,
+        data = res,
+        image_header = image_header.super_resolve(3),
+        eval_brightness = eval_brightness,
+        _depth = _depth+1,
+        max_depth = max_depth,
+        _reference_brightness = _reference_brightness,
+        integrate_threshold = integrate_threshold,
+    )
+    
+    # Update the pixels with the new integrated values
+    data[select] = res.sum(axis = (1,2))
```

## Comparing `autoprof-0.6.1.dist-info/LICENSE` & `autoprof-0.6.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `autoprof-0.6.1.dist-info/METADATA` & `autoprof-0.6.2.dist-info/METADATA`

 * *Files 10% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 Metadata-Version: 2.1
 Name: autoprof
-Version: 0.6.1
-Summary: A fast, flexible, and automated astronomical image modelling tool for precise parallel multi-wavelength photometry
+Version: 0.6.2
+Summary: A fast, flexible, differentiable, and automated astronomical image modelling tool for precise parallel multi-wavelength photometry
 Home-page: https://github.com/ConnorStoneAstro/AutoProf
 Author: Connor Stone
 Author-email: connorstone628@gmail.com
 License: GPL-3.0 license
 Platform: UNKNOWN
 Classifier: Development Status :: 1 - Planning
 Classifier: Intended Audience :: Science/Research
@@ -42,15 +42,15 @@
 
 AutoProf can be installed with pip:
 
 ```
 pip install autoprof
 ```
 
-However, for AutoProf to run you will need to install pytorch as well. Installing pytorch is very user specific, though also not very hard. Follow the instructions on the [pytorch website](https://pytorch.org/) to install a version for your system.
+If PyTorch gives you any trouble on your system, just follow the instructions on the [pytorch website](https://pytorch.org/) to install a version for your system.
 
 Also note that AutoProf is only available for python3.
 
 See [the documentation](https://connorstoneastro.github.io/AutoProf/) for more details.
 
 ## Documentation
```

## Comparing `autoprof-0.6.1.dist-info/RECORD` & `autoprof-0.6.2.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,62 +1,63 @@
 autoprof/AP_config.py,sha256=fIxYQr2lxsjA3ONiSLuKMAKmL2BPM7Xz99L3gGab8AQ,3171
-autoprof/__init__.py,sha256=9QwUdjH9NbiaLPE1JtKKwvqIDL7DPJX96x2U7s_kwNc,5650
+autoprof/__init__.py,sha256=m4UMWYCukOJraotyPnE1MbttldIGgclZKXi9LRGjixc,5650
 autoprof/__main__.py,sha256=Ce_QoW2_-x3o3ZXbErfBFzJ_M20JzoaB3dUvwzAhXx4,281
 autoprof/fit/__init__.py,sha256=uXsyhfhD3BguZRE0BrDexHqYU2FlGbBg9sLWgH4CSSM,1072
 autoprof/fit/base.py,sha256=xaA0lqrVvM7xoiSWYPPA2laz1VfpXaXLifM1iS5qbvE,6689
 autoprof/fit/gp.py,sha256=PvMC6LeAIYWwDteVVo3AY7lb_TkQOY2-C5yWfK4CpUY,30
 autoprof/fit/gradient.py,sha256=N1RzlOkWjo0yoNxD483v6EVFtvNODiypLkCy7WoeEQw,6704
-autoprof/fit/hmc.py,sha256=qXxvYGHMZjxcNqIScbPNIk2AJyaHYxTgSiOrSGCHZC8,7538
+autoprof/fit/hmc.py,sha256=87LD6oFB4ncTlUcWe2ybJYzzRbGofAOzybY8lWxclS4,9341
 autoprof/fit/iterative.py,sha256=5Jaa5ks3gsr0u1stvMNSZbCpJ71Q4E1RGhVV8SGWnJA,13206
-autoprof/fit/lm.py,sha256=NpS4mcy0BhaLV6hkl8PPRbeku-RJkDPNNxh0yi40LDw,29711
+autoprof/fit/lm.py,sha256=bv0B8RC8ePitca99wFCZji6EIh4oGBmmHli_jg5rCGA,29711
 autoprof/fit/mhmcmc.py,sha256=jwdU4lmte10YNyWexhhaShYIjavQSFrU-kwwvM2bCcg,4546
-autoprof/image/__init__.py,sha256=sDrdWXsCamN8jcpIeZi2nKjPeOAYNjTTaOE_F9S9AJY,1097
-autoprof/image/image_object.py,sha256=N1Qh9uTB1ekM94Ao75lz-YMNU1SzV_WzGl8WUjPnqOs,24702
-autoprof/image/jacobian_image.py,sha256=gjve0vCW7ZObDGOp9Bz1JBMhlSn7qPQmOy6yrICWKls,5277
-autoprof/image/model_image.py,sha256=MXAZctVRWcwLmvj07mcNDZe6JsJBNXGRLL7y8647l08,6627
-autoprof/image/target_image.py,sha256=Nq6m_F-dqTpbVDk5B6Qca_zJDUk5xuKx62R7COb1kB8,16047
-autoprof/image/window_object.py,sha256=2AVFaHmz3fgNmxnZwkw5WQTvCZTMKoG6jgW0E1lA8_0,22851
+autoprof/image/__init__.py,sha256=ksiEV0RXh6C7szKH1TQQm6kAWG_S2fx4-4NqAmAyMLc,1125
+autoprof/image/image_header.py,sha256=EZc6YXArsirO5TIYOHzAcQXQseINYGmQBK6RbU637cI,11195
+autoprof/image/image_object.py,sha256=qqZ6hKfp3c0et4om-AmagH7Np4vZmjxaROJ_ysq81z8,21644
+autoprof/image/jacobian_image.py,sha256=1s3eAq3xB353i_srThMZVaJ7Wp-aXu0POLfvZ1JQSgg,5269
+autoprof/image/model_image.py,sha256=kcGG1_d-mYMUmDsYcJ9bTA7XNecS1rbt4dkRbcPNoLk,6615
+autoprof/image/target_image.py,sha256=y-_69PPjh2kKO5C9MoF4F3GeJdRtEekhblb_4U_Bd18,15842
+autoprof/image/window_object.py,sha256=DbOnZQmbYMQKB1T_Z-Fpc6PdEENFIGxy7QSGLY0s4jI,22836
 autoprof/models/__init__.py,sha256=V5ziTIawNay5GoqHIz-x1rnIOZqARe5VmMyFPCWLOR8,654
-autoprof/models/_model_methods.py,sha256=nbDblPhwf6QHTxyzLvLzKMQi5qo3dporu9kOKOQDCw4,4281
+autoprof/models/_model_methods.py,sha256=rZh4fuVedfFmzaLJsrdZ_a4WG96JTk9E76SX8Pe0Ues,4277
 autoprof/models/_shared_methods.py,sha256=DPuQmdddnpnWIpRrGoh2eB2sxoa3pwI1U_YABxCogJo,20263
-autoprof/models/core_model.py,sha256=KJ-yXt-IhPSmef7ft-J2uLeWAqPvhoJQnNqdyUt2lsE,18946
-autoprof/models/edgeon_model.py,sha256=0nvaWSbMWzo0Q5ymKiHXXffGraALnOJ7kJjN59s-CWo,5774
+autoprof/models/core_model.py,sha256=GB8zXtIEP_IVLFwmBWcpoRhfi8qQJ1sttEAIOYBONQk,18936
+autoprof/models/edgeon_model.py,sha256=N7tz7U0cV1jzX7RUgAGKngVFvm73aPJEO-0PCgEN05U,5837
 autoprof/models/exponential_model.py,sha256=P3H5OYQAQSdkqiX3QRosLZ8nBSKXmhupxyLO9L_PnZc,12642
-autoprof/models/flatsky_model.py,sha256=uBWNg8hfHLz-5ThaLZvWOnh2hDpPNunv2Fp52dMqSqc,1824
+autoprof/models/flatsky_model.py,sha256=8WJ6YRTzepIO11S_Rd6OaL8XO1BopwNcgt5nT8UdSEU,1945
 autoprof/models/foureirellipse_model.py,sha256=SSbySZcb_GWQtdm1valPRVq41m8tvthDKKlt-vWw7BU,8266
-autoprof/models/galaxy_model_object.py,sha256=p27xKLARdFWtbtkj1T0eJppc6GMDpnNi2SVwOL1ooRE,4748
+autoprof/models/galaxy_model_object.py,sha256=ZzqxORKqjkhqiYk_WopuaSe8RiSZOFBiE2kRH9F6sa4,4825
 autoprof/models/gaussian_model.py,sha256=ZYdjl1C5oOK5DVUFie57MNc8PJxB1q2Qala_W7ja_YU,11660
-autoprof/models/group_model_object.py,sha256=zHBB4grFesVZC2XsWm3G5pUIfEfrxOVSOn2LgnlNvaQ,14768
-autoprof/models/model_object.py,sha256=23uLOLO8_kr2SyRbgGvvBsbMPcD_nxhUWyzywgv54Ps,23773
+autoprof/models/group_model_object.py,sha256=i5WwS6KZwFAYBwdYa2mebMvuJxXfeee9LAlV_mCIMyI,14753
+autoprof/models/model_object.py,sha256=5e_gQGEZd-u-SEU3Nto6_F6IoQCNj_kK37n0sKQW7Bo,25819
 autoprof/models/moffat_model.py,sha256=meLcfV_DFt39ESSP1P3dner_2ZjZZBb0Eq-Vi29f2yo,3380
 autoprof/models/nuker_model.py,sha256=KWx8lFVWhguLkEybcOAV3nMCBsiKOCdYGtNj7JzhYgU,17123
 autoprof/models/parameter_object.py,sha256=9kzWwOlzyvtkm0kf8rTyeN441xdnufJnJT4QMEWKHSE,17262
-autoprof/models/planesky_model.py,sha256=AstaYxzkC-WDH60bdp67bbAaJIA7hKWuCnRyPtNbUrs,2135
-autoprof/models/psf_model.py,sha256=3DkAhjPTSYLaBcmBFE1IRU-Bt_JetX_NdIvaBqMTmXY,3342
-autoprof/models/ray_model.py,sha256=4ALHeDHbtokaFzs61CPrIkn1ekP9t35ynURo_0UiYHE,4523
+autoprof/models/planesky_model.py,sha256=w2pQ25uvtgnWxydt5jMSX_wwMNysETPUOIfTeNVwc5M,2212
+autoprof/models/psf_model.py,sha256=EjdYEFzv1Tx82f0bdYTHf5NjV7IYYSmRC554UlF66Jc,3439
+autoprof/models/ray_model.py,sha256=7h9zfKopqiTbTEkyXh7FVC0SXRshriHv3tpNd_FtW_U,4600
 autoprof/models/sersic_model.py,sha256=atmSKyTJ_3b_K9lvuQoDYoLsjMn4d2JfPn1Ng5TrnMc,14516
 autoprof/models/sky_model_object.py,sha256=WD450x05pnwMDOzDo5yhQfqYXAHgE8QPkJLVyKJeFGQ,875
 autoprof/models/spline_model.py,sha256=SjZOm8tKIBWY2alXsgrBAOJN5vQQgIJ7YfBOgdsJJ6o,10193
 autoprof/models/star_model_object.py,sha256=kc57eQy02n02zCJKKqLDXoqoa0ZqgrzBONLpJUgRKZ4,1125
 autoprof/models/superellipse_model.py,sha256=6zfF5DJ2__qu_-2R6Attn4UKC8Z-ru8YL6Rpem40hqw,3073
 autoprof/models/warp_model.py,sha256=5r_RniJvEDQFVm60FtXQsUucHIeBOY6AG3RXjof_oxo,4332
-autoprof/models/wedge_model.py,sha256=H18yT8OaywdZNE0JwG2Mkkbllt67g3zGnPqJQN4-rzA,3469
+autoprof/models/wedge_model.py,sha256=VKOs0mEYm3WUB-xj_CePEWDxZVj63hxmrKVmiqHtVqw,3546
 autoprof/parse_config/__init__.py,sha256=CT6gEcILfcEdz3I5nbgqKeno8tyGZsMtHmfXQUWejUA,57
 autoprof/parse_config/basic_config.py,sha256=amQEkpKoDp04RVYXNrO4lO0j4T_G1BK7MaLmemtaN-4,4147
 autoprof/parse_config/galfit_config.py,sha256=0HqkIJwgv3XKiW2EZAidXqneDQRaOkgfMxNksxKwuBY,4590
 autoprof/parse_config/shared_methods.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 autoprof/plots/__init__.py,sha256=p8lf0eFHXeZ7-3s0SHRb3zIH1pR6x_jnMxIPUHaEPls,67
 autoprof/plots/image.py,sha256=-mYWeoWr72FUZ-TOE7Cx7UkjFSIo8UUL7ZcpFbJON58,6449
 autoprof/plots/profile.py,sha256=zyyFTrW-u44d9xt4bX0euAXz765KU7XGZfrmQvgYC8I,7559
 autoprof/plots/shared_elements.py,sha256=-9cuoMXOSZh7uB32Fkn2R5wCFecNU3WIGvVrjmP8ESY,3048
 autoprof/plots/visuals.py,sha256=v_DWht98km2HVHpwWBwNGHNW4Jc3_sHCWXhHhOZUKGU,3527
 autoprof/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 autoprof/utils/angle_operations.py,sha256=oJp9g9v5v7SGzuDRXILg7B0d2CcXygT3GXAzoUbtPGk,800
 autoprof/utils/interpolate.py,sha256=LCI4utAYOay8IsNQIpq0FHhj6MUpigOYii5cEnEhADA,8609
-autoprof/utils/operations.py,sha256=sK8LR_L6uYKK-MXQEfRxZquIdT4gKLp_I89Qg-cx7P0,1977
+autoprof/utils/operations.py,sha256=LLO8LB02L3sJMsmlKiH3XS2m4dZBFxwq2aN8o8v0evs,6369
 autoprof/utils/optimization.py,sha256=VkAusKnyc4zyztbceazKADVy85g6VV7qqYIw6V7cQos,963
 autoprof/utils/parametric_profiles.py,sha256=JY6reeVkATmuPkhKY7cWPgrnpZaM--simb-8mS2xh1A,5990
 autoprof/utils/conversions/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 autoprof/utils/conversions/coordinates.py,sha256=zXdOxPurvoh3szKaEKx02jRCK_z1JfWeg17Z1Hl9a7g,2317
 autoprof/utils/conversions/dict_to_hdf5.py,sha256=XIy_JvX8Nrp-9N3sk5F3gIY2YLX5I88HxEYWbPGbfsU,1095
 autoprof/utils/conversions/functions.py,sha256=YEKL5WAZvBQkR_P0DK255SIYtJoRzQmnbcEoCbweHIs,1829
 autoprof/utils/conversions/optimization.py,sha256=FGlTum_ddqPDXYSN2mkFJUuRaPj385chL49nV3HABa8,2728
@@ -66,13 +67,13 @@
 autoprof/utils/initialize/construct_psf.py,sha256=DaIQhQOn9cqn7q_NGIK8zWXNL1WV4n2hiSTeDOlBIao,3296
 autoprof/utils/initialize/initialize.py,sha256=V1Epy8vudmquOxSB5S-moCcS7bvjdr5qze6IcqihsSs,3921
 autoprof/utils/initialize/segmentation_map.py,sha256=6fG5U4y5P6MDnX4_6ROBdRhUq6TxgbwVG1V3MaLf7-c,7036
 autoprof/utils/isophote/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 autoprof/utils/isophote/ellipse.py,sha256=p2SGzU067jBIBrKjhYKpnJQF7MSThMcnLajsaXeJ23k,1085
 autoprof/utils/isophote/extract.py,sha256=ousarHmkj7GrgAZ6mO3G-QY0tXjhd8bBh6lHzb--d6U,8531
 autoprof/utils/isophote/integrate.py,sha256=jNOCbSYC1dZO1pasEMcRUqZCpt43DEPVME4Hsa37DKQ,7012
-autoprof-0.6.1.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
-autoprof-0.6.1.dist-info/METADATA,sha256=OxN0JqV74jq4txi05CxqHw6xLGJDL37wE4_f4n_kMhg,3858
-autoprof-0.6.1.dist-info/WHEEL,sha256=kGT74LWyRUZrL4VgLh6_g12IeVl_9u9ZVhadrgXZUEY,110
-autoprof-0.6.1.dist-info/entry_points.txt,sha256=CJw03tyO_XyE5_-xxRzbAg74ITHASY47DhUiRVsn67s,57
-autoprof-0.6.1.dist-info/top_level.txt,sha256=8N1I5eyEKnh1QOUENsiy7fDvtU-sfyRCYsUJPzrvPvc,9
-autoprof-0.6.1.dist-info/RECORD,,
+autoprof-0.6.2.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
+autoprof-0.6.2.dist-info/METADATA,sha256=tWTJR-k6mIMSsPzLUgqt56F6i5lN3hIMPCPbjMdIcFg,3788
+autoprof-0.6.2.dist-info/WHEEL,sha256=kGT74LWyRUZrL4VgLh6_g12IeVl_9u9ZVhadrgXZUEY,110
+autoprof-0.6.2.dist-info/entry_points.txt,sha256=CJw03tyO_XyE5_-xxRzbAg74ITHASY47DhUiRVsn67s,57
+autoprof-0.6.2.dist-info/top_level.txt,sha256=8N1I5eyEKnh1QOUENsiy7fDvtU-sfyRCYsUJPzrvPvc,9
+autoprof-0.6.2.dist-info/RECORD,,
```

