# Comparing `tmp/pyquokka-0.2.1-py3-none-any.whl.zip` & `tmp/pyquokka-0.2.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,34 +1,36 @@
-Zip file size: 140248 bytes, number of entries: 32
--rw-rw-r--  2.0 unx      228 b- defN 23-Mar-26 17:19 pyquokka/__init__.py
--rw-rw-r--  2.0 unx     4242 b- defN 23-Mar-24 01:07 pyquokka/catalog.py
--rw-rw-r--  2.0 unx      138 b- defN 23-Mar-25 18:25 pyquokka/common_startup.sh
--rw-rw-r--  2.0 unx    28239 b- defN 23-Mar-24 19:57 pyquokka/coordinator.py
--rw-rw-r--  2.0 unx    46535 b- defN 23-Mar-27 05:45 pyquokka/core.py
--rw-rw-r--  2.0 unx    46811 b- defN 23-Mar-25 02:59 pyquokka/dataset.py
--rw-rw-r--  2.0 unx    84427 b- defN 23-Apr-04 04:39 pyquokka/datastream.py
+Zip file size: 275479 bytes, number of entries: 34
+-rw-rw-r--  2.0 unx      256 b- defN 23-Apr-15 21:15 pyquokka/__init__.py
+-rw-rw-r--  2.0 unx     4291 b- defN 23-Apr-19 05:39 pyquokka/catalog.py
+-rw-rw-r--  2.0 unx      372 b- defN 23-Apr-12 19:11 pyquokka/common_startup.sh
+-rw-rw-r--  2.0 unx    28239 b- defN 23-Apr-16 23:17 pyquokka/coordinator.py
+-rw-rw-r--  2.0 unx    48229 b- defN 23-Apr-16 23:13 pyquokka/core.py
+-rw-rw-r--  2.0 unx    48426 b- defN 23-Apr-19 05:39 pyquokka/dataset.py
+-rw-rw-r--  2.0 unx    96482 b- defN 23-Apr-19 18:58 pyquokka/datastream.py
 -rw-rw-r--  2.0 unx     1438 b- defN 22-Nov-11 18:24 pyquokka/debugger.py
--rw-rw-r--  2.0 unx    71649 b- defN 23-Apr-01 02:44 pyquokka/df.py
--rw-rw-r--  2.0 unx    33759 b- defN 23-Mar-29 22:38 pyquokka/executors.py
+-rw-rw-r--  2.0 unx    75930 b- defN 23-Apr-16 23:19 pyquokka/df.py
+-rw-rw-r--  2.0 unx    37360 b- defN 23-Apr-16 23:19 pyquokka/executors.py
 -rw-rw-r--  2.0 unx    12657 b- defN 23-Mar-28 22:05 pyquokka/expression.py
 -rw-rw-r--  2.0 unx    17483 b- defN 23-Mar-26 21:54 pyquokka/flight.py
 -rw-rw-r--  2.0 unx     3108 b- defN 23-Mar-20 21:38 pyquokka/hbq.py
+-rwxrwxr-x  2.0 unx   368480 b- defN 23-Apr-11 05:32 pyquokka/ldb.so
 -rw-rw-r--  2.0 unx      274 b- defN 23-Mar-25 18:43 pyquokka/leader_start_ray.sh
 -rw-rw-r--  2.0 unx      619 b- defN 23-Mar-25 18:43 pyquokka/leader_startup.sh
--rw-rw-r--  2.0 unx    26540 b- defN 23-Mar-29 06:41 pyquokka/logical.py
+-rw-rw-r--  2.0 unx    27823 b- defN 23-Apr-13 23:50 pyquokka/logical.py
+-rw-rw-r--  2.0 unx     4314 b- defN 23-Apr-15 21:27 pyquokka/orderedstream.py
 -rw-rw-r--  2.0 unx      886 b- defN 23-Mar-15 22:44 pyquokka/placement_strategy.py
 -rw-rw-r--  2.0 unx     3717 b- defN 23-Mar-29 00:48 pyquokka/quokka_dataset.py
--rw-rw-r--  2.0 unx    19752 b- defN 23-Mar-25 00:17 pyquokka/quokka_runtime.py
+-rw-rw-r--  2.0 unx    19755 b- defN 23-Apr-14 04:33 pyquokka/quokka_runtime.py
 -rw-rw-r--  2.0 unx    93718 b- defN 22-Oct-04 03:50 pyquokka/redis.conf
 -rw-rw-r--  2.0 unx    16471 b- defN 23-Mar-27 22:47 pyquokka/sql_utils.py
 -rw-rw-r--  2.0 unx     2371 b- defN 22-Jul-15 20:59 pyquokka/state.py
--rw-rw-r--  2.0 unx    11213 b- defN 23-Mar-20 21:30 pyquokka/tables.py
+-rw-rw-r--  2.0 unx    11695 b- defN 23-Apr-09 03:05 pyquokka/tables.py
 -rw-rw-r--  2.0 unx     2351 b- defN 23-Jan-28 04:17 pyquokka/target_info.py
 -rw-rw-r--  2.0 unx     5752 b- defN 23-Mar-20 21:30 pyquokka/task.py
--rw-rw-r--  2.0 unx    25660 b- defN 23-Mar-31 23:28 pyquokka/utils.py
+-rw-rw-r--  2.0 unx    29075 b- defN 23-Apr-20 17:57 pyquokka/utils.py
 -rw-rw-r--  2.0 unx     4081 b- defN 23-Jan-25 17:17 pyquokka/windowtypes.py
--rw-rw-r--  2.0 unx    11357 b- defN 23-Apr-05 05:05 pyquokka-0.2.1.dist-info/LICENSE
--rw-rw-r--  2.0 unx      959 b- defN 23-Apr-05 05:05 pyquokka-0.2.1.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Apr-05 05:05 pyquokka-0.2.1.dist-info/WHEEL
--rw-rw-r--  2.0 unx        9 b- defN 23-Apr-05 05:05 pyquokka-0.2.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2502 b- defN 23-Apr-05 05:05 pyquokka-0.2.1.dist-info/RECORD
-32 files, 579038 bytes uncompressed, 136338 bytes compressed:  76.5%
+-rw-rw-r--  2.0 unx    11357 b- defN 23-Apr-20 18:01 pyquokka-0.2.3.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     1012 b- defN 23-Apr-20 18:01 pyquokka-0.2.3.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Apr-20 18:01 pyquokka-0.2.3.dist-info/WHEEL
+-rw-rw-r--  2.0 unx        9 b- defN 23-Apr-20 18:01 pyquokka-0.2.3.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     2659 b- defN 23-Apr-20 18:01 pyquokka-0.2.3.dist-info/RECORD
+34 files, 980782 bytes uncompressed, 271337 bytes compressed:  72.3%
```

## zipnote {}

```diff
@@ -33,23 +33,29 @@
 
 Filename: pyquokka/flight.py
 Comment: 
 
 Filename: pyquokka/hbq.py
 Comment: 
 
+Filename: pyquokka/ldb.so
+Comment: 
+
 Filename: pyquokka/leader_start_ray.sh
 Comment: 
 
 Filename: pyquokka/leader_startup.sh
 Comment: 
 
 Filename: pyquokka/logical.py
 Comment: 
 
+Filename: pyquokka/orderedstream.py
+Comment: 
+
 Filename: pyquokka/placement_strategy.py
 Comment: 
 
 Filename: pyquokka/quokka_dataset.py
 Comment: 
 
 Filename: pyquokka/quokka_runtime.py
@@ -75,23 +81,23 @@
 
 Filename: pyquokka/utils.py
 Comment: 
 
 Filename: pyquokka/windowtypes.py
 Comment: 
 
-Filename: pyquokka-0.2.1.dist-info/LICENSE
+Filename: pyquokka-0.2.3.dist-info/LICENSE
 Comment: 
 
-Filename: pyquokka-0.2.1.dist-info/METADATA
+Filename: pyquokka-0.2.3.dist-info/METADATA
 Comment: 
 
-Filename: pyquokka-0.2.1.dist-info/WHEEL
+Filename: pyquokka-0.2.3.dist-info/WHEEL
 Comment: 
 
-Filename: pyquokka-0.2.1.dist-info/top_level.txt
+Filename: pyquokka-0.2.3.dist-info/top_level.txt
 Comment: 
 
-Filename: pyquokka-0.2.1.dist-info/RECORD
+Filename: pyquokka-0.2.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pyquokka/__init__.py

```diff
@@ -1,11 +1,12 @@
 from . import sql_utils
 from . import expression
 from . import quokka_runtime
 from . import executors
 from . import dataset
 from . import utils
 from . import datastream
+from . import orderedstream
 from . import tables
 from . import task
 from . import hbq
```

## pyquokka/catalog.py

```diff
@@ -1,13 +1,12 @@
 import pyarrow
 import duckdb
 import ray
 import sqlglot
 import os
-import numpy as np
 import polars
 from pyarrow.fs import S3FileSystem
 from pyquokka.sql_utils import filters_to_expression
 import pyarrow.parquet as pq
 import boto3
 
 @ray.remote
@@ -23,28 +22,32 @@
         self.samples[self.table_id] = sample
         self.ratio[self.table_id] = ratio
         self.table_id += 1
         return self.table_id - 1
     
     def register_s3_csv_source(self, bucket, key, schema, sep, total_size):
 
+        import numpy as np
+
         s3 = boto3.client('s3')
         response = s3.head_object(Bucket= bucket, Key=key)
         size = response['ContentLength']
         
         start_pos = np.random.randint(0, max(size - 10 * 1024 * 1024,1))
         sample = s3.get_object(Bucket=bucket, Key=key, Range='bytes={}-{}'.format(start_pos, min(start_pos + 10 * 1024 * 1024, size - 1)))['Body'].read()
         first_new_line = sample.find(b'\n')
         last_new_line = sample.rfind(b'\n')
         sample = sample[first_new_line + 1 : last_new_line]
-        sample = polars.read_csv(sample, new_columns = schema, sep = sep, has_header = False).to_arrow()
+        sample = polars.read_csv(sample, new_columns = schema, separator = sep, has_header = False).to_arrow()
         return self.register_table_data_and_return_ticket(sample, ratio = total_size / (last_new_line - first_new_line))
 
     def register_disk_csv_source(self, filename, schema, sep):
 
+        import numpy as np
+
         if os.path.isfile(filename):
             files = [filename]
             sizes = [os.path.getsize(filename)]
         else:
             assert os.path.isdir(filename), "Does not support prefix, must give absolute directory path for a list of files, will read everything in there!"
             files = [filename + "/" + file for file in os.listdir(filename)]
             sizes = [os.path.getsize(file) for file in files]
@@ -56,15 +59,15 @@
         start_pos = np.random.randint(0, max(os.path.getsize(file_to_do) - 10 * 1024 * 1024,1))
         with open(file_to_do, 'rb') as f:
             f.seek(start_pos)
             sample = f.read(10 * 1024 * 1024)
         first_new_line = sample.find(b'\n')
         last_new_line = sample.rfind(b'\n')
         sample = sample[first_new_line + 1 : last_new_line]
-        sample = polars.read_csv(sample, new_columns = schema, sep = sep, has_header = False).to_arrow()
+        sample = polars.read_csv(sample, new_columns = schema, separator = sep, has_header = False).to_arrow()
         return self.register_table_data_and_return_ticket(sample, ratio = sum(sizes) / (last_new_line - first_new_line))
     
     def register_s3_parquet_source(self, filepath, total_files):
         s3fs = S3FileSystem()
         dataset = pq.ParquetDataset(filepath, filesystem=s3fs )
         # very cursory estimate
         sample = dataset.fragments[0].to_table()
```

## pyquokka/common_startup.sh

```diff
@@ -1,5 +1,11 @@
 sudo apt-get update
-sudo apt install -y python3-pip
-sudo apt install -y awscli
+sudo apt-get install -y python3-pip
+sudo apt-get install -y curl
+sudo apt-get install -y python3.8-dev
+sudo apt-get install -y unzip
 sudo apt install -y nvme-cli
+curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
+unzip -o  awscliv2.zip
+sudo ./aws/install
 pip3 install --upgrade awscli
+sudo apt-get install -y python-cffi
```

## pyquokka/core.py

```diff
@@ -90,14 +90,15 @@
         self.LT = LineageTable()
         self.DST = DoneSeqTable()
         self.CLT = ChannelLocationTable()
         self.FOT = FunctionObjectTable()
         self.SAT = SortedActorsTable()
         self.PFT = PartitionFunctionTable()
         self.AST = ActorStageTable()
+        self.EWT = ExecutorWatermarkTable()
 
         # populate this dictionary from the initial assignment 
             
         self.self_flight_client = pyarrow.flight.connect("grpc://0.0.0.0:5005")
 
         # self.tasks = deque()
 
@@ -147,20 +148,22 @@
     def register_partition_function(self, source_actor_ids, target_actor_id, number_target_channels, mapping):
 
         self.mappings[target_actor_id] = mapping
 
         def partition_fn(predicate_fn, partitioner_fn, batch_funcs, projection, num_target_channels, x, source_channel):
 
             start = time.time()
+            # x could be either a pyarrow table of a polars dataframe
             # print(predicate_fn)
             if type(predicate_fn) == str:
                 con = duckdb.connect().execute('PRAGMA threads=%d' % 8)
-                batch_arrow = x.to_arrow()
+                batch_arrow = x.to_arrow() if type(x) == polars.internals.DataFrame else x
                 x = polars.from_arrow(con.execute(predicate_fn).arrow())
             else:
+                x = x if type(x) == polars.internals.DataFrame else polars.from_arrow(x)
                 x = x.filter(predicate_fn)
             # print("filter time", time.time() - start)
 
             for func in batch_funcs:
                 x = func(x)
                 if x is None or len(x) == 0:
                     return {}
@@ -271,23 +274,18 @@
         
         if target_mask is not None:
             print_if_debug("TARGET_MASK", target_mask)
         
         partition_fns = self.partition_fns[source_actor_id]
 
         start_convert = time.time()
-        if type(output) == pyarrow.Table:
-            output = polars.from_arrow(output)
-        elif type(output) == polars.internals.DataFrame:
-            pass
-        elif output is None:
+        if output is None:
             assert from_local
         else:
-            print(output)
-            raise Exception("push data type not understood")
+            assert type(output) == pyarrow.Table or type(output) == polars.internals.DataFrame, "push data type {} not understood".format(type(output))
 
         print_if_profile("convert time", time.time() - start_convert)
 
         for target_actor_id in partition_fns:
 
             if target_mask is not None and target_actor_id not in target_mask:
                 continue
@@ -329,15 +327,15 @@
                     name = (source_actor_id, source_channel_id, seq, target_actor_id, 0, target_channel_id)
                     batches = [pyarrow.RecordBatch.from_pydict({"__empty__":[]})]
 
                 assert len(batches) == 1, batches
 
                 client = self.actor_flight_clients[target_actor_id][target_channel_id]
 
-                print_if_debug("pushing", source_actor_id, source_channel_id, seq, target_actor_id, target_channel_id, len(batches[0]))
+                # print("pushing", source_actor_id, source_channel_id, seq, target_actor_id, target_channel_id, len(batches[0]))
 
                 try:
                     if not self.check_puttable(client):
                         return False
                     upload_descriptor = pyarrow.flight.FlightDescriptor.for_command(pickle.dumps((True, name, my_format)))
                     batch = batches[0]
                     writer, _ = client.do_put(upload_descriptor, batch.schema)
@@ -632,15 +630,15 @@
                     source_channel_ids = [i for i in source_channel_seqs]
                     source_channel_progress = [len(source_channel_seqs[i]) for i in source_channel_ids]
                     progress = polars.from_dict({"source_actor_id": [source_actor_id] * len(source_channel_ids) , "source_channel_id": source_channel_ids, "progress": source_channel_progress})
                     # progress is guaranteed to have something since len(batches) > 0
 
                     # print("starting with input reqs", input_requirements[0])      
                     new_input_reqs = input_requirements[0].join(progress, on = ["source_actor_id", "source_channel_id"], how = "left").fill_null(0)
-                    new_input_reqs = new_input_reqs.with_column(polars.Series(name = "min_seq", values = new_input_reqs["progress"] + new_input_reqs["min_seq"]))
+                    new_input_reqs = new_input_reqs.with_columns(polars.Series(name = "min_seq", values = new_input_reqs["progress"] + new_input_reqs["min_seq"]))
                     
                     new_input_reqs = new_input_reqs.drop("progress")
                     # print("progress", actor_id, channel_id, progress, new_input_reqs)
                     # if self.dst is not None:
                     #     print(input_requirements, self.dst.filter(polars.col("source_actor_id") == 0))
                     # else:
                     #     print(input_requirements, "None")
@@ -673,14 +671,17 @@
 
                     self.LCT.rpush(transaction, pickle.dumps((actor_id, channel_id)), pickle.dumps((state_seq, out_seq)))
                     self.IRT.set(transaction, pickle.dumps((actor_id, channel_id, state_seq)), pickle.dumps([new_input_reqs] + input_requirements[1:]))
                 # this way of logging the lineage probably use less space than a Polars table actually.                        
 
                 self.EST.set(transaction, pickle.dumps((actor_id, channel_id)), state_seq)                    
                 lineage = pickle.dumps((source_actor_id, source_channel_seqs))
+                for source_channel_id in source_channel_seqs:
+                    max_seq = max(source_channel_seqs[source_channel_id])
+                    self.EWT.set(transaction, pickle.dumps((source_actor_id, source_channel_id)), max_seq)
                 self.state_commit(transaction, actor_id, channel_id, state_seq, lineage)
                 self.task_commit(transaction, candidate_task, next_task)
                 
                 executed = transaction.execute()
                 #if not all(executed):
                 #    raise Exception(executed)
                 
@@ -709,101 +710,111 @@
                     assert (actor_id, channel_id) not in self.tape_input_reqs
                     self.tape_input_reqs[actor_id, channel_id] = new_input_reqs
 
                 name_prefix = pickle.dumps(('s', actor_id, channel_id, state_seq))
                 input_requirements = self.LT.get(self.r, name_prefix)
                 assert input_requirements is not None, pickle.loads(name_prefix)
                 
-                if actor_id in self.sat:
-                    request = ("cache", actor_id, channel_id, input_requirements, True, self.sat[actor_id])
-                else:
-                    request = ("cache", actor_id, channel_id, input_requirements, True, None)
+                transaction = self.r.pipeline()
 
-                reader = self.flight_client.do_get(pyarrow.flight.Ticket(pickle.dumps(request)))
+                if len(pickle.loads(input_requirements)[1]) > 0:
+                    if actor_id in self.sat:
+                        request = ("cache", actor_id, channel_id, input_requirements, True, self.sat[actor_id])
+                    else:
+                        request = ("cache", actor_id, channel_id, input_requirements, True, None)
 
-                chunks_list = []
-                names = []
-                while True:
-                    try:
-                        chunk, metadata = reader.read_chunk()
-                        name, format = pickle.loads(metadata)
-                        assert format == "polars"
-                        if len(names) == 0 or name != names[-1]:
-                            chunks_list.append([chunk])
-                            names.append(name)
-                        else:
-                            chunks_list[-1].append(chunk)
-                            print("creating multi-chunk")
+                    reader = self.flight_client.do_get(pyarrow.flight.Ticket(pickle.dumps(request)))
 
-                    except StopIteration:
-                        break
+                    chunks_list = []
+                    names = []
+                    while True:
+                        try:
+                            chunk, metadata = reader.read_chunk()
+                            name, format = pickle.loads(metadata)
+                            assert format == "polars"
+                            if len(names) == 0 or name != names[-1]:
+                                chunks_list.append([chunk])
+                                names.append(name)
+                            else:
+                                chunks_list[-1].append(chunk)
+                                print("creating multi-chunk")
 
-                # we are going to assume the Flight server gives us results sorted by source_actor_id
-                batches = chunks_list
-                input_names = names
+                        except StopIteration:
+                            break
 
-                if len(batches) == 0:
-                    self.index += 1
-                    continue
+                    # we are going to assume the Flight server gives us results sorted by source_actor_id
+                    batches = chunks_list
+                    input_names = names
 
-                source_actor_id, source_channel_seqs = pickle.loads(input_requirements)
-                source_channel_ids = list(source_channel_seqs.keys())
-                source_channel_progress = [len(source_channel_seqs[k]) for k in source_channel_ids]
-                progress = polars.from_dict({"source_actor_id": [source_actor_id] * len(source_channel_ids) , "source_channel_id": source_channel_ids, "progress": source_channel_progress})
-
-                new_input_reqs = self.tape_input_reqs[actor_id, channel_id][0].join(progress, on = ["source_actor_id", "source_channel_id"], how = "left").fill_null(0)
-                new_input_reqs = new_input_reqs.with_column(polars.Series(name = "min_seq", values = new_input_reqs["progress"] + new_input_reqs["min_seq"]))
-                new_input_reqs = new_input_reqs.select(["source_actor_id", "source_channel_id","min_seq"])
-                self.update_dst()
-                if self.dst is not None:
-                    new_input_reqs = new_input_reqs.join(self.dst, on = ["source_actor_id", "source_channel_id"], how = "left")\
-                                                    .fill_null(MAX_SEQ)\
-                                                    .filter(polars.col("min_seq") <= polars.col("done_seq"))\
-                                                    .drop("done_seq")
-                
-                if len(new_input_reqs) == 0:
-                    self.tape_input_reqs[actor_id, channel_id] =  self.tape_input_reqs[actor_id, channel_id][1:]
-                else:
-                    self.tape_input_reqs[actor_id, channel_id] = [new_input_reqs] +  self.tape_input_reqs[actor_id, channel_id][1:]
+                    if len(batches) == 0:
+                        self.index += 1
+                        continue
 
-                input = [record_batches_to_table(batch) for batch in batches if sum([len(b) for b in batch]) > 0]
+                    source_actor_id, source_channel_seqs = pickle.loads(input_requirements)
+                    source_channel_ids = list(source_channel_seqs.keys())
+                    source_channel_progress = [len(source_channel_seqs[k]) for k in source_channel_ids]
+                    progress = polars.from_dict({"source_actor_id": [source_actor_id] * len(source_channel_ids) , "source_channel_id": source_channel_ids, "progress": source_channel_progress})
 
-                if len(input) > 0:
-                    output, state_seq, out_seq = candidate_task.execute(self.function_objects[actor_id, channel_id], input, self.mappings[actor_id][source_actor_id] , channel_id)
-                else:
-                    state_seq = candidate_task.state_seq
-                    out_seq = candidate_task.out_seq
-                    output = None
-                
-                transaction = self.r.pipeline()
+                    new_input_reqs = self.tape_input_reqs[actor_id, channel_id][0].join(progress, on = ["source_actor_id", "source_channel_id"], how = "left").fill_null(0)
+                    new_input_reqs = new_input_reqs.with_columns(polars.Series(name = "min_seq", values = new_input_reqs["progress"] + new_input_reqs["min_seq"]))
+                    new_input_reqs = new_input_reqs.select(["source_actor_id", "source_channel_id","min_seq"])
+                    self.update_dst()
+                    if self.dst is not None:
+                        new_input_reqs = new_input_reqs.join(self.dst, on = ["source_actor_id", "source_channel_id"], how = "left")\
+                                                        .fill_null(MAX_SEQ)\
+                                                        .filter(polars.col("min_seq") <= polars.col("done_seq"))\
+                                                        .drop("done_seq")
                     
-                out_seq = self.process_output(actor_id, channel_id, output, transaction, state_seq, out_seq)
-                if out_seq == -1:
-                    continue
-                    
-                if state_seq + 1 > candidate_task.last_state_seq:
-                    next_task = ExecutorTask(actor_id, channel_id, state_seq + 1, out_seq, self.tape_input_reqs[actor_id, channel_id])
-                    # print("finished exectape, next input requirement is ", self.tape_input_reqs[actor_id, channel_id])
+                    if len(new_input_reqs) == 0:
+                        self.tape_input_reqs[actor_id, channel_id] =  self.tape_input_reqs[actor_id, channel_id][1:]
+                    else:
+                        self.tape_input_reqs[actor_id, channel_id] = [new_input_reqs] +  self.tape_input_reqs[actor_id, channel_id][1:]
+
+                    input = [record_batches_to_table(batch) for batch in batches if sum([len(b) for b in batch]) > 0]
+
+                    if len(input) > 0:
+                        output, state_seq, out_seq = candidate_task.execute(self.function_objects[actor_id, channel_id], input, self.mappings[actor_id][source_actor_id] , channel_id)
+                    else:
+                        state_seq = candidate_task.state_seq
+                        out_seq = candidate_task.out_seq
+                        output = None
+                        
+                    out_seq = self.process_output(actor_id, channel_id, output, transaction, state_seq, out_seq)
+                    if out_seq == -1:
+                        continue
+                        
+                    if state_seq + 1 > candidate_task.last_state_seq:
+                        next_task = ExecutorTask(actor_id, channel_id, state_seq + 1, out_seq, self.tape_input_reqs[actor_id, channel_id])
+                        # print("finished exectape, next input requirement is ", self.tape_input_reqs[actor_id, channel_id])
+
+                    else:
+                        next_task = TapedExecutorTask(actor_id, channel_id, state_seq + 1, out_seq, candidate_task.last_state_seq)
 
+                # this is a "done task"
                 else:
-                    next_task = TapedExecutorTask(actor_id, channel_id, state_seq + 1, out_seq, candidate_task.last_state_seq)
+                    output = self.function_objects[actor_id, channel_id].done(channel_id)
+                    next_task = None
+                    out_seq = candidate_task.out_seq
+                    out_seq = self.process_output(actor_id, channel_id, output, transaction, state_seq, out_seq)
+                    if out_seq == -1:
+                        continue
 
-                # this way of logging the lineage probably use less space than a Polars table actually.
 
                 self.EST.set(transaction, pickle.dumps((actor_id, channel_id)), state_seq)
                 self.task_commit(transaction, candidate_task, next_task)
-                
                 executed = transaction.execute()
                 #if not all(executed):
                 #    raise Exception(executed)
             
-                message = pyarrow.py_buffer(pickle.dumps(input_names))
-                action = pyarrow.flight.Action("cache_garbage_collect", message)
-                for result in list(self.flight_client.do_action(action)):
-                    assert result.body.to_pybytes().decode("utf-8") == "True"
+                # if it's a done task this garbage collection shouldn't happen cause you didn't use any inputs.
+                if next_task is not None:
+                    message = pyarrow.py_buffer(pickle.dumps(input_names))
+                    action = pyarrow.flight.Action("cache_garbage_collect", message)
+                    for result in list(self.flight_client.do_action(action)):
+                        assert result.body.to_pybytes().decode("utf-8") == "True"
 
 
 @ray.remote
 class IOTaskManager(TaskManager):
     def __init__(self, node_id: int, coordinator_ip: str, worker_ips: list, configs = None) -> None:
         super().__init__(node_id, coordinator_ip, worker_ips, configs = configs)
         self.GIT = GeneratedInputTable()
@@ -873,41 +884,51 @@
                 continue
             
             candidate_task = random.sample(candidate_tasks,1 )[0]
             task_type, tup = pickle.loads(candidate_task)
 
             if task_type == "input":
                 
-                candidate_task = InputTask.from_tuple(tup)
-                actor_id = candidate_task.actor_id
-                channel_id = candidate_task.channel_id
+                # candidate_task = InputTask.from_tuple(tup)
+                # actor_id = candidate_task.actor_id
+                # channel_id = candidate_task.channel_id
 
-                if (actor_id, channel_id) not in self.function_objects:
-                    self.function_objects[actor_id, channel_id] = ray.cloudpickle.loads(self.FOT.get(self.r, actor_id))
+                # if (actor_id, channel_id) not in self.function_objects:
+                #     self.function_objects[actor_id, channel_id] = ray.cloudpickle.loads(self.FOT.get(self.r, actor_id))
 
-                functionObject = self.function_objects[actor_id, channel_id]
+                # functionObject = self.function_objects[actor_id, channel_id]
                 
-                start = time.time()
+                # start = time.time()
 
-                next_task, output, seq, lineage = candidate_task.execute(functionObject)
+                # next_task, output, seq, lineage = candidate_task.execute(functionObject)
 
-                print_if_profile("read time", time.time() - start)
+                # print_if_profile("read time", time.time() - start)
+                raise Exception("only taped input tasks are supported now. Raise Github issue with this error message.")
             
             elif task_type == "inputtape":
                 candidate_task = TapedInputTask.from_tuple(tup)
+                seq = candidate_task.tape[0]
+
+                max_consumed_seq = self.EWT.get(self.r, pickle.dumps((candidate_task.actor_id, candidate_task.channel_id)))
+                if max_consumed_seq is not None:
+                    max_consumed_seq = int(max_consumed_seq)
+                else:
+                    max_consumed_seq = 0
+                if seq > max_consumed_seq + self.configs["max_pipeline"]:
+                    continue
+
                 actor_id = candidate_task.actor_id
                 channel_id = candidate_task.channel_id
                 
                 if (actor_id, channel_id) not in self.function_objects:
                     self.function_objects[actor_id, channel_id] = ray.cloudpickle.loads(self.FOT.get(self.r, actor_id))
                 
                 start = time.time()
 
                 functionObject = self.function_objects[actor_id, channel_id]
-                seq = candidate_task.tape[0]
                 input_object = pickle.loads(self.LT.get(self.r, pickle.dumps((actor_id, channel_id, seq))))
                 print_if_profile("lineage  time", time.time() - start)
                 start = time.time()
 
                 next_task, output, seq, lineage = candidate_task.execute(functionObject, input_object)
                 print_if_profile("read  time", time.time() - start)
             
@@ -945,15 +966,15 @@
         # plan is going to be a polars dataframe with three columns: seq, target_actor, target_channel
 
         seqs = plan['seq'].unique().to_list()
         
         for seq in seqs:
 
             targets = plan.filter(polars.col('seq') == seq).select(["target_actor_id", "target_channel_id"])
-            target_mask_list = targets.groupby('target_actor_id').agg_list().to_dicts()
+            target_mask_list = targets.groupby('target_actor_id').all().to_dicts()
             target_mask = {k['target_actor_id'] : k['target_channel_id'] for k in target_mask_list}
 
             # figure out a pythonic way to convert a dataframe to a dict of lists
 
             pushed = self.push(source_actor_id, source_channel_id, seq, None, target_mask, True)
             if not pushed:
                 return False
```

## pyquokka/dataset.py

```diff
@@ -6,15 +6,14 @@
 import pyarrow.json as pajson
 from io import BytesIO
 import boto3
 import os
 import redis
 from collections import deque
 import polars
-import numpy as np
 import gc
 from pyarrow.fs import S3FileSystem, LocalFileSystem
 from pyarrow.dataset import FileSystemDataset, ParquetFileFormat
 from pyquokka.sql_utils import filters_to_expression
 import multiprocessing
 import concurrent.futures
 import time
@@ -139,14 +138,56 @@
             return ret
         
         results = asyncio.run(main(self.arguments[state[0]: state[1]]))
         # find the first non-None result
         results = polars.concat([i for i in results if i is not None]).select(self.projection)
         return None, results
 
+class InputLanceQVDataset:
+
+    """
+    Let's hope this works!
+    """
+
+    def __init__(self, uri, vec_column, query_vectors, k, columns = None, filters = None, batch_size = 100) -> None:
+        
+        assert type(columns) == list, "columns must be a list of strings"
+        self.columns = columns
+        assert type(filters) == str, "sql predicate supported"
+        self.filters = filters
+        self.query_vector = None
+        self.k = None
+        import numpy as np
+        assert type(query_vectors) == np.ndarray
+        assert len(query_vectors.shape) == 2, "must supply 2d query_vectors"
+        assert type(k) == int
+        self.query_vectors = query_vectors
+        self.k = k
+        self.uri = uri
+        self.batch_size = batch_size
+        self.vec_column = vec_column
+        
+    def get_own_state(self, num_channels):
+
+        self.num_channels = num_channels
+        vectors_per_channel = len(self.query_vectors) // num_channels + 1
+        channel_infos = {}
+        for channel in range(num_channels):
+            my_query_vectors = self.query_vectors[channel * vectors_per_channel : (channel + 1) * vectors_per_channel]
+            channel_infos[channel] = []
+            for pos in range(0, len(my_query_vectors), self.batch_size):
+                channel_infos[channel].append(my_query_vectors[pos:pos+self.batch_size])
+        return channel_infos
+    
+    def execute(self, mapper_id, query_vec_batch):
+
+        import lance
+        dataset = lance.dataset(self.uri)
+        return dataset.to_table(columns = self.columns, filters = self.filters, nearest={"column": self.vec_column, "k": self.k, "q": query_vec_batch})
+
 class InputEC2ParquetDataset:
 
     # filter pushdown could be profitable in the future, especially when you can skip entire Parquet files
     # but when you can't it seems like you still read in the entire thing anyways
     # might as well do the filtering at the Pandas step. Also you need to map filters to the DNF form of tuples, which could be
     # an interesting project in itself. Time for an intern?
 
@@ -211,15 +252,14 @@
 
 class InputSortedEC2ParquetDataset:
 
     def __init__(self, files, partitioner, columns=None, filters=None, mode = "stride") -> None:
 
         self.files = files
         self.partitioner = partitioner
-        assert self.prefix is not None
 
         self.num_channels = None
         self.columns = columns
         self.filters = filters
 
         self.length = 0
         self.workers = 1
@@ -234,16 +274,16 @@
 
     def _get_bounds(self, num_channels):
         
         channel_infos = {}
         fragments = []
         self.num_channels = num_channels
         s3fs = S3FileSystem()
-        dataset = pq.ParquetDataset(self.files, filesystem=s3fs )
-        for fragment in dataset.fragments:
+        dataset = ds.dataset(self.files, format = "parquet", filesystem=s3fs )
+        for fragment in dataset.get_fragments():
             field_index = fragment.physical_schema.get_field_index(self.partitioner)
             metadata = fragment.metadata
             min_timestamp = None
             max_timestamp = None
             for row_group_index in range(metadata.num_row_groups):
                 stats = metadata.row_group(row_group_index).column(field_index).statistics
                 # Parquet files can be created without statistics
@@ -304,15 +344,15 @@
         if self.s3 is None:
             self.s3 = S3FileSystem()
             self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=self.workers)
 
         def download(file):
             with warnings.catch_warnings():
                 warnings.simplefilter("ignore")
-                return pq.read_table(file, columns=self.columns, filters=self.filters, use_threads= False, use_legacy_dataset = True, filesystem = self.s3)
+                return pq.read_table(file, columns=self.columns, filters=self.filters, use_threads= False, filesystem = self.s3)
 
         assert self.num_channels is not None
 
         if files_to_do is None:
             raise Exception("dynamic lineage for inputs not supported anymore")
 
         if len(files_to_do) == 0:
@@ -355,16 +395,16 @@
         def overlap(a, b):
             return max(-1, min(a[1], b[1]) - max(a[0], b[0]))
         
         channel_infos = {channel: [] for channel in channel_bounds}
         assert len(channel_bounds) == num_channels, "must provide bounds for all the channel"
         self.num_channels = num_channels
         s3fs = S3FileSystem()
-        dataset = pq.ParquetDataset(self.bucket + "/" + self.prefix, filesystem=s3fs )
-        for fragment in dataset.fragments:
+        dataset = ds.dataset(self.bucket + "/" + self.prefix, format = "parquet", filesystem=s3fs )
+        for fragment in dataset.get_fragments():
             field_index = fragment.physical_schema.get_field_index(self.partitioner)
             metadata = fragment.metadata
             min_timestamp = None
             max_timestamp = None
             for row_group_index in range(metadata.num_row_groups):
                 stats = metadata.row_group(row_group_index).column(field_index).statistics
                 # Parquet files can be created without statistics
@@ -689,15 +729,15 @@
                 first_newline = resp.find(bytes('\n','utf-8'))
                 if first_newline == -1:
                     raise Exception
                 resp = resp[first_newline + 1:]
 
             # bump = csv.read_csv(BytesIO(resp), read_options=csv.ReadOptions(
             #     column_names=self.names), parse_options=csv.ParseOptions(delimiter=self.sep))
-            bump = polars.read_csv(resp, new_columns = self.names, sep = self.sep, has_header = False, try_parse_dates=True)
+            bump = polars.read_csv(resp, new_columns = self.names, separator = self.sep, has_header = False, try_parse_dates=True)
             
             bump = bump.select(self.columns) if self.columns is not None else bump
 
             return None, bump
 
 
 class InputDiskJSONDataset:
```

## pyquokka/datastream.py

```diff
@@ -5,14 +5,15 @@
 from pyquokka.quokka_runtime import *
 from pyquokka.expression import * 
 from pyquokka.utils import EC2Cluster, LocalCluster
 from pyquokka.sql_utils import required_columns_from_exp, label_sample_table_names
 from functools import partial
 import pyarrow as pa
 from sqlglot.dataframe.sql import functions as F
+import numpy as np
 
 class DataStream:
 
     """
     Quokka DataStream class is how most users are expected to interact with Quokka.
     However users are not expected to create a DataStream directly by calling its constructor.
     Note that constructor takes an argument called `source_node_id`, which would confuse 
@@ -32,15 +33,17 @@
 
     """
 
     def __init__(self, quokka_context, schema: list, source_node_id: int, sorted_reqs = None, materialized = False) -> None:
         self.quokka_context = quokka_context
         self.schema = schema
         self.source_node_id = source_node_id
-        self.sorted = sorted_reqs
+        self.sorted = None
+        if sorted_reqs is not None:
+            self._set_sorted(sorted_reqs)
         self.materialized = materialized # this is used to indicate whether or not the source is materializable from a polars dataframe
 
     def _get_materialized_df(self):
         assert self.materialized == True
         return self.quokka_context.nodes[self.source_node_id].df
     
     def _set_materialized_df(self, df):
@@ -384,14 +387,22 @@
         
             transformed = self.transform(f, new_schema = self.schema, required_columns=self.schema)
             return transformed
         else:
             return self.quokka_context.new_stream(sources={0: self}, partitioners={0: PassThroughPartitioner()}, node=FilterNode(self.schema, predicate),
                                               schema=self.schema, sorted = self.sorted)
 
+
+    def filter_ann(self, vec_column, query_vectors, k):
+
+        """
+        This will filter the DataStream where the vec_column is a 
+        
+        """
+
     def select(self, columns: list):
 
         """
         This will create a new DataStream that contains only selected columns from the source DataStream.
 
         Since a DataStream is implemented as a stream of batches, you might be tempted to think of a filtered DataStream as a stream of batches where each
         batch directly results from selecting columns from a batch in the source DataStream. While this certainly may be the case, `select()` is aggressively 
@@ -756,15 +767,181 @@
             sources={0: self, 1: other},
             partitioners={0: PassThroughPartitioner(), 1: PassThroughPartitioner()},
             node=node,
             schema=self.schema,
             sorted=None
         )
 
-    def gramian(self, columns):
+    def clip(self, columns):
+
+        """
+        Clip the values of the specified columns to the specified min and max values.
+        The min and max values can be either a single value or a list of values (one for each column).
+        If a single value is specified, it will be used for all columns.
+
+        Args:
+            columns (dict): A dictionary of the form {column_name: (min_value, max_value)}.
+        Return:
+            A new DataStream with the specified columns clipped.
+
+        Examples:
+
+            >>> d = qc.read_csv("lineitem.csv")
+            >>> d1 = d.clip({"l_quantity": (0, 10), "l_extendedprice": (0, 1000)})
+
+        """
+        
+        # can't use with_columns or transform because we want some columns to be passed through but not all.
+
+        def polars_func(batch):
+            return batch.with_columns([polars.col(col).clip(min_, max_) for col, (min_, max_) in columns.items()])
+
+        return self.quokka_context.new_stream(
+            sources={0: self},
+            partitioners={0: PassThroughPartitioner()},
+            node=MapNode(
+                schema=self.schema,
+                schema_mapping={
+                    **{column: {-1: column} for column in columns}, **{col: {0: col} for col in self.schema if col not in columns}},
+                required_columns={0: set(columns)},
+                function=polars_func,
+                foldable=True),
+            schema=self.schema,
+            sorted = self.sorted
+            )
+
+    def approximate_median(self, columns, sample_factor = 1):
+
+        """
+        Use the t-digest algorithm to compute approximate median.
+
+        Args:
+            columns (list): A list of columns to compute the median for.
+            sample_factor (int): The factor by which to sample the data before computing the median.
+
+        Return:
+            A new DataStream with number of columns = len(columns) and one row.
+        
+        """
+
+        return self.approximate_quantile(columns, 0.5, sample_factor)
+
+    def approximate_quantile(self, columns, quantiles, sample_factor = 1):
+
+        """
+        Use the t-digest algorithm to compute approximate quantiles.
+        This operator requires [ldbpy](https://pypi.org/project/ldbpy/#description) to be installed on the cluster. 
+        You can optionally supply a sample factor to sample the data before computing the quantiles.
+        Quokka does not implement exact T-Digest. Instead it computes the approximate quantiles on different portions of the data,
+        then computes the mean of the quantiles. This is not the same as computing the quantiles on the entire dataset, but it is
+        much faster and still gives a good approximation of the quantiles. Usually the number of portions is on the same order
+        of magnitude as nodes in the cluster.
+
+        The result will be a DataStream with as many rows as quantiles you want to compute, and as many columns as the number of
+        columns you want to compute the quantiles for. 
+
+        Args:
+            columns (list): A list of the columns to compute the quantiles for.
+            quantiles (list or float): A list of quantiles to compute, or a single quantile.
+            sample_factor (float): The fraction of the data to sample. Must be between 0 and 1.
+
+        Return:
+            A new DataStream. Number of columns = len(columns). Number of rows = len(quantiles).
+            The rows will be laid out in the order in which you supplied the quantiles argument. Row i will be quantile i for all the columns.
+        """
+
+        class TDigestExecutor(Executor):
+            def __init__(self, columns, quantiles) -> None:
+                self.columns = columns
+                assert type(quantiles) == list or type(quantiles) == float, "quantile must be a list or a float"
+                if type(quantiles) == float:
+                    assert 0 <= quantiles <= 1, "quantile must be between 0 and 1"
+                    quantiles = [quantiles]
+                self.quantiles = quantiles
+                self.state = None
+                assert 0 < sample_factor <= 1
+                self.sample_factor = sample_factor
+
+            def execute(self,batches,stream_id, executor_id):
+                from pyarrow.cffi import ffi
+                os.environ["OMP_NUM_THREADS"] = "8"
+                import time
+                if self.state is None:
+                    import ldbpy
+                    self.state = ldbpy.NTDigest(len(self.columns), 100,500) 
+         
+                arrow_batch = pa.concat_tables(batches)
+                if self.sample_factor < 1:
+                    indices = np.random.choice(len(arrow_batch), int(len(arrow_batch) * self.sample_factor), replace=False)
+                    arrow_batch = arrow_batch.take(pa.array(indices))
+                
+                array_ptrs = []
+                schema_ptrs = []
+                c_schemas = []
+                c_arrays = []
+                list_of_arrs = []
+                for i, col in enumerate(self.columns):
+                    c_schema = ffi.new("struct ArrowSchema*")
+                    c_array = ffi.new("struct ArrowArray*")
+                    c_schemas.append(c_schema)
+                    c_arrays.append(c_array)
+                    schema_ptr = int(ffi.cast("uintptr_t", c_schema))
+                    array_ptr = int(ffi.cast("uintptr_t", c_array))
+                    list_of_arrs.append(arrow_batch[col].combine_chunks())
+                    list_of_arrs[-1]._export_to_c(array_ptr, schema_ptr)
+                    array_ptrs.append(array_ptr)
+                    schema_ptrs.append(schema_ptr)
+
+                # start = time.time()
+                # print(array_ptrs, schema_ptrs)
+                self.state.batch_add_arrow(array_ptrs, schema_ptrs)
+                del c_schemas
+                del c_arrays
+                # print("TIME", time.time() - start)
+                
+            def done(self,executor_id):
+                dicts = []
+                for quantile in self.quantiles:
+                    values = [self.state.quantile(i, quantile) for i in range(len(self.columns))]
+                    dicts.append({col: value for col, value in zip(self.columns, values)})
+                return polars.from_dicts(dicts)
+
+        class MeanExecutor(Executor):
+            def __init__(self) -> None:
+                self.state = None
+                self.count = 0
+            def execute(self,batches,stream_id, executor_id):
+                for batch in batches:
+                    #print(batch)
+                    self.count += 1
+                    if self.state is None:
+                        self.state = polars.from_arrow(batch)
+                    else:
+                        self.state += polars.from_arrow(batch)
+            def done(self,executor_id):
+                return self.state / self.count
+
+        assert type(quantiles) == float or type(quantiles) == list, "quantiles must be a float or a list"
+        if type(quantiles) == float:
+            quantiles = [quantiles]
+
+        if self.materialized:
+            df = self._get_materialized_df()
+            frames = []
+            for quantile in quantiles:
+                frames.append(df.select([polars.col(col).quantile(quantile) for col in columns]))
+            return self.quokka_context.from_polars(polars.concat(frames))
+        
+        selected_stream = self.select(columns)
+        executor = TDigestExecutor(columns, quantiles)
+        stream = selected_stream.stateful_transform( executor, columns, required_columns = set(columns), partitioner = PassThroughPartitioner())
+        return stream.stateful_transform( MeanExecutor() , columns, required_columns = set(columns),
+                            partitioner=BroadcastPartitioner(), placement_strategy = SingleChannelStrategy())
+
+    def gramian(self, columns, demean = None):
 
         """
         This will compute DataStream[columns]^T * DataStream[columns]. The result will be len(columns) * len(columns), with schema same as columns.
 
         Args:
             columns (list): List of columns.
 
@@ -779,48 +956,106 @@
 
             >>> d = d.gramian(["l_quantity", "l_extendedprice"])
 
             Result will be a 2x2 matrix.
         
         """
 
+        def udf2(x):
+            from threadpoolctl import threadpool_limits
+            x = x.select(columns).to_numpy() - demean
+            with threadpool_limits(limits=8, user_api='blas'):
+                product = np.dot(x.transpose(), x)
+            return polars.from_numpy(product, schema = columns)
+
         for col in columns:
             assert col in self.schema
+        
+        if demean is not None:
+            assert type(demean) == np.ndarray, "demean must be a numpy array"
+            assert len(demean) == len(columns), "demean must be the same length as columns"
+        else:
+            demean = np.zeros(len(columns))
 
         if self.materialized:
             df = self._get_materialized_df()
-            stuff = df.select(columns).to_numpy()
-            product = np.dot(stuff.transpose(), stuff)
-            return self.quokka_context.from_polars(polars.from_numpy(product, columns = columns))
+            stuff = df.select(columns).to_numpy() - demean
+            product = np.dot(stuff.transpose(), stuff)  
+            return self.quokka_context.from_polars(polars.from_numpy(product, schema = columns))
 
         class AggExecutor(Executor):
             def __init__(self) -> None:
                 self.state = None
             def execute(self,batches,stream_id, executor_id):
                 for batch in batches:
                     #print(batch)
                     if self.state is None:
                         self.state = polars.from_arrow(batch)
                     else:
                         self.state += polars.from_arrow(batch)
             def done(self,executor_id):
                 return self.state
-
+                
+        local_agg_executor = AggExecutor()
         agg_executor = AggExecutor()
-        def udf2(x):
-            x = x.select(columns).to_numpy()
-            product = np.dot(x.transpose(), x)
-            return polars.from_numpy(product, columns = columns)
 
         stream = self.select(columns)
         stream = stream.transform( udf2, new_schema = columns, required_columns = set(columns), foldable=True)
-
+        stream = stream.stateful_transform( local_agg_executor , columns, required_columns = set(columns),
+                            partitioner=PassThroughPartitioner(), placement_strategy = CustomChannelsStrategy(1))
         return stream.stateful_transform( agg_executor , columns, required_columns = set(columns),
                             partitioner=BroadcastPartitioner(), placement_strategy = SingleChannelStrategy())
 
+    def covariance(self, columns):
+
+        """
+        Computes the covariance matrix of the columns.
+
+        Args:
+            columns (list): List of columns.
+
+        Return:
+            A Polars DataFrame of shape len(columns) * len(columns) which is the covariance matrix. This call is blocking.
+        
+        """
+
+        assert "__len__" not in self.schema
+
+        class AggExecutor(Executor):
+            def __init__(self) -> None:
+                self.state = None
+            def execute(self,batches,stream_id, executor_id):
+                psum = polars.from_arrow(pa.concat_tables(batches)).select(columns + ["__len__"]).to_numpy().sum(axis=0)
+                for batch in batches:
+                    #print(batch)
+                    if self.state is None:
+                        self.state = psum
+                    else:
+                        self.state += psum
+            def done(self,executor_id):
+                return polars.from_numpy(np.expand_dims(self.state,0), schema = columns + ["__len__"])
+            
+        def udf2(x):
+            x =  polars.from_numpy(np.expand_dims(x.select(columns).to_numpy().sum(axis = 0),0) , schema = columns).hstack(polars.from_dict({"__len__": [len(x)]}))
+            return x
+
+        stream = self.transform( udf2, new_schema = columns + ["__len__"], required_columns = set(columns), foldable=True)
+        local_agg_executor = AggExecutor()
+        agg_executor = AggExecutor()
+        stream = stream.stateful_transform( local_agg_executor , columns + ["__len__"], required_columns = set(columns + ["__len__"]),
+                            partitioner=PassThroughPartitioner(), placement_strategy = CustomChannelsStrategy(1))
+        mean = stream.stateful_transform( agg_executor , columns + ["__len__"], required_columns = set(columns + ["__len__"]),
+                            partitioner=BroadcastPartitioner(), placement_strategy = SingleChannelStrategy()).collect()
+        count = mean["__len__"][0]
+        print(count)
+        mean = mean.select(columns)
+        print(mean)
+        mean /= count
+        
+        return self.gramian(columns, demean = np.squeeze(mean.to_numpy())).collect() / count
 
     def with_columns_sql(self, new_columns: str, foldable = True):
 
         """
         This is the SQL analog of with_columns. 
 
         Args:
@@ -1636,85 +1871,108 @@
 
         """
         Alias of `agg`.
         """
 
         return self.agg(aggregations)
     
-    def count(self):
+    def count(self, collect = True):
 
         """
         Return total row count.
-        """
 
-        return self.agg({"*":"count"}).collect()
+        Args:
+            collect (bool): if True, return a Polars DataFrame. If False, return a Quokka DataStream.
+        """
+        if collect:
+            return self.agg({"*":"count"}).collect()
+        else:
+            return self.agg({"*":"count"})
 
-    def sum(self, columns):
+    def sum(self, columns, collect = True):
 
         """
         Return the sums of the specified columns.
 
         Args:
             columns (str or list): the column name or a list of column names to sum.
+            collect (bool): if True, return a Polars DataFrame. If False, return a Quokka DataStream.
         """
 
         assert type(columns) == str or type(columns) == list
         if type(columns) == str:
             columns = [columns]
         for col in columns:
             assert col in self.schema
-        return self.agg({col: "sum" for col in columns}).collect()
-    
-    def max(self, columns):
+        
+        if collect:
+            return self.agg({col: "sum" for col in columns}).collect()
+        else:
+            return self.agg({col: "sum" for col in columns})
+
+    def max(self, columns, collect = True):
 
         """
         Return the maximum values of the specified columns.
 
         Args:
             columns (str or list): the column name or a list of column names.
+            collect (bool): if True, return a Polars DataFrame. If False, return a Quokka DataStream.
         """
 
         assert type(columns) == str or type(columns) == list
         if type(columns) == str:
             columns = [columns]
         for col in columns:
             assert col in self.schema
-        return self.agg({col: "max" for col in columns}).collect()
+        
+        if collect:
+            return self.agg({col: "max" for col in columns}).collect()
+        else:
+            return self.agg({col: "max" for col in columns})
 
-    def min(self, columns):
+    def min(self, columns, collect = True):
 
         """
         Return the minimum values of the specified columns.
 
         Args:
             columns (str or list): the column name or a list of column names.
+            collect (bool): if True, return a Polars DataFrame. If False, return a Quokka DataStream.
         """
 
         assert type(columns) == str or type(columns) == list
         if type(columns) == str:
             columns = [columns]
         for col in columns:
             assert col in self.schema
-        return self.agg({col: "min" for col in columns}).collect()
+        if collect:
+            return self.agg({col: "min" for col in columns}).collect()
+        else:
+            return self.agg({col: "min" for col in columns})
     
-    def mean(self, columns):
+    def mean(self, columns, collect = True):
 
         """
         Return the mean values of the specified columns.
 
         Args:
             columns (str or list): the column name or a list of column names.
+            collect (bool): if True, return a Polars DataFrame. If False, return a Quokka DataStream.
         """
 
         assert type(columns) == str or type(columns) == list
         if type(columns) == str:
             columns = [columns]
         for col in columns:
             assert col in self.schema
-        return self.agg({col: "mean" for col in columns}).collect()
+        if collect:
+            return self.agg({col: "mean" for col in columns}).collect()
+        else:
+            return self.agg({col: "mean" for col in columns})
 
 
 class GroupedDataStream:
     def __init__(self, source_data_stream: DataStream, groupby, orderby) -> None:
         
         self.source_data_stream = source_data_stream
         self.groupby = groupby if type(groupby) == list else [groupby]
```

## pyquokka/df.py

```diff
@@ -58,15 +58,15 @@
             >>> cluster = manager.from_json("my_cluster.json")
 
         """
 
         self.sql_config= {"optimize_joins" : True, "s3_csv_materialize_threshold" : 10 * 1048576, "disk_csv_materialize_threshold" : 1048576,
                       "s3_parquet_materialize_threshold" : 10 * 1048576, "disk_parquet_materialize_threshold" : 1048576}
         self.exec_config = {"hbq_path": "/data/", "fault_tolerance": False, "memory_limit": 0.25, "max_pipeline_batches": 30, 
-                        "checkpoint_interval": None, "checkpoint_bucket": "quokka-checkpoint", "batch_attempt": 5}
+                        "checkpoint_interval": None, "checkpoint_bucket": "quokka-checkpoint", "batch_attempt": 20, "max_pipeline": 3}
 
         self.latest_node_id = 0
         self.nodes = {}
         self.cluster = LocalCluster() if cluster is None else cluster
         if type(self.cluster) == LocalCluster:
             self.exec_config["fault_tolerance"] = False, "Fault tolerance is not supported in local mode, turning it off"
         self.io_per_node = io_per_node
@@ -292,17 +292,17 @@
             if resp[-1] == sep:
                 resp = resp[:-1]
             schema = resp.decode("utf-8").split(sep)
             return schema
     
         def return_materialized_stream(where, my_schema = None):
             if has_header:
-                df = polars.read_csv(where, has_header = True,sep = sep)
+                df = polars.read_csv(where, has_header = True,separator = sep)
             else:
-                df = polars.read_csv(where, new_columns = my_schema, has_header = False,sep = sep)
+                df = polars.read_csv(where, new_columns = my_schema, has_header = False,separator = sep)
             self.nodes[self.latest_node_id] = InputPolarsNode(df)
             self.latest_node_id += 1
             return DataStream(self, df.columns, self.latest_node_id - 1, materialized=True)
 
         if schema is None:
             assert has_header, "if not provide schema, must have header."
         if schema is not None and has_header:
@@ -665,23 +665,22 @@
 
         self.nodes[self.latest_node_id] = InputPolarsNode(polars.from_arrow(df))
         self.latest_node_id += 1
         return DataStream(self, df.columns, self.latest_node_id - 1, materialized=True)
 
     def read_sorted_parquet(self, table_location: str, sorted_by: str, schema = None):
         assert type(sorted_by) == str
-        stream = self.read_parquet(table_location, schema)
-        stream._set_sorted({sorted_by : "stride"})
-        return stream
+        stream = self.read_parquet(table_location)
+        assert sorted_by in stream.schema, f"sorted_by column {sorted_by} not in schema {stream.schema}"
+        return OrderedStream(stream, {sorted_by : "stride"})
     
     def read_sorted_csv(self, table_location: str,sorted_by: str, schema = None, has_header = False, sep=","):
         assert type(sorted_by) == str
         stream = self.read_csv(table_location, schema, has_header, sep)
-        stream._set_sorted({sorted_by : "stride"})
-        return stream
+        return OrderedStream(stream, {sorted_by : "stride"})
 
     def read_iceberg(self, table, snapshot = None):
 
         """
         Must have pyiceberg installed on the client. This is a new API. This only supports AWS Glue catalog so far. 
 
         Args:
@@ -759,14 +758,15 @@
     '''
     def new_dataset(self, source, schema: list):
         stream = self.new_stream(sources={0: source}, partitioners={
                                  0: PassThroughPartitioner()}, node=DataSetNode(schema), schema=schema)
         return DataSet(self, schema, stream.source_node_id)
 
     def optimize(self, node_id):
+        self.__push_ann__()
         self.__push_filter__(node_id)
         self.__early_projection__(node_id)
         self.__fold_map__(node_id)
         if self.sql_config["optimize_joins"]:
             self.__merge_joins__(node_id)
         self.__propagate_cardinality__(node_id)
         self.__determine_stages__(node_id)
@@ -1133,14 +1133,95 @@
                         # if for some reason the parent's projection is not None then it has to contain whatever you are already projecting, or else that specified projection is wrong
                         if parent.targets[node_id].projection is not None:
                             assert pushed_projections[parent_id].issubset(
                                 set(parent.targets[node_id].projection))
 
                         parent.targets[node_id].projection = pushed_projections[parent_id]
                     self.__early_projection__(parent_id)
+    
+    def __push_ann__(self):
+
+        ann_nodes = []
+        for node_id in self.execution_nodes:
+            if issubclass(type(self.execution_nodes[node_id]), NearestNeighborFilterNode):
+                ann_nodes.append(node_id)
+        
+        for node_id in ann_nodes:
+
+            node = self.execution_nodes[node_id]
+            targets = copy.deepcopy(node.targets)
+            assert len(node.parents) == 1 and len(targets) != 0
+            # figure out which source node is the parent by walking the graph. currently the vector must come from a source node. It cannot be generated by something.
+            
+            curr_node_id = node.parents[0]
+            curr_col_name = node.vec_column
+            curr_target_id = node_id
+            while True:
+                if issubclass(type(self.execution_nodes[curr_node_id]), SourceNode):
+                    break
+                else:
+                    schema_mapping = self.execution_nodes[curr_node_id].schema_mapping
+                    interested = schema_mapping[curr_col_name]
+                    assert len(interested) == 1, "vector column must only originate from one source currently, i.e. unions don't work. Please raise Github issue."
+                    my_parent = list(interested.keys())[0]
+                    if my_parent == -1:
+                        break
+                    else:
+                        curr_node_id = self.execution_nodes[curr_node_id].parents[my_parent]
+                        curr_col_name = interested[my_parent]
+                        curr_target_id = self.execution_nodes[curr_node_id]
+            
+            # curr_node_id and col_name should now be the source node and the column name of the vector
+
+            if issubclass(type(self.execution_nodes[curr_node_id]), InputLanceNode):
+                
+                lance_node = self.execution_nodes[curr_node_id]
+                lance_node.query_vectors = copy.deepcopy(node.query_vectors)
+                lance_node.k = node.k
+                lance_node.vec_column = curr_col_name
+                
+                # delete yourself
+                parent = self.execution_nodes[node.parents[0]]
+                for target_id in targets:
+                    parent.targets[target_id] = targets[target_id]
+                del parent.targets[node_id]
+                del self.execution_nodes[node_id]
+
+            # otherwise you need to maintain yourself
+            else:
+
+                # first remove yourself from the current location in the graph, i.e. wire up your parent to your targets.
+                parent = self.execution_nodes[node.parents[0]]
+                for target_id in targets:
+                    parent.targets[target_id] = targets[target_id]
+                del parent.targets[node_id]
+                # set your targets parents to your parent
+                for target_id in targets:
+                    success = False
+                    for key in self.execution_nodes[target_id].parents:
+                        if self.execution_nodes[target_id].parents[key] == node_id:
+                            self.execution_nodes[target_id].parents[key] = node.parents[0]
+                            success = True
+                            break
+                    assert success
+                node.targets = {}
+                
+                # now insert yourself between curr_node_id and curr_target_id
+
+                self.execution_nodes[curr_node_id].targets[node_id] = TargetInfo(PassThroughPartitioner(), None, None, [])
+                node.parents[0] = curr_node_id
+                node.targets[curr_target_id] = copy.deepcopy(self.execution_nodes[curr_node_id].targets[curr_target_id])
+                del self.execution_nodes[curr_node_id].targets[curr_target_id]
+                success = False
+                for key in self.execution_nodes[curr_target_id].parents:
+                    if self.execution_nodes[target_id].parents[key] == curr_node_id:
+                        self.execution_nodes[target_id].parents[key] = node_id
+                        success = True
+                        break
+                assert success
 
     def __fold_map__(self, node_id):
 
         node = self.execution_nodes[node_id]
         targets = node.targets
 
         if issubclass(type(node), SourceNode):
```

## pyquokka/executors.py

```diff
@@ -1,15 +1,14 @@
 import os
 import polars
 import pandas as pd
 os.environ['ARROW_DEFAULT_MEMORY_POOL'] = 'system'
 import redis
 import pyarrow as pa
 import time
-import numpy as np
 import os, psutil
 import pyarrow.parquet as pq
 import pyarrow.csv as csv
 from collections import deque
 import pyarrow.compute as compute
 import random
 import sys
@@ -402,14 +401,16 @@
             else:
                 futures.append(self.executor.submit(upload_csv, current_batch, basename_template))
         
         assert all([fut.result() for fut in futures])
 
     def execute(self,batches,stream_id, executor_id):
 
+        import numpy as np
+
         self.my_batches.extend([i for i in batches if i is not None and len(i) > 0])
 
         lengths = [len(batch) for batch in self.my_batches]
         total_len = np.sum(lengths)
 
         if total_len <= self.row_group_size:
             return
@@ -606,25 +607,85 @@
             self.agg_clause = self.agg_clause[:-1]
         
         self.state = None
     
     def execute(self, batches, stream_id, executor_id):
         batch = pa.concat_tables(batches)
         self.state = batch if self.state is None else pa.concat_tables([self.state, batch])
-    
-    def done(self, executor_id):
 
+    def done(self, executor_id):
         if self.state is None:
             return None
         con = duckdb.connect().execute('PRAGMA threads=%d' % 8)
         batch_arrow = self.state
         self.state = polars.from_arrow(con.execute(self.agg_clause).arrow())
         del batch_arrow        
         return self.state
+
+class SortedAsofExecutor(Executor):
+    def __init__(self, time_col_trades = 'time', time_col_quotes = 'time', symbol_col_trades = 'symbol', symbol_col_quotes = 'symbol', suffix = "_right") -> None:
+        self.trade_state = None
+        self.quote_state = None
+        self.join_state = None
+        self.time_col_trades = time_col_trades
+        self.time_col_quotes = time_col_quotes
+        self.symbol_col_trades = symbol_col_trades
+        self.symbol_col_quotes = symbol_col_quotes
+        self.suffix = suffix
+
+    def execute(self,batches,stream_id, executor_id):    
+        # sort_col = self.time_col_trades if stream_id == 0 else self.time_col_quotes
+        # batch = polars.from_arrow(pa.concat_tables([batch.sort_by(sort_col) for batch in batches]))
+        batch = polars.from_arrow(pa.concat_tables(batches))
+        if stream_id == 0:
+            # assert batch[self.time_col_trades].is_sorted()
+            if self.trade_state is None:
+                self.trade_state = batch
+            else:
+                if len(self.trade_state) > 0:
+                    assert self.trade_state[self.time_col_trades][-1] <= batch[self.time_col_trades][0]
+                self.trade_state.vstack(batch, in_place = True)
+        else:
+            # assert batch[self.time_col_quotes].is_sorted()
+            if self.quote_state is None:
+                self.quote_state = batch
+            else:
+                if len(self.quote_state) > 0:
+                    assert self.quote_state[self.time_col_quotes][-1] <= batch[self.time_col_quotes][0]
+                self.quote_state.vstack(batch, in_place = True)
+
+        if self.trade_state is None or self.quote_state is None or len(self.trade_state) == 0 or len(self.quote_state) == 0:
+            return
+
+        joinable_trades = self.trade_state.filter(polars.col(self.time_col_trades) < self.quote_state[self.time_col_quotes][-1])
+        if len(joinable_trades) == 0:
+            return
+        
+        joinable_quotes = self.quote_state.filter(polars.col(self.time_col_quotes) <= joinable_trades[self.time_col_trades][-1])
+        if len(joinable_quotes) == 0:
+            return
+
+        self.trade_state =  self.trade_state.filter(polars.col(self.time_col_trades) >= self.quote_state[self.time_col_quotes][-1])
+
+        result = joinable_trades.join_asof(joinable_quotes, left_on = self.time_col_trades, right_on = self.time_col_quotes, by_left = self.symbol_col_trades, by_right = self.symbol_col_quotes, suffix = self.suffix)
+
+        mock_result = joinable_quotes.join_asof(joinable_trades, left_on = self.time_col_quotes, right_on = self.time_col_trades, by_left = self.symbol_col_quotes, by_right = self.symbol_col_trades, suffix = self.suffix, strategy = "forward").drop_nulls()
+        latest_joined_quotes = mock_result.groupby(self.symbol_col_quotes).agg([polars.max(self.time_col_quotes)])
+        start = time.time()
+        new_quote_state = self.quote_state.join(latest_joined_quotes, on = self.symbol_col_quotes, how = "left", suffix = "_latest").fill_null(-1)
+        print("join time: ", time.time() - start)
+        self.quote_state = new_quote_state.filter(polars.col(self.time_col_quotes) >= polars.col(self.time_col_quotes + "_latest")).drop([self.time_col_quotes + "_latest"])
+
+        # print(len(result))
+
+        return result
     
+    def done(self, executor_id):
+        return self.trade_state.join_asof(self.quote_state, left_on = self.time_col_trades, right_on = self.time_col_quotes, by_left = self.symbol_col_trades, by_right = self.symbol_col_quotes, suffix = self.suffix)
+
 class ConcatThenSQLExecutor(Executor):
     def __init__(self, sql_statement) -> None:
         self.statement = sql_statement
         self.state = None
 
     def checkpoint(self, conn, actor_id, channel_id, seq):
         pass
@@ -688,14 +749,16 @@
         writer.close()
         # input_mem_table.write_parquet(target_filepath, row_group_size = self.record_batch_rows, use_pyarrow =True)
 
         return True
 
     def execute(self, batches, stream_id, executor_id):
 
+        import numpy as np
+
         # if self.executor is None:
         #     self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)
 
         # we are going to update the in memory index and flush out the sorted stuff
         
         flush_file_name = self.data_dir + self.prefix + "-" + str(executor_id) + "-" + str(self.fileno) + ".arrow"
         batches = [polars.from_arrow(i) for i in batches if i is not None and len(i) > 0]
```

## pyquokka/logical.py

```diff
@@ -267,14 +267,26 @@
         return node
     
     def __str__(self):
         result = str(type(self)) + '\nPredicate: ' + str(self.predicate) + '\nProjection: ' + str(self.projection) + '\nTargets:' 
         for target in self.targets:
             result += "\n\t" + str(target) + " " + str(self.targets[target])
         return result
+    
+class InputLanceNode(SourceNode):
+    def __init__(self, uri, schema, predicate = None, projection = None, query_vectors = None, k = None) -> None:
+        super().__init__(schema)
+        self.uri = uri
+        self.predicate = predicate
+        self.projection = projection
+        self.query_vectors = query_vectors
+        self.k = k
+    
+    def set_cardinality(self, catalog):
+        return super().set_cardinality(catalog)
 
 class InputS3ParquetNode(SourceNode):
     def __init__(self, files, schema, predicate = None, projection = None) -> None:
         super().__init__(schema)
         
         self.files = files
         self.predicate = predicate
@@ -290,15 +302,15 @@
     
     def lower(self, task_graph):
 
         if self.output_sorted_reqs is not None:
             assert len(self.output_sorted_reqs) == 1
             key = list(self.output_sorted_reqs.keys())[0]
             val = self.output_sorted_reqs[key]
-            parquet_reader = InputSortedEC2ParquetDataset(self.files, columns = list(self.projection) if self.projection is not None else None, filters = self.predicate, mode=val)
+            parquet_reader = InputSortedEC2ParquetDataset(self.files, key, columns = list(self.projection) if self.projection is not None else None, filters = self.predicate, mode=val)
         else:
             parquet_reader = InputEC2ParquetDataset(self.files, columns = list(self.projection) if self.projection is not None else None, filters = self.predicate)
         node = task_graph.new_input_reader_node(parquet_reader, self.stage, self.placement_strategy)
         return node
     
     def __str__(self):
         result = str(type(self)) + '\nPredicate: ' + str(self.predicate) + '\nProjection: ' + str(self.projection) + '\nTargets:' 
@@ -557,14 +569,34 @@
         result = "filter node: " + self.predicate.sql(dialect = "duckdb")
         return result
     
     def lower(self, task_graph, parent_nodes, parent_source_info):
         print("Tried to lower a filter node. This means the optimization probably failed and something bad is happening.")
         raise NotImplementedError
 
+class NearestNeighborFilterNode(TaskNode):
+
+    def __init__(self, schema: list, vec_column: str, query_vectors,  k: int) -> None:
+        super().__init__(
+            schema = schema,
+            schema_mapping = {column: {0: column} for column in schema},
+            required_columns = {0: {vec_column}})
+        self.vec_column = vec_column
+        self.query_vectors = query_vectors
+        self.k = k
+    
+    def __str__(self):
+        result = "nearest neighbor filter node on vec column {} with {} query vectors".format(self.vec_column, len(self.query_vectors))
+        return result
+    
+    def lower(self, task_graph, parent_nodes, parent_source_info):
+        print("Tried to lower a nearest neighbor filter node. This means the optimization probably failed and something bad is happening.")
+        raise NotImplementedError
+
+
 class ProjectionNode(TaskNode):
 
     # predicate must be CNF
     def __init__(self, projection: set) -> None:
         super().__init__(
             schema = projection, 
             schema_mapping = {column: {0: column} for column in projection},
```

## pyquokka/quokka_runtime.py

```diff
@@ -211,32 +211,32 @@
     def prologue(self, streams, placement_strategy, source_target_info):
 
         def partition_key_str(key, data, source_channel, num_target_channels):
 
             result = {}
             assert type(data) == polars.internals.DataFrame
             if "int" in str(data[key].dtype).lower():
-                partitions = data.with_column(polars.Series(name="__partition__", values=(data[key] % num_target_channels))).partition_by("__partition__")
+                partitions = data.with_columns(polars.Series(name="__partition__", values=(data[key] % num_target_channels))).partition_by("__partition__")
             elif data[key].dtype == polars.datatypes.Utf8 or data[key].dtype == polars.datatypes.Float32 or data[key].dtype == polars.datatypes.Float64:
-                partitions = data.with_column(polars.Series(name="__partition__", values=(data[key].hash() % num_target_channels))).partition_by("__partition__")
+                partitions = data.with_columns(polars.Series(name="__partition__", values=(data[key].hash() % num_target_channels))).partition_by("__partition__")
             else:
                 print(data[key])
                 raise Exception("partition key type not supported")
             for partition in partitions:
                 target = partition["__partition__"][0]
                 result[target] = partition.drop("__partition__")   
             return result
         
 
         def partition_key_range(key, total_range, data, source_channel, num_target_channels):
 
             per_channel_range = total_range // num_target_channels
             result = {}
             assert type(data) == polars.internals.DataFrame
-            partitions = data.with_column(polars.Series(name="__partition__", values=((data[key] - 1) // per_channel_range))).partition_by("__partition__")
+            partitions = data.with_columns(polars.Series(name="__partition__", values=((data[key] - 1) // per_channel_range))).partition_by("__partition__")
             for partition in partitions:
                 target = partition["__partition__"][0]
                 result[target] = partition.drop("__partition__")   
             return result 
 
         def broadcast(data, source_channel, num_target_channels):
             return {i: data for i in range(num_target_channels)}
```

## pyquokka/tables.py

```diff
@@ -318,8 +318,22 @@
 class LastInputTable(ClientWrapper):
     def __init__(self) -> None:
         super().__init__("LIT")
     
     def to_dict(self, redis_client):
         keys = self.keys(redis_client)
         values = self.mget(redis_client, keys)
+        return {pickle.loads(key): int(value) for key, value in zip(keys, values)}
+
+'''
+- Executor Watermark Table (EWT): this keeps track of the latest processed seq for each actor_id, channel_id to affect
+   backpressure similar to Storm
+'''
+
+class ExecutorWatermarkTable(ClientWrapper):
+    def __init__(self) -> None:
+        super().__init__("EWT")
+    
+    def to_dict(self, redis_client):
+        keys = self.keys(redis_client)
+        values = self.mget(redis_client, keys)
         return {pickle.loads(key): int(value) for key, value in zip(keys, values)}
```

## pyquokka/utils.py

```diff
@@ -4,14 +4,15 @@
 import subprocess
 import multiprocessing
 import pyquokka
 import ray
 import json
 import signal
 import polars
+import multiprocessing
 from pssh.clients import ParallelSSHClient
 
 def preexec_function():
     # Ignore the SIGINT signal by setting the handler to the standard
     # signal handler SIG_IGN.
     signal.signal(signal.SIGINT, signal.SIG_IGN)
 
@@ -170,16 +171,23 @@
     def str_key_to_int(self, d):
         return {int(i):d[i] for i in d}
     
     def install_python_package(self, cluster, req):
         assert type(cluster) == EC2Cluster
         self.launch_all("pip3 install " + req, list(cluster.public_ips.values()), "Failed to install " + req)
 
-    def launch_all(self, command, ips, error = "Error", ignore_error = False):
+    def launch_ssh_command(self, command, ip, ignore_error=False):
+        launch_command = "ssh -oStrictHostKeyChecking=no -oConnectTimeout=2 -i " + self.key_location + " ubuntu@" + ip + " " + command
+        try:
+            result = subprocess.run(launch_command, shell=True, capture_output=True, check=True)
+            return result.stdout.decode().strip()
+        except subprocess.CalledProcessError as e:
+            raise Exception(f"launch_ssh_command failed with exit code: {e.returncode}")
 
+    def launch_all(self, command, ips, error = "Error", ignore_error = False):
         client = ParallelSSHClient(ips, user="ubuntu", pkey=self.key_location, timeout=5)
         output = client.run_command(command)
         result = []
         for host_output in output:
             for line in host_output.stdout:
                 result.append(line)
             exit_code = host_output.exit_code
@@ -227,15 +235,14 @@
         command ="/home/ubuntu/.local/bin/ray start --address='" + str(leader_private_ip) + ":6380' --redis-password='5241590000000000'"
         self.launch_all(command, public_ips, "ray workers failed to connect to ray head node")
 
         self.copy_and_launch_flight(public_ips)
 
     def set_up_envs(self, public_ips, requirements, aws_access_key, aws_access_id):
             
-        import multiprocessing
         pool = multiprocessing.Pool(multiprocessing.cpu_count())        
         pool.starmap(execute_script, [(self.key_location, public_ip) for public_ip in public_ips])
 
         self.launch_all("aws configure set aws_secret_access_key " + str(aws_access_key), public_ips, "Failed to set AWS access key")
         self.launch_all("aws configure set aws_access_key_id " + str(aws_access_id), public_ips, "Failed to set AWS access id")
 
         # cluster must have same ray version as client.
@@ -253,14 +260,17 @@
         self.launch_all("export GLIBC_TUNABLES=glibc.malloc.trim_threshold=524288", public_ips, "Failed to set malloc limit")
         self.launch_all("nohup python3 -u flight.py > foo.out 2> foo.err < /dev/null &", public_ips, "Failed to start flight servers on workers.")
 
     def set_up_spill_dir(self, public_ips, spill_dir):
         print("Trying to set up spill dir.")   
         result = self.launch_all("sudo nvme list", public_ips, "failed to list nvme devices")
         devices = [sentence.split(" ")[0] for sentence in result if "Amazon EC2 NVMe Instance Storage" in sentence]
+        if len(devices) == 0:
+            print("No nvme devices found. Skipping.")
+            return
         assert all([device == devices[0] for device in devices]), "All instances must have same nvme device location. Raise Github issue if you see this."
         device = devices[0]
         print("Found nvme device: ", device)
         
         try:
             self.launch_all("sudo mkfs.ext4 -F -E nodiscard {};".format(device), public_ips, "failed to format nvme ssd")
             self.launch_all("sudo mount {} {};".format(device, spill_dir), public_ips, "failed to mount nvme ssd")
@@ -430,22 +440,30 @@
             a = ec2.describe_instances(InstanceIds = instance_ids)
 
             public_ips = [k['PublicIpAddress'] for reservation in a['Reservations'] for k in reservation['Instances']] 
             private_ips = [k['PrivateIpAddress'] for reservation in a['Reservations'] for k in reservation['Instances']] 
 
             return EC2Cluster(public_ips, private_ips, instance_ids, cpu_count, spill_dir)
         if sum([i=="running" for i in states]) == len(states):
+            request_instance_ids = [k['InstanceId'] for reservation in a['Reservations'] for k in reservation['Instances']]
             public_ips = [k['PublicIpAddress'] for reservation in a['Reservations'] for k in reservation['Instances']] 
             private_ips = [k['PrivateIpAddress'] for reservation in a['Reservations'] for k in reservation['Instances']] 
+
+            # figure out where in request_instance_ids is instance_ids[0]
+            leader_index = request_instance_ids.index(instance_ids[0])
+            assert leader_index != -1, "Leader instance not found in request_instance_ids"
+            public_ips = public_ips[leader_index:] + public_ips[:leader_index]
+            private_ips = private_ips[leader_index:] + private_ips[:leader_index]
+
             return EC2Cluster(public_ips, private_ips, instance_ids, cpu_count, spill_dir)
         else:
             print("Cluster in an inconsistent state. Either only some machines are running or some machines have been terminated.")
             return False
     
-    def get_cluster_from_ray(self, path_to_yaml, aws_access_key, aws_access_id, requirements = [], spill_dir = "/data"):
+    def get_cluster_from_ray(self, path_to_yaml, aws_access_key, aws_access_id, requirements = [], spill_dir = "/data", cluster_name = None):
 
         """
         Connect to a Ray cluster. This will set up the Quokka runtime on the cluster. The Ray cluster must be in a running state and created by 
         the `ray up` command. The `ray up` command creates a yaml file that is used to connect to the cluster. This function will read the yaml file
         and connect to the cluster.
 
         Make sure all the instances are running before calling this function! Best wait for a few minutes after calling `ray up` before calling this function.
@@ -481,15 +499,16 @@
         import yaml
         ec2 = boto3.client("ec2")
         with open(path_to_yaml, 'r') as f:
             config = yaml.safe_load(f)
         
     
         tag_key = "ray-cluster-name"
-        cluster_name = config['cluster_name']
+        if cluster_name is None:
+            cluster_name = config['cluster_name']
         instance_type = config["available_node_types"]['ray.worker.default']["node_config"]["InstanceType"]
         cpu_count = ec2.describe_instance_types(InstanceTypes=[instance_type])['InstanceTypes'][0]['VCpuInfo']['DefaultVCpus']
 
         filters = [{'Name': 'instance-state-name', 'Values': ['running']},
                    {'Name': f'tag:{tag_key}', 'Values': [cluster_name]}]
         response = ec2.describe_instances(Filters=filters)
         instance_ids = []
@@ -521,7 +540,50 @@
         self.set_up_spill_dir(public_ips, spill_dir)
 
         z = os.system("ssh -oStrictHostKeyChecking=no -i " + self.key_location + " ubuntu@" + public_ips[0] + " 'bash -s' < " + pyquokka.__file__.replace("__init__.py","leader_startup.sh"))
         print(z)
 
         self.copy_and_launch_flight(public_ips)
         return EC2Cluster(public_ips, private_ips, instance_ids, cpu_count, spill_dir)
+
+
+    def get_cluster_from_dockerized_ray(self, path_to_yaml, spill_dir = "/data", cluster_name = None):
+
+        """
+        """
+
+        import yaml
+        ec2 = boto3.client("ec2")
+        with open(path_to_yaml, 'r') as f:
+            config = yaml.safe_load(f)
+    
+        tag_key = "ray-cluster-name"
+        if cluster_name is None:
+            cluster_name = config['cluster_name']
+        instance_type = config["available_node_types"]['ray.worker.default']["node_config"]["InstanceType"]
+        cpu_count = ec2.describe_instance_types(InstanceTypes=[instance_type])['InstanceTypes'][0]['VCpuInfo']['DefaultVCpus']
+
+        filters = [{'Name': 'instance-state-name', 'Values': ['running']},
+                   {'Name': f'tag:{tag_key}', 'Values': [cluster_name]}]
+        response = ec2.describe_instances(Filters=filters)
+
+        instance_names = [[k for k in instance['Tags'] if k['Key'] == 'ray-user-node-type'][0]['Value'] for reservation in response['Reservations'] for instance in reservation['Instances']]
+        instance_ids = [instance['InstanceId'] for reservation in response['Reservations'] for instance in reservation['Instances']]
+        public_ips = [instance['PublicIpAddress'] for reservation in response['Reservations'] for instance in reservation['Instances']]
+        private_ips = [instance['PrivateIpAddress'] for reservation in response['Reservations'] for instance in reservation['Instances']]
+
+        try:
+            head_index = instance_names.index("ray.head.default")
+        except:
+            print("No head node found. Please make sure that the cluster is running.")
+            return False
+    
+        # rotate instance_ids, public_ips, private_ips so that head is first
+        instance_ids = instance_ids[head_index:] + instance_ids[:head_index]
+        public_ips = public_ips[head_index:] + public_ips[:head_index]
+        private_ips = private_ips[head_index:] + private_ips[:head_index]
+
+        assert len(instance_ids) == len(public_ips) == len(private_ips)
+        print("Detected {} instances in running ray cluster {}".format(len(instance_ids), cluster_name))
+
+        print(public_ips)
+        return EC2Cluster(public_ips, private_ips, instance_ids, cpu_count, spill_dir)
```

## Comparing `pyquokka-0.2.1.dist-info/LICENSE` & `pyquokka-0.2.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `pyquokka-0.2.1.dist-info/METADATA` & `pyquokka-0.2.3.dist-info/METADATA`

 * *Files 9% similar despite different names*

```diff
@@ -1,38 +1,40 @@
 Metadata-Version: 2.1
 Name: pyquokka
-Version: 0.2.1
+Version: 0.2.3
 Summary: Quokka
 Author: Tony Wang
 Author-email: zihengw@stanford.edu
 License: http://www.apache.org/licenses/LICENSE-2.0
 Keywords: python
 Platform: UNKNOWN
 Classifier: Development Status :: 3 - Alpha
 Classifier: Programming Language :: Python :: 3
 Classifier: Operating System :: MacOS :: MacOS X
 Classifier: Operating System :: POSIX :: Linux
 License-File: LICENSE
+Requires-Dist: cffi
 Requires-Dist: pyarrow
 Requires-Dist: duckdb (>=0.6.0)
 Requires-Dist: redis
 Requires-Dist: boto3
 Requires-Dist: numpy
 Requires-Dist: pandas
 Requires-Dist: protobuf
 Requires-Dist: ray (>=2.0.0)
 Requires-Dist: psutil
 Requires-Dist: h5py
-Requires-Dist: polars (>=0.16.8)
-Requires-Dist: sqlglot
+Requires-Dist: polars (>=0.17.0)
+Requires-Dist: sqlglot (>=11.4.2)
 Requires-Dist: graphviz
 Requires-Dist: tqdm
 Requires-Dist: aiohttp
 Requires-Dist: botocore
-Requires-Dist: parallel-ssh
+Requires-Dist: ldbpy
+Requires-Dist: threadpoolctl
 
 
 Dope way to do cloud analytics
 
 Check out https://github.com/marsupialtail/quokka
 
 or https://marsupialtail.github.io/quokka/
```

## Comparing `pyquokka-0.2.1.dist-info/RECORD` & `pyquokka-0.2.3.dist-info/RECORD`

 * *Files 17% similar despite different names*

```diff
@@ -1,32 +1,34 @@
-pyquokka/__init__.py,sha256=1oqvc7hTUUOHMkeXs1bCQv5jcsBs0Wf4zOfsgSoLpcA,228
-pyquokka/catalog.py,sha256=K7bkrML7TC4sl9YBroGNxRT6teJEfafG8Y7dFTPzwkU,4242
-pyquokka/common_startup.sh,sha256=xrGYHVANLgzKaESDTw-MuBV64GHUecj23N_236HFIlk,138
+pyquokka/__init__.py,sha256=vifW-I2hoQKvSNKPyT0BAX1iVntZ7erS3xXhjq4eH3k,256
+pyquokka/catalog.py,sha256=XczH9nRAjYE0eWvPdoI_sQE1N85vflPouzjpIe0lqOA,4291
+pyquokka/common_startup.sh,sha256=Rst9lyiO_Fx34swl6_-Z8w7lRHQM-8GDhsUFCbS_kP8,372
 pyquokka/coordinator.py,sha256=s7yjNB_F6bEoWoko_5cTfN0BAyH9AcYm2PKIbqJexhI,28239
-pyquokka/core.py,sha256=kXNAo_HfUcFgaeBxoxUjzWuugmkgpgfzc-XETYBFQwg,46535
-pyquokka/dataset.py,sha256=maqXM5IeC4fdJHWura028UaUFFgRGRvdovxgp3olLYs,46811
-pyquokka/datastream.py,sha256=v6FvefRtLElSO6mjpFft2wPlW2i5yE81UPxNPkrXvVU,84427
+pyquokka/core.py,sha256=wGUxg3M0bIUzBoVbQbAWg-vg3sFhpj4auBVF-cKr784,48229
+pyquokka/dataset.py,sha256=vgKT07HkV_P0LVu5JHijEHvYUkeQZElbgLnMC8yw_jg,48426
+pyquokka/datastream.py,sha256=thsNg6wKI5BYTbxD3OksCws_Kszfv69uGhhc32Q91lA,96482
 pyquokka/debugger.py,sha256=Yi1CqGHbV2y2bszUhKuxcQ561vugc6hs6xfnpJ8HIjU,1438
-pyquokka/df.py,sha256=T4f8JiosHTIkLZ1Hrw9LpXMaVldz3MqtzkzCrdJXJ2I,71649
-pyquokka/executors.py,sha256=FKx9hLggJM92PfsQHOabfqi7hX4K1En8TO6ItkGqF0k,33759
+pyquokka/df.py,sha256=iU3bNgO8PPcDW78jkGDTcZv2zog8De16fWw-VVLecd0,75930
+pyquokka/executors.py,sha256=dVzy3_WbsMtbAWay2UlEgE1GmxEVbT6Fz1X9i2elboY,37360
 pyquokka/expression.py,sha256=SmJxvGrSgW3ra8EU5SLGbQ8AxbJ2bPxKHLvbXpQ-8WY,12657
 pyquokka/flight.py,sha256=ri1aEXtn4s2_ACD8PfU5EIy_fA7FlNrM61WuUjQ9Pl8,17483
 pyquokka/hbq.py,sha256=tF679UdNRkjXDdEvBXv6_aXd4dh4nQgIUHRjknKX1LQ,3108
+pyquokka/ldb.so,sha256=SO70uhXN219Ns5RNIEePv1fs0qRSksc6yq2eN0tucw8,368480
 pyquokka/leader_start_ray.sh,sha256=vyZi-Utmj7r8Qr43YmOQq3uIuAN5Nn4F61MIszzdit4,274
 pyquokka/leader_startup.sh,sha256=yOP5vjuLS9D2WtcozcFewXQB84p8jm8IcyltYcuWNss,619
-pyquokka/logical.py,sha256=MAPd9dwzyJG8jahacdFll_ad2x9i2EHx00dUczZxKic,26540
+pyquokka/logical.py,sha256=u9sua5Pwt-_xm_QGnQ_0dx7Wf0sVXWOyB3mAKaIE0Yg,27823
+pyquokka/orderedstream.py,sha256=_ADqgW0GxSffcXCs0Z1TDpdwGtxgDDwxzOCtoPr14lQ,4314
 pyquokka/placement_strategy.py,sha256=KX1hEDHCTBUfUoQ64Zv5e7iVjTKRh4oAFFs03HIxDqQ,886
 pyquokka/quokka_dataset.py,sha256=_b7L8zWCimtiR3kMbCZWLH4g-w3SZ-J8gw8bjqhv-lA,3717
-pyquokka/quokka_runtime.py,sha256=SmvYFezNHCBM6V4982y-dYFtS7fznBA09EzUsfNfhB0,19752
+pyquokka/quokka_runtime.py,sha256=ADyM7i5KiJxERReBQdesm_kuYxGwPWccG5RiK4ywMK0,19755
 pyquokka/redis.conf,sha256=Hk0GU-BnDDpMZ6Gmit1-Ct_iyO7ttCvzyfz5PLFVkJY,93718
 pyquokka/sql_utils.py,sha256=1vWMrQft37so5KtOLlsqLk8MpaFB0XO9Ib9XwQEuw90,16471
 pyquokka/state.py,sha256=wGt5uh_ZS-xV-HqVgWmdTZUnQoabpvSOHiQXOejg7L4,2371
-pyquokka/tables.py,sha256=cBcbDxU76VEukcogxz5uJS5IBn0ddYOCOxdUkN1Pyyg,11213
+pyquokka/tables.py,sha256=58vsFPBKOEKZ-7Ei7GbhXKsAgw6JguwdfQhayRJrjEI,11695
 pyquokka/target_info.py,sha256=RVldOYWGgxv9YXMSN3J3S-UbSEJxeUMoYY1LcHT3edg,2351
 pyquokka/task.py,sha256=_O_itxDxzXnjojys8___CKSaApTnEpUxVrAvWaU-AVI,5752
-pyquokka/utils.py,sha256=b2RaikOjQNWT7ki3CoYTCdQ4f2sJ-bxQ3ZXjPakz7oE,25660
+pyquokka/utils.py,sha256=_QXhE8OrDT3w17GqfZoQ03hMzJ9wL7nfw3XL5ntWAts,29075
 pyquokka/windowtypes.py,sha256=zPh9QgwSQ3E00eNQM8jk2iQZNMuzAQ3eoaFDm1H5NGk,4081
-pyquokka-0.2.1.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-pyquokka-0.2.1.dist-info/METADATA,sha256=kpaSP9H5JLwaJ3dlagvnKsOmBDYCHR30H_s8i7EZ7Z4,959
-pyquokka-0.2.1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-pyquokka-0.2.1.dist-info/top_level.txt,sha256=u5sX_ng3imCHha6-wOUpEO0V2TufF_OHADKxb38hwHg,9
-pyquokka-0.2.1.dist-info/RECORD,,
+pyquokka-0.2.3.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+pyquokka-0.2.3.dist-info/METADATA,sha256=_LfvOYmLgT42h0Ym1CZNkPFTbMaJsDtL4uEEGh_7hxo,1012
+pyquokka-0.2.3.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+pyquokka-0.2.3.dist-info/top_level.txt,sha256=u5sX_ng3imCHha6-wOUpEO0V2TufF_OHADKxb38hwHg,9
+pyquokka-0.2.3.dist-info/RECORD,,
```

