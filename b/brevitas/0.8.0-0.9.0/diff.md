# Comparing `tmp/brevitas-0.8.0.tar.gz` & `tmp/brevitas-0.9.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "brevitas-0.8.0.tar", last modified: Tue Jan 10 09:16:50 2023, max compression
+gzip compressed data, was "brevitas-0.9.0.tar", last modified: Fri Apr 21 17:52:33 2023, max compression
```

## Comparing `brevitas-0.8.0.tar` & `brevitas-0.9.0.tar`

### file list

```diff
@@ -1,351 +1,379 @@
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:50.020359 brevitas-0.8.0/
--rw-rw-rw-   0        0        0     2136 2023-01-10 09:12:31.000000 brevitas-0.8.0/LICENSE
--rw-rw-rw-   0        0        0      361 2023-01-10 09:12:31.000000 brevitas-0.8.0/MANIFEST.in
--rw-rw-rw-   0        0        0     3691 2023-01-10 09:16:50.019359 brevitas-0.8.0/PKG-INFO
--rw-rw-rw-   0        0        0     3062 2023-01-10 09:12:31.000000 brevitas-0.8.0/README.md
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.262481 brevitas-0.8.0/notebooks/
--rw-rw-rw-   0        0        0    60657 2023-01-10 09:12:32.000000 brevitas-0.8.0/notebooks/01_quant_tensor_quant_conv2d_overview.ipynb
--rw-rw-rw-   0        0        0    34646 2023-01-10 09:12:32.000000 brevitas-0.8.0/notebooks/02_quant_activation_overview.ipynb
--rw-rw-rw-   0        0        0    70426 2023-01-10 09:12:32.000000 brevitas-0.8.0/notebooks/03_anatomy_of_a_quantizer.ipynb
--rw-rw-rw-   0        0        0    98682 2023-01-10 09:12:32.000000 brevitas-0.8.0/notebooks/Brevitas_TVMCon2021.ipynb
--rw-rw-rw-   0        0        0    52191 2023-01-10 09:12:32.000000 brevitas-0.8.0/notebooks/ONNX_export_tutorial.ipynb
--rw-rw-rw-   0        0        0    59257 2023-01-10 09:12:32.000000 brevitas-0.8.0/notebooks/quantized_recurrent.ipynb
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.295903 brevitas-0.8.0/requirements/
--rw-rw-rw-   0        0        0      145 2023-01-10 09:12:32.000000 brevitas-0.8.0/requirements/requirements-docs.txt
--rw-rw-rw-   0        0        0       19 2023-01-10 09:12:32.000000 brevitas-0.8.0/requirements/requirements-export.txt
--rw-rw-rw-   0        0        0       56 2023-01-10 09:12:32.000000 brevitas-0.8.0/requirements/requirements-finn-integration.txt
--rw-rw-rw-   0        0        0        5 2023-01-10 09:12:32.000000 brevitas-0.8.0/requirements/requirements-hadamard.txt
--rw-rw-rw-   0        0        0       15 2023-01-10 09:12:32.000000 brevitas-0.8.0/requirements/requirements-notebook.txt
--rw-rw-rw-   0        0        0       20 2023-01-10 09:12:32.000000 brevitas-0.8.0/requirements/requirements-nox.txt
--rw-rw-rw-   0        0        0       30 2023-01-10 09:12:32.000000 brevitas-0.8.0/requirements/requirements-ort-integration.txt
--rw-rw-rw-   0        0        0       14 2023-01-10 09:12:32.000000 brevitas-0.8.0/requirements/requirements-setup.txt
--rw-rw-rw-   0        0        0       97 2023-01-10 09:12:32.000000 brevitas-0.8.0/requirements/requirements-stt.txt
--rw-rw-rw-   0        0        0       66 2023-01-10 09:12:32.000000 brevitas-0.8.0/requirements/requirements-test.txt
--rw-rw-rw-   0        0        0       49 2023-01-10 09:12:32.000000 brevitas-0.8.0/requirements/requirements-tts.txt
--rw-rw-rw-   0        0        0       11 2023-01-10 09:12:32.000000 brevitas-0.8.0/requirements/requirements-vision.txt
--rw-rw-rw-   0        0        0       79 2023-01-10 09:12:32.000000 brevitas-0.8.0/requirements/requirements.txt
--rw-rw-rw-   0        0        0       42 2023-01-10 09:16:50.021359 brevitas-0.8.0/setup.cfg
--rw-rw-rw-   0        0        0     2601 2023-01-10 09:12:32.000000 brevitas-0.8.0/setup.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.221472 brevitas-0.8.0/src/
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.305905 brevitas-0.8.0/src/brevitas/
--rw-rw-rw-   0        0        0     2794 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/__init__.py
--rw-rw-rw-   0        0        0     1751 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/common.py
--rw-rw-rw-   0        0        0      887 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/config.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.334911 brevitas-0.8.0/src/brevitas/core/
--rw-rw-rw-   0        0        0        1 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/__init__.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.342913 brevitas-0.8.0/src/brevitas/core/bit_width/
--rw-rw-rw-   0        0        0      381 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/bit_width/__init__.py
--rw-rw-rw-   0        0        0     2066 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/bit_width/const.py
--rw-rw-rw-   0        0        0     5834 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/bit_width/parameter.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.356917 brevitas-0.8.0/src/brevitas/core/function_wrapper/
--rw-rw-rw-   0        0        0      546 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/function_wrapper/__init__.py
--rw-rw-rw-   0        0        0     2002 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/function_wrapper/clamp.py
--rw-rw-rw-   0        0        0     2053 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/function_wrapper/misc.py
--rw-rw-rw-   0        0        0     3136 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/function_wrapper/ops_ste.py
--rw-rw-rw-   0        0        0     3875 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/function_wrapper/shape.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.372920 brevitas-0.8.0/src/brevitas/core/quant/
--rw-rw-rw-   0        0        0      524 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/quant/__init__.py
--rw-rw-rw-   0        0        0     4785 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/quant/binary.py
--rw-rw-rw-   0        0        0     1781 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/quant/delay.py
--rw-rw-rw-   0        0        0     9354 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/quant/int.py
--rw-rw-rw-   0        0        0     6778 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/quant/int_base.py
--rw-rw-rw-   0        0        0     2475 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/quant/ternary.py
--rw-rw-rw-   0        0        0     4907 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/restrict_val.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.382922 brevitas-0.8.0/src/brevitas/core/scaling/
--rw-rw-rw-   0        0        0      363 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/scaling/__init__.py
--rw-rw-rw-   0        0        0     1089 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/scaling/int_scaling.py
--rw-rw-rw-   0        0        0     4722 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/scaling/runtime.py
--rw-rw-rw-   0        0        0    14561 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/scaling/standalone.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.393924 brevitas-0.8.0/src/brevitas/core/stats/
--rw-rw-rw-   0        0        0      503 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/stats/__init__.py
--rw-rw-rw-   0        0        0    14409 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/stats/stats_op.py
--rw-rw-rw-   0        0        0     4372 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/stats/stats_wrapper.py
--rw-rw-rw-   0        0        0     2690 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/stats/view_wrapper.py
--rw-rw-rw-   0        0        0     1926 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/utils.py
--rw-rw-rw-   0        0        0     9116 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/core/zero_point.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.396925 brevitas-0.8.0/src/brevitas/csrc/
--rw-rw-rw-   0        0        0     7056 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/csrc/autograd_ste_ops.cpp
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.401926 brevitas-0.8.0/src/brevitas/export/
--rw-rw-rw-   0        0        0     1365 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/__init__.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.404927 brevitas-0.8.0/src/brevitas/export/common/
--rw-rw-rw-   0        0        0      423 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/common/__init__.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.412928 brevitas-0.8.0/src/brevitas/export/common/handler/
--rw-rw-rw-   0        0        0      111 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/common/handler/__init__.py
--rw-rw-rw-   0        0        0     5505 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/common/handler/base.py
--rw-rw-rw-   0        0        0    12400 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/common/handler/qcdq.py
--rw-rw-rw-   0        0        0    11796 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/manager.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.426427 brevitas-0.8.0/src/brevitas/export/onnx/
--rw-rw-rw-   0        0        0      474 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/__init__.py
--rw-rw-rw-   0        0        0     1982 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/debug.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.437429 brevitas-0.8.0/src/brevitas/export/onnx/finn/
--rw-rw-rw-   0        0        0      111 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/finn/__init__.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.449432 brevitas-0.8.0/src/brevitas/export/onnx/finn/function/
--rw-rw-rw-   0        0        0      155 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/finn/function/__init__.py
--rw-rw-rw-   0        0        0      936 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/finn/function/acc.py
--rw-rw-rw-   0        0        0     1349 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/finn/function/act.py
--rw-rw-rw-   0        0        0     2295 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/finn/function/parameter.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.462443 brevitas-0.8.0/src/brevitas/export/onnx/finn/handler/
--rw-rw-rw-   0        0        0      111 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/finn/handler/__init__.py
--rw-rw-rw-   0        0        0     2162 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/finn/handler/acc.py
--rw-rw-rw-   0        0        0     5624 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/finn/handler/act.py
--rw-rw-rw-   0        0        0     3093 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/finn/handler/base.py
--rw-rw-rw-   0        0        0     6301 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/finn/handler/parameter.py
--rw-rw-rw-   0        0        0     5595 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/finn/manager.py
--rw-rw-rw-   0        0        0     2173 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/finn/transform.py
--rw-rw-rw-   0        0        0      585 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/finn/utils.py
--rw-rw-rw-   0        0        0     1902 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/function.py
--rw-rw-rw-   0        0        0    10492 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/handler.py
--rw-rw-rw-   0        0        0     6787 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/manager.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.473446 brevitas-0.8.0/src/brevitas/export/onnx/qonnx/
--rw-rw-rw-   0        0        0      111 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/qonnx/__init__.py
--rw-rw-rw-   0        0        0    14773 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/qonnx/function.py
--rw-rw-rw-   0        0        0     6199 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/qonnx/handler.py
--rw-rw-rw-   0        0        0     2197 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/qonnx/manager.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.481439 brevitas-0.8.0/src/brevitas/export/onnx/standard/
--rw-rw-rw-   0        0        0      112 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/standard/__init__.py
--rw-rw-rw-   0        0        0     2518 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/standard/function.py
--rw-rw-rw-   0        0        0     1623 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/standard/manager.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.489441 brevitas-0.8.0/src/brevitas/export/onnx/standard/qcdq/
--rw-rw-rw-   0        0        0      111 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/standard/qcdq/__init__.py
--rw-rw-rw-   0        0        0     3204 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/standard/qcdq/handler.py
--rw-rw-rw-   0        0        0     2026 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/standard/qcdq/manager.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.498442 brevitas-0.8.0/src/brevitas/export/onnx/standard/qoperator/
--rw-rw-rw-   0        0        0      111 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/standard/qoperator/__init__.py
--rw-rw-rw-   0        0        0     3139 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/standard/qoperator/function.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.511446 brevitas-0.8.0/src/brevitas/export/onnx/standard/qoperator/handler/
--rw-rw-rw-   0        0        0        0 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/standard/qoperator/handler/__init__.py
--rw-rw-rw-   0        0        0     5566 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/standard/qoperator/handler/act.py
--rw-rw-rw-   0        0        0     6807 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/standard/qoperator/handler/base.py
--rw-rw-rw-   0        0        0     7963 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/standard/qoperator/handler/parameter.py
--rw-rw-rw-   0        0        0     1316 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/standard/qoperator/handler/pool.py
--rw-rw-rw-   0        0        0     3292 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/onnx/standard/qoperator/manager.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.206469 brevitas-0.8.0/src/brevitas/export/torch/
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.520448 brevitas-0.8.0/src/brevitas/export/torch/qcdq/
--rw-rw-rw-   0        0        0      111 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/torch/qcdq/__init__.py
--rw-rw-rw-   0        0        0     3869 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/torch/qcdq/handler.py
--rw-rw-rw-   0        0        0     1348 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/torch/qcdq/manager.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.525449 brevitas-0.8.0/src/brevitas/export/torch/qoperator/
--rw-rw-rw-   0        0        0      111 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/torch/qoperator/__init__.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.538451 brevitas-0.8.0/src/brevitas/export/torch/qoperator/handler/
--rw-rw-rw-   0        0        0      213 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/torch/qoperator/handler/__init__.py
--rw-rw-rw-   0        0        0     2008 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/torch/qoperator/handler/act.py
--rw-rw-rw-   0        0        0     2263 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/torch/qoperator/handler/base.py
--rw-rw-rw-   0        0        0     4738 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/torch/qoperator/handler/parameter.py
--rw-rw-rw-   0        0        0     1788 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/torch/qoperator/handler/pool.py
--rw-rw-rw-   0        0        0     2413 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/export/torch/qoperator/manager.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.549454 brevitas-0.8.0/src/brevitas/function/
--rw-rw-rw-   0        0        0      157 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/function/__init__.py
--rw-rw-rw-   0        0        0     5516 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/function/ops.py
--rw-rw-rw-   0        0        0    12920 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/function/ops_ste.py
--rw-rw-rw-   0        0        0     2315 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/function/shape.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.557456 brevitas-0.8.0/src/brevitas/fx/
--rw-rw-rw-   0        0        0     2281 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/fx/__init__.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.583462 brevitas-0.8.0/src/brevitas/fx/backport/
--rw-rw-rw-   0        0        0      115 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/fx/backport/__init__.py
--rw-rw-rw-   0        0        0    39630 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/fx/backport/graph.py
--rw-rw-rw-   0        0        0    17256 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/fx/backport/graph_module.py
--rw-rw-rw-   0        0        0     3411 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/fx/backport/immutable_collections.py
--rw-rw-rw-   0        0        0    17967 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/fx/backport/interpreter.py
--rw-rw-rw-   0        0        0    13800 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/fx/backport/node.py
--rw-rw-rw-   0        0        0    11670 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/fx/backport/proxy.py
--rw-rw-rw-   0        0        0    16943 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/fx/backport/subgraph_rewriter.py
--rw-rw-rw-   0        0        0    27968 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/fx/backport/symbolic_trace.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.594887 brevitas-0.8.0/src/brevitas/fx/backport/torch_function/
--rw-rw-rw-   0        0        0      190 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/fx/backport/torch_function/__init__.py
--rw-rw-rw-   0        0        0     7743 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/fx/backport/torch_function/_overrides.py
--rw-rw-rw-   0        0        0     4319 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/fx/backport/torch_function/patch.py
--rw-rw-rw-   0        0        0    46068 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/fx/backport/torch_function/signatures.py
--rw-rw-rw-   0        0        0     2266 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/fx/brevitas_tracer.py
--rw-rw-rw-   0        0        0    21367 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/fx/value_tracer.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.616902 brevitas-0.8.0/src/brevitas/graph/
--rw-rw-rw-   0        0        0      261 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/graph/__init__.py
--rw-rw-rw-   0        0        0    10353 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/graph/base.py
--rw-rw-rw-   0        0        0    11064 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/graph/calibrate.py
--rw-rw-rw-   0        0        0    12213 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/graph/equalize.py
--rw-rw-rw-   0        0        0     8088 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/graph/fixed_point.py
--rw-rw-rw-   0        0        0     4996 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/graph/per_input.py
--rw-rw-rw-   0        0        0     4821 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/graph/standardize.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.621895 brevitas-0.8.0/src/brevitas/graph/target/
--rw-rw-rw-   0        0        0      111 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/graph/target/__init__.py
--rw-rw-rw-   0        0        0    19435 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/graph/target/flexml.py
--rw-rw-rw-   0        0        0     4858 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/graph/utils.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.629896 brevitas-0.8.0/src/brevitas/inject/
--rw-rw-rw-   0        0        0     7648 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/inject/__init__.py
--rw-rw-rw-   0        0        0      211 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/inject/defaults.py
--rw-rw-rw-   0        0        0     1293 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/inject/enum.py
--rw-rw-rw-   0        0        0      790 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/jit.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.638898 brevitas-0.8.0/src/brevitas/loss/
--rw-rw-rw-   0        0        0      285 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/loss/__init__.py
--rw-rw-rw-   0        0        0      728 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/loss/base_loss.py
--rw-rw-rw-   0        0        0     3760 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/loss/weighted_bit_width.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.688910 brevitas-0.8.0/src/brevitas/nn/
--rw-rw-rw-   0        0        0     1063 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/__init__.py
--rw-rw-rw-   0        0        0     3518 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/hadamard_classifier.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.705411 brevitas-0.8.0/src/brevitas/nn/mixin/
--rw-rw-rw-   0        0        0      355 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/mixin/__init__.py
--rw-rw-rw-   0        0        0     1897 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/mixin/acc.py
--rw-rw-rw-   0        0        0     4173 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/mixin/act.py
--rw-rw-rw-   0        0        0    12564 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/mixin/base.py
--rw-rw-rw-   0        0        0     6235 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/mixin/parameter.py
--rw-rw-rw-   0        0        0      260 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/mixin/utils.py
--rw-rw-rw-   0        0        0     1882 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/quant_accumulator.py
--rw-rw-rw-   0        0        0     2954 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/quant_activation.py
--rw-rw-rw-   0        0        0     5430 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/quant_avg_pool.py
--rw-rw-rw-   0        0        0     3780 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/quant_bn.py
--rw-rw-rw-   0        0        0     8159 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/quant_conv.py
--rw-rw-rw-   0        0        0     8117 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/quant_convtranspose.py
--rw-rw-rw-   0        0        0      896 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/quant_dropout.py
--rw-rw-rw-   0        0        0     3249 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/quant_eltwise.py
--rw-rw-rw-   0        0        0     2541 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/quant_embedding.py
--rw-rw-rw-   0        0        0    13023 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/quant_layer.py
--rw-rw-rw-   0        0        0     2474 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/quant_linear.py
--rw-rw-rw-   0        0        0     2486 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/quant_max_pool.py
--rw-rw-rw-   0        0        0    40111 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/quant_rnn.py
--rw-rw-rw-   0        0        0     3140 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/quant_scale_bias.py
--rw-rw-rw-   0        0        0     3713 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/quant_upsample.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.710413 brevitas-0.8.0/src/brevitas/nn/target/
--rw-rw-rw-   0        0        0      176 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/target/__init__.py
--rw-rw-rw-   0        0        0     3936 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/target/flexml.py
--rw-rw-rw-   0        0        0     2431 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/nn/utils.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.713415 brevitas-0.8.0/src/brevitas/onnx/
--rw-rw-rw-   0        0        0      451 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/onnx/__init__.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.718870 brevitas-0.8.0/src/brevitas/ops/
--rw-rw-rw-   0        0        0      419 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/ops/__init__.py
--rw-rw-rw-   0        0        0    14578 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/ops/autograd_ste_ops.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.732875 brevitas-0.8.0/src/brevitas/proxy/
--rw-rw-rw-   0        0        0      458 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/proxy/__init__.py
--rw-rw-rw-   0        0        0     6341 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/proxy/parameter_quant.py
--rw-rw-rw-   0        0        0     4885 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/proxy/quant_proxy.py
--rw-rw-rw-   0        0        0     7578 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/proxy/runtime_quant.py
--rw-rw-rw-   0        0        0     1573 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/proxy/utils.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.754880 brevitas-0.8.0/src/brevitas/quant/
--rw-rw-rw-   0        0        0      265 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant/__init__.py
--rw-rw-rw-   0        0        0     9120 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant/base.py
--rw-rw-rw-   0        0        0     1176 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant/binary.py
--rw-rw-rw-   0        0        0     3762 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant/fixed_point.py
--rw-rw-rw-   0        0        0     1312 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant/none.py
--rw-rw-rw-   0        0        0     7675 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant/scaled_int.py
--rw-rw-rw-   0        0        0     2163 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant/shifted_scaled_int.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.775884 brevitas-0.8.0/src/brevitas/quant/solver/
--rw-rw-rw-   0        0        0      289 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant/solver/__init__.py
--rw-rw-rw-   0        0        0     4773 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant/solver/act.py
--rw-rw-rw-   0        0        0     3363 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant/solver/bias.py
--rw-rw-rw-   0        0        0     2128 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant/solver/clamp.py
--rw-rw-rw-   0        0        0     5438 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant/solver/common.py
--rw-rw-rw-   0        0        0     4445 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant/solver/parameter.py
--rw-rw-rw-   0        0        0     1444 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant/solver/trunc.py
--rw-rw-rw-   0        0        0     3920 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant/solver/weight.py
--rw-rw-rw-   0        0        0     1242 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant/ternary.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.781886 brevitas-0.8.0/src/brevitas/quant_tensor/
--rw-rw-rw-   0        0        0    16391 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant_tensor/__init__.py
--rw-rw-rw-   0        0        0     3073 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/quant_tensor/torch_handler.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.797889 brevitas-0.8.0/src/brevitas/utils/
--rw-rw-rw-   0        0        0      111 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/utils/__init__.py
--rw-rw-rw-   0        0        0     1257 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/utils/jit_utils.py
--rw-rw-rw-   0        0        0     1649 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/utils/logging.py
--rw-rw-rw-   0        0        0      799 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/utils/python_utils.py
--rw-rw-rw-   0        0        0     1792 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/utils/quant_utils.py
--rw-rw-rw-   0        0        0      901 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas/utils/torch_utils.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.323908 brevitas-0.8.0/src/brevitas.egg-info/
--rw-rw-rw-   0        0        0     3691 2023-01-10 09:16:46.000000 brevitas-0.8.0/src/brevitas.egg-info/PKG-INFO
--rw-rw-rw-   0        0        0    12399 2023-01-10 09:16:47.000000 brevitas-0.8.0/src/brevitas.egg-info/SOURCES.txt
--rw-rw-rw-   0        0        0        1 2023-01-10 09:16:46.000000 brevitas-0.8.0/src/brevitas.egg-info/dependency_links.txt
--rw-rw-rw-   0        0        0      614 2023-01-10 09:16:46.000000 brevitas-0.8.0/src/brevitas.egg-info/entry_points.txt
--rw-rw-rw-   0        0        0        2 2023-01-10 09:14:58.000000 brevitas-0.8.0/src/brevitas.egg-info/not-zip-safe
--rw-rw-rw-   0        0        0      688 2023-01-10 09:16:46.000000 brevitas-0.8.0/src/brevitas.egg-info/requires.txt
--rw-rw-rw-   0        0        0       27 2023-01-10 09:16:46.000000 brevitas-0.8.0/src/brevitas.egg-info/top_level.txt
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.800890 brevitas-0.8.0/src/brevitas_examples/
--rw-rw-rw-   0        0        0      111 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/__init__.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.814454 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/
--rw-rw-rw-   0        0        0     2825 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/README.md
--rw-rw-rw-   0        0        0      329 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/__init__.py
--rw-rw-rw-   0        0        0     4401 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/bnn_pynq_train.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.845488 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/cfg/
--rw-rw-rw-   0        0        0      111 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/cfg/__init__.py
--rw-rw-rw-   0        0        0      338 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/cfg/cnv_1w1a.ini
--rw-rw-rw-   0        0        0      338 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/cfg/cnv_1w2a.ini
--rw-rw-rw-   0        0        0      338 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/cfg/cnv_2w2a.ini
--rw-rw-rw-   0        0        0      365 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/cfg/lfc_1w1a.ini
--rw-rw-rw-   0        0        0      365 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/cfg/lfc_1w2a.ini
--rw-rw-rw-   0        0        0      362 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/cfg/sfc_1w1a.ini
--rw-rw-rw-   0        0        0      362 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/cfg/sfc_1w2a.ini
--rw-rw-rw-   0        0        0      362 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/cfg/sfc_2w2a.ini
--rw-rw-rw-   0        0        0      359 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/cfg/tfc_1w1a.ini
--rw-rw-rw-   0        0        0      359 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/cfg/tfc_1w2a.ini
--rw-rw-rw-   0        0        0      359 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/cfg/tfc_2w2a.ini
--rw-rw-rw-   0        0        0     3484 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/logger.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.862448 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/models/
--rw-rw-rw-   0        0        0     4022 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/models/CNV.py
--rw-rw-rw-   0        0        0     2909 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/models/FC.py
--rw-rw-rw-   0        0        0     2299 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/models/__init__.py
--rw-rw-rw-   0        0        0     1275 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/models/common.py
--rw-rw-rw-   0        0        0     1092 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/models/losses.py
--rw-rw-rw-   0        0        0     1397 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/models/tensor_norm.py
--rw-rw-rw-   0        0        0    13110 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/bnn_pynq/trainer.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.870451 brevitas-0.8.0/src/brevitas_examples/imagenet_classification/
--rw-rw-rw-   0        0        0     4596 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/imagenet_classification/README.md
--rw-rw-rw-   0        0        0      135 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/imagenet_classification/__init__.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.884177 brevitas-0.8.0/src/brevitas_examples/imagenet_classification/cfg/
--rw-rw-rw-   0        0        0        0 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/imagenet_classification/cfg/__init__.py
--rw-rw-rw-   0        0        0      296 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/imagenet_classification/cfg/quant_mobilenet_v1_4b.ini
--rw-rw-rw-   0        0        0      500 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/imagenet_classification/cfg/quant_proxylessnas_mobile14_4b.ini
--rw-rw-rw-   0        0        0      504 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/imagenet_classification/cfg/quant_proxylessnas_mobile14_4b5b.ini
--rw-rw-rw-   0        0        0      518 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/imagenet_classification/cfg/quant_proxylessnas_mobile14_hadamard_4b.ini
--rw-rw-rw-   0        0        0     4444 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/imagenet_classification/imagenet_val.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.898180 brevitas-0.8.0/src/brevitas_examples/imagenet_classification/models/
--rw-rw-rw-   0        0        0     1564 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/imagenet_classification/models/__init__.py
--rw-rw-rw-   0        0        0     1458 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/imagenet_classification/models/common.py
--rw-rw-rw-   0        0        0     6304 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/imagenet_classification/models/mobilenetv1.py
--rw-rw-rw-   0        0        0    11279 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/imagenet_classification/models/proxylessnas.py
--rw-rw-rw-   0        0        0     5779 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/imagenet_classification/models/vgg.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.909182 brevitas-0.8.0/src/brevitas_examples/speech_to_text/
--rw-rw-rw-   0        0        0     3071 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/README.md
--rw-rw-rw-   0        0        0      138 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/__init__.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.920186 brevitas-0.8.0/src/brevitas_examples/speech_to_text/cfg/
--rw-rw-rw-   0        0        0        0 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/cfg/__init__.py
--rw-rw-rw-   0        0        0      603 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/cfg/quant_quartznet_perchannelscaling_4b.ini
--rw-rw-rw-   0        0        0      606 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/cfg/quant_quartznet_perchannelscaling_8b.ini
--rw-rw-rw-   0        0        0      604 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/cfg/quant_quartznet_pertensorscaling_8b.ini
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.925186 brevitas-0.8.0/src/brevitas_examples/speech_to_text/cfg/topology/
--rw-rw-rw-   0        0        0        0 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/cfg/topology/__init__.py
--rw-rw-rw-   0        0        0     3475 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/cfg/topology/quartznet15x5.yaml
--rw-rw-rw-   0        0        0     5214 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/get_librispeech_data.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.947191 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/
--rw-rw-rw-   0        0        0     3181 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/__init__.py
--rw-rw-rw-   0        0        0    19382 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/audio_preprocessing.py
--rw-rw-rw-   0        0        0     6115 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/data_layer.py
--rw-rw-rw-   0        0        0     1103 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/greedy_ctc_decoder.py
--rw-rw-rw-   0        0        0     8301 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/helpers.py
--rw-rw-rw-   0        0        0     1917 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/losses.py
--rw-rw-rw-   0        0        0     2724 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/metrics.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.976197 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/
--rw-rw-rw-   0        0        0      940 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/__init__.py
--rw-rw-rw-   0        0        0     7776 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/cleaners.py
--rw-rw-rw-   0        0        0     3781 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/common.py
--rw-rw-rw-   0        0        0     8047 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/dataset.py
--rw-rw-rw-   0        0        0    12213 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/features.py
--rw-rw-rw-   0        0        0     7798 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/manifest.py
--rw-rw-rw-   0        0        0     6783 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/perturb.py
--rw-rw-rw-   0        0        0    19875 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/quartznet.py
--rw-rw-rw-   0        0        0     8566 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/segment.py
--rw-rw-rw-   0        0        0     3858 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/spectr_augment.py
--rw-rw-rw-   0        0        0    12018 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/quartznet.py
--rw-rw-rw-   0        0        0     3849 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet_val.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.988531 brevitas-0.8.0/src/brevitas_examples/text_to_speech/
--rw-rw-rw-   0        0        0     2195 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/text_to_speech/README.md
--rw-rw-rw-   0        0        0      135 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/text_to_speech/__init__.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:49.993847 brevitas-0.8.0/src/brevitas_examples/text_to_speech/cfg/
--rw-rw-rw-   0        0        0        0 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/text_to_speech/cfg/__init__.py
--rw-rw-rw-   0        0        0      365 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/text_to_speech/cfg/quant_melgan_8b.ini
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:50.007851 brevitas-0.8.0/src/brevitas_examples/text_to_speech/melgan/
--rw-rw-rw-   0        0        0      948 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/text_to_speech/melgan/__init__.py
--rw-rw-rw-   0        0        0     6162 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/text_to_speech/melgan/common.py
--rw-rw-rw-   0        0        0     4961 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/text_to_speech/melgan/generator_brevitas.py
--rw-rw-rw-   0        0        0     1959 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/text_to_speech/melgan/melgan.py
--rw-rw-rw-   0        0        0     3337 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/text_to_speech/melgan/res_stack_brevitas.py
--rw-rw-rw-   0        0        0     1963 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/text_to_speech/melgan_val.py
--rw-rw-rw-   0        0        0     2480 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/text_to_speech/preprocess_dataset.py
-drwxrwxrwx   0        0        0        0 2023-01-10 09:16:50.015852 brevitas-0.8.0/src/brevitas_examples/text_to_speech/utilities/
--rw-rw-rw-   0        0        0       72 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/text_to_speech/utilities/__init__.py
--rw-rw-rw-   0        0        0     4624 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/text_to_speech/utilities/audio_processing.py
--rw-rw-rw-   0        0        0     7979 2023-01-10 09:12:32.000000 brevitas-0.8.0/src/brevitas_examples/text_to_speech/utilities/stft.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.742040 brevitas-0.9.0/
+-rw-rw-rw-   0        0        0      824 2023-04-21 17:11:25.000000 brevitas-0.9.0/.pre-commit-config.yaml
+-rw-rw-rw-   0        0        0     2137 2023-04-21 17:11:25.000000 brevitas-0.9.0/LICENSE
+-rw-rw-rw-   0        0        0      361 2023-01-10 09:12:31.000000 brevitas-0.9.0/MANIFEST.in
+-rw-rw-rw-   0        0        0     3844 2023-04-21 17:52:33.741040 brevitas-0.9.0/PKG-INFO
+-rw-rw-rw-   0        0        0     3220 2023-04-21 17:12:45.000000 brevitas-0.9.0/README.md
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.700482 brevitas-0.9.0/notebooks/
+-rw-rw-rw-   0        0        0    60778 2023-04-21 17:11:25.000000 brevitas-0.9.0/notebooks/01_quant_tensor_quant_conv2d_overview.ipynb
+-rw-rw-rw-   0        0        0    35108 2023-04-21 17:11:25.000000 brevitas-0.9.0/notebooks/02_quant_activation_overview.ipynb
+-rw-rw-rw-   0        0        0    70607 2023-04-21 17:11:25.000000 brevitas-0.9.0/notebooks/03_anatomy_of_a_quantizer.ipynb
+-rw-rw-rw-   0        0        0    98682 2023-01-10 09:12:32.000000 brevitas-0.9.0/notebooks/Brevitas_TVMCon2021.ipynb
+-rw-rw-rw-   0        0        0    52335 2023-04-21 17:11:25.000000 brevitas-0.9.0/notebooks/ONNX_export_tutorial.ipynb
+-rw-rw-rw-   0        0        0    59257 2023-01-10 09:12:32.000000 brevitas-0.9.0/notebooks/quantized_recurrent.ipynb
+-rw-rw-rw-   0        0        0      396 2023-04-21 17:11:25.000000 brevitas-0.9.0/pyproject.toml
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.745839 brevitas-0.9.0/requirements/
+-rw-rw-rw-   0        0        0      145 2023-04-21 17:11:25.000000 brevitas-0.9.0/requirements/requirements-docs.txt
+-rw-rw-rw-   0        0        0       19 2023-01-10 09:12:32.000000 brevitas-0.9.0/requirements/requirements-export.txt
+-rw-rw-rw-   0        0        0       56 2023-04-21 17:11:25.000000 brevitas-0.9.0/requirements/requirements-finn-integration.txt
+-rw-rw-rw-   0        0        0        6 2023-04-21 17:11:25.000000 brevitas-0.9.0/requirements/requirements-hadamard.txt
+-rw-rw-rw-   0        0        0       15 2023-01-10 09:12:32.000000 brevitas-0.9.0/requirements/requirements-notebook.txt
+-rw-rw-rw-   0        0        0       21 2023-04-21 17:11:25.000000 brevitas-0.9.0/requirements/requirements-nox.txt
+-rw-rw-rw-   0        0        0       31 2023-04-21 17:11:25.000000 brevitas-0.9.0/requirements/requirements-ort-integration.txt
+-rw-rw-rw-   0        0        0       15 2023-04-21 17:11:25.000000 brevitas-0.9.0/requirements/requirements-setup.txt
+-rw-rw-rw-   0        0        0       97 2023-04-21 17:11:25.000000 brevitas-0.9.0/requirements/requirements-stt.txt
+-rw-rw-rw-   0        0        0       79 2023-04-21 17:11:25.000000 brevitas-0.9.0/requirements/requirements-test.txt
+-rw-rw-rw-   0        0        0       49 2023-04-21 17:11:25.000000 brevitas-0.9.0/requirements/requirements-tts.txt
+-rw-rw-rw-   0        0        0       12 2023-04-21 17:11:25.000000 brevitas-0.9.0/requirements/requirements-vision.txt
+-rw-rw-rw-   0        0        0      126 2023-04-21 17:11:25.000000 brevitas-0.9.0/requirements/requirements.txt
+-rw-rw-rw-   0        0        0       42 2023-04-21 17:52:33.742040 brevitas-0.9.0/setup.cfg
+-rw-rw-rw-   0        0        0     2641 2023-04-21 17:11:25.000000 brevitas-0.9.0/setup.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.627483 brevitas-0.9.0/src/
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.758483 brevitas-0.9.0/src/brevitas/
+-rw-rw-rw-   0        0        0     2796 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/__init__.py
+-rw-rw-rw-   0        0        0     1766 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/common.py
+-rw-rw-rw-   0        0        0      887 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/config.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.792482 brevitas-0.9.0/src/brevitas/core/
+-rw-rw-rw-   0        0        0        0 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.801484 brevitas-0.9.0/src/brevitas/core/bit_width/
+-rw-rw-rw-   0        0        0      463 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/bit_width/__init__.py
+-rw-rw-rw-   0        0        0     3721 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/bit_width/const.py
+-rw-rw-rw-   0        0        0     6202 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/bit_width/parameter.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.821482 brevitas-0.9.0/src/brevitas/core/function_wrapper/
+-rw-rw-rw-   0        0        0      803 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/function_wrapper/__init__.py
+-rw-rw-rw-   0        0        0     2002 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/function_wrapper/clamp.py
+-rw-rw-rw-   0        0        0     3115 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/function_wrapper/learned_round.py
+-rw-rw-rw-   0        0        0     2047 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/function_wrapper/misc.py
+-rw-rw-rw-   0        0        0     3137 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/function_wrapper/ops_ste.py
+-rw-rw-rw-   0        0        0     3944 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/function_wrapper/shape.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.844482 brevitas-0.9.0/src/brevitas/core/quant/
+-rw-rw-rw-   0        0        0      596 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/quant/__init__.py
+-rw-rw-rw-   0        0        0     4783 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/quant/binary.py
+-rw-rw-rw-   0        0        0     1776 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/quant/delay.py
+-rw-rw-rw-   0        0        0    10619 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/quant/int.py
+-rw-rw-rw-   0        0        0     6694 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/quant/int_base.py
+-rw-rw-rw-   0        0        0     2475 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/quant/ternary.py
+-rw-rw-rw-   0        0        0     5018 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/restrict_val.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.861482 brevitas-0.9.0/src/brevitas/core/scaling/
+-rw-rw-rw-   0        0        0      571 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/scaling/__init__.py
+-rw-rw-rw-   0        0        0     1121 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/scaling/int_scaling.py
+-rw-rw-rw-   0        0        0    10521 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/scaling/pre_scaling.py
+-rw-rw-rw-   0        0        0     5412 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/scaling/runtime.py
+-rw-rw-rw-   0        0        0    14748 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/scaling/standalone.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.875485 brevitas-0.9.0/src/brevitas/core/stats/
+-rw-rw-rw-   0        0        0      829 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/stats/__init__.py
+-rw-rw-rw-   0        0        0    15934 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/stats/stats_op.py
+-rw-rw-rw-   0        0        0     4500 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/stats/stats_wrapper.py
+-rw-rw-rw-   0        0        0     2699 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/stats/view_wrapper.py
+-rw-rw-rw-   0        0        0     2523 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/utils.py
+-rw-rw-rw-   0        0        0     9053 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/core/zero_point.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.878485 brevitas-0.9.0/src/brevitas/csrc/
+-rw-rw-rw-   0        0        0     7051 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/csrc/autograd_ste_ops.cpp
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.883483 brevitas-0.9.0/src/brevitas/export/
+-rw-rw-rw-   0        0        0     1363 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.887483 brevitas-0.9.0/src/brevitas/export/common/
+-rw-rw-rw-   0        0        0      425 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/common/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.896483 brevitas-0.9.0/src/brevitas/export/common/handler/
+-rw-rw-rw-   0        0        0      112 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/common/handler/__init__.py
+-rw-rw-rw-   0        0        0     5533 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/common/handler/base.py
+-rw-rw-rw-   0        0        0    12025 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/common/handler/qcdq.py
+-rw-rw-rw-   0        0        0    11884 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/manager.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.913485 brevitas-0.9.0/src/brevitas/export/onnx/
+-rw-rw-rw-   0        0        0      472 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/__init__.py
+-rw-rw-rw-   0        0        0     1935 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/debug.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.926483 brevitas-0.9.0/src/brevitas/export/onnx/finn/
+-rw-rw-rw-   0        0        0      112 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/finn/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.938485 brevitas-0.9.0/src/brevitas/export/onnx/finn/function/
+-rw-rw-rw-   0        0        0      154 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/finn/function/__init__.py
+-rw-rw-rw-   0        0        0      947 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/finn/function/acc.py
+-rw-rw-rw-   0        0        0     1347 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/finn/function/act.py
+-rw-rw-rw-   0        0        0     2607 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/finn/function/parameter.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.955484 brevitas-0.9.0/src/brevitas/export/onnx/finn/handler/
+-rw-rw-rw-   0        0        0      112 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/finn/handler/__init__.py
+-rw-rw-rw-   0        0        0     2163 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/finn/handler/acc.py
+-rw-rw-rw-   0        0        0     5692 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/finn/handler/act.py
+-rw-rw-rw-   0        0        0     3096 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/finn/handler/base.py
+-rw-rw-rw-   0        0        0     6371 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/finn/handler/parameter.py
+-rw-rw-rw-   0        0        0     5803 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/finn/manager.py
+-rw-rw-rw-   0        0        0     2138 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/finn/transform.py
+-rw-rw-rw-   0        0        0      586 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/finn/utils.py
+-rw-rw-rw-   0        0        0     1805 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/function.py
+-rw-rw-rw-   0        0        0    10476 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/handler.py
+-rw-rw-rw-   0        0        0     6871 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/manager.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.968485 brevitas-0.9.0/src/brevitas/export/onnx/qonnx/
+-rw-rw-rw-   0        0        0      112 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/qonnx/__init__.py
+-rw-rw-rw-   0        0        0    13891 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/qonnx/function.py
+-rw-rw-rw-   0        0        0     6196 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/qonnx/handler.py
+-rw-rw-rw-   0        0        0     2179 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/qonnx/manager.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.979483 brevitas-0.9.0/src/brevitas/export/onnx/standard/
+-rw-rw-rw-   0        0        0      112 2023-01-10 09:12:32.000000 brevitas-0.9.0/src/brevitas/export/onnx/standard/__init__.py
+-rw-rw-rw-   0        0        0     1966 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/standard/function.py
+-rw-rw-rw-   0        0        0     1613 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/standard/manager.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.990485 brevitas-0.9.0/src/brevitas/export/onnx/standard/qcdq/
+-rw-rw-rw-   0        0        0      112 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/standard/qcdq/__init__.py
+-rw-rw-rw-   0        0        0     4002 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/standard/qcdq/handler.py
+-rw-rw-rw-   0        0        0     2125 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/standard/qcdq/manager.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.001484 brevitas-0.9.0/src/brevitas/export/onnx/standard/qoperator/
+-rw-rw-rw-   0        0        0      112 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/standard/qoperator/__init__.py
+-rw-rw-rw-   0        0        0     3231 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/standard/qoperator/function.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.020484 brevitas-0.9.0/src/brevitas/export/onnx/standard/qoperator/handler/
+-rw-rw-rw-   0        0        0        0 2023-01-10 09:12:32.000000 brevitas-0.9.0/src/brevitas/export/onnx/standard/qoperator/handler/__init__.py
+-rw-rw-rw-   0        0        0     5759 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/standard/qoperator/handler/act.py
+-rw-rw-rw-   0        0        0     7390 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/standard/qoperator/handler/base.py
+-rw-rw-rw-   0        0        0     8186 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/standard/qoperator/handler/parameter.py
+-rw-rw-rw-   0        0        0     1370 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/standard/qoperator/handler/pool.py
+-rw-rw-rw-   0        0        0     3364 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/onnx/standard/qoperator/manager.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.609964 brevitas-0.9.0/src/brevitas/export/torch/
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.030483 brevitas-0.9.0/src/brevitas/export/torch/qcdq/
+-rw-rw-rw-   0        0        0      112 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/torch/qcdq/__init__.py
+-rw-rw-rw-   0        0        0     4304 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/torch/qcdq/handler.py
+-rw-rw-rw-   0        0        0     1361 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/torch/qcdq/manager.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.037483 brevitas-0.9.0/src/brevitas/export/torch/qoperator/
+-rw-rw-rw-   0        0        0      112 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/torch/qoperator/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.056483 brevitas-0.9.0/src/brevitas/export/torch/qoperator/handler/
+-rw-rw-rw-   0        0        0      213 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/torch/qoperator/handler/__init__.py
+-rw-rw-rw-   0        0        0     2054 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/torch/qoperator/handler/act.py
+-rw-rw-rw-   0        0        0     2373 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/torch/qoperator/handler/base.py
+-rw-rw-rw-   0        0        0     4781 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/torch/qoperator/handler/parameter.py
+-rw-rw-rw-   0        0        0     1812 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/torch/qoperator/handler/pool.py
+-rw-rw-rw-   0        0        0     2509 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/export/torch/qoperator/manager.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.070483 brevitas-0.9.0/src/brevitas/function/
+-rw-rw-rw-   0        0        0      155 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/function/__init__.py
+-rw-rw-rw-   0        0        0     5513 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/function/ops.py
+-rw-rw-rw-   0        0        0    13010 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/function/ops_ste.py
+-rw-rw-rw-   0        0        0     2314 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/function/shape.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.082482 brevitas-0.9.0/src/brevitas/fx/
+-rw-rw-rw-   0        0        0     3549 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/fx/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.121484 brevitas-0.9.0/src/brevitas/fx/backport/
+-rw-rw-rw-   0        0        0      112 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/fx/backport/__init__.py
+-rw-rw-rw-   0        0        0    39947 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/fx/backport/graph.py
+-rw-rw-rw-   0        0        0    17454 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/fx/backport/graph_module.py
+-rw-rw-rw-   0        0        0     3427 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/fx/backport/immutable_collections.py
+-rw-rw-rw-   0        0        0    18108 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/fx/backport/interpreter.py
+-rw-rw-rw-   0        0        0    14017 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/fx/backport/node.py
+-rw-rw-rw-   0        0        0    11952 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/fx/backport/proxy.py
+-rw-rw-rw-   0        0        0    16627 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/fx/backport/subgraph_rewriter.py
+-rw-rw-rw-   0        0        0    28505 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/fx/backport/symbolic_trace.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.141484 brevitas-0.9.0/src/brevitas/fx/backport/torch_function/
+-rw-rw-rw-   0        0        0      190 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/fx/backport/torch_function/__init__.py
+-rw-rw-rw-   0        0        0     7731 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/fx/backport/torch_function/_overrides.py
+-rw-rw-rw-   0        0        0     4319 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/fx/backport/torch_function/patch.py
+-rw-rw-rw-   0        0        0    61914 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/fx/backport/torch_function/signatures.py
+-rw-rw-rw-   0        0        0     2274 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/fx/brevitas_tracer.py
+-rw-rw-rw-   0        0        0    21509 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/fx/value_tracer.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.188485 brevitas-0.9.0/src/brevitas/graph/
+-rw-rw-rw-   0        0        0      261 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/graph/__init__.py
+-rw-rw-rw-   0        0        0    10632 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/graph/base.py
+-rw-rw-rw-   0        0        0    11684 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/graph/calibrate.py
+-rw-rw-rw-   0        0        0    20501 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/graph/equalize.py
+-rw-rw-rw-   0        0        0     8309 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/graph/fixed_point.py
+-rw-rw-rw-   0        0        0     5021 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/graph/per_input.py
+-rw-rw-rw-   0        0        0    13412 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/graph/quantize.py
+-rw-rw-rw-   0        0        0    19261 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/graph/quantize_impl.py
+-rw-rw-rw-   0        0        0     5927 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/graph/standardize.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.197483 brevitas-0.9.0/src/brevitas/graph/target/
+-rw-rw-rw-   0        0        0      112 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/graph/target/__init__.py
+-rw-rw-rw-   0        0        0     6919 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/graph/target/flexml.py
+-rw-rw-rw-   0        0        0     4880 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/graph/utils.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.210637 brevitas-0.9.0/src/brevitas/inject/
+-rw-rw-rw-   0        0        0     7787 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/inject/__init__.py
+-rw-rw-rw-   0        0        0      203 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/inject/defaults.py
+-rw-rw-rw-   0        0        0     1451 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/inject/enum.py
+-rw-rw-rw-   0        0        0      788 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/jit.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.222485 brevitas-0.9.0/src/brevitas/loss/
+-rw-rw-rw-   0        0        0      346 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/loss/__init__.py
+-rw-rw-rw-   0        0        0      743 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/loss/base_loss.py
+-rw-rw-rw-   0        0        0     3790 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/loss/weighted_bit_width.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.288483 brevitas-0.9.0/src/brevitas/nn/
+-rw-rw-rw-   0        0        0     1516 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/__init__.py
+-rw-rw-rw-   0        0        0     3439 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/hadamard_classifier.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.307483 brevitas-0.9.0/src/brevitas/nn/mixin/
+-rw-rw-rw-   0        0        0      468 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/mixin/__init__.py
+-rw-rw-rw-   0        0        0     1919 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/mixin/acc.py
+-rw-rw-rw-   0        0        0     4085 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/mixin/act.py
+-rw-rw-rw-   0        0        0    12643 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/mixin/base.py
+-rw-rw-rw-   0        0        0     7083 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/mixin/parameter.py
+-rw-rw-rw-   0        0        0      261 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/mixin/utils.py
+-rw-rw-rw-   0        0        0     1828 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/quant_accumulator.py
+-rw-rw-rw-   0        0        0     3013 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/quant_activation.py
+-rw-rw-rw-   0        0        0     5880 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/quant_avg_pool.py
+-rw-rw-rw-   0        0        0     3823 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/quant_bn.py
+-rw-rw-rw-   0        0        0     8226 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/quant_conv.py
+-rw-rw-rw-   0        0        0     8260 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/quant_convtranspose.py
+-rw-rw-rw-   0        0        0      871 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/quant_dropout.py
+-rw-rw-rw-   0        0        0     3122 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/quant_eltwise.py
+-rw-rw-rw-   0        0        0     2485 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/quant_embedding.py
+-rw-rw-rw-   0        0        0    12986 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/quant_layer.py
+-rw-rw-rw-   0        0        0     2519 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/quant_linear.py
+-rw-rw-rw-   0        0        0     2457 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/quant_max_pool.py
+-rw-rw-rw-   0        0        0    36694 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/quant_mha.py
+-rw-rw-rw-   0        0        0    40303 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/quant_rnn.py
+-rw-rw-rw-   0        0        0     3077 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/quant_scale_bias.py
+-rw-rw-rw-   0        0        0     3627 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/quant_upsample.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.313483 brevitas-0.9.0/src/brevitas/nn/target/
+-rw-rw-rw-   0        0        0      195 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/target/__init__.py
+-rw-rw-rw-   0        0        0     5852 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/target/flexml.py
+-rw-rw-rw-   0        0        0     6092 2023-04-21 17:11:25.000000 brevitas-0.9.0/src/brevitas/nn/utils.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.316484 brevitas-0.9.0/src/brevitas/onnx/
+-rw-rw-rw-   0        0        0      452 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/onnx/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.322483 brevitas-0.9.0/src/brevitas/ops/
+-rw-rw-rw-   0        0        0      418 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/ops/__init__.py
+-rw-rw-rw-   0        0        0    14701 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/ops/autograd_ste_ops.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.339483 brevitas-0.9.0/src/brevitas/proxy/
+-rw-rw-rw-   0        0        0      533 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/proxy/__init__.py
+-rw-rw-rw-   0        0        0     7483 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/proxy/parameter_quant.py
+-rw-rw-rw-   0        0        0     4943 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/proxy/quant_proxy.py
+-rw-rw-rw-   0        0        0     7804 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/proxy/runtime_quant.py
+-rw-rw-rw-   0        0        0     1572 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/proxy/utils.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.367012 brevitas-0.9.0/src/brevitas/quant/
+-rw-rw-rw-   0        0        0      265 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant/__init__.py
+-rw-rw-rw-   0        0        0    15008 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant/base.py
+-rw-rw-rw-   0        0        0     1237 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant/binary.py
+-rw-rw-rw-   0        0        0     6619 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant/fixed_point.py
+-rw-rw-rw-   0        0        0     1289 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant/none.py
+-rw-rw-rw-   0        0        0    13323 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant/scaled_int.py
+-rw-rw-rw-   0        0        0     3144 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant/shifted_scaled_int.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.394012 brevitas-0.9.0/src/brevitas/quant/solver/
+-rw-rw-rw-   0        0        0      289 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant/solver/__init__.py
+-rw-rw-rw-   0        0        0     5181 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant/solver/act.py
+-rw-rw-rw-   0        0        0     3591 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant/solver/bias.py
+-rw-rw-rw-   0        0        0     2208 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant/solver/clamp.py
+-rw-rw-rw-   0        0        0     6661 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant/solver/common.py
+-rw-rw-rw-   0        0        0     4509 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant/solver/parameter.py
+-rw-rw-rw-   0        0        0     1491 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant/solver/trunc.py
+-rw-rw-rw-   0        0        0     4168 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant/solver/weight.py
+-rw-rw-rw-   0        0        0     1416 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant/ternary.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.401012 brevitas-0.9.0/src/brevitas/quant_tensor/
+-rw-rw-rw-   0        0        0    17983 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant_tensor/__init__.py
+-rw-rw-rw-   0        0        0     4117 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/quant_tensor/torch_handler.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.421012 brevitas-0.9.0/src/brevitas/utils/
+-rw-rw-rw-   0        0        0      112 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/utils/__init__.py
+-rw-rw-rw-   0        0        0     1258 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/utils/jit_utils.py
+-rw-rw-rw-   0        0        0     1668 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/utils/logging.py
+-rw-rw-rw-   0        0        0      914 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/utils/python_utils.py
+-rw-rw-rw-   0        0        0     1836 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/utils/quant_utils.py
+-rw-rw-rw-   0        0        0      901 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas/utils/torch_utils.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:32.779484 brevitas-0.9.0/src/brevitas.egg-info/
+-rw-rw-rw-   0        0        0     3844 2023-04-21 17:52:29.000000 brevitas-0.9.0/src/brevitas.egg-info/PKG-INFO
+-rw-rw-rw-   0        0        0    13924 2023-04-21 17:52:32.000000 brevitas-0.9.0/src/brevitas.egg-info/SOURCES.txt
+-rw-rw-rw-   0        0        0        1 2023-04-21 17:52:29.000000 brevitas-0.9.0/src/brevitas.egg-info/dependency_links.txt
+-rw-rw-rw-   0        0        0      699 2023-04-21 17:52:29.000000 brevitas-0.9.0/src/brevitas.egg-info/entry_points.txt
+-rw-rw-rw-   0        0        0        2 2023-01-10 09:14:58.000000 brevitas-0.9.0/src/brevitas.egg-info/not-zip-safe
+-rw-rw-rw-   0        0        0      751 2023-04-21 17:52:29.000000 brevitas-0.9.0/src/brevitas.egg-info/requires.txt
+-rw-rw-rw-   0        0        0       27 2023-04-21 17:52:29.000000 brevitas-0.9.0/src/brevitas.egg-info/top_level.txt
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.427011 brevitas-0.9.0/src/brevitas_examples/
+-rw-rw-rw-   0        0        0      413 2023-04-21 17:12:45.000000 brevitas-0.9.0/src/brevitas_examples/README.md
+-rw-rw-rw-   0        0        0      112 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.443013 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/
+-rw-rw-rw-   0        0        0     2972 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/README.md
+-rw-rw-rw-   0        0        0      330 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/__init__.py
+-rw-rw-rw-   0        0        0     4429 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/bnn_pynq_train.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.483013 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/cfg/
+-rw-rw-rw-   0        0        0      112 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/cfg/__init__.py
+-rw-rw-rw-   0        0        0      337 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/cfg/cnv_1w1a.ini
+-rw-rw-rw-   0        0        0      337 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/cfg/cnv_1w2a.ini
+-rw-rw-rw-   0        0        0      337 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/cfg/cnv_2w2a.ini
+-rw-rw-rw-   0        0        0      364 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/cfg/lfc_1w1a.ini
+-rw-rw-rw-   0        0        0      364 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/cfg/lfc_1w2a.ini
+-rw-rw-rw-   0        0        0      102 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/cfg/resnet18_3w3a.ini
+-rw-rw-rw-   0        0        0      102 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/cfg/resnet18_4w4a.ini
+-rw-rw-rw-   0        0        0      361 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/cfg/sfc_1w1a.ini
+-rw-rw-rw-   0        0        0      361 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/cfg/sfc_1w2a.ini
+-rw-rw-rw-   0        0        0      361 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/cfg/sfc_2w2a.ini
+-rw-rw-rw-   0        0        0      358 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/cfg/tfc_1w1a.ini
+-rw-rw-rw-   0        0        0      358 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/cfg/tfc_1w2a.ini
+-rw-rw-rw-   0        0        0      358 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/cfg/tfc_2w2a.ini
+-rw-rw-rw-   0        0        0     3318 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/logger.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.504011 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/models/
+-rw-rw-rw-   0        0        0     4226 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/models/CNV.py
+-rw-rw-rw-   0        0        0     3032 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/models/FC.py
+-rw-rw-rw-   0        0        0     2635 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/models/__init__.py
+-rw-rw-rw-   0        0        0     1346 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/models/common.py
+-rw-rw-rw-   0        0        0     1100 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/models/losses.py
+-rw-rw-rw-   0        0        0     7535 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/models/resnet.py
+-rw-rw-rw-   0        0        0     1468 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/models/tensor_norm.py
+-rw-rw-rw-   0        0        0    12702 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/bnn_pynq/trainer.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.513011 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/
+-rw-rw-rw-   0        0        0     1735 2023-04-21 17:12:45.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/README.md
+-rw-rw-rw-   0        0        0      135 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.534012 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/
+-rw-rw-rw-   0        0        0     2464 2023-04-21 17:12:45.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/README.md
+-rw-rw-rw-   0        0        0     3318 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.558014 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/cfg/
+-rw-rw-rw-   0        0        0        0 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/cfg/__init__.py
+-rw-rw-rw-   0        0        0      199 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/cfg/quant_mobilenet_v1.ini
+-rw-rw-rw-   0        0        0      438 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/cfg/quant_mobilenet_v1_4b.ini
+-rw-rw-rw-   0        0        0      437 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/cfg/quant_mobilenet_v1_4b_round_avgpool.ini
+-rw-rw-rw-   0        0        0      524 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/cfg/quant_proxylessnas_mobile14_4b.ini
+-rw-rw-rw-   0        0        0      528 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/cfg/quant_proxylessnas_mobile14_4b5b.ini
+-rw-rw-rw-   0        0        0      523 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/cfg/quant_proxylessnas_mobile14_4b_round_avgpool.ini
+-rw-rw-rw-   0        0        0      542 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/cfg/quant_proxylessnas_mobile14_hadamard_4b.ini
+-rw-rw-rw-   0        0        0     1387 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/common.py
+-rw-rw-rw-   0        0        0     6859 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/mobilenetv1.py
+-rw-rw-rw-   0        0        0    11786 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/proxylessnas.py
+-rw-rw-rw-   0        0        0     6268 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/vgg.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.585011 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/ptq/
+-rw-rw-rw-   0        0        0     9027 2023-04-21 17:12:45.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/ptq/README.md
+-rw-rw-rw-   0        0        0      325 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/ptq/RESULTS_IMGCLSMOB.csv
+-rw-rw-rw-   0        0        0    76880 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/ptq/RESULTS_TORCHVISION.csv
+-rw-rw-rw-   0        0        0     8983 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/ptq/RESULTS_TORCHVISION_BEST_CONFIGS.csv
+-rw-rw-rw-   0        0        0    12900 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/ptq/ptq_benchmark.py
+-rw-rw-rw-   0        0        0     7437 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/ptq/ptq_common.py
+-rw-rw-rw-   0        0        0     9803 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/ptq/ptq_evaluate.py
+-rw-rw-rw-   0        0        0     1676 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/ptq/utils.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.594012 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/qat/
+-rw-rw-rw-   0        0        0     2973 2023-04-21 17:12:45.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/qat/README.md
+-rw-rw-rw-   0        0        0      112 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/qat/__init__.py
+-rw-rw-rw-   0        0        0     2743 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/qat/imagenet_val.py
+-rw-rw-rw-   0        0        0     3697 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/imagenet_classification/utils.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.608013 brevitas-0.9.0/src/brevitas_examples/speech_to_text/
+-rw-rw-rw-   0        0        0     3069 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/README.md
+-rw-rw-rw-   0        0        0      138 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.622013 brevitas-0.9.0/src/brevitas_examples/speech_to_text/cfg/
+-rw-rw-rw-   0        0        0        0 2023-01-10 09:12:32.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/cfg/__init__.py
+-rw-rw-rw-   0        0        0      601 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/cfg/quant_quartznet_perchannelscaling_4b.ini
+-rw-rw-rw-   0        0        0      603 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/cfg/quant_quartznet_perchannelscaling_8b.ini
+-rw-rw-rw-   0        0        0      602 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/cfg/quant_quartznet_pertensorscaling_8b.ini
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.629012 brevitas-0.9.0/src/brevitas_examples/speech_to_text/cfg/topology/
+-rw-rw-rw-   0        0        0        0 2023-01-10 09:12:32.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/cfg/topology/__init__.py
+-rw-rw-rw-   0        0        0     3475 2023-01-10 09:12:32.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/cfg/topology/quartznet15x5.yaml
+-rw-rw-rw-   0        0        0     5304 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/get_librispeech_data.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.656012 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/
+-rw-rw-rw-   0        0        0     3172 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/__init__.py
+-rw-rw-rw-   0        0        0    19330 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/audio_preprocessing.py
+-rw-rw-rw-   0        0        0     5995 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/data_layer.py
+-rw-rw-rw-   0        0        0     1105 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/greedy_ctc_decoder.py
+-rw-rw-rw-   0        0        0     7802 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/helpers.py
+-rw-rw-rw-   0        0        0     1818 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/losses.py
+-rw-rw-rw-   0        0        0     2634 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/metrics.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.690012 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/
+-rw-rw-rw-   0        0        0      961 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/__init__.py
+-rw-rw-rw-   0        0        0     6487 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/cleaners.py
+-rw-rw-rw-   0        0        0     3501 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/common.py
+-rw-rw-rw-   0        0        0     7744 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/dataset.py
+-rw-rw-rw-   0        0        0    11831 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/features.py
+-rw-rw-rw-   0        0        0     7658 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/manifest.py
+-rw-rw-rw-   0        0        0     6652 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/perturb.py
+-rw-rw-rw-   0        0        0    20949 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/quartznet.py
+-rw-rw-rw-   0        0        0     8321 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/segment.py
+-rw-rw-rw-   0        0        0     3634 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/spectr_augment.py
+-rw-rw-rw-   0        0        0    11990 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/quartznet.py
+-rw-rw-rw-   0        0        0     3823 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet_val.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.705011 brevitas-0.9.0/src/brevitas_examples/text_to_speech/
+-rw-rw-rw-   0        0        0     2193 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/text_to_speech/README.md
+-rw-rw-rw-   0        0        0      135 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/text_to_speech/__init__.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.710011 brevitas-0.9.0/src/brevitas_examples/text_to_speech/cfg/
+-rw-rw-rw-   0        0        0        0 2023-01-10 09:12:32.000000 brevitas-0.9.0/src/brevitas_examples/text_to_speech/cfg/__init__.py
+-rw-rw-rw-   0        0        0      365 2023-01-10 09:12:32.000000 brevitas-0.9.0/src/brevitas_examples/text_to_speech/cfg/quant_melgan_8b.ini
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.727013 brevitas-0.9.0/src/brevitas_examples/text_to_speech/melgan/
+-rw-rw-rw-   0        0        0      946 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/text_to_speech/melgan/__init__.py
+-rw-rw-rw-   0        0        0     4942 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/text_to_speech/melgan/common.py
+-rw-rw-rw-   0        0        0     4914 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/text_to_speech/melgan/generator_brevitas.py
+-rw-rw-rw-   0        0        0     1959 2023-01-10 09:12:32.000000 brevitas-0.9.0/src/brevitas_examples/text_to_speech/melgan/melgan.py
+-rw-rw-rw-   0        0        0     3588 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/text_to_speech/melgan/res_stack_brevitas.py
+-rw-rw-rw-   0        0        0     1969 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/text_to_speech/melgan_val.py
+-rw-rw-rw-   0        0        0     2403 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/text_to_speech/preprocess_dataset.py
+drwxrwxrwx   0        0        0        0 2023-04-21 17:52:33.737040 brevitas-0.9.0/src/brevitas_examples/text_to_speech/utilities/
+-rw-rw-rw-   0        0        0       73 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/text_to_speech/utilities/__init__.py
+-rw-rw-rw-   0        0        0     4614 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/text_to_speech/utilities/audio_processing.py
+-rw-rw-rw-   0        0        0     8078 2023-04-21 17:11:26.000000 brevitas-0.9.0/src/brevitas_examples/text_to_speech/utilities/stft.py
```

### Comparing `brevitas-0.8.0/LICENSE` & `brevitas-0.9.0/LICENSE`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -14,8 +14,8 @@
 Brevitas contains third party software which is under different license
 terms. All such code will be identified clearly using at least one of two
 mechanisms:
 1) It will be in a separate directory tree with its own `LICENSE.txt` or
    `LICENSE` file at the top containing the specific license and restrictions
    which apply to that software, or
 2) It will contain specific license and restriction terms at the top of every
-   file.
+   file.
```

### Comparing `brevitas-0.8.0/PKG-INFO` & `brevitas-0.9.0/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: brevitas
-Version: 0.8.0
+Version: 0.9.0
 Summary: Quantization-aware training in PyTorch
 Home-page: https://github.com/Xilinx/brevitas
 Author: Alessandro Pappalardo
 Author-email: alessand@xilinx.com
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown
 Provides-Extra: notebook
@@ -23,19 +23,19 @@
 
 [![Downloads](https://pepy.tech/badge/brevitas)](https://pepy.tech/project/brevitas)
 [![Gitter](https://badges.gitter.im/xilinx-brevitas/community.svg)](https://gitter.im/xilinx-brevitas/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)
 ![Pytest](https://github.com/Xilinx/brevitas/workflows/Pytest/badge.svg?branch=master)
 ![Examples Pytest](https://github.com/Xilinx/brevitas/workflows/Examples%20Pytest/badge.svg?branch=master)
 [![DOI](https://zenodo.org/badge/140494324.svg)](https://zenodo.org/badge/latestdoi/140494324)
 
-Brevitas is a PyTorch library for neural network quantization, with a focus on *quantization-aware training (QAT)*.
+Brevitas is a PyTorch library for neural network quantization, with support for both *post-training quantization (PTQ)* and *quantization-aware training (QAT)*.
 
 **Please note that Brevitas is a research project and not an official Xilinx product.**
 
-If you like this project please consider  this repo, as it is the simplest and best way to support it. 
+If you like this project please consider  this repo, as it is the simplest and best way to support it.
 
 If you have issues, comments, or are just looking for advices on training quantized neural networks, open an issue or a discussion.
 
 ## Cite as
 
 If you adopt Brevitas in your work, please cite it as:
 ```
@@ -48,21 +48,22 @@
   url          = {https://doi.org/10.5281/zenodo.3333552}
 }
 ```
 
 
 ## History
 
+- *2023/04/21* - Release version 0.9.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.9.0).
 - *2023/01/10* - Release version 0.8.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.8.0).
 - *2021/12/13* - Release version 0.7.1, fix a bunch of issues. Added TVMCon 2021 tutorial notebook.
 - *2021/11/03* - Re-release version 0.7.0 (build 1) on PyPI to fix a packaging issue.
 - *2021/10/29* - Release version 0.7.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.7.0).
 - *2021/06/04* - Release version 0.6.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.6.0).
 - *2021/05/24* - Release version 0.5.1, fix a bunch of minor issues. See [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.5.1).
-- *2021/05/06* - Release version 0.5.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.5.0). 
+- *2021/05/06* - Release version 0.5.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.5.0).
 - *2021/03/15* - Release version 0.4.0, add support for \_\_torch_function\_\_ to QuantTensor.
 - *2021/03/04* - Release version 0.3.1, fix bug w/ act initialization from statistics w/ IGNORE_MISSING_KEYS=1.
 - *2021/03/01* - Release version 0.3.0, implements enum and shape solvers within extended dependency injectors. This allows declarative quantizers to be self-contained.
 - *2021/02/04* - Release version 0.2.1, includes various bugfixes of QuantTensor w/ zero-point.
 - *2021/01/30* - First release version 0.2.0 on PyPI.
 
 ## Requirements
@@ -78,13 +79,7 @@
 ```bash
 pip install brevitas
 ```
 
 ## Getting started
 
 Check out available info at https://xilinx.github.io/brevitas/getting_started .
-
-
-
-
-
-
```

### Comparing `brevitas-0.8.0/README.md` & `brevitas-0.9.0/README.md`

 * *Files 4% similar despite different names*

```diff
@@ -2,19 +2,19 @@
 
 [![Downloads](https://pepy.tech/badge/brevitas)](https://pepy.tech/project/brevitas)
 [![Gitter](https://badges.gitter.im/xilinx-brevitas/community.svg)](https://gitter.im/xilinx-brevitas/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)
 ![Pytest](https://github.com/Xilinx/brevitas/workflows/Pytest/badge.svg?branch=master)
 ![Examples Pytest](https://github.com/Xilinx/brevitas/workflows/Examples%20Pytest/badge.svg?branch=master)
 [![DOI](https://zenodo.org/badge/140494324.svg)](https://zenodo.org/badge/latestdoi/140494324)
 
-Brevitas is a PyTorch library for neural network quantization, with a focus on *quantization-aware training (QAT)*.
+Brevitas is a PyTorch library for neural network quantization, with support for both *post-training quantization (PTQ)* and *quantization-aware training (QAT)*.
 
 **Please note that Brevitas is a research project and not an official Xilinx product.**
 
-If you like this project please consider  this repo, as it is the simplest and best way to support it. 
+If you like this project please consider  this repo, as it is the simplest and best way to support it.
 
 If you have issues, comments, or are just looking for advices on training quantized neural networks, open an issue or a discussion.
 
 ## Cite as
 
 If you adopt Brevitas in your work, please cite it as:
 ```
@@ -27,21 +27,22 @@
   url          = {https://doi.org/10.5281/zenodo.3333552}
 }
 ```
 
 
 ## History
 
+- *2023/04/21* - Release version 0.9.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.9.0).
 - *2023/01/10* - Release version 0.8.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.8.0).
 - *2021/12/13* - Release version 0.7.1, fix a bunch of issues. Added TVMCon 2021 tutorial notebook.
 - *2021/11/03* - Re-release version 0.7.0 (build 1) on PyPI to fix a packaging issue.
 - *2021/10/29* - Release version 0.7.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.7.0).
 - *2021/06/04* - Release version 0.6.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.6.0).
 - *2021/05/24* - Release version 0.5.1, fix a bunch of minor issues. See [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.5.1).
-- *2021/05/06* - Release version 0.5.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.5.0). 
+- *2021/05/06* - Release version 0.5.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.5.0).
 - *2021/03/15* - Release version 0.4.0, add support for \_\_torch_function\_\_ to QuantTensor.
 - *2021/03/04* - Release version 0.3.1, fix bug w/ act initialization from statistics w/ IGNORE_MISSING_KEYS=1.
 - *2021/03/01* - Release version 0.3.0, implements enum and shape solvers within extended dependency injectors. This allows declarative quantizers to be self-contained.
 - *2021/02/04* - Release version 0.2.1, includes various bugfixes of QuantTensor w/ zero-point.
 - *2021/01/30* - First release version 0.2.0 on PyPI.
 
 ## Requirements
@@ -57,13 +58,7 @@
 ```bash
 pip install brevitas
 ```
 
 ## Getting started
 
 Check out available info at https://xilinx.github.io/brevitas/getting_started .
-
-
-
-
-
-
```

### Comparing `brevitas-0.8.0/notebooks/01_quant_tensor_quant_conv2d_overview.ipynb` & `brevitas-0.9.0/notebooks/03_anatomy_of_a_quantizer.ipynb`

 * *Files 12% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9509605505689367%*

 * *Differences: {"'cells'": '{0: {\'source\': {insert: [(0, \'# Anatomy of a Quantizer\\n\'), (2, "## What\'s in a '*

 * *            'Quantizer? \\n"), (4, \'In a broad sense, a quantizer is anything that implements a '*

 * *            'quantization technique, and the flexibility of Brevitas means that there are '*

 * *            "different ways to do so.  \\n'), (5, 'However, to keep our terminology straight, we "*

 * *            'refer to a quantizer as a specific kind of way to implement quantization, the one '*

 * *            'preferred a []*

```diff
@@ -2,1181 +2,1495 @@
     "cells": [
         {
             "cell_type": "markdown",
             "metadata": {
                 "collapsed": true
             },
             "source": [
-                "# An overview of QuantTensor and QuantConv2d\n",
+                "# Anatomy of a Quantizer\n",
                 "\n",
-                "In this initial tutorial, we take a first look at `QuantTensor`, a basic data structure in Brevitas, and at `QuantConv2d`, a typical quantized layer. `QuantConv2d` is an instance of a `QuantWeightBiasInputOutputLayer` (typically imported as `QuantWBIOL`), meaning that it supports quantization of its weight, bias, input and output. Other instances of `QuantWBIOL` are `QuantLinear`, `QuantConv1d`, `QuantConvTranspose1d` and `QuantConvTranspose2d`, and they all follow the same principles.\n",
+                "## What's in a Quantizer? \n",
                 "\n",
-                "If we take a look at the `__init__` method of `QuantConv2d`, we notice a few things:"
+                "In a broad sense, a quantizer is anything that implements a quantization technique, and the flexibility of Brevitas means that there are different ways to do so.  \n",
+                "However, to keep our terminology straight, we refer to a quantizer as a specific kind of way to implement quantization, the one preferred and adopted by default. That is, a quantizer is a subclass of a `brevitas.inject.ExtendedInjector` that carries a `tensor_quant` attribute, which points to an instance of a torch `Module` that implements quantization.\n",
+                "\n",
+                "We have seen in previous tutorials quantizers being imported from `brevitas.quant` and passed on to quantized layers. We can easily very what we just said on one of them:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
-                        "text/markdown": [
-                            "```python\n",
-                            "    def __init__(\n",
-                            "            self,\n",
-                            "            in_channels: int,\n",
-                            "            out_channels: int,\n",
-                            "            kernel_size: Union[int, Tuple[int, int]],\n",
-                            "            stride: Union[int, Tuple[int, int]] = 1,\n",
-                            "            padding: Union[int, Tuple[int, int]] = 0,\n",
-                            "            dilation: Union[int, Tuple[int, int]] = 1,\n",
-                            "            groups: int = 1,\n",
-                            "            bias: bool = True,\n",
-                            "            padding_type: str = 'standard',\n",
-                            "            weight_quant: Optional[WeightQuantType] = Int8WeightPerTensorFloat,\n",
-                            "            bias_quant: Optional[BiasQuantType] = None,\n",
-                            "            input_quant: Optional[ActQuantType] = None,\n",
-                            "            output_quant: Optional[ActQuantType] = None,\n",
-                            "            return_quant_tensor: bool = False,\n",
-                            "            **kwargs) -> None:\n",
-                            "        Conv2d.__init__(\n",
-                            "            self,\n",
-                            "            in_channels=in_channels,\n",
-                            "            out_channels=out_channels,\n",
-                            "            kernel_size=kernel_size,\n",
-                            "            stride=stride,\n",
-                            "            padding=padding,\n",
-                            "            dilation=dilation,\n",
-                            "            groups=groups,\n",
-                            "            bias=bias)\n",
-                            "        QuantWBIOL.__init__(\n",
-                            "            self,\n",
-                            "            weight_quant=weight_quant,\n",
-                            "            bias_quant=bias_quant,\n",
-                            "            input_quant=input_quant,\n",
-                            "            output_quant=output_quant,\n",
-                            "            return_quant_tensor=return_quant_tensor,\n",
-                            "            **kwargs)\n",
-                            "        assert self.padding_mode == 'zeros'\n",
-                            "        assert not (padding_type == 'same' and padding != 0)\n",
-                            "        self.padding_type = padding_type\n",
-                            "\n",
-                            "```"
-                        ],
                         "text/plain": [
-                            "<IPython.core.display.Markdown object>"
+                            "True"
                         ]
                     },
+                    "execution_count": 1,
                     "metadata": {},
-                    "output_type": "display_data"
+                    "output_type": "execute_result"
                 }
             ],
             "source": [
-                "import inspect\n",
-                "from brevitas.nn import QuantConv2d\n",
-                "from IPython.display import Markdown, display\n",
-                "\n",
-                "def pretty_print_source(source):\n",
-                "    display(Markdown('```python\\n' + source + '\\n```'))\n",
-                "    \n",
-                "source = inspect.getsource(QuantConv2d.__init__)  \n",
-                "pretty_print_source(source)"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "`QuantConv2d` is an instance of both `Conv2d` and `QuantWBIOL`. Its initialization method exposes the usual arguments of a `Conv2d`, as well as: an extra flag to support *same padding*; *four* different arguments to set a quantizer for - respectively - *weight*, *bias*, *input*, and *output*; a `return_quant_tensor` boolean flag; the `**kwargs` placeholder to intercept additional arbitrary keyword arguments.  \n",
-                "In this tutorial we will focus on how to set the four quantizer arguments and the return flags; arbitrary kwargs will be explained in a separate tutorial dedicated to defining and overriding quantizers.\n",
+                "from brevitas.inject import ExtendedInjector\n",
+                "from brevitas.quant.scaled_int import Int8ActPerTensorFloat \n",
                 "\n",
-                "By default `weight_quant=Int8WeightPerTensorFloat`, while `bias_quant`, `input_quant` and `output_quant` are set to `None`. That means that by default weights are quantized to *8-bit signed integer with a per-tensor floating-point scale factor* (a very common type of quantization adopted by e.g. the ONNX standard opset), while quantization of bias, input, and output are disabled. We can easily verify all of this at runtime on an example:"
+                "issubclass(Int8ActPerTensorFloat, ExtendedInjector)"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 2,
             "metadata": {},
-            "outputs": [],
-            "source": [
-                "default_quant_conv = QuantConv2d(\n",
-                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False)"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 3,
-            "metadata": {},
             "outputs": [
                 {
-                    "name": "stdout",
-                    "output_type": "stream",
-                    "text": [
-                        "Is weight quant enabled: True\n",
-                        "Is bias quant enabled: False\n",
-                        "Is input quant enabled: False\n",
-                        "Is output quant enabled: False\n"
-                    ]
+                    "data": {
+                        "text/plain": [
+                            "RescalingIntQuant(\n",
+                            "  (int_quant): IntQuant(\n",
+                            "    (float_to_int_impl): RoundSte()\n",
+                            "    (tensor_clamp_impl): TensorClamp()\n",
+                            "    (delay_wrapper): DelayWrapper(\n",
+                            "      (delay_impl): _NoDelay()\n",
+                            "    )\n",
+                            "  )\n",
+                            "  (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
+                            "    (stats_input_view_shape_impl): OverTensorView()\n",
+                            "    (stats): _Stats(\n",
+                            "      (stats_impl): AbsPercentile()\n",
+                            "    )\n",
+                            "    (restrict_scaling): _RestrictValue(\n",
+                            "      (restrict_value_impl): FloatRestrictValue()\n",
+                            "    )\n",
+                            "    (clamp_scaling): _ClampValue(\n",
+                            "      (clamp_min_ste): ScalarClampMinSte()\n",
+                            "    )\n",
+                            "    (restrict_inplace_preprocess): Identity()\n",
+                            "    (restrict_preprocess): Identity()\n",
+                            "  )\n",
+                            "  (int_scaling_impl): IntScaling()\n",
+                            "  (zero_point_impl): ZeroZeroPoint(\n",
+                            "    (zero_point): StatelessBuffer()\n",
+                            "  )\n",
+                            "  (msb_clamp_bit_width_impl): BitWidthConst(\n",
+                            "    (bit_width): StatelessBuffer()\n",
+                            "  )\n",
+                            ")"
+                        ]
+                    },
+                    "execution_count": 2,
+                    "metadata": {},
+                    "output_type": "execute_result"
                 }
             ],
             "source": [
-                "print(f'Is weight quant enabled: {default_quant_conv.is_weight_quant_enabled}')\n",
-                "print(f'Is bias quant enabled: {default_quant_conv.is_bias_quant_enabled}')\n",
-                "print(f'Is input quant enabled: {default_quant_conv.is_input_quant_enabled}')\n",
-                "print(f'Is output quant enabled: {default_quant_conv.is_output_quant_enabled}')"
+                "Int8ActPerTensorFloat.tensor_quant"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "If we now try to pass in a random floating-point tensor as input, as expected we get the output of the convolution:"
+                "Note how we said *subclass* and not *instance*. To understand why that's the case, we have to understand what an `ExtendedInjector` is and why it's used in the first place."
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": 4,
-            "metadata": {
-                "scrolled": true
-            },
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "tensor([[[[-0.1007, -0.2631,  0.9119],\n",
-                            "          [ 0.3060,  0.3174,  0.6748],\n",
-                            "          [-0.2179, -0.4119, -0.4807]],\n",
-                            "\n",
-                            "         [[ 0.4296,  0.0781, -0.2309],\n",
-                            "          [ 0.3522, -0.6440, -0.5089],\n",
-                            "          [ 0.8133,  0.3387, -0.0395]],\n",
-                            "\n",
-                            "         [[-0.7194,  0.9901,  0.5440],\n",
-                            "          [-1.1865,  1.5809, -1.0971],\n",
-                            "          [-0.7248, -0.1470, -0.0498]]]], grad_fn=<ThnnConv2DBackward>)"
-                        ]
-                    },
-                    "execution_count": 4,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
+            "cell_type": "markdown",
+            "metadata": {},
             "source": [
-                "import torch\n",
+                "## Quantization with auto-wiring Dependency Injection\n",
+                "\n",
+                "Pytorch has exploded in popularity thanks to its straightforward numpy-like *define-by-run* execution model. However, when it comes to applying quantization, this style of programming poses a problem.  \n",
                 "\n",
-                "out = default_quant_conv(torch.randn(1, 2, 5, 5))\n",
-                "out"
+                "Many quantization methods depend on making decisions based on the (in Pytorch terms) `state_dict` of the original floating-point model to finetune with quantization. However, when we instantiate a model in Pytorch we can't know on the spot if a state_dict is going to be loaded a few lines of code later or not. Yet, because Pytorch is define-by-run, we need our model to work consistently both before and after a `state_dict` is possibly loaded. In a traditional scenario that wouldn't pose a problem. However, with quantization in the loop, the way a quantizer is defined might change before and after a pretrained `state_dict` is loaded.\n",
+                "\n",
+                "That means that we need a way to define our quantized model such that it can react appropriately in case the `state_dict` changes. In a Python-only world that wouldn't be too hard. However, in order to mitigate the performance impact of quantization-aware training, Brevitas makes extended use of Pytorch's JIT compiler for a custom subset of Python, TorchScript. That means that in most scenarios, when a `state_dict` is loaded, we need to recompile parts of the model. Because compilation in general is a lossy process, a TorchScript component cannot simply re-compile itself based on new input information. \n",
+                "\n",
+                "We need then a way to *declare* a quantization method such that it can be re-initialized and JIT compiled any time the `state_dict` changes. Because we want to support arbitrarly-complex user-defined quantization algorithms, this method has to be generic, i.e. it cannot depend on the specifics of the quantization algorithm implemented.\n",
+                "\n",
+                "Implementing a quantizer with an `ExtendedInjector` is a way to do so. Specifically, an `ExtendedInjector` extends an `Injector` from an older version (*0.2.1*) of the excellent dependency-injection library *[dependencies](https://github.com/proofit404/dependencies)* with support for a couple of extra features that are specific to Brevitas' needs.\n",
+                "\n",
+                "An `Injector` (and an `ExtendedInjector`) allows to take what might be a very complicated graph of interwined objects and turns it into a flat list of variables that are capable of auto-assembly by matching variable names to arguments names. This technique typically goes under the name of auto-wiring dependency injection. \n",
+                "\n",
+                "In the context of Brevitas, the goal is gather all the modules and hyperparameters that contribute to a quantization implementation such that they can be re-assembled automatically on demand. What comes out of this process is a `tensor_quant` object."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "In this case we are computing the convolution between an unquantized input tensor and quantized weights, so the output in general is unquantized."
+                "## A Practical Example: Binary Quantization\n",
+                "\n",
+                "To make things practical, let's look at how we can implement a simple variant of binary quantization. All the components typically used to implement quantization can be found under `brevitas.core`. As mentioned before, Brevitas makes heavy use of TorchScript. In particular, all the components found under `brevitas.core` are implemented as `ScriptModule` that can be assembled together.\n",
+                "The core `ScriptModule` that implements binarization can be found under `brevitas.core.quant`: "
             ]
         },
         {
-            "cell_type": "markdown",
+            "cell_type": "code",
+            "execution_count": 3,
             "metadata": {},
+            "outputs": [],
             "source": [
-                "A QuantConv2d with quantization disabled everywhere behaves like a standard `Conv2d`. Again can easily verify this:"
+                "import inspect\n",
+                "from IPython.display import Markdown, display\n",
+                "\n",
+                "def pretty_print_source(source):\n",
+                "    display(Markdown('```python\\n' + source + '\\n```'))"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 5,
+            "execution_count": 4,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
+                        "text/markdown": [
+                            "```python\n",
+                            "class BinaryQuant(brevitas.jit.ScriptModule):\n",
+                            "    \"\"\"\n",
+                            "    ScriptModule that implements scaled uniform binary quantization of an input tensor.\n",
+                            "    Quantization is performed with :func:`~brevitas.function.ops_ste.binary_sign_ste`.\n",
+                            "\n",
+                            "    Args:\n",
+                            "        scaling_impl (Module): Module that returns a scale factor.\n",
+                            "        quant_delay_steps (int): Number of training steps to delay quantization for. Default: 0\n",
+                            "\n",
+                            "    Returns:\n",
+                            "        Tuple[Tensor, Tensor, Tensor, Tensor]: Quantized output in de-quantized format, scale, zero-point, bit_width.\n",
+                            "\n",
+                            "    Examples:\n",
+                            "        >>> from brevitas.core.scaling import ConstScaling\n",
+                            "        >>> binary_quant = BinaryQuant(ConstScaling(0.1))\n",
+                            "        >>> inp = torch.Tensor([0.04, -0.6, 3.3])\n",
+                            "        >>> out, scale, zero_point, bit_width = binary_quant(inp)\n",
+                            "        >>> out\n",
+                            "        tensor([ 0.1000, -0.1000,  0.1000])\n",
+                            "        >>> scale\n",
+                            "        tensor(0.1000)\n",
+                            "        >>> zero_point\n",
+                            "        tensor(0.)\n",
+                            "        >>> bit_width\n",
+                            "        tensor(1.)\n",
+                            "\n",
+                            "    Note:\n",
+                            "        Maps to quant_type == QuantType.BINARY == 'BINARY' == 'binary' when applied to weights in higher-level APIs.\n",
+                            "\n",
+                            "    Note:\n",
+                            "        Set env variable BREVITAS_JIT=1 to enable TorchScript compilation of this module.\n",
+                            "    \"\"\"\n",
+                            "\n",
+                            "    def __init__(self, scaling_impl: Module, quant_delay_steps: int = 0):\n",
+                            "        super(BinaryQuant, self).__init__()\n",
+                            "        self.scaling_impl = scaling_impl\n",
+                            "        self.bit_width = BitWidthConst(1)\n",
+                            "        self.zero_point = StatelessBuffer(torch.tensor(0.0))\n",
+                            "        self.delay_wrapper = DelayWrapper(quant_delay_steps)\n",
+                            "\n",
+                            "    @brevitas.jit.script_method\n",
+                            "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
+                            "        scale = self.scaling_impl(x)\n",
+                            "        y = binary_sign_ste(x) * scale\n",
+                            "        y = self.delay_wrapper(x, y)\n",
+                            "        return y, scale, self.zero_point(), self.bit_width()\n",
+                            "\n",
+                            "```"
+                        ],
                         "text/plain": [
-                            "True"
+                            "<IPython.core.display.Markdown object>"
                         ]
                     },
-                    "execution_count": 5,
                     "metadata": {},
-                    "output_type": "execute_result"
+                    "output_type": "display_data"
                 }
             ],
             "source": [
-                "from torch.nn import Conv2d\n",
+                "from brevitas.core.quant import BinaryQuant\n",
                 "\n",
-                "torch.manual_seed(0)  # set a seed to make sure the random weight init is reproducible\n",
-                "disabled_quant_conv = QuantConv2d(\n",
-                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False, weight_quant=None)\n",
-                "torch.manual_seed(0)  # reproduce the same random weight init as above\n",
-                "float_conv = Conv2d(\n",
-                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False)\n",
-                "inp = torch.randn(1, 2, 5, 5)\n",
-                "torch.isclose(disabled_quant_conv(inp), float_conv(inp)).all().item()"
+                "source = inspect.getsource(BinaryQuant)  \n",
+                "pretty_print_source(source)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "As we have just seen, Brevitas allows users as much freedom as possible to experiment with quantization, meaning that computation between quantized and unquantized values is considered legal. This allows users to mix Brevitas layers with Pytorch layers with little restrictions.  \n",
-                "To make this possible, quantized values are typically represented in *dequantized format*, meaning that - in the case of affine quantization implemented in Brevitas - zero-point and scale factor are applied to their integer values according to the formula **quant_value = (integer_value - zero_point) * scale**."
+                "The implementation is quite simple. Apart from `quant_delay_steps`, which allows to delay quantization by a certain number of training steps (*default = 0*), the only other argument that BinaryQuant accepts is an implementation to compute the scale factor. `bit_width` is fixed to 1 and `zero-point` is fixed to 0.\n",
+                "\n",
+                "We pick as scale factor implementation a `ScriptModule` called `ParameterScaling`, which implements a learned parameter with user-defined initialization. It can be found under `brevitas.core.scaling`:"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": 5,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "from brevitas.core.scaling import ParameterScaling"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "## QuantTensor\n",
+                "### Manual Binary Quantization\n",
                 "\n",
-                "We can directly observe the quantized weights by calling the weight quantizer on the layer's weights: `default_quant_conv.weight_quant(quant_conv.weight)`, which for shortness is already implemented as `default_quant_conv.quant_weight()` :"
+                "As a first step, we simply instantiate `BinaryQuant` with `ParameterScaling` using `scaling_init` equal *0.1* and we call it on a random floating-point input tensor:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 6,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[-0.1684, -0.0722,  0.1554],\n",
-                            "          [-0.1499, -0.1554, -0.1332],\n",
-                            "          [-0.1665,  0.1388,  0.2220]],\n",
-                            "\n",
-                            "         [[-0.1277,  0.1813, -0.2294],\n",
-                            "          [ 0.0740, -0.0259, -0.1628],\n",
-                            "          [ 0.1425, -0.1906, -0.2109]]],\n",
-                            "\n",
-                            "\n",
-                            "        [[[ 0.0463,  0.2350, -0.1480],\n",
-                            "          [-0.2350,  0.1221, -0.0074],\n",
-                            "          [ 0.1369,  0.0814, -0.0185]],\n",
-                            "\n",
-                            "         [[-0.0648,  0.1684, -0.1517],\n",
-                            "          [ 0.1628,  0.1517,  0.1998],\n",
-                            "          [-0.0130,  0.2257, -0.1221]]],\n",
-                            "\n",
-                            "\n",
-                            "        [[[-0.1758,  0.1166, -0.0592],\n",
-                            "          [ 0.1425, -0.0796, -0.1499],\n",
-                            "          [-0.1832, -0.0278, -0.2294]],\n",
-                            "\n",
-                            "         [[ 0.2054, -0.0296,  0.1702],\n",
-                            "          [ 0.0185, -0.2294,  0.0426],\n",
-                            "          [ 0.0352,  0.0037, -0.1258]]]], grad_fn=<MulBackward0>), scale=tensor(0.0019, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))"
+                            "(tensor([[ 0.1000,  0.1000,  0.1000,  0.1000],\n",
+                            "         [-0.1000, -0.1000,  0.1000,  0.1000],\n",
+                            "         [ 0.1000, -0.1000,  0.1000, -0.1000],\n",
+                            "         [ 0.1000, -0.1000,  0.1000, -0.1000]], grad_fn=<MulBackward0>),\n",
+                            " tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>),\n",
+                            " tensor(0.),\n",
+                            " tensor(1.))"
                         ]
                     },
                     "execution_count": 6,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "default_quant_conv.quant_weight()"
+                "import torch\n",
+                "\n",
+                "manual_tensor_quant = BinaryQuant(scaling_impl=ParameterScaling(scaling_init=0.1))\n",
+                "manual_tensor_quant(torch.randn(4, 4))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Notice how the quantized weights are wrapped in a data structure implemented by Brevitas called `QuantTensor`. A `QuantTensor` is a way to represent an affine quantized tensor with all its metadata, meaning: the `value` of the quantized tensor in *dequantized* format, `scale`, `zero_point`, `bit_width`, whether the quantized value it's `signed` or not, and whether the tensor was generated in `training` mode. "
+                "Nothing too surprising here, as expected the tensor is binarized with the scale factor we defined. Note however how `manual_tensor_quant` is returning a `tuple` and not a `QuantTensor`. This is because support for custom data structures in TorchScript is still quite limited, so `QuantTensor` are allocated only in Python-world abstractions."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "As expected, we have that the quantized value (in dequantized format) can be computer from its integer representation, together with zero-point and scale:"
+                "### Binary Quantization with an ExtendedInjector\n",
+                "\n",
+                "Let's now declare `tensor_quant` through an `ExtendedInjector`:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 7,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "True"
+                            "(tensor([[-0.1000,  0.1000, -0.1000,  0.1000],\n",
+                            "         [ 0.1000,  0.1000, -0.1000, -0.1000],\n",
+                            "         [-0.1000,  0.1000, -0.1000,  0.1000],\n",
+                            "         [-0.1000,  0.1000,  0.1000,  0.1000]], grad_fn=<MulBackward0>),\n",
+                            " tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>),\n",
+                            " tensor(0.),\n",
+                            " tensor(1.))"
                         ]
                     },
                     "execution_count": 7,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "int_weight = default_quant_conv.int_weight()\n",
-                "zero_point = default_quant_conv.quant_weight_zero_point()\n",
-                "scale = default_quant_conv.quant_weight_scale()\n",
-                "quant_weight_manually = (int_weight - zero_point) * scale\n",
-                "default_quant_conv.quant_weight().value.isclose(quant_weight_manually).all().item()"
+                "from brevitas.inject import ExtendedInjector\n",
+                "\n",
+                "class MyBinaryQuantizer(ExtendedInjector):\n",
+                "    tensor_quant = BinaryQuant\n",
+                "    scaling_impl=ParameterScaling\n",
+                "    scaling_init=0.1\n",
+                "\n",
+                "inj_tensor_quant = MyBinaryQuantizer.tensor_quant\n",
+                "inj_tensor_quant(torch.randn(4, 4))"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "Any time `MyBinaryQuantizer.tensor_quant` is called, a new instance of `BinaryQuant` is created. Note how the attributes of `MyBinaryQuantizer` are designed to match the name of the arguments of each other, except for `tensor_quant`, which is what we are interested in retrieving from the outside."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "A *valid* QuantTensor correctly populates all its fields with values `!= None` and respect the **affine quantization invariant**, i.e. `value / scale + zero_point` is (accounting for rounding errors) an *integer* that can be represented within the interval defined by the `bit_width` and `signed` fields of the `QuantTensor`. A *non-valid* one doesn't.\n",
-                "We can observe that the quantized weights are indeed marked as valid:"
+                "## Inheritance and Composition of Quantizers\n",
+                "\n",
+                "The advantage of expressing a quantizer through a Python class also means that we can leverage both *inheritance* and *composition*. So for example we can inherit from `MyBinaryQuantizer` and override `scaling_init` with a new value:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 8,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "True"
+                            "(tensor([[ 1., -1.,  1.,  1.],\n",
+                            "         [ 1.,  1., -1.,  1.],\n",
+                            "         [ 1.,  1.,  1., -1.],\n",
+                            "         [-1.,  1., -1., -1.]], grad_fn=<MulBackward0>),\n",
+                            " tensor(1., grad_fn=<AbsBinarySignGradFnBackward>),\n",
+                            " tensor(0.),\n",
+                            " tensor(1.))"
                         ]
                     },
                     "execution_count": 8,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "default_quant_conv.quant_weight().is_valid"
+                "class MyChildBinaryQuantizer(MyBinaryQuantizer):\n",
+                "    scaling_init=1.0\n",
+                "    \n",
+                "child_inj_tensor_quant = MyChildBinaryQuantizer.tensor_quant\n",
+                "child_inj_tensor_quant(torch.randn(4, 4))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Calling `is_valid` is relative expensive, so it should be using sparingly, but there are a few cases where a non-valid QuantTensor might be generated that is important to be aware of. Say we instantiate the layer again, this time with `return_quant_tensor=True`:"
+                "Or we can leverage composition by assembling together various classes containing different pieces of a quantizer:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 9,
             "metadata": {},
-            "outputs": [],
+            "outputs": [
+                {
+                    "data": {
+                        "text/plain": [
+                            "(tensor([[ 0.1000, -0.1000, -0.1000, -0.1000],\n",
+                            "         [-0.1000,  0.1000, -0.1000,  0.1000],\n",
+                            "         [ 0.1000, -0.1000,  0.1000,  0.1000],\n",
+                            "         [-0.1000,  0.1000, -0.1000,  0.1000]], grad_fn=<MulBackward0>),\n",
+                            " tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>),\n",
+                            " tensor(0.),\n",
+                            " tensor(1.))"
+                        ]
+                    },
+                    "execution_count": 9,
+                    "metadata": {},
+                    "output_type": "execute_result"
+                }
+            ],
             "source": [
-                "return_quant_conv = QuantConv2d(\n",
-                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False, return_quant_tensor=True)"
+                "class MyBinaryImpl(ExtendedInjector):\n",
+                "    tensor_quant = BinaryQuant\n",
+                "\n",
+                "class MyScalingImpl(ExtendedInjector):\n",
+                "    scaling_impl=ParameterScaling\n",
+                "    scaling_init=0.1\n",
+                "    \n",
+                "class MyComposedBinaryQuantizer(MyBinaryImpl, MyScalingImpl):\n",
+                "    pass\n",
+                "\n",
+                "comp_inj_tensor_quant = MyComposedBinaryQuantizer.tensor_quant\n",
+                "comp_inj_tensor_quant(torch.randn(4, 4))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "We then again pass as input a random floating-point tensor. Because `input_quant=None` and `output_quant=None` (i.e. both input and output quantization are disabled), again as before we are performing a convolution between a quantized and an unquantized tensor, which in general returns an unquantized tensor:"
+                "## Interfacing a Quantizer with a Quantized Layer\n",
+                "\n",
+                "Before we can pass the quantizer to a quantized layer such as `QuantConv2d`, we need a last component to define, a proxy. A proxy (found under `brevitas.proxy`) is an `nn.Module` that serve as interface between a quantizer and a quantized layer. \n",
+                "\n",
+                "While a quantizer lives mostly in JIT-land, a proxy lives mostly in Python-land, and as such can afford much more flexibility. Proxies take care of returning a `QuantTensor` and re-initializing the output of quantizer whenever a new `state_dict` is loaded. \n",
+                "\n",
+                "Proxies are specific to the kind of tensor being quantized, as in weights vs biases vs activations. For convenience, they are declared as part of the quantizer itself under the attribute `proxy_class`. For example, for weights we can use `WeightQuantProxyFromInjector`:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 10,
             "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "QuantTensor(value=tensor([[[[-0.6238, -0.1567,  0.5639],\n",
-                            "          [-0.3426,  0.0662,  0.6296],\n",
-                            "          [-0.6507,  0.4468, -0.4465]],\n",
-                            "\n",
-                            "         [[ 1.0313,  0.7856, -0.2931],\n",
-                            "          [-0.6213,  0.5228,  0.7288],\n",
-                            "          [ 0.1397,  0.0216,  0.7518]],\n",
-                            "\n",
-                            "         [[ 0.0763, -0.3561, -0.0491],\n",
-                            "          [-0.5127,  0.2945,  0.5501],\n",
-                            "          [-0.2460, -0.0052,  0.0395]]]], grad_fn=<ThnnConv2DBackward>), scale=None, zero_point=None, bit_width=None, signed_t=None, training_t=tensor(True))"
-                        ]
-                    },
-                    "execution_count": 10,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
+            "outputs": [],
             "source": [
-                "out_tensor = return_quant_conv(torch.randn(1, 2, 5, 5))\n",
-                "out_tensor"
+                "from brevitas.proxy import WeightQuantProxyFromInjector\n",
+                "\n",
+                "class MyBinaryWeightQuantizer(MyBinaryQuantizer):\n",
+                "    proxy_class = WeightQuantProxyFromInjector"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Because we set `return_quant_tensor=True`, we get a `QuantTensor` as output object. However, we observe that `scale`, `zero_point` and `bit_width` of the output `QuantTensor` are set to `None`. This is expected since the output tensor is unquantized. In this case then the `QuantTensor` is really just acting as a wrapper around a `torch.Tensor`, and as such is market as non-valid."
+                "We can now use `MyBinaryWeightQuantizer` as the weight quantizer of a layer:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 11,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "False"
+                            "QuantTensor(value=tensor([[[[ 0.1000,  0.1000, -0.1000],\n",
+                            "          [-0.1000,  0.1000, -0.1000],\n",
+                            "          [ 0.1000, -0.1000, -0.1000]],\n",
+                            "\n",
+                            "         [[-0.1000,  0.1000, -0.1000],\n",
+                            "          [ 0.1000, -0.1000,  0.1000],\n",
+                            "          [-0.1000, -0.1000,  0.1000]],\n",
+                            "\n",
+                            "         [[ 0.1000, -0.1000, -0.1000],\n",
+                            "          [ 0.1000,  0.1000, -0.1000],\n",
+                            "          [-0.1000, -0.1000,  0.1000]]],\n",
+                            "\n",
+                            "\n",
+                            "        [[[ 0.1000, -0.1000,  0.1000],\n",
+                            "          [ 0.1000, -0.1000, -0.1000],\n",
+                            "          [ 0.1000, -0.1000,  0.1000]],\n",
+                            "\n",
+                            "         [[-0.1000,  0.1000, -0.1000],\n",
+                            "          [ 0.1000,  0.1000,  0.1000],\n",
+                            "          [-0.1000, -0.1000, -0.1000]],\n",
+                            "\n",
+                            "         [[ 0.1000,  0.1000, -0.1000],\n",
+                            "          [-0.1000,  0.1000, -0.1000],\n",
+                            "          [ 0.1000,  0.1000,  0.1000]]]], grad_fn=<MulBackward0>), scale=tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=None, training_t=tensor(True))"
                         ]
                     },
                     "execution_count": 11,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "out_tensor.is_valid"
+                "from brevitas.nn import QuantConv2d\n",
+                "\n",
+                "binary_weight_quant_conv = QuantConv2d(3, 2, (3,3), weight_quant=MyBinaryWeightQuantizer)\n",
+                "quant_weight = binary_weight_quant_conv.quant_weight()\n",
+                "quant_weight"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "`QuantTensor` implements `__torch_function__` to handle being called from torch functional operators (e.g. ops under `torch.nn.functional`). Passing a QuantTensor to supported ops that are invariant to quantization, e.g. max-pooling, preserve the the validity of a QuantTensor. Example:"
+                "Note however how the `QuantTensor` is not properly formed, as the `signed` attribute is `None`. This means that `quant_weight` is not considered valid, as the affine quantization invariant cannot be computed:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 12,
             "metadata": {},
-            "outputs": [
-                {
-                    "name": "stderr",
-                    "output_type": "stream",
-                    "text": [
-                        "C:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
-                        "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
-                    ]
-                },
-                {
-                    "data": {
-                        "text/plain": [
-                            "QuantTensor(value=tensor([[[[ 1.4362,  0.5809],\n",
-                            "          [ 1.0327,  1.9041]],\n",
-                            "\n",
-                            "         [[ 0.5809, -0.1452],\n",
-                            "          [ 1.3878,  2.0494]],\n",
-                            "\n",
-                            "         [[ 0.7100,  0.5809],\n",
-                            "          [ 1.1780,  0.9359]]]]), scale=tensor(0.0161), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))"
-                        ]
-                    },
-                    "execution_count": 12,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
+            "outputs": [],
             "source": [
-                "import torch\n",
-                "from brevitas.nn import QuantIdentity\n",
-                "\n",
-                "quant_identity = QuantIdentity(return_quant_tensor=True)\n",
-                "quant_tensor = quant_identity(torch.randn(1, 3, 4, 4))\n",
-                "torch.nn.functional.max_pool2d(quant_tensor, kernel_size=2, stride=2)"
+                "assert not quant_weight.is_valid"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "For ops that are not invariant to quantization, a `QuantTensor` decays into a floating-point `torch.Tensor`. Example:"
+                "`signed` is one of those attributes that in the case of binary quantization has to be explicitly defined by the user. The idea is that it informs the proxy on whether the value generated by our quantizer should be considered signed or not. We can do so by simply setting it in the quantizer:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 13,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "tensor([[[[-0.9023, -0.9424,  0.0805, -0.7936],\n",
-                            "          [ 0.8929, -0.1125,  0.5233, -0.1600],\n",
-                            "          [-0.0645,  0.7750,  0.7181, -0.4366],\n",
-                            "          [-0.7021,  0.1442,  0.9566,  0.3690]],\n",
-                            "\n",
-                            "         [[-0.0323, -0.6678, -0.4623, -0.3408],\n",
-                            "          [ 0.5233, -0.8216, -0.1442, -0.1913],\n",
-                            "          [-0.0805,  0.8827,  0.5350, -0.4495],\n",
-                            "          [-0.3550, -0.8676,  0.9347,  0.9674]],\n",
-                            "\n",
-                            "         [[-0.2221,  0.6107,  0.5233,  0.2676],\n",
-                            "          [-0.9684,  0.4873, -0.4873, -0.7021],\n",
-                            "          [-0.0161,  0.8268,  0.3829, -0.1913],\n",
-                            "          [-0.8216, -0.3120,  0.7333, -0.8163]]]])"
+                            "QuantTensor(value=tensor([[[[ 0.1000,  0.1000, -0.1000],\n",
+                            "          [-0.1000, -0.1000,  0.1000],\n",
+                            "          [ 0.1000,  0.1000, -0.1000]],\n",
+                            "\n",
+                            "         [[ 0.1000,  0.1000,  0.1000],\n",
+                            "          [ 0.1000, -0.1000,  0.1000],\n",
+                            "          [ 0.1000, -0.1000, -0.1000]],\n",
+                            "\n",
+                            "         [[-0.1000,  0.1000,  0.1000],\n",
+                            "          [ 0.1000, -0.1000, -0.1000],\n",
+                            "          [-0.1000, -0.1000, -0.1000]]],\n",
+                            "\n",
+                            "\n",
+                            "        [[[ 0.1000,  0.1000,  0.1000],\n",
+                            "          [ 0.1000,  0.1000, -0.1000],\n",
+                            "          [-0.1000, -0.1000,  0.1000]],\n",
+                            "\n",
+                            "         [[-0.1000, -0.1000,  0.1000],\n",
+                            "          [-0.1000,  0.1000,  0.1000],\n",
+                            "          [-0.1000, -0.1000, -0.1000]],\n",
+                            "\n",
+                            "         [[-0.1000,  0.1000, -0.1000],\n",
+                            "          [-0.1000,  0.1000, -0.1000],\n",
+                            "          [-0.1000,  0.1000, -0.1000]]]], grad_fn=<MulBackward0>), scale=tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
                     "execution_count": 13,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "torch.tanh(quant_tensor)"
+                "class MySignedBinaryWeightQuantizer(MyBinaryWeightQuantizer):\n",
+                "    signed = True\n",
+                "    \n",
+                "binary_weight_quant_conv = QuantConv2d(3, 2, (3,3), weight_quant=MySignedBinaryWeightQuantizer)\n",
+                "signed_quant_weight = binary_weight_quant_conv.quant_weight()\n",
+                "signed_quant_weight"
             ]
         },
         {
-            "cell_type": "markdown",
+            "cell_type": "code",
+            "execution_count": 15,
             "metadata": {},
+            "outputs": [],
             "source": [
-                "## Input Quantization\n",
-                "\n",
-                "We can obtain a valid output `QuantTensor` by making sure that both input and weight of `QuantConv2d` are quantized. To do so, we can set a quantizer for `input_quant`. In this example we pick a *signed 8-bit* quantizer with *per-tensor floating-point scale factor*:"
+                "assert signed_quant_weight.is_valid == True"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": 14,
+            "cell_type": "markdown",
             "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "QuantTensor(value=tensor([[[[-0.1760, -0.3239,  0.8647],\n",
-                            "          [ 0.2300, -0.9457, -0.5969],\n",
-                            "          [-0.1486,  0.2389, -0.1381]],\n",
-                            "\n",
-                            "         [[ 0.4634, -0.4049, -0.3049],\n",
-                            "          [-0.0643,  1.0154,  0.6058],\n",
-                            "          [-0.0367, -0.9156,  0.1461]],\n",
-                            "\n",
-                            "         [[ 0.7388,  0.6103,  0.8035],\n",
-                            "          [ 0.3011,  1.0519, -0.2473],\n",
-                            "          [-0.3113, -1.8302,  0.0223]]]], grad_fn=<ThnnConv2DBackward>), scale=tensor([[[[3.8195e-05]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(21.), signed_t=tensor(True), training_t=tensor(True))"
-                        ]
-                    },
-                    "execution_count": 14,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
             "source": [
-                "from brevitas.quant.scaled_int import Int8ActPerTensorFloat\n",
+                "And now the quant weights are valid.\n",
                 "\n",
-                "input_quant_conv = QuantConv2d(\n",
-                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False, \n",
-                "    input_quant=Int8ActPerTensorFloat, return_quant_tensor=True)\n",
-                "out_tensor = input_quant_conv(torch.randn(1, 2, 5, 5))\n",
-                "out_tensor"
+                "When we want to add or override an single attribute of a quantizer passed to a layer, defining a whole new quantizer can be too verbose. There is a simpler syntax to achieve the same goal. Let's say we want to have add the `signed` attribute to `MyBinaryQuantizer`, as we just did. We could have also simply done the following:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 15,
+            "execution_count": 16,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "True"
+                            "QuantTensor(value=tensor([[[[-0.1000, -0.1000,  0.1000],\n",
+                            "          [-0.1000, -0.1000, -0.1000],\n",
+                            "          [ 0.1000, -0.1000,  0.1000]],\n",
+                            "\n",
+                            "         [[-0.1000,  0.1000, -0.1000],\n",
+                            "          [-0.1000,  0.1000,  0.1000],\n",
+                            "          [ 0.1000, -0.1000, -0.1000]],\n",
+                            "\n",
+                            "         [[-0.1000,  0.1000, -0.1000],\n",
+                            "          [-0.1000, -0.1000,  0.1000],\n",
+                            "          [-0.1000, -0.1000, -0.1000]]],\n",
+                            "\n",
+                            "\n",
+                            "        [[[-0.1000, -0.1000, -0.1000],\n",
+                            "          [-0.1000, -0.1000, -0.1000],\n",
+                            "          [ 0.1000,  0.1000, -0.1000]],\n",
+                            "\n",
+                            "         [[-0.1000, -0.1000,  0.1000],\n",
+                            "          [-0.1000,  0.1000, -0.1000],\n",
+                            "          [ 0.1000, -0.1000,  0.1000]],\n",
+                            "\n",
+                            "         [[ 0.1000,  0.1000, -0.1000],\n",
+                            "          [ 0.1000,  0.1000,  0.1000],\n",
+                            "          [ 0.1000, -0.1000,  0.1000]]]], grad_fn=<MulBackward0>), scale=tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
-                    "execution_count": 15,
+                    "execution_count": 16,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "out_tensor.is_valid"
+                "small_scale_quant_conv = QuantConv2d(3, 2, (3,3), weight_quant=MyBinaryWeightQuantizer, weight_signed=True)\n",
+                "small_scale_quant_conv.quant_weight()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "What happens internally is that the input tensor passed to `input_quant_conv` is being quantized before being passed to the convolution operator. That means we are now computing a convolution between two quantized tensors, which mimplies that the output of the operation is also quantized. As expected then `out_tensor` is marked as valid. \n",
+                "What we did was to take the name of the attribute `signed`, add the prefix `weight_`, and pass it as a keyword argument to `QuantConv2d`. What happens in the background is that the keyword arguments prefixed with `weight_` are set as attributes of `weight_quant`, possibly overriding any pre-existing value. The same principle applies to `input_`, `output_` and `bias_`.\n",
                 "\n",
-                "Another important thing to notice is how the `bit_width` field of `out_tensor` is relatively high at *21 bits*. In Brevitas, the assumption is always that the output bit-width of an operator reflects the worst-case size of the *accumulator* required by that operation. In other terms, given the *size* of the input and weight tensors and their *bit-widths*, 21 is the bit-width that would be required to represent the largest possible output value that could be generated. This makes sure that the affine quantization invariant is always respected."
+                "This is the reason why, as it was mentioned in the first tutorial, quantized layers can accept arbitrary keyword arguments. It's really just a way to support different styles of syntax when defining a quantizer."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "We could have obtained a similar result by directly passing as input a QuantTensor. In this example we are directly defining a QuantTensor ourselves, but it could also be the output of a previous layer."
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 16,
-            "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "QuantTensor(value=tensor([[[[-1.1500e-02, -5.8000e-03, -9.3000e-03,  1.0000e-02,  3.5000e-03],\n",
-                            "          [-6.8000e-03,  1.1500e-02, -1.0600e-02, -1.5000e-03, -1.9000e-03],\n",
-                            "          [ 2.9000e-03,  9.5000e-03,  7.2000e-03, -3.7000e-03,  7.7000e-03],\n",
-                            "          [-2.4000e-03, -8.9000e-03, -1.2000e-02, -8.1000e-03,  7.2000e-03],\n",
-                            "          [-1.1300e-02, -9.7000e-03, -1.0000e-03,  1.0100e-02,  3.8000e-03]],\n",
-                            "\n",
-                            "         [[-1.1900e-02,  6.9000e-03,  8.3000e-03,  1.0000e-04, -6.9000e-03],\n",
-                            "          [ 3.9000e-03, -5.4000e-03,  1.1300e-02, -6.0000e-03,  9.7000e-03],\n",
-                            "          [ 0.0000e+00,  1.0900e-02, -1.0900e-02,  1.1400e-02, -6.4000e-03],\n",
-                            "          [ 9.2000e-03,  7.1000e-03, -6.0000e-04,  9.2000e-03, -8.5000e-03],\n",
-                            "          [ 5.0000e-03,  6.5000e-03, -8.3000e-03, -1.2000e-03,  7.4000e-03]]]]), scale=tensor(1.0000e-04), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))"
-                        ]
-                    },
-                    "execution_count": 16,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
-            "source": [
-                "from brevitas.quant_tensor import QuantTensor\n",
+                "## Passing a custom quantizer to QuantIdentity\n",
                 "\n",
-                "scale = 0.0001\n",
-                "bit_width = 8\n",
-                "zero_point = 0.\n",
-                "int_value = torch.randint(low=- 2 ** (bit_width - 1), high=2 ** (bit_width - 1) - 1, size=(1, 2, 5, 5))\n",
-                "quant_value = (int_value - zero_point) * scale\n",
-                "quant_tensor_input = QuantTensor(\n",
-                "    quant_value, \n",
-                "    scale=torch.tensor(scale), \n",
-                "    zero_point=torch.tensor(zero_point), \n",
-                "    bit_width=torch.tensor(float(bit_width)),\n",
-                "    signed=True,\n",
-                "    training=True)\n",
-                "quant_tensor_input"
+                "We can do a similar thing with quantized activations:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 17,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "True"
+                            "QuantTensor(value=tensor([[-0.1000,  0.1000, -0.1000,  0.1000],\n",
+                            "        [ 0.1000,  0.1000,  0.1000,  0.1000],\n",
+                            "        [-0.1000,  0.1000,  0.1000,  0.1000],\n",
+                            "        [-0.1000, -0.1000,  0.1000, -0.1000]], grad_fn=<MulBackward0>), scale=tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
                     "execution_count": 17,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "quant_tensor_input.is_valid"
+                "from brevitas.proxy import ActQuantProxyFromInjector\n",
+                "from brevitas.nn import QuantIdentity\n",
+                "\n",
+                "class MySignedBinaryActQuantizer(MyBinaryQuantizer):\n",
+                "    proxy_class = ActQuantProxyFromInjector\n",
+                "    signed = True\n",
+                "\n",
+                "binary_relu = QuantIdentity(act_quant=MySignedBinaryActQuantizer, return_quant_tensor=True)\n",
+                "binary_relu(torch.randn(4, 4))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "**Note**: how we are explicitly forcing `value`, `scale`, `zero_point` and `bit_width` to be floating-point `torch.Tensor`, as this is expected by Brevitas but it's currently not enforced automatically at initialization time.\n",
-                "\n",
-                "If we now pass in `quant_tensor_input` to `return_quant_conv`, we will see that indeed the output is a valid 21-bit `QuantTensor`:"
+                "So there isn't really much difference between a quantizer for weights and a quantizer for activations, they are just wrapped by different proxies. Also, with activations  a prefix is not required when passing keyword arguments. For example, when can override the existing `scaling_init` defined in `MyBinaryQuantizer` with a new value passed in as a keywork argument:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 18,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[ 0.0002, -0.0017, -0.0038],\n",
-                            "          [-0.0020,  0.0038,  0.0001],\n",
-                            "          [ 0.0022,  0.0034, -0.0024]],\n",
-                            "\n",
-                            "         [[-0.0010, -0.0078,  0.0010],\n",
-                            "          [ 0.0002,  0.0014,  0.0059],\n",
-                            "          [ 0.0050, -0.0020, -0.0069]],\n",
-                            "\n",
-                            "         [[ 0.0006, -0.0071, -0.0012],\n",
-                            "          [-0.0011,  0.0057,  0.0055],\n",
-                            "          [-0.0003,  0.0040, -0.0019]]]], grad_fn=<ThnnConv2DBackward>), scale=tensor([[[[1.7899e-07]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(21.), signed_t=tensor(True), training_t=tensor(True))"
+                            "QuantTensor(value=tensor([[ 0.0010,  0.0010,  0.0010, -0.0010],\n",
+                            "        [ 0.0010, -0.0010,  0.0010, -0.0010],\n",
+                            "        [-0.0010, -0.0010, -0.0010, -0.0010],\n",
+                            "        [ 0.0010,  0.0010,  0.0010,  0.0010]], grad_fn=<MulBackward0>), scale=tensor(0.0010, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
                     "execution_count": 18,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "out_tensor = return_quant_conv(quant_tensor_input)\n",
-                "out_tensor"
+                "small_scale_binary_identity = QuantIdentity(\n",
+                "    act_quant=MySignedBinaryActQuantizer, scaling_init=0.001, return_quant_tensor=True)\n",
+                "small_scale_binary_identity(torch.randn(4, 4))"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## A Custom Quantizer initialized with Weight Statistics\n",
+                "\n",
+                "So far we have seen use-cases where an `ExtendedInjector` provides, at best, a different kind of syntax to define a quantizer, without any particular other advantage. Let's now make things a bit more complicated to show the sort of situations where it really shines.\n",
+                "\n",
+                "Let's say we want to define a binary weight quantizer where `scaling_impl` is still `ParameterScaling`. However, instead of being user-defined, we want `scaling_init` to be the maximum value found in the weight tensor of the quantized layer.\n",
+                "To support this sort of use cases where the quantizer depends on the layer, a quantized layer automatically passes itself to all its quantizers under the name of `module`.\n",
+                "With only a few lines of code then, we can achieve our goal:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 19,
             "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "True"
-                        ]
-                    },
-                    "execution_count": 19,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
+            "outputs": [],
             "source": [
-                "out_tensor.is_valid"
+                "from brevitas.inject import value\n",
+                "\n",
+                "class ParamFromMaxWeightQuantizer(MySignedBinaryWeightQuantizer):\n",
+                "    \n",
+                "    @value\n",
+                "    def scaling_init(module):\n",
+                "        return module.weight.abs().max()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "We can also pass in an input `QuantTensor` to a layer that has `input_quant` enabled. In that case, the input gets re-quantized:"
+                "Note how we are leveraging the `@value` *decorator* to define a function that is executed at *dependency-injection (DI) time*. This kind of behaviour is similar in spirit to defining a `@property` instead of an *attribute*, with the difference that a `@value` function can depend on other attributes of the Injector, which are automatically passed in as arguments of the function during DI.\n",
+                "\n",
+                "Let's now pass the quantizer to a QuantConv2d and retrieve its quantized weights:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 20,
-            "metadata": {},
+            "metadata": {
+                "scrolled": true
+            },
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[ 0.0045, -0.0035, -0.0022],\n",
-                            "          [ 0.0027,  0.0001,  0.0051],\n",
-                            "          [ 0.0030, -0.0050, -0.0006]],\n",
-                            "\n",
-                            "         [[-0.0079,  0.0035,  0.0032],\n",
-                            "          [-0.0026, -0.0034, -0.0005],\n",
-                            "          [ 0.0022,  0.0095, -0.0014]],\n",
-                            "\n",
-                            "         [[-0.0030, -0.0045,  0.0031],\n",
-                            "          [ 0.0038,  0.0023,  0.0054],\n",
-                            "          [ 0.0094,  0.0156, -0.0074]]]], grad_fn=<ThnnConv2DBackward>), scale=tensor([[[[1.7393e-07]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(21.), signed_t=tensor(True), training_t=tensor(True))"
+                            "QuantTensor(value=tensor([[[[-0.1876, -0.1876, -0.1876],\n",
+                            "          [ 0.1876,  0.1876,  0.1876],\n",
+                            "          [-0.1876, -0.1876,  0.1876]],\n",
+                            "\n",
+                            "         [[-0.1876, -0.1876,  0.1876],\n",
+                            "          [ 0.1876,  0.1876, -0.1876],\n",
+                            "          [-0.1876,  0.1876,  0.1876]],\n",
+                            "\n",
+                            "         [[-0.1876, -0.1876, -0.1876],\n",
+                            "          [ 0.1876,  0.1876,  0.1876],\n",
+                            "          [-0.1876,  0.1876, -0.1876]]],\n",
+                            "\n",
+                            "\n",
+                            "        [[[-0.1876, -0.1876, -0.1876],\n",
+                            "          [ 0.1876,  0.1876, -0.1876],\n",
+                            "          [ 0.1876, -0.1876, -0.1876]],\n",
+                            "\n",
+                            "         [[-0.1876,  0.1876, -0.1876],\n",
+                            "          [ 0.1876, -0.1876, -0.1876],\n",
+                            "          [-0.1876, -0.1876,  0.1876]],\n",
+                            "\n",
+                            "         [[-0.1876,  0.1876,  0.1876],\n",
+                            "          [ 0.1876, -0.1876,  0.1876],\n",
+                            "          [-0.1876, -0.1876, -0.1876]]]], grad_fn=<MulBackward0>), scale=tensor(0.1876, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
                     "execution_count": 20,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "input_quant_conv(quant_tensor_input)"
+                "param_from_max_quant_conv = QuantConv2d(3, 2, (3, 3), weight_quant=ParamFromMaxWeightQuantizer)\n",
+                "param_from_max_quant_conv.quant_weight()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "## Output Quantization\n",
-                "\n",
-                "Let's now look at would have happened if we instead enabled output quantization:"
+                "Indeed we can verify that `quant_weight_scale()` is equal to `weight.abs().max()`:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 21,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[ 0.1127,  0.7247,  0.4509],\n",
-                            "          [ 0.0805, -0.5153, -0.7569],\n",
-                            "          [-0.0483, -0.9662, -0.1610]],\n",
-                            "\n",
-                            "         [[ 0.3060,  0.7730, -0.3704],\n",
-                            "          [ 0.1771,  0.5153, -0.0805],\n",
-                            "          [ 0.6925, -0.4831,  1.1272]],\n",
-                            "\n",
-                            "         [[ 2.0451, -0.8535,  0.1932],\n",
-                            "          [ 0.1610,  1.2239, -0.4670],\n",
-                            "          [-0.7086,  0.6441, -0.9984]]]], grad_fn=<MulBackward0>), scale=tensor(0.0161, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))"
+                            "True"
                         ]
                     },
                     "execution_count": 21,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "from brevitas.quant.scaled_int import Int8ActPerTensorFloat\n",
-                "\n",
-                "output_quant_conv = QuantConv2d(\n",
-                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False, \n",
-                "    output_quant=Int8ActPerTensorFloat, return_quant_tensor=True)\n",
-                "out_tensor = output_quant_conv(torch.randn(1, 2, 5, 5))\n",
-                "out_tensor"
+                "(param_from_max_quant_conv.quant_weight_scale() == param_from_max_quant_conv.weight.abs().max()).item()"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "Let's say now that we want to load a pretrained floating-point weight tensor on top of our quantized model. We simuate this scenario by defining a separate `nn.Conv2d` layer with the same weight shape:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 22,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "True"
+                            "tensor(0.1897, grad_fn=<MaxBackward1>)"
                         ]
                     },
                     "execution_count": 22,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "out_tensor.is_valid"
+                "from torch import nn\n",
+                "\n",
+                "float_conv = nn.Conv2d(3, 2, (3, 3))\n",
+                "float_conv.weight.abs().max()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "We can see again that the output is a valid `QuantTensor`. However, what happened internally is quite different from before.  \n",
-                "Previously, we computed the convolution between two quantized tensors, and got a quantized tensor as output.  \n",
-                "In this case instead, we compute the convolution between a quantized and an unquantized tensor, we take its unquantized output and we quantize it.  \n",
-                "The difference is obvious once we look at the output `bit_width`. In the previous case, we had that the `bit_width` reflected the size of the output accumulator. In this case instead, we have `bit_width=tensor(8.)`, which is what we expected since `output_quant` had been set to an *Int8* quantizer."
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "## Bias Quantization\n",
-                "\n",
-                "There is an important scenario where the various options we just saw make a practical difference, and it's quantization of *bias*. In many contexts, such as in the ONNX standard opset and in FINN, bias is assumed to be quantized with scale factor equal to *input scale * weight scale*, which means that we need a valid quantized input somehow. A predefined bias quantizer that reflects that assumption is `brevitas.quant.scaled_int.Int8Bias`. If we simply tried to set it to a `QuantConv2d` without any sort of input quantization, we would get an error:"
+                "and then we load it on top of `param_from_max_quant_conv`:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 23,
             "metadata": {
                 "tags": [
                     "raises-exception"
                 ]
             },
             "outputs": [
                 {
                     "ename": "RuntimeError",
-                    "evalue": "Input scale required",
+                    "evalue": "Error(s) in loading state_dict for QuantConv2d:\n\tMissing key(s) in state_dict: \"weight_quant.tensor_quant.scaling_impl.value\". ",
                     "output_type": "error",
                     "traceback": [
                         "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                         "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
-                        "\u001b[1;32m<ipython-input-23-301e46b56837>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0min_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     bias_quant=Int8Bias, return_quant_tensor=True)\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mbias_quant_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
-                        "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\nn\\quant_conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquant_weight\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquant_bias\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\nn\\quant_layer.py\u001b[0m in \u001b[0;36mforward_impl\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mquant_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_quant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_bit_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_inference_quant_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cached_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_CachedIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquant_bias\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\proxy\\parameter_quant.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, input_scale, input_bit_width)\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mimpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_handler\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_mode\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_quant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_input_scale\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_scale\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input scale required\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_input_bit_width\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_bit_width\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input bit-width required\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;31mRuntimeError\u001b[0m: Input scale required"
+                        "\u001b[1;32m<ipython-input-22-5b3646241211>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparam_from_max_quant_conv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat_conv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
+                        "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m-> 1407\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m   1408\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
+                        "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for QuantConv2d:\n\tMissing key(s) in state_dict: \"weight_quant.tensor_quant.scaling_impl.value\". "
                     ]
                 }
             ],
             "source": [
-                "from brevitas.quant.scaled_int import Int8Bias\n",
-                "\n",
-                "bias_quant_conv = QuantConv2d(\n",
-                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
-                "    bias_quant=Int8Bias, return_quant_tensor=True)\n",
-                "bias_quant_conv(torch.randn(1, 2, 5, 5))"
+                "param_from_max_quant_conv.load_state_dict(float_conv.state_dict())"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "We can solve the issue by passing in a valid `QuantTensor`, e.g. the `quant_tensor_input`  we defined above:"
+                "Ouch, we get an error. This is because `ParameterScaling` contains a learned `torch.nn.Parameter`, and Pytorch expects all learned parameters of a model to be contained in a `state_dict` that is being loaded.\n",
+                "We can work around the issue by either setting the `IGNORE_MISSING_KEYS` config flag in Brevitas, or by passing `strict=False` to load_state_dict. We go with the former as setting `strict=False` is too forgiving to other kind of problems:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 24,
+            "execution_count": 23,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[-0.0040, -0.0009,  0.0006],\n",
-                            "          [ 0.0053, -0.0011,  0.0011],\n",
-                            "          [ 0.0043,  0.0068,  0.0016]],\n",
-                            "\n",
-                            "         [[ 0.0010,  0.0020,  0.0037],\n",
-                            "          [-0.0032, -0.0055,  0.0034],\n",
-                            "          [ 0.0078, -0.0071,  0.0038]],\n",
-                            "\n",
-                            "         [[ 0.0012, -0.0061,  0.0010],\n",
-                            "          [ 0.0008,  0.0001, -0.0060],\n",
-                            "          [ 0.0030, -0.0052,  0.0061]]]], grad_fn=<ThnnConv2DBackward>), scale=tensor([[[[1.8108e-07]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(22.), signed_t=tensor(True), training_t=tensor(True))"
+                            "<All keys matched successfully>"
                         ]
                     },
-                    "execution_count": 24,
+                    "execution_count": 23,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "bias_quant_conv(quant_tensor_input)"
+                "from brevitas import config\n",
+                "config.IGNORE_MISSING_KEYS = True\n",
+                "\n",
+                "param_from_max_quant_conv.load_state_dict(float_conv.state_dict())"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Or by enabling input quantization and then passing in a float a `torch.Tensor` or a `QuantTensor`:"
+                "Note that we could have also achieve the same goal by setting the *env variable* `BREVITAS_IGNORE_MISSING_KEYS=1`.\n",
+                "\n",
+                "And now if we take a look at the quantized weights again:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 25,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[-1.1932, -0.3228, -0.4671],\n",
-                            "          [ 0.0132,  0.0988,  0.2729],\n",
-                            "          [-0.8529,  0.4697, -0.5951]],\n",
-                            "\n",
-                            "         [[-0.6967,  0.4200,  0.3516],\n",
-                            "          [ 0.1296, -0.1609, -0.0758],\n",
-                            "          [ 1.2978,  0.0923, -0.1931]],\n",
-                            "\n",
-                            "         [[-0.6142,  0.8017, -0.1383],\n",
-                            "          [-0.7863,  0.1125, -0.2210],\n",
-                            "          [ 0.3591, -0.5942, -0.1165]]]], grad_fn=<ThnnConv2DBackward>), scale=tensor([[[[4.9213e-05]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(22.), signed_t=tensor(True), training_t=tensor(True))"
+                            "QuantTensor(value=tensor([[[[ 0.1897, -0.1897,  0.1897],\n",
+                            "          [-0.1897,  0.1897, -0.1897],\n",
+                            "          [-0.1897,  0.1897, -0.1897]],\n",
+                            "\n",
+                            "         [[-0.1897,  0.1897,  0.1897],\n",
+                            "          [ 0.1897, -0.1897, -0.1897],\n",
+                            "          [ 0.1897, -0.1897,  0.1897]],\n",
+                            "\n",
+                            "         [[-0.1897,  0.1897, -0.1897],\n",
+                            "          [-0.1897,  0.1897,  0.1897],\n",
+                            "          [-0.1897,  0.1897,  0.1897]]],\n",
+                            "\n",
+                            "\n",
+                            "        [[[ 0.1897,  0.1897,  0.1897],\n",
+                            "          [-0.1897,  0.1897, -0.1897],\n",
+                            "          [ 0.1897,  0.1897, -0.1897]],\n",
+                            "\n",
+                            "         [[ 0.1897, -0.1897, -0.1897],\n",
+                            "          [ 0.1897,  0.1897, -0.1897],\n",
+                            "          [ 0.1897,  0.1897,  0.1897]],\n",
+                            "\n",
+                            "         [[-0.1897,  0.1897, -0.1897],\n",
+                            "          [-0.1897,  0.1897, -0.1897],\n",
+                            "          [ 0.1897,  0.1897,  0.1897]]]], grad_fn=<MulBackward0>), scale=tensor(0.1897, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
                     "execution_count": 25,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "input_bias_quant_conv = QuantConv2d(\n",
-                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
-                "    input_quant=Int8ActPerTensorFloat, bias_quant=Int8Bias, return_quant_tensor=True)\n",
-                "input_bias_quant_conv(torch.randn(1, 2, 5, 5))"
+                "param_from_max_quant_conv.quant_weight()"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "We see that, as expected, the scale factor has been updated to the new `weight.abs().max()`.\n",
+                "\n",
+                "What happens internally is that after `load_state_dict` is called on the layer, `ParamFromMaxWeightQuantizer.tensor_quant` gets called again to re-initialize `BinaryQuant`, and in turn `ParameterScaling` is re-initialized with a new `scaling_init` value computed based on the updated `module.weight` tensor. This whole process wouldn't have been possible without an `ExtendedInjector` behind it. "
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## Sharing a Quantizer\n",
+                "\n",
+                "There are two ways to share a quantizer between multiple layers, with importance differences.\n",
+                "\n",
+                "The first one, which we have seen so far, is to simply pass the same ExtendedInjector to multiple layers. What that does is sharing the same quantization strategy among different layers. Each layer still gets its own instance of the quantization implementation."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 26,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[-5.1220e-03,  5.3110e-03, -1.7256e-03],\n",
-                            "          [-3.5191e-03, -4.1349e-03,  1.1166e-03],\n",
-                            "          [-2.6435e-03, -4.1546e-03,  7.3403e-03]],\n",
-                            "\n",
-                            "         [[ 6.7945e-07, -6.7448e-03,  6.8607e-03],\n",
-                            "          [ 2.3195e-04, -2.5737e-03, -5.3764e-03],\n",
-                            "          [ 2.7155e-03,  5.5203e-03, -1.0607e-03]],\n",
-                            "\n",
-                            "         [[ 4.6902e-03, -6.1470e-03,  5.9844e-03],\n",
-                            "          [ 3.4126e-03, -6.6340e-03,  5.9087e-03],\n",
-                            "          [-4.5733e-03,  3.8523e-03, -3.5290e-03]]]],\n",
-                            "       grad_fn=<ThnnConv2DBackward>), scale=tensor([[[[1.6992e-07]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(22.), signed_t=tensor(True), training_t=tensor(True))"
+                            "False"
                         ]
                     },
-                    "execution_count": 26,
+                    "execution_count": 25,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "input_bias_quant_conv(quant_tensor_input)"
+                "quant_conv1 = QuantConv2d(3, 2, (3, 3), weight_quant=MySignedBinaryWeightQuantizer)\n",
+                "quant_conv2 = QuantConv2d(3, 2, (3, 3), weight_quant=MySignedBinaryWeightQuantizer)\n",
+                "\n",
+                "quant_conv1.weight_quant is quant_conv2.weight_quant"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Notice how the output `bit_width=tensor(22.)`. This is because, in the worst-case, summing a *21-bit* integer (the size of the accumulator before bias is added) and an *8-bit* integer (the size of quantized bias) gives a *22-bit* integer.\n",
+                "### Sharing a proxy\n",
+                "\n",
+                "The second one, which we are introducing now, allows to share the same quantization instance among multiple layers. This is done by simply sharing the proxy wrapping it. This can be useful in those scenarios where, for example, we want different layers to share the same scale factor. The syntax goes as follows:"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": 26,
+            "metadata": {},
+            "outputs": [
+                {
+                    "data": {
+                        "text/plain": [
+                            "True"
+                        ]
+                    },
+                    "execution_count": 26,
+                    "metadata": {},
+                    "output_type": "execute_result"
+                }
+            ],
+            "source": [
+                "quant_conv1 = QuantConv2d(3, 2, (3, 3), weight_quant=MySignedBinaryWeightQuantizer)\n",
+                "quant_conv2 = QuantConv2d(3, 2, (3, 3), weight_quant=quant_conv1.weight_quant)\n",
                 "\n",
-                "Let's try now to enable output quantization instead of input quantization. That wouldn't have solved the problem with bias quantization, as output quantization is performed after bias is added:"
+                "assert quant_conv1.weight_quant is quant_conv2.weight_quant"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 27,
-            "metadata": {
-                "tags": [
-                    "raises-exception"
-                ]
-            },
+            "metadata": {},
             "outputs": [
                 {
-                    "ename": "RuntimeError",
-                    "evalue": "Input scale required",
+                    "ename": "AssertionError",
+                    "evalue": "",
                     "output_type": "error",
                     "traceback": [
-                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
-                        "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
-                        "\u001b[1;32m<ipython-input-27-7b506e3334dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0min_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     output_quant=Int8ActPerTensorFloat, bias_quant=Int8Bias, return_quant_tensor=True)\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0moutput_bias_quant_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
-                        "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\nn\\quant_conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquant_weight\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquant_bias\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\nn\\quant_layer.py\u001b[0m in \u001b[0;36mforward_impl\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mquant_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_quant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_bit_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_inference_quant_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cached_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_CachedIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquant_bias\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\proxy\\parameter_quant.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, input_scale, input_bit_width)\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mimpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_handler\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_mode\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_quant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_input_scale\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_scale\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input scale required\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_input_bit_width\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_bit_width\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input bit-width required\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;31mRuntimeError\u001b[0m: Input scale required"
+                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+                        "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
+                        "\u001b[0;32m/tmp/ipykernel_58415/1066539094.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquant_conv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_weight_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mquant_conv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_weight_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
+                        "\u001b[0;31mAssertionError\u001b[0m: "
                     ]
                 }
             ],
             "source": [
-                "output_bias_quant_conv = QuantConv2d(\n",
-                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
-                "    output_quant=Int8ActPerTensorFloat, bias_quant=Int8Bias, return_quant_tensor=True)\n",
-                "output_bias_quant_conv(torch.randn(1, 2, 5, 5))"
+                "assert (quant_conv1.quant_weight_scale() == quant_conv2.quant_weight_scale()).item()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Not all scenarios require bias quantization to depend on the scale factor of the input. In those cases, biases can be quantized the same way weights are quantized, and have their own scale factor. In Brevitas, a predefined quantizer that reflects this other scenario is `Int8BiasPerTensorFloatInternalScaling`. In this case then a valid quantized input is not required:"
+                "What happens in background is that the weight quantizer now has access to both `quant_conv1` and `quant_conv2`. \n",
+                "So let's say we want to build a quantizer similar to `ParamFromMaxWeightQuantizer`, but in this case we want the scale factor to be initialized with the average of both weight tensors. When a quantizer has access to multiple parent modules, they are passed in at dependency injection time as a *tuple* under the same name `module` as before. So we can do the following:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 28,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[ 3.9541e-01,  1.0925e-01,  5.7339e-01],\n",
-                            "          [ 2.8455e-01,  2.5993e-01, -6.4358e-01],\n",
-                            "          [-1.5946e-02, -2.6222e-01,  1.1759e+00]],\n",
-                            "\n",
-                            "         [[ 2.9224e-01, -7.9820e-01,  7.9893e-01],\n",
-                            "          [-1.2683e-01, -5.2149e-01,  5.0705e-01],\n",
-                            "          [-1.2161e+00, -3.3960e-01,  2.7555e-03]],\n",
-                            "\n",
-                            "         [[-5.0872e-02, -2.2190e-01,  4.6538e-04],\n",
-                            "          [-7.2407e-02,  6.3366e-04, -7.2510e-01],\n",
-                            "          [-2.2703e-02, -7.2701e-01,  8.6453e-02]]]],\n",
-                            "       grad_fn=<ThnnConv2DBackward>), scale=None, zero_point=None, bit_width=None, signed_t=None, training_t=tensor(True))"
+                            "False"
                         ]
                     },
                     "execution_count": 28,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "from brevitas.quant.scaled_int import Int8BiasPerTensorFloatInternalScaling\n",
+                "class SharedParamFromMeanWeightQuantizer(MySignedBinaryWeightQuantizer):\n",
+                "    \n",
+                "    @value\n",
+                "    def scaling_init(module):\n",
+                "        if isinstance(module, tuple):\n",
+                "            return torch.cat((module[0].weight.view(-1), module[1].weight.view(-1))).abs().mean()\n",
+                "        else:\n",
+                "            return module.weight.abs().mean()\n",
+                "        \n",
+                "quant_conv1 = QuantConv2d(3, 2, (3, 3), weight_quant=SharedParamFromMeanWeightQuantizer)\n",
+                "old_quant_conv1_scale = quant_conv1.quant_weight_scale()\n",
+                "quant_conv2 = QuantConv2d(3, 2, (3, 3), weight_quant=quant_conv1.weight_quant)\n",
+                "new_quant_conv1_scale = quant_conv1.quant_weight_scale()\n",
+                "\n",
+                "assert not (old_quant_conv1_scale == new_quant_conv1_scale).item()"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": 31,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "assert (new_quant_conv1_scale == quant_conv2.quant_weight_scale()).item()"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "Note how, when `quant_conv2` is initialized using the `weight_quant` of `quant_conv1`, weight quantization is re-initialized for both layers such that they end up having the same scale.\n",
+                "\n",
+                "We can see in this example how Brevitas works consistently with Pytorch's eager execution model. When we initialize `quant_conv1` we still don't know that its weight quantizer is going to be shared with `quant_conv2`, and the semantics of Pytorch impose that `quant_conv1` should work correctly both before and after `quant_conv2` is declared. The way we take advantage of dependency injection allows to do so."
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "### Sharing an instance of Activation Quantization\n",
+                "\n",
+                "Sharing an instance of activation quantization is easier because for most scenarios it's enough to simply share the whole layer itself, e.g. calling the same `QuantReLU` from multiple places in the forward pass.\n",
+                "\n",
+                "For those scenarios where sharing the whole layer is not possible, there is something important to keep in mind. Instances of activation quantization include (for performance reasons) the implementation of the non-linear activation itself (if any). So, for example, using a `QuantReLU.act_quant` to initialize a `QuantConv2d.output_quant` should be avoided as we would not share not only the quantizer, but also the relu activation function.  \n",
+                "In general then sharing of instances of activations quantization should be done only between activations of the same *kind*. \n",
+                "\n",
+                "*Note*: we say kind and not type because `input_quant`, `output_quant` and `IdentityQuant` count as being the same kind of activation, even though they belong to different type of layers."
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "## Dealing with Weight Initialization\n",
+                "\n",
+                "There is a type of situation that Brevitas cannot deal with automatically. That is, when the initialization of the quantizer depends on the layer to which it is applied (like with the `ParamFromMaxWeightQuantizer` or `SharedParamFromMeanWeightQuantizer` quantizers), but the layer gets modified after it is initialized. \n",
+                "\n",
+                "The typical example is with weight initialization when training from scratch (so rather than loading from a floating-point state_dict):"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": 32,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "quant_conv_w_init = QuantConv2d(3, 2, (3, 3), weight_quant=ParamFromMaxWeightQuantizer)\n",
+                "torch.nn.init.uniform_(quant_conv_w_init.weight)\n",
+                "\n",
+                "assert not (quant_conv_w_init.weight.abs().max() == quant_conv_w_init.quant_weight_scale()).item()"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "We can see how the scale factor is not initialized correctly anymore. In this case we can simply trigger re-initialization of the weight quantizer manually:"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": 33,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "quant_conv_w_init.weight_quant.init_tensor_quant()\n",
                 "\n",
-                "bias_internal_scale_quant_conv = QuantConv2d(\n",
-                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
-                "    bias_quant=Int8BiasPerTensorFloatInternalScaling, return_quant_tensor=True)\n",
-                "bias_internal_scale_quant_conv(torch.randn(1, 2, 5, 5))"
+                "assert (quant_conv_w_init.weight.abs().max() == quant_conv_w_init.quant_weight_scale()).item()"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "*Note*: because the way weights are initialized is often the same as to how an optimizer performs the weight update step, there are currently no plans to try to perform re-initialization automatically (as it happens e.g. when a `state_dict` is loaded) since it wouldn't be possible to distinguish between the two scenarios."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "There are a couple of situations to be aware of concerning bias quantization that can lead to changes in the output `zero_point`.\n",
+                "## Building a Custom Quantization API\n",
+                "\n",
+                "Finally, let's go through an even more complicated example. We are going to look at a scenario that illustrates the differences between a standard `Injector` (implemented in the dependencies library) and our `ExtendedInjector` extension.\n",
                 "\n",
-                "Let's consider the scenario where we compute the convolution between a quantized input tensor and quantized weights. In the first case, we then add an *unquantized* bias on top of the output. In the second one, we add a bias quantized with its own scale factor, e.g. with the `Int8BiasPerTensorFloatInternalScaling` quantizer. In both cases, in order to make sure the output `QuantTensor` is valid (i.e. the affine quantization invariant is respected), the output `zero_point` becomes non-zero:"
+                "Let's say we want to build two quantizers for respectively weights and activations and build a simple API on top of them.\n",
+                "In particular, we want to be able to switch between `BinaryQuant` and `ClampedBinaryQuant` (a variant of binary quantization with clamping), and we want to optionally perform *per-channel scaling*.\n",
+                "To do so, we are going to implement the controlling logic through a hierarchy of ExtendedInjector, leaving two boolean flags exposed as arguments of the quantizers, with the idea then that the flags can be set through keyword arguments of the respective quantized layers.\n",
+                "\n",
+                "We can go as follows:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 29,
+            "execution_count": 32,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "from brevitas.core.quant import ClampedBinaryQuant\n",
+                "from brevitas.proxy import WeightQuantProxyFromInjector, ActQuantProxyFromInjector\n",
+                "from brevitas.inject import this\n",
+                "\n",
+                "\n",
+                "class CommonQuantizer(ExtendedInjector):\n",
+                "    scaling_impl = ParameterScaling\n",
+                "    signed=True\n",
+                "    \n",
+                "    @value\n",
+                "    def tensor_quant(is_clamped):\n",
+                "        # returning a class to auto-wire from a value function\n",
+                "        # wouldn't be allowed in a standard Injector\n",
+                "        if is_clamped:\n",
+                "            return ClampedBinaryQuant\n",
+                "        else:\n",
+                "            return BinaryQuant\n",
+                "    \n",
+                "    @value\n",
+                "    def scaling_shape(scaling_per_output_channel):\n",
+                "        if scaling_per_output_channel:\n",
+                "            # returning this.something from a value function \n",
+                "            # wouldn't be allowed in a standard Injector\n",
+                "            return this.per_channel_broadcastable_shape\n",
+                "        else:\n",
+                "            return ()\n",
+                "        \n",
+                "        \n",
+                "class AdvancedWeightQuantizer(CommonQuantizer):\n",
+                "    proxy_class = WeightQuantProxyFromInjector\n",
+                "        \n",
+                "    @value\n",
+                "    def per_channel_broadcastable_shape(module):\n",
+                "        return (module.weight.shape[0], 1, 1, 1)\n",
+                "    \n",
+                "    @value\n",
+                "    def scaling_init(module, scaling_per_output_channel):\n",
+                "        if scaling_per_output_channel:\n",
+                "            num_ch = module.weight.shape[0]\n",
+                "            return module.weight.abs().view(num_ch, -1).max(dim=1)[0].view(-1, 1, 1, 1)\n",
+                "        else:\n",
+                "            return module.weight.abs().max()\n",
+                "        \n",
+                "        \n",
+                "class AdvancedActQuantizer(CommonQuantizer):\n",
+                "    scaling_init = 0.01\n",
+                "    proxy_class = ActQuantProxyFromInjector"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "There are a bunch of things going on here to unpack.  \n",
+                "\n",
+                "The first one is that a `@value` function can return a class to auto-wire and inject, as seen in the definition of `tensor_quant`. This wouldn't normally be possible with a standard `Injector`, but it's possible with an `ExtendedInjector`. This way we can switch between different implementations of `tensor_quant`.\n",
+                "\n",
+                "The second one is the special object `this`. `this` is already present in the *dependencies* library, and it's used as a way to retrieve attributes of the quantizer from within the quantizer itself. However, normally it wouldn't be possible to return a reference to `this` from a `@value` function. Again this is something that only a `ExtendedInjector` supports, and it allows to chain different attributes in a way such that the chained values are computed only when necessary. \n",
+                "\n",
+                "Let's see the quantizers applied to a layer:"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": 35,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[-0.2864,  0.0194, -0.5835],\n",
-                            "          [ 0.0089, -0.4669,  0.1854],\n",
-                            "          [-0.6356, -0.1184, -0.4888]],\n",
+                            "QuantTensor(value=tensor([[[[-0.1842,  0.1842, -0.1842],\n",
+                            "          [-0.1842, -0.1842,  0.1842],\n",
+                            "          [-0.1842, -0.1842,  0.1842]],\n",
                             "\n",
-                            "         [[-0.4935,  0.1282, -0.0513],\n",
-                            "          [ 0.2893, -0.4362, -0.8152],\n",
-                            "          [ 0.4719, -0.1783,  0.3152]],\n",
+                            "         [[-0.1842, -0.1842,  0.1842],\n",
+                            "          [ 0.1842, -0.1842,  0.1842],\n",
+                            "          [ 0.1842,  0.1842, -0.1842]],\n",
                             "\n",
-                            "         [[ 0.1789, -0.2934,  0.2483],\n",
-                            "          [ 0.3184, -0.5840, -0.3555],\n",
-                            "          [-0.5761, -0.1768, -0.8496]]]], grad_fn=<ThnnConv2DBackward>), scale=tensor([[[[3.5363e-05]]]], grad_fn=<MulBackward0>), zero_point=tensor([[[[6285.0532]],\n",
+                            "         [[-0.1842, -0.1842,  0.1842],\n",
+                            "          [ 0.1842,  0.1842,  0.1842],\n",
+                            "          [-0.1842,  0.1842, -0.1842]]],\n",
                             "\n",
-                            "         [[ 936.8170]],\n",
                             "\n",
-                            "         [[4303.7090]]]], grad_fn=<DivBackward0>), bit_width=tensor(21.), signed_t=tensor(True), training_t=tensor(True))"
+                            "        [[[ 0.1838,  0.1838,  0.1838],\n",
+                            "          [-0.1838, -0.1838, -0.1838],\n",
+                            "          [ 0.1838,  0.1838, -0.1838]],\n",
+                            "\n",
+                            "         [[ 0.1838, -0.1838,  0.1838],\n",
+                            "          [ 0.1838,  0.1838,  0.1838],\n",
+                            "          [-0.1838,  0.1838, -0.1838]],\n",
+                            "\n",
+                            "         [[-0.1838,  0.1838, -0.1838],\n",
+                            "          [ 0.1838, -0.1838, -0.1838],\n",
+                            "          [ 0.1838, -0.1838,  0.1838]]]], grad_fn=<MulBackward0>), scale=tensor([[[[0.1842]]],\n",
+                            "\n",
+                            "\n",
+                            "        [[[0.1838]]]], grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
-                    "execution_count": 29,
+                    "execution_count": 35,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "unquant_bias_input_quant_conv = QuantConv2d(\n",
-                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
-                "    input_quant=Int8ActPerTensorFloat, return_quant_tensor=True)\n",
-                "out_tensor = unquant_bias_input_quant_conv(torch.randn(1, 2, 5, 5))\n",
-                "out_tensor"
+                "per_channel_quant_conv = QuantConv2d(\n",
+                "    3, 2, (3, 3), \n",
+                "    weight_quant=AdvancedWeightQuantizer, \n",
+                "    weight_is_clamped=False, \n",
+                "    weight_scaling_per_output_channel=True)\n",
+                "per_channel_quant_conv.quant_weight()"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "As expected the weight scale is now a vector. Everything we said so far about quantizers still applies, so for example we can load the floating-point state dict we defined before and observe how it triggers an update of the weight scale:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 30,
+            "execution_count": 36,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "True"
+                            "QuantTensor(value=tensor([[[[ 0.1875, -0.1875,  0.1875],\n",
+                            "          [-0.1875,  0.1875, -0.1875],\n",
+                            "          [-0.1875,  0.1875, -0.1875]],\n",
+                            "\n",
+                            "         [[-0.1875,  0.1875,  0.1875],\n",
+                            "          [ 0.1875, -0.1875, -0.1875],\n",
+                            "          [ 0.1875, -0.1875,  0.1875]],\n",
+                            "\n",
+                            "         [[-0.1875,  0.1875, -0.1875],\n",
+                            "          [-0.1875,  0.1875,  0.1875],\n",
+                            "          [-0.1875,  0.1875,  0.1875]]],\n",
+                            "\n",
+                            "\n",
+                            "        [[[ 0.1897,  0.1897,  0.1897],\n",
+                            "          [-0.1897,  0.1897, -0.1897],\n",
+                            "          [ 0.1897,  0.1897, -0.1897]],\n",
+                            "\n",
+                            "         [[ 0.1897, -0.1897, -0.1897],\n",
+                            "          [ 0.1897,  0.1897, -0.1897],\n",
+                            "          [ 0.1897,  0.1897,  0.1897]],\n",
+                            "\n",
+                            "         [[-0.1897,  0.1897, -0.1897],\n",
+                            "          [-0.1897,  0.1897, -0.1897],\n",
+                            "          [ 0.1897,  0.1897,  0.1897]]]], grad_fn=<MulBackward0>), scale=tensor([[[[0.1875]]],\n",
+                            "\n",
+                            "\n",
+                            "        [[[0.1897]]]], grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
-                    "execution_count": 30,
+                    "execution_count": 36,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "out_tensor.is_valid"
+                "per_channel_quant_conv.load_state_dict(float_conv.state_dict())\n",
+                "per_channel_quant_conv.quant_weight()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Finally, an important point about `QuantTensor`. With the exception of learned bit-width (which will be the subject of a separate tutorial) and some of the bias quantization scenarios we have just seen, usually returing a `QuantTensor` is not necessary and can create extra complexity. This is why currently `return_quant_tensor` defaults to `False`. We can easily see it in an example:"
+                "In this case we have a per-channel quantizer, so the original floating-point weight tensor is now quantized per channel. \n",
+                "\n",
+                "Similarly, we can apply our custom activation quantizer to e.g. a `QuantIdentity` layer:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 31,
+            "execution_count": 37,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "tensor([[[[ 1.0190,  1.2963,  0.0597],\n",
-                            "          [-1.0870, -0.0248,  0.6649],\n",
-                            "          [-0.3103,  0.6573, -0.3369]],\n",
-                            "\n",
-                            "         [[ 0.6602, -0.6170, -0.6805],\n",
-                            "          [-0.4438,  0.9469, -0.5658],\n",
-                            "          [ 0.0269,  1.0239, -0.0896]],\n",
-                            "\n",
-                            "         [[ 1.5136,  0.8112,  0.7560],\n",
-                            "          [-0.4665,  0.2027, -1.1200],\n",
-                            "          [-1.0580,  1.2795, -0.0788]]]], grad_fn=<ThnnConv2DBackward>)"
+                            "tensor([[-0.0100, -0.0100,  0.0100, -0.0100],\n",
+                            "        [-0.0100, -0.0100, -0.0100,  0.0100],\n",
+                            "        [-0.0100,  0.0100,  0.0100,  0.0100],\n",
+                            "        [-0.0100,  0.0100,  0.0100,  0.0100]], grad_fn=<MulBackward0>)"
+                        ]
+                    },
+                    "execution_count": 37,
+                    "metadata": {},
+                    "output_type": "execute_result"
+                }
+            ],
+            "source": [
+                "from brevitas.nn import QuantIdentity\n",
+                "\n",
+                "quant_identity = QuantIdentity(\n",
+                "    act_quant=AdvancedActQuantizer, is_clamped=True, scaling_per_output_channel=False)\n",
+                "quant_identity(torch.randn(4, 4))"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "Note how `AdvancedActQuantizer` doesn't define a `per_channel_broadcastable_shape`, yet no errors are triggered. This is because `this.per_channel_broadcastable_shape` is required only when `scaling_per_output_channel` is `True`, while in this case `scaling_per_output_channel` is `False`.\n",
+                "Let' try to set it to `True` then:"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": 36,
+            "metadata": {
+                "tags": [
+                    "raises-exception"
+                ]
+            },
+            "outputs": [
+                {
+                    "ename": "DependencyError",
+                    "evalue": "'AdvancedActQuantizer' can not resolve attribute 'per_channel_broadcastable_shape'",
+                    "output_type": "error",
+                    "traceback": [
+                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
+                        "\u001b[1;31mDependencyError\u001b[0m                           Traceback (most recent call last)",
+                        "\u001b[1;32m<ipython-input-36-b3479e90d1a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m quant_identity = QuantIdentity(\n\u001b[1;32m----> 4\u001b[1;33m     act_quant=AdvancedActQuantizer, is_clamped=True, scaling_per_output_channel=True)\n\u001b[0m",
+                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\nn\\quant_activation.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, act_quant, return_quant_tensor, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[0mact_quant\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mact_quant\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mreturn_quant_tensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_quant_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
+                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\nn\\quant_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, act_impl, passthrough_act, input_quant, act_quant, return_quant_tensor, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mpassthrough_act\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mact_quant\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\nn\\mixin\\act.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, act_impl, passthrough_act, act_quant, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[0mproxy_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'act_'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[0mkwargs_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\nn\\mixin\\base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, quant, proxy_protocol, none_quant_injector, proxy_prefix, kwargs_prefix, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mquant_injector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0mquant_injector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquant_injector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfilter_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0mquant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquant_injector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproxy_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquant_injector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxy_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\proxy\\runtime_quant.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, quant_layer, quant_injector)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquant_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquant_injector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActQuantProxyFromInjector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquant_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquant_injector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_passthrough_act\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_is_passthrough_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquant_injector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
+                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\proxy\\quant_proxy.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, quant_layer, quant_injector, export_mode, export_handler)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# Use a normal list and not a ModuleList since this is a pointer to parent modules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtracked_module_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_tracked_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquant_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexport_handler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexport_mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\proxy\\quant_proxy.py\u001b[0m in \u001b[0;36madd_tracked_module\u001b[1;34m(self, module)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtracked_module_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_tracked_modules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_tensor_quant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Trying to add None as a parent module.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\proxy\\runtime_quant.py\u001b[0m in \u001b[0;36minit_tensor_quant\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_tensor_quant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mtensor_quant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquant_injector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_quant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mact_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquant_injector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact_impl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mis_act_enabled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_is_act_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_impl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_quant\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+                        "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
+                        "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\_dependencies\\this.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, __self__)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\".\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mDependencyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                     message = (\n",
+                        "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
+                        "\u001b[1;31mDependencyError\u001b[0m: 'AdvancedActQuantizer' can not resolve attribute 'per_channel_broadcastable_shape'"
+                    ]
+                }
+            ],
+            "source": [
+                "from brevitas.nn import QuantIdentity\n",
+                "\n",
+                "quant_identity = QuantIdentity(\n",
+                "    act_quant=AdvancedActQuantizer, is_clamped=True, scaling_per_output_channel=True)"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "As expected we get an error saying that the quantizer cannot resolve `per_channel_broadcastable_shape`. If we pass it in then we can get a per-channel quantizer:"
+            ]
+        },
+        {
+            "cell_type": "code",
+            "execution_count": 39,
+            "metadata": {},
+            "outputs": [
+                {
+                    "data": {
+                        "text/plain": [
+                            "QuantTensor(value=tensor([[-0.0100,  0.0100, -0.0100, -0.0100],\n",
+                            "        [-0.0100,  0.0100, -0.0100, -0.0100],\n",
+                            "        [ 0.0100, -0.0100,  0.0100, -0.0100],\n",
+                            "        [ 0.0100, -0.0100, -0.0100, -0.0100]], grad_fn=<MulBackward0>), scale=tensor([[0.0100],\n",
+                            "        [0.0100],\n",
+                            "        [0.0100],\n",
+                            "        [0.0100]], grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
-                    "execution_count": 31,
+                    "execution_count": 39,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "bias_input_quant_conv = QuantConv2d(\n",
-                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
-                "    input_quant=Int8ActPerTensorFloat, bias_quant=Int8Bias)\n",
-                "bias_input_quant_conv(torch.randn(1, 2, 5, 5))"
+                "quant_identity = QuantIdentity(\n",
+                "    act_quant=AdvancedActQuantizer, is_clamped=True, scaling_per_output_channel=True,\n",
+                "    per_channel_broadcastable_shape=(4, 1), return_quant_tensor=True)\n",
+                "quant_identity(torch.randn(4, 4))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Altough not obvious, the output is actually implicitly quantized."
+                "We have seen how powerful dependency injection is. In a way, it's even too expressive. For users that are not interesting in building completely custom quantizers, it can be hard to make sense of how the various components available under `brevitas.core` can be assembled together according to best practices."
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
             "display_name": "Python 3",
             "language": "python",
```

### Comparing `brevitas-0.8.0/notebooks/02_quant_activation_overview.ipynb` & `brevitas-0.9.0/notebooks/02_quant_activation_overview.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9983363687738507%*

 * *Differences: {"'cells'": "{2: {'source': {insert: [(17, 'assert "*

 * *            "out_tensor1.isclose(out_tensor2).all().item()')], delete: [17]}}, 4: {'source': "*

 * *            "{insert: [(15, 'assert out_tensor1.isclose(out_tensor2).all().item()')], delete: "*

 * *            "[15]}}, 9: {'outputs': {0: {'data': {'text/plain': {insert: [(10, '          "*

 * *            "[-0.3615, -1.2175, -0.6278, -0.4566,  1.9214]]]],\\n'), (11, '       "*

 * *            'grad_fn=<MulBackward0>), scale=tensor(0.0190, grad_fn=<DivBackward0>), '*

 * *       []*

```diff
@@ -50,15 +50,15 @@
                 "    in_channels=2, out_channels=3, kernel_size=(3,3))\n",
                 "output_identity_quant = QuantIdentity()\n",
                 "\n",
                 "inp = torch.randn(1, 2, 5, 5)\n",
                 "out_tensor1 = output_quant_conv(inp)\n",
                 "out_tensor2 = output_identity_quant(default_quant_conv(inp))\n",
                 "\n",
-                "out_tensor1.isclose(out_tensor2).all().item()"
+                "assert out_tensor1.isclose(out_tensor2).all().item()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "We can observe a similar behaviour if we enable input quantization too:"
@@ -92,15 +92,15 @@
                 "input_identity_quant = QuantIdentity()\n",
                 "output_identity_quant = QuantIdentity()\n",
                 "\n",
                 "inp = torch.randn(1, 2, 5, 5)\n",
                 "out_tensor1 = input_output_quant_conv(inp)\n",
                 "out_tensor2 = output_identity_quant(default_quant_conv(input_identity_quant(inp)))\n",
                 "\n",
-                "out_tensor1.isclose(out_tensor2).all().item()"
+                "assert out_tensor1.isclose(out_tensor2).all().item()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "From an algorithmic point of view then the two different implementation are doing the same thing. However, as it will become clearer in later tutorials, there are currently some scenarios where picking one style over the other can make a difference when it comes to exporting to a format such as standard ONNX. In the meantime, we can just keep in mind that both alternatives exist."
@@ -155,15 +155,16 @@
                             "          [-1.6361,  2.0736,  1.7122,  2.3780, -1.1224],\n",
                             "          [-0.3234, -1.0844, -0.0761, -0.0951, -0.7610]],\n",
                             "\n",
                             "         [[-1.5980,  0.0190, -0.7419,  0.1902,  0.6278],\n",
                             "          [ 0.6468, -0.2473, -0.5327,  1.1605,  0.4376],\n",
                             "          [-0.7990, -1.2936, -0.7419, -1.3127, -0.2283],\n",
                             "          [-2.4351, -0.0761,  0.2283,  0.7990, -0.1902],\n",
-                            "          [-0.3615, -1.2175, -0.6278, -0.4566,  1.9214]]]]), scale=tensor(0.0190), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))"
+                            "          [-0.3615, -1.2175, -0.6278, -0.4566,  1.9214]]]],\n",
+                            "       grad_fn=<MulBackward0>), scale=tensor(0.0190, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
                     "execution_count": 4,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -186,15 +187,15 @@
                     },
                     "execution_count": 5,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "out_tensor.is_valid"
+                "assert out_tensor.is_valid"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "As expected, a `QuantIdentity` with quantization disabled behaves like an identity function also when a `QuantTensor` is passed in. However, depending on whather `return_quant_tensor` is set to `False` or not, quantization metadata might be stripped out, i.e. the input `QuantTensor` is going to be returned as an implicitly quantized `torch.Tensor`:"
@@ -214,15 +215,16 @@
                             "          [-1.6361,  2.0736,  1.7122,  2.3780, -1.1224],\n",
                             "          [-0.3234, -1.0844, -0.0761, -0.0951, -0.7610]],\n",
                             "\n",
                             "         [[-1.5980,  0.0190, -0.7419,  0.1902,  0.6278],\n",
                             "          [ 0.6468, -0.2473, -0.5327,  1.1605,  0.4376],\n",
                             "          [-0.7990, -1.2936, -0.7419, -1.3127, -0.2283],\n",
                             "          [-2.4351, -0.0761,  0.2283,  0.7990, -0.1902],\n",
-                            "          [-0.3615, -1.2175, -0.6278, -0.4566,  1.9214]]]])"
+                            "          [-0.3615, -1.2175, -0.6278, -0.4566,  1.9214]]]],\n",
+                            "       grad_fn=<MulBackward0>)"
                         ]
                     },
                     "execution_count": 6,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -245,15 +247,16 @@
                             "          [-1.6361,  2.0736,  1.7122,  2.3780, -1.1224],\n",
                             "          [-0.3234, -1.0844, -0.0761, -0.0951, -0.7610]],\n",
                             "\n",
                             "         [[-1.5980,  0.0190, -0.7419,  0.1902,  0.6278],\n",
                             "          [ 0.6468, -0.2473, -0.5327,  1.1605,  0.4376],\n",
                             "          [-0.7990, -1.2936, -0.7419, -1.3127, -0.2283],\n",
                             "          [-2.4351, -0.0761,  0.2283,  0.7990, -0.1902],\n",
-                            "          [-0.3615, -1.2175, -0.6278, -0.4566,  1.9214]]]]), scale=tensor(0.0190), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))"
+                            "          [-0.3615, -1.2175, -0.6278, -0.4566,  1.9214]]]],\n",
+                            "       grad_fn=<MulBackward0>), scale=tensor(0.0190, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
                     "execution_count": 7,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -284,15 +287,15 @@
                             "          [0.0000, 2.0817, 1.7083, 2.3804, 0.0000],\n",
                             "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
                             "\n",
                             "         [[0.0000, 0.0187, 0.0000, 0.1867, 0.6254],\n",
                             "          [0.6348, 0.0000, 0.0000, 1.1668, 0.4387],\n",
                             "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
                             "          [0.0000, 0.0000, 0.2334, 0.7935, 0.0000],\n",
-                            "          [0.0000, 0.0000, 0.0000, 0.0000, 1.9230]]]]), scale=tensor(0.0093), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(False), training_t=tensor(True))"
+                            "          [0.0000, 0.0000, 0.0000, 0.0000, 1.9230]]]], grad_fn=<MulBackward0>), scale=tensor(0.0093, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(False), training_t=tensor(True))"
                         ]
                     },
                     "execution_count": 8,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -310,30 +313,22 @@
                 "`QuantReLU`, like `QuantIdentity`, is also special compared to other non-linear quantized activation layers as it preserves the metadata of an input `QuantTensor` even when quantization is disabled:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 9,
             "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "True"
-                        ]
-                    },
-                    "execution_count": 9,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
+            "outputs": [],
             "source": [
                 "return_disabled_quant_relu = QuantReLU(act_quant=None, return_quant_tensor=True)\n",
                 "relu_out_tensor = return_disabled_quant_relu(out_tensor)\n",
-                "relu_out_tensor.is_valid"
+                "assert relu_out_tensor.is_valid==True\n",
+                "assert relu_out_tensor.scale == out_tensor.scale\n",
+                "assert relu_out_tensor.zero_point == out_tensor.zero_point\n",
+                "assert relu_out_tensor.bit_width == out_tensor.bit_width"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "That doesn't apply to other layers like, say, `QuantSigmoid`:"
@@ -343,25 +338,26 @@
             "cell_type": "code",
             "execution_count": 10,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[0.3878, 0.3611, 0.3655, 0.6433, 0.8236],\n",
+                            "QuantTensor(value=(tensor([[[[0.3878, 0.3611, 0.3655, 0.6433, 0.8236],\n",
                             "          [0.6257, 0.3567, 0.3611, 0.5474, 0.4810],\n",
                             "          [0.3788, 0.1820, 0.4526, 0.6077, 0.7911],\n",
                             "          [0.1630, 0.8883, 0.8471, 0.9151, 0.2456],\n",
                             "          [0.4198, 0.2527, 0.4810, 0.4762, 0.3184]],\n",
                             "\n",
                             "         [[0.1683, 0.5048, 0.3226, 0.5474, 0.6520],\n",
                             "          [0.6563, 0.4385, 0.3699, 0.7614, 0.6077],\n",
                             "          [0.3102, 0.2152, 0.3226, 0.2120, 0.4432],\n",
                             "          [0.0805, 0.4810, 0.5568, 0.6898, 0.4526],\n",
-                            "          [0.4106, 0.2284, 0.3480, 0.3878, 0.8723]]]]), scale=None, zero_point=None, bit_width=None, signed_t=None, training_t=tensor(True))"
+                            "          [0.4106, 0.2284, 0.3480, 0.3878, 0.8723]]]],\n",
+                            "       grad_fn=<SigmoidBackward0>), None, None, None), scale=None, zero_point=None, bit_width=None, signed_t=None, training_t=tensor(True))"
                         ]
                     },
                     "execution_count": 10,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -386,15 +382,15 @@
                     },
                     "execution_count": 11,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "sigmoid_out_tensor.is_valid"
+                "assert not sigmoid_out_tensor.is_valid"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "Something to always keep in mind is that the non-linearity of a quantized activation layer is always called on the *dequantized* representation of the input.\n",
@@ -416,15 +412,15 @@
                             "          [0.0000, 2.0772, 1.6996, 2.3794, 0.0000],\n",
                             "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
                             "\n",
                             "         [[0.0000, 0.0189, 0.0000, 0.1888, 0.6232],\n",
                             "          [0.6421, 0.0000, 0.0000, 1.1708, 0.4343],\n",
                             "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
                             "          [0.0000, 0.0000, 0.2266, 0.7931, 0.0000],\n",
-                            "          [0.0000, 0.0000, 0.0000, 0.0000, 1.9262]]]]), scale=tensor(0.0189), zero_point=tensor(129.), bit_width=tensor(8.), signed_t=tensor(False), training_t=tensor(True))"
+                            "          [0.0000, 0.0000, 0.0000, 0.0000, 1.9262]]]], grad_fn=<ReluBackward0>), scale=tensor(0.0189, grad_fn=<DivBackward0>), zero_point=tensor(129., grad_fn=<SWhereBackward0>), bit_width=tensor(8.), signed_t=tensor(False), training_t=tensor(True))"
                         ]
                     },
                     "execution_count": 12,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
@@ -570,15 +566,15 @@
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "out1_train = quant_identity(inp1)\n",
                 "out2_train = quant_identity(inp2)\n",
-                "out1_train.scale.isclose(out2_train.scale).item()"
+                "assert not out1_train.scale.isclose(out2_train.scale).item()"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 17,
             "metadata": {},
             "outputs": [
@@ -593,15 +589,15 @@
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "quant_identity.eval()\n",
                 "out1_eval = quant_identity(inp1)\n",
                 "out2_eval = quant_identity(inp2)\n",
-                "out1_eval.scale.isclose(out2_eval.scale).item()"
+                "assert out1_eval.scale.isclose(out2_eval.scale).item()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "By default, the only layer that is an exception to this is `QuantHardTanh`. That is because the interface to `torch.nn.HardTanh` already requires users to manually specify `min_val` and `max_val`, so Brevitas preserves that both when quantization is enabled or disabled. With quantization enabled, by default those values are used for initialization, but then the range is learned. Let's look at an example:"
@@ -682,15 +678,15 @@
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "out1_train = quant_hard_tanh(inp1)\n",
                 "quant_hard_tanh.eval()\n",
                 "out2_eval = quant_hard_tanh(inp2)\n",
-                "out1_train.scale.isclose(out2_eval.scale).item()"
+                "assert out1_train.scale.isclose(out2_eval.scale).item()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "Finally, a reminder that mixing things up is perfectly legal and encouraged in Brevitas.\n",
```

### Comparing `brevitas-0.8.0/notebooks/03_anatomy_of_a_quantizer.ipynb` & `brevitas-0.9.0/notebooks/01_quant_tensor_quant_conv2d_overview.ipynb`

 * *Files 27% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.903276803208073%*

 * *Differences: {"'cells'": "{0: {'source': {insert: [(0, '# An overview of QuantTensor and QuantConv2d\\n'), (2, "*

 * *            "'In this initial tutorial, we take a first look at `QuantTensor`, a basic data "*

 * *            'structure in Brevitas, and at `QuantConv2d`, a typical quantized layer. `QuantConv2d` '*

 * *            'is an instance of a `QuantWeightBiasInputOutputLayer` (typically imported as '*

 * *            '`QuantWBIOL`), meaning that it supports quantization of its weight, bias, input and '*

 * *            'output. Other []*

```diff
@@ -2,1565 +2,1183 @@
     "cells": [
         {
             "cell_type": "markdown",
             "metadata": {
                 "collapsed": true
             },
             "source": [
-                "# Anatomy of a Quantizer\n",
+                "# An overview of QuantTensor and QuantConv2d\n",
                 "\n",
-                "## What's in a Quantizer? \n",
+                "In this initial tutorial, we take a first look at `QuantTensor`, a basic data structure in Brevitas, and at `QuantConv2d`, a typical quantized layer. `QuantConv2d` is an instance of a `QuantWeightBiasInputOutputLayer` (typically imported as `QuantWBIOL`), meaning that it supports quantization of its weight, bias, input and output. Other instances of `QuantWBIOL` are `QuantLinear`, `QuantConv1d`, `QuantConvTranspose1d` and `QuantConvTranspose2d`, and they all follow the same principles.\n",
                 "\n",
-                "In a broad sense, a quantizer is anything that implements a quantization technique, and the flexibility of Brevitas means that there are different ways to do so.  \n",
-                "However, to keep our terminology straight, we refer to a quantizer as a specific kind of way to implement quantization, the one preferred and adopted by default. That is, a quantizer is a subclass of a `brevitas.inject.ExtendedInjector` that carries a `tensor_quant` attribute, which points to an instance of a torch `Module` that implements quantization.\n",
-                "\n",
-                "We have seen in previous tutorials quantizers being imported from `brevitas.quant` and passed on to quantized layers. We can easily very what we just said on one of them:"
+                "If we take a look at the `__init__` method of `QuantConv2d`, we notice a few things:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [
                 {
-                    "data": {
-                        "text/plain": [
-                            "True"
-                        ]
-                    },
-                    "execution_count": 1,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
-            "source": [
-                "from brevitas.inject import ExtendedInjector\n",
-                "from brevitas.quant.scaled_int import Int8ActPerTensorFloat \n",
-                "\n",
-                "issubclass(Int8ActPerTensorFloat, ExtendedInjector)"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 2,
-            "metadata": {},
-            "outputs": [
+                    "name": "stderr",
+                    "output_type": "stream",
+                    "text": [
+                        "/home/user/.local/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
+                        "  from .autonotebook import tqdm as notebook_tqdm\n"
+                    ]
+                },
                 {
                     "data": {
+                        "text/markdown": [
+                            "```python\n",
+                            "    def __init__(\n",
+                            "            self,\n",
+                            "            in_channels: int,\n",
+                            "            out_channels: int,\n",
+                            "            kernel_size: Union[int, Tuple[int, int]],\n",
+                            "            stride: Union[int, Tuple[int, int]] = 1,\n",
+                            "            padding: Union[int, Tuple[int, int]] = 0,\n",
+                            "            dilation: Union[int, Tuple[int, int]] = 1,\n",
+                            "            groups: int = 1,\n",
+                            "            bias: bool = True,\n",
+                            "            padding_type: str = 'standard',\n",
+                            "            weight_quant: Optional[WeightQuantType] = Int8WeightPerTensorFloat,\n",
+                            "            bias_quant: Optional[BiasQuantType] = None,\n",
+                            "            input_quant: Optional[ActQuantType] = None,\n",
+                            "            output_quant: Optional[ActQuantType] = None,\n",
+                            "            return_quant_tensor: bool = False,\n",
+                            "            **kwargs) -> None:\n",
+                            "        Conv2d.__init__(\n",
+                            "            self,\n",
+                            "            in_channels=in_channels,\n",
+                            "            out_channels=out_channels,\n",
+                            "            kernel_size=kernel_size,\n",
+                            "            stride=stride,\n",
+                            "            padding=padding,\n",
+                            "            dilation=dilation,\n",
+                            "            groups=groups,\n",
+                            "            bias=bias)\n",
+                            "        QuantWBIOL.__init__(\n",
+                            "            self,\n",
+                            "            weight_quant=weight_quant,\n",
+                            "            bias_quant=bias_quant,\n",
+                            "            input_quant=input_quant,\n",
+                            "            output_quant=output_quant,\n",
+                            "            return_quant_tensor=return_quant_tensor,\n",
+                            "            **kwargs)\n",
+                            "        assert self.padding_mode == 'zeros'\n",
+                            "        assert not (padding_type == 'same' and padding != 0)\n",
+                            "        self.padding_type = padding_type\n",
+                            "\n",
+                            "```"
+                        ],
                         "text/plain": [
-                            "RescalingIntQuant(\n",
-                            "  (int_quant): IntQuant(\n",
-                            "    (float_to_int_impl): RoundSte()\n",
-                            "    (tensor_clamp_impl): TensorClamp()\n",
-                            "    (delay_wrapper): DelayWrapper(\n",
-                            "      (delay_impl): _NoDelay()\n",
-                            "    )\n",
-                            "  )\n",
-                            "  (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
-                            "    (stats_input_view_shape_impl): OverTensorView()\n",
-                            "    (stats): _Stats(\n",
-                            "      (stats_impl): AbsPercentile()\n",
-                            "    )\n",
-                            "    (restrict_clamp_scaling): _RestrictClampValue(\n",
-                            "      (clamp_min_ste): Identity()\n",
-                            "      (restrict_value_impl): FloatRestrictValue()\n",
-                            "    )\n",
-                            "    (restrict_inplace_preprocess): Identity()\n",
-                            "    (restrict_preprocess): Identity()\n",
-                            "  )\n",
-                            "  (int_scaling_impl): IntScaling()\n",
-                            "  (zero_point_impl): ZeroZeroPoint(\n",
-                            "    (zero_point): StatelessBuffer()\n",
-                            "  )\n",
-                            "  (msb_clamp_bit_width_impl): BitWidthConst(\n",
-                            "    (bit_width): StatelessBuffer()\n",
-                            "  )\n",
-                            ")"
+                            "<IPython.core.display.Markdown object>"
                         ]
                     },
-                    "execution_count": 2,
                     "metadata": {},
-                    "output_type": "execute_result"
+                    "output_type": "display_data"
                 }
             ],
             "source": [
-                "Int8ActPerTensorFloat.tensor_quant"
+                "import inspect\n",
+                "from brevitas.nn import QuantConv2d\n",
+                "from brevitas.nn import QuantIdentity\n",
+                "from IPython.display import Markdown, display\n",
+                "\n",
+                "def pretty_print_source(source):\n",
+                "    display(Markdown('```python\\n' + source + '\\n```'))\n",
+                "    \n",
+                "source = inspect.getsource(QuantConv2d.__init__)  \n",
+                "pretty_print_source(source)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Note how we said *subclass* and not *instance*. To understand why that's the case, we have to understand what an `ExtendedInjector` is and why it's used in the first place."
+                "`QuantConv2d` is an instance of both `Conv2d` and `QuantWBIOL`. Its initialization method exposes the usual arguments of a `Conv2d`, as well as: an extra flag to support *same padding*; *four* different arguments to set a quantizer for - respectively - *weight*, *bias*, *input*, and *output*; a `return_quant_tensor` boolean flag; the `**kwargs` placeholder to intercept additional arbitrary keyword arguments.  \n",
+                "In this tutorial we will focus on how to set the four quantizer arguments and the return flags; arbitrary kwargs will be explained in a separate tutorial dedicated to defining and overriding quantizers.\n",
+                "\n",
+                "By default `weight_quant=Int8WeightPerTensorFloat`, while `bias_quant`, `input_quant` and `output_quant` are set to `None`. That means that by default weights are quantized to *8-bit signed integer with a per-tensor floating-point scale factor* (a very common type of quantization adopted by e.g. the ONNX standard opset), while quantization of bias, input, and output are disabled. We can easily verify all of this at runtime on an example:"
             ]
         },
         {
-            "cell_type": "markdown",
+            "cell_type": "code",
+            "execution_count": 2,
             "metadata": {},
+            "outputs": [],
             "source": [
-                "## Quantization with auto-wiring Dependency Injection\n",
-                "\n",
-                "Pytorch has exploded in popularity thanks to its straightforward numpy-like *define-by-run* execution model. However, when it comes to applying quantization, this style of programming poses a problem.  \n",
-                "\n",
-                "Many quantization methods depend on making decisions based on the (in Pytorch terms) `state_dict` of the original floating-point model to finetune with quantization. However, when we instantiate a model in Pytorch we can't know on the spot if a state_dict is going to be loaded a few lines of code later or not. Yet, because Pytorch is define-by-run, we need our model to work consistently both before and after a `state_dict` is possibly loaded. In a traditional scenario that wouldn't pose a problem. However, with quantization in the loop, the way a quantizer is defined might change before and after a pretrained `state_dict` is loaded.\n",
-                "\n",
-                "That means that we need a way to define our quantized model such that it can react appropriately in case the `state_dict` changes. In a Python-only world that wouldn't be too hard. However, in order to mitigate the performance impact of quantization-aware training, Brevitas makes extended use of Pytorch's JIT compiler for a custom subset of Python, TorchScript. That means that in most scenarios, when a `state_dict` is loaded, we need to recompile parts of the model. Because compilation in general is a lossy process, a TorchScript component cannot simply re-compile itself based on new input information. \n",
-                "\n",
-                "We need then a way to *declare* a quantization method such that it can be re-initialized and JIT compiled any time the `state_dict` changes. Because we want to support arbitrarly-complex user-defined quantization algorithms, this method has to be generic, i.e. it cannot depend on the specifics of the quantization algorithm implemented.\n",
-                "\n",
-                "Implementing a quantizer with an `ExtendedInjector` is a way to do so. Specifically, an `ExtendedInjector` extends an `Injector` from an older version (*0.2.1*) of the excellent dependency-injection library *[dependencies](https://github.com/proofit404/dependencies)* with support for a couple of extra features that are specific to Brevitas' needs.\n",
-                "\n",
-                "An `Injector` (and an `ExtendedInjector`) allows to take what might be a very complicated graph of interwined objects and turns it into a flat list of variables that are capable of auto-assembly by matching variable names to arguments names. This technique typically goes under the name of auto-wiring dependency injection. \n",
-                "\n",
-                "In the context of Brevitas, the goal is gather all the modules and hyperparameters that contribute to a quantization implementation such that they can be re-assembled automatically on demand. What comes out of this process is a `tensor_quant` object."
+                "default_quant_conv = QuantConv2d(\n",
+                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False)"
             ]
         },
         {
-            "cell_type": "markdown",
+            "cell_type": "code",
+            "execution_count": 3,
             "metadata": {},
+            "outputs": [
+                {
+                    "name": "stdout",
+                    "output_type": "stream",
+                    "text": [
+                        "Is weight quant enabled: True\n",
+                        "Is bias quant enabled: False\n",
+                        "Is input quant enabled: False\n",
+                        "Is output quant enabled: False\n"
+                    ]
+                }
+            ],
             "source": [
-                "## A Practical Example: Binary Quantization\n",
-                "\n",
-                "To make things practical, let's look at how we can implement a simple variant of binary quantization. All the components typically used to implement quantization can be found under `brevitas.core`. As mentioned before, Brevitas makes heavy use of TorchScript. In particular, all the components found under `brevitas.core` are implemented as `ScriptModule` that can be assembled together.\n",
-                "The core `ScriptModule` that implements binarization can be found under `brevitas.core.quant`: "
+                "print(f'Is weight quant enabled: {default_quant_conv.is_weight_quant_enabled}')\n",
+                "print(f'Is bias quant enabled: {default_quant_conv.is_bias_quant_enabled}')\n",
+                "print(f'Is input quant enabled: {default_quant_conv.is_input_quant_enabled}')\n",
+                "print(f'Is output quant enabled: {default_quant_conv.is_output_quant_enabled}')"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": 3,
+            "cell_type": "markdown",
             "metadata": {},
-            "outputs": [],
             "source": [
-                "import inspect\n",
-                "from IPython.display import Markdown, display\n",
-                "\n",
-                "def pretty_print_source(source):\n",
-                "    display(Markdown('```python\\n' + source + '\\n```'))"
+                "If we now try to pass in a random floating-point tensor as input, as expected we get the output of the convolution:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 4,
-            "metadata": {},
+            "metadata": {
+                "scrolled": true
+            },
             "outputs": [
                 {
                     "data": {
-                        "text/markdown": [
-                            "```python\n",
-                            "class BinaryQuant(brevitas.jit.ScriptModule):\n",
-                            "    \"\"\"\n",
-                            "    ScriptModule that implements scaled uniform binary quantization of an input tensor.\n",
-                            "    Quantization is performed with :func:`~brevitas.function.ops_ste.binary_sign_ste`.\n",
-                            "\n",
-                            "    Args:\n",
-                            "        scaling_impl (Module): Module that returns a scale factor.\n",
-                            "        quant_delay_steps (int): Number of training steps to delay quantization for. Default: 0\n",
-                            "\n",
-                            "    Returns:\n",
-                            "        Tuple[Tensor, Tensor, Tensor, Tensor]: Quantized output in de-quantized format, scale, zero-point, bit_width.\n",
-                            "\n",
-                            "    Examples:\n",
-                            "        >>> from brevitas.core.scaling import ConstScaling\n",
-                            "        >>> binary_quant = BinaryQuant(ConstScaling(0.1))\n",
-                            "        >>> inp = torch.Tensor([0.04, -0.6, 3.3])\n",
-                            "        >>> out, scale, zero_point, bit_width = binary_quant(inp)\n",
-                            "        >>> out\n",
-                            "        tensor([ 0.1000, -0.1000,  0.1000])\n",
-                            "        >>> scale\n",
-                            "        tensor(0.1000)\n",
-                            "        >>> zero_point\n",
-                            "        tensor(0.)\n",
-                            "        >>> bit_width\n",
-                            "        tensor(1.)\n",
-                            "\n",
-                            "    Note:\n",
-                            "        Maps to quant_type == QuantType.BINARY == 'BINARY' == 'binary' when applied to weights in higher-level APIs.\n",
-                            "\n",
-                            "    Note:\n",
-                            "        Set env variable BREVITAS_JIT=1 to enable TorchScript compilation of this module.\n",
-                            "    \"\"\"\n",
-                            "\n",
-                            "    def __init__(self, scaling_impl: Module, quant_delay_steps: int = 0):\n",
-                            "        super(BinaryQuant, self).__init__()\n",
-                            "        self.scaling_impl = scaling_impl\n",
-                            "        self.bit_width = BitWidthConst(1)\n",
-                            "        self.zero_point = StatelessBuffer(torch.tensor(0.0))\n",
-                            "        self.delay_wrapper = DelayWrapper(quant_delay_steps)\n",
-                            "\n",
-                            "    @brevitas.jit.script_method\n",
-                            "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
-                            "        scale = self.scaling_impl(x)\n",
-                            "        y = binary_sign_ste(x) * scale\n",
-                            "        y = self.delay_wrapper(x, y)\n",
-                            "        return y, scale, self.zero_point(), self.bit_width()\n",
-                            "\n",
-                            "```"
-                        ],
                         "text/plain": [
-                            "<IPython.core.display.Markdown object>"
+                            "tensor([[[[-0.2594,  0.5392,  0.5916],\n",
+                            "          [ 0.3493,  0.6813,  0.2499],\n",
+                            "          [ 1.3732,  0.1229, -0.0084]],\n",
+                            "\n",
+                            "         [[ 0.0031, -0.1702,  0.1069],\n",
+                            "          [-0.8181, -0.8056,  0.0385],\n",
+                            "          [-0.4738,  0.0589,  0.1278]],\n",
+                            "\n",
+                            "         [[-0.1718, -0.1162, -0.1526],\n",
+                            "          [-0.9903, -0.3541,  0.1645],\n",
+                            "          [ 0.0557, -0.4458, -0.2080]]]], grad_fn=<ThnnConv2DBackward0>)"
                         ]
                     },
+                    "execution_count": 4,
                     "metadata": {},
-                    "output_type": "display_data"
+                    "output_type": "execute_result"
                 }
             ],
             "source": [
-                "from brevitas.core.quant import BinaryQuant\n",
+                "import torch\n",
                 "\n",
-                "source = inspect.getsource(BinaryQuant)  \n",
-                "pretty_print_source(source)"
+                "out = default_quant_conv(torch.randn(1, 2, 5, 5))\n",
+                "out"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "The implementation is quite simple. Apart from `quant_delay_steps`, which allows to delay quantization by a certain number of training steps (*default = 0*), the only other argument that BinaryQuant accepts is an implementation to compute the scale factor. `bit_width` is fixed to 1 and `zero-point` is fixed to 0.\n",
-                "\n",
-                "We pick as scale factor implementation a `ScriptModule` called `ParameterScaling`, which implements a learned parameter with user-defined initialization. It can be found under `brevitas.core.scaling`:"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 5,
-            "metadata": {},
-            "outputs": [],
-            "source": [
-                "from brevitas.core.scaling import ParameterScaling"
+                "In this case we are computing the convolution between an unquantized input tensor and quantized weights, so the output in general is unquantized."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Manual Binary Quantization\n",
-                "\n",
-                "As a first step, we simply instantiate `BinaryQuant` with `ParameterScaling` using `scaling_init` equal *0.1* and we call it on a random floating-point input tensor:"
+                "A QuantConv2d with quantization disabled everywhere behaves like a standard `Conv2d`. Again can easily verify this:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 6,
+            "execution_count": 5,
             "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "(tensor([[-0.1000, -0.1000, -0.1000, -0.1000],\n",
-                            "         [ 0.1000, -0.1000, -0.1000,  0.1000],\n",
-                            "         [ 0.1000,  0.1000,  0.1000,  0.1000],\n",
-                            "         [-0.1000,  0.1000, -0.1000, -0.1000]], grad_fn=<MulBackward0>),\n",
-                            " tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>),\n",
-                            " tensor(0.),\n",
-                            " tensor(1.))"
-                        ]
-                    },
-                    "execution_count": 6,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
+            "outputs": [],
             "source": [
-                "import torch\n",
+                "from torch.nn import Conv2d\n",
                 "\n",
-                "manual_tensor_quant = BinaryQuant(scaling_impl=ParameterScaling(scaling_init=0.1))\n",
-                "manual_tensor_quant(torch.randn(4, 4))"
+                "torch.manual_seed(0)  # set a seed to make sure the random weight init is reproducible\n",
+                "disabled_quant_conv = QuantConv2d(\n",
+                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False, weight_quant=None)\n",
+                "torch.manual_seed(0)  # reproduce the same random weight init as above\n",
+                "float_conv = Conv2d(\n",
+                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False)\n",
+                "inp = torch.randn(1, 2, 5, 5)\n",
+                "assert torch.isclose(disabled_quant_conv(inp), float_conv(inp)).all().item()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Nothing too surprising here, as expected the tensor is binarized with the scale factor we defined. Note however how `manual_tensor_quant` is returning a `tuple` and not a `QuantTensor`. This is because support for custom data structures in TorchScript is still quite limited, so `QuantTensor` are allocated only in Python-world abstractions."
+                "As we have just seen, Brevitas allows users as much freedom as possible to experiment with quantization, meaning that computation between quantized and unquantized values is considered legal. This allows users to mix Brevitas layers with Pytorch layers with little restrictions.  \n",
+                "To make this possible, quantized values are typically represented in *dequantized format*, meaning that - in the case of affine quantization implemented in Brevitas - zero-point and scale factor are applied to their integer values according to the formula **quant_value = (integer_value - zero_point) * scale**."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "### Binary Quantization with an ExtendedInjector\n",
+                "## QuantTensor\n",
                 "\n",
-                "Let's now declare `tensor_quant` through an `ExtendedInjector`:"
+                "We can directly observe the quantized weights by calling the weight quantizer on the layer's weights: `default_quant_conv.weight_quant(quant_conv.weight)`, which for shortness is already implemented as `default_quant_conv.quant_weight()` :"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 7,
+            "execution_count": 6,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "(tensor([[-0.1000,  0.1000,  0.1000, -0.1000],\n",
-                            "         [ 0.1000, -0.1000, -0.1000,  0.1000],\n",
-                            "         [ 0.1000, -0.1000,  0.1000, -0.1000],\n",
-                            "         [ 0.1000,  0.1000, -0.1000,  0.1000]], grad_fn=<MulBackward0>),\n",
-                            " tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>),\n",
-                            " tensor(0.),\n",
-                            " tensor(1.))"
+                            "QuantTensor(value=tensor([[[[-0.0790,  0.0503, -0.0934],\n",
+                            "          [-0.1149, -0.1903, -0.1329],\n",
+                            "          [-0.1813,  0.0108,  0.0593]],\n",
+                            "\n",
+                            "         [[ 0.0970, -0.0215, -0.0144],\n",
+                            "          [ 0.2280,  0.1239, -0.0090],\n",
+                            "          [ 0.1957, -0.2011, -0.0108]]],\n",
+                            "\n",
+                            "\n",
+                            "        [[[-0.0018, -0.1957,  0.1993],\n",
+                            "          [-0.0359,  0.1778, -0.1400],\n",
+                            "          [ 0.0916,  0.1059,  0.2173]],\n",
+                            "\n",
+                            "         [[-0.1670,  0.1939, -0.2191],\n",
+                            "          [-0.0215,  0.1688, -0.1383],\n",
+                            "          [-0.0449, -0.1185,  0.1742]]],\n",
+                            "\n",
+                            "\n",
+                            "        [[[-0.0808, -0.1652, -0.0233],\n",
+                            "          [-0.0700,  0.0467, -0.0485],\n",
+                            "          [ 0.1059,  0.1418,  0.1077]],\n",
+                            "\n",
+                            "         [[-0.0593,  0.0108,  0.0036],\n",
+                            "          [-0.1508,  0.0808,  0.1616],\n",
+                            "          [ 0.0144, -0.0287, -0.1365]]]], grad_fn=<MulBackward0>), scale=tensor(0.0018, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
-                    "execution_count": 7,
+                    "execution_count": 6,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "from brevitas.inject import ExtendedInjector\n",
-                "\n",
-                "class MyBinaryQuantizer(ExtendedInjector):\n",
-                "    tensor_quant = BinaryQuant\n",
-                "    scaling_impl=ParameterScaling\n",
-                "    scaling_init=0.1\n",
-                "\n",
-                "inj_tensor_quant = MyBinaryQuantizer.tensor_quant\n",
-                "inj_tensor_quant(torch.randn(4, 4))"
+                "default_quant_conv.quant_weight()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Any time `MyBinaryQuantizer.tensor_quant` is called, a new instance of `BinaryQuant` is created. Note how the attributes of `MyBinaryQuantizer` are designed to match the name of the arguments of each other, except for `tensor_quant`, which is what we are interested in retrieving from the outside."
+                "Notice how the quantized weights are wrapped in a data structure implemented by Brevitas called `QuantTensor`. A `QuantTensor` is a way to represent an affine quantized tensor with all its metadata, meaning: the `value` of the quantized tensor in *dequantized* format, `scale`, `zero_point`, `bit_width`, whether the quantized value it's `signed` or not, and whether the tensor was generated in `training` mode. "
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "## Inheritance and Composition of Quantizers\n",
-                "\n",
-                "The advantage of expressing a quantizer through a Python class also means that we can leverage both *inheritance* and *composition*. So for example we can inherit from `MyBinaryQuantizer` and override `scaling_init` with a new value:"
+                "As expected, we have that the quantized value (in dequantized format) can be computer from its integer representation, together with zero-point and scale:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 8,
+            "execution_count": 7,
             "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "(tensor([[ 1., -1., -1.,  1.],\n",
-                            "         [-1., -1., -1.,  1.],\n",
-                            "         [ 1., -1., -1., -1.],\n",
-                            "         [ 1.,  1., -1.,  1.]], grad_fn=<MulBackward0>),\n",
-                            " tensor(1., grad_fn=<AbsBinarySignGradFnBackward>),\n",
-                            " tensor(0.),\n",
-                            " tensor(1.))"
-                        ]
-                    },
-                    "execution_count": 8,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
+            "outputs": [],
             "source": [
-                "class MyChildBinaryQuantizer(MyBinaryQuantizer):\n",
-                "    scaling_init=1.0\n",
-                "    \n",
-                "child_inj_tensor_quant = MyChildBinaryQuantizer.tensor_quant\n",
-                "child_inj_tensor_quant(torch.randn(4, 4))"
+                "int_weight = default_quant_conv.int_weight()\n",
+                "zero_point = default_quant_conv.quant_weight_zero_point()\n",
+                "scale = default_quant_conv.quant_weight_scale()\n",
+                "quant_weight_manually = (int_weight - zero_point) * scale\n",
+                "\n",
+                "assert default_quant_conv.quant_weight().value.isclose(quant_weight_manually).all().item()"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Or we can leverage composition by assembling together various classes containing different pieces of a quantizer:"
+                "A *valid* QuantTensor correctly populates all its fields with values `!= None` and respect the **affine quantization invariant**, i.e. `value / scale + zero_point` is (accounting for rounding errors) an *integer* that can be represented within the interval defined by the `bit_width` and `signed` fields of the `QuantTensor`. A *non-valid* one doesn't.\n",
+                "We can observe that the quantized weights are indeed marked as valid:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 9,
+            "execution_count": 8,
             "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "(tensor([[ 0.1000,  0.1000, -0.1000,  0.1000],\n",
-                            "         [-0.1000,  0.1000,  0.1000,  0.1000],\n",
-                            "         [-0.1000, -0.1000,  0.1000,  0.1000],\n",
-                            "         [ 0.1000,  0.1000, -0.1000,  0.1000]], grad_fn=<MulBackward0>),\n",
-                            " tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>),\n",
-                            " tensor(0.),\n",
-                            " tensor(1.))"
-                        ]
-                    },
-                    "execution_count": 9,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
+            "outputs": [],
             "source": [
-                "class MyBinaryImpl(ExtendedInjector):\n",
-                "    tensor_quant = BinaryQuant\n",
-                "\n",
-                "class MyScalingImpl(ExtendedInjector):\n",
-                "    scaling_impl=ParameterScaling\n",
-                "    scaling_init=0.1\n",
-                "    \n",
-                "class MyComposedBinaryQuantizer(MyBinaryImpl, MyScalingImpl):\n",
-                "    pass\n",
-                "\n",
-                "comp_inj_tensor_quant = MyComposedBinaryQuantizer.tensor_quant\n",
-                "comp_inj_tensor_quant(torch.randn(4, 4))"
+                "assert default_quant_conv.quant_weight().is_valid"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "## Interfacing a Quantizer with a Quantized Layer\n",
-                "\n",
-                "Before we can pass the quantizer to a quantized layer such as `QuantConv2d`, we need a last component to define, a proxy. A proxy (found under `brevitas.proxy`) is an `nn.Module` that serve as interface between a quantizer and a quantized layer. \n",
-                "\n",
-                "While a quantizer lives mostly in JIT-land, a proxy lives mostly in Python-land, and as such can afford much more flexibility. Proxies take care of returning a `QuantTensor` and re-initializing the output of quantizer whenever a new `state_dict` is loaded. \n",
-                "\n",
-                "Proxies are specific to the kind of tensor being quantized, as in weights vs biases vs activations. For convenience, they are declared as part of the quantizer itself under the attribute `proxy_class`. For example, for weights we can use `WeightQuantProxyFromInjector`:"
+                "Calling `is_valid` is relative expensive, so it should be using sparingly, but there are a few cases where a non-valid QuantTensor might be generated that is important to be aware of. Say we have two QuantTensor as output of the same quantized activation, and we want to sum them together:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 10,
             "metadata": {},
-            "outputs": [],
+            "outputs": [
+                {
+                    "name": "stdout",
+                    "output_type": "stream",
+                    "text": [
+                        "tensor(0.0173, grad_fn=<DivBackward0>)\n",
+                        "tensor(0.0307, grad_fn=<DivBackward0>)\n"
+                    ]
+                }
+            ],
             "source": [
-                "from brevitas.proxy import WeightQuantProxyFromInjector\n",
+                "from brevitas.quant_tensor import QuantTensor\n",
                 "\n",
-                "class MyBinaryWeightQuantizer(MyBinaryQuantizer):\n",
-                "    proxy_class = WeightQuantProxyFromInjector"
+                "quant_act = QuantIdentity(return_quant_tensor=True)\n",
+                "\n",
+                "out_tensor_0 = quant_act(torch.randn(1,2,5,5))\n",
+                "out_tensor_1 = quant_act(torch.randn(1,2,5,5))\n",
+                "\n",
+                "assert out_tensor_0.is_valid\n",
+                "assert out_tensor_1.is_valid\n",
+                "print(out_tensor_0.scale)\n",
+                "print(out_tensor_1.scale)"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "We can now use `MyBinaryWeightQuantizer` as the weight quantizer of a layer:"
+                "Both QuantTensor are valid but since the quantized activation is in training mode by default, their scale factors are going to be different. It is important to note that the behaviour is different at evaluation time, where the two scale factors will be the same."
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 11,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[ 0.1000,  0.1000, -0.1000],\n",
-                            "          [-0.1000,  0.1000, -0.1000],\n",
-                            "          [ 0.1000, -0.1000, -0.1000]],\n",
-                            "\n",
-                            "         [[-0.1000,  0.1000, -0.1000],\n",
-                            "          [ 0.1000, -0.1000,  0.1000],\n",
-                            "          [-0.1000, -0.1000,  0.1000]],\n",
-                            "\n",
-                            "         [[ 0.1000, -0.1000, -0.1000],\n",
-                            "          [ 0.1000,  0.1000, -0.1000],\n",
-                            "          [-0.1000, -0.1000,  0.1000]]],\n",
-                            "\n",
-                            "\n",
-                            "        [[[ 0.1000, -0.1000,  0.1000],\n",
-                            "          [ 0.1000, -0.1000, -0.1000],\n",
-                            "          [ 0.1000, -0.1000,  0.1000]],\n",
-                            "\n",
-                            "         [[-0.1000,  0.1000, -0.1000],\n",
-                            "          [ 0.1000,  0.1000,  0.1000],\n",
-                            "          [-0.1000, -0.1000, -0.1000]],\n",
-                            "\n",
-                            "         [[ 0.1000,  0.1000, -0.1000],\n",
-                            "          [-0.1000,  0.1000, -0.1000],\n",
-                            "          [ 0.1000,  0.1000,  0.1000]]]], grad_fn=<MulBackward0>), scale=tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=None, training_t=tensor(True))"
+                            "QuantTensor(value=tensor([[[[ 0.9489, -0.9111, -0.0536,  0.5788,  0.3645],\n",
+                            "          [ 0.3401,  1.4325,  0.6498,  0.6411, -1.4390],\n",
+                            "          [-1.9029,  0.7012,  0.1591,  1.9235,  0.5883],\n",
+                            "          [-2.7258,  2.5330,  0.9165, -0.0820,  3.4148],\n",
+                            "          [-0.3651,  1.0164,  0.9567, -0.2758, -1.1376]],\n",
+                            "\n",
+                            "         [[-0.2414,  2.2111, -1.9124, -2.3814, -0.8805],\n",
+                            "          [ 1.3191, -0.8965, -0.2048, -3.8113,  1.1142],\n",
+                            "          [-0.3381, -0.2238,  1.2661,  0.0068,  0.2567],\n",
+                            "          [ 0.0731, -0.4280,  0.0909,  0.0875, -1.6851],\n",
+                            "          [-0.7744, -1.4127, -0.8143,  1.3557, -0.2802]]]],\n",
+                            "       grad_fn=<AddBackward0>), scale=tensor(0.0240, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(9.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
                     "execution_count": 11,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "from brevitas.nn import QuantConv2d\n",
-                "\n",
-                "binary_weight_quant_conv = QuantConv2d(3, 2, (3,3), weight_quant=MyBinaryWeightQuantizer)\n",
-                "quant_weight = binary_weight_quant_conv.quant_weight()\n",
-                "quant_weight"
+                "out_tensor = out_tensor_0 + out_tensor_1\n",
+                "out_tensor"
             ]
         },
         {
+            "attachments": {},
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Note however how the `QuantTensor` is not properly formed, as the `signed` attribute is `None`. This means that `quant_weight` is not considered valid, as the affine quantization invariant cannot be computed:"
+                "Because we set `training` to `True` for both of them, we are allowed to sum them even if they have different scale factors. The output QuantTensor will have the correct `bit_width`, and a scale which is the average of the two original scale factors. This is done only at training time, in order to propagate gradient information, however the consequence is that the resulting QuantTensor is no longer valid:"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 12,
             "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "False"
-                        ]
-                    },
-                    "execution_count": 12,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
+            "outputs": [],
             "source": [
-                "quant_weight.is_valid"
+                "assert not out_tensor.is_valid"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "`signed` is one of those attributes that in the case of binary quantization has to be explicitly defined by the user. The idea is that it informs the proxy on whether the value generated by our quantizer should be considered signed or not. We can do so by simply setting it in the quantizer:"
+                "`QuantTensor` implements `__torch_function__` to handle being called from torch functional operators (e.g. ops under `torch.nn.functional`). Passing a QuantTensor to supported ops that are invariant to quantization, e.g. max-pooling, preserve the the validity of a QuantTensor. Example:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 13,
+            "execution_count": 108,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[ 0.1000,  0.1000, -0.1000],\n",
-                            "          [-0.1000, -0.1000,  0.1000],\n",
-                            "          [ 0.1000,  0.1000, -0.1000]],\n",
-                            "\n",
-                            "         [[ 0.1000,  0.1000,  0.1000],\n",
-                            "          [ 0.1000, -0.1000,  0.1000],\n",
-                            "          [ 0.1000, -0.1000, -0.1000]],\n",
-                            "\n",
-                            "         [[-0.1000,  0.1000,  0.1000],\n",
-                            "          [ 0.1000, -0.1000, -0.1000],\n",
-                            "          [-0.1000, -0.1000, -0.1000]]],\n",
-                            "\n",
+                            "QuantTensor(value=tensor([[[[1.5800, 1.0157],\n",
+                            "          [1.4445, 0.8577]],\n",
                             "\n",
-                            "        [[[ 0.1000,  0.1000,  0.1000],\n",
-                            "          [ 0.1000,  0.1000, -0.1000],\n",
-                            "          [-0.1000, -0.1000,  0.1000]],\n",
+                            "         [[0.5643, 1.2414],\n",
+                            "          [1.0383, 0.9028]],\n",
                             "\n",
-                            "         [[-0.1000, -0.1000,  0.1000],\n",
-                            "          [-0.1000,  0.1000,  0.1000],\n",
-                            "          [-0.1000, -0.1000, -0.1000]],\n",
-                            "\n",
-                            "         [[-0.1000,  0.1000, -0.1000],\n",
-                            "          [-0.1000,  0.1000, -0.1000],\n",
-                            "          [-0.1000,  0.1000, -0.1000]]]], grad_fn=<MulBackward0>), scale=tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
+                            "         [[0.5191, 0.6546],\n",
+                            "          [2.1442, 0.5868]]]], grad_fn=<MaxPool2DWithIndicesBackward0>), scale=tensor(0.0226, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
-                    "execution_count": 13,
+                    "execution_count": 108,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "class MySignedBinaryWeightQuantizer(MyBinaryWeightQuantizer):\n",
-                "    signed = True\n",
-                "    \n",
-                "binary_weight_quant_conv = QuantConv2d(3, 2, (3,3), weight_quant=MySignedBinaryWeightQuantizer)\n",
-                "signed_quant_weight = binary_weight_quant_conv.quant_weight()\n",
-                "signed_quant_weight"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 14,
-            "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "True"
-                        ]
-                    },
-                    "execution_count": 14,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
-            "source": [
-                "signed_quant_weight.is_valid"
+                "import torch\n",
+                "\n",
+                "quant_identity = QuantIdentity(return_quant_tensor=True)\n",
+                "quant_tensor = quant_identity(torch.randn(1, 3, 4, 4))\n",
+                "torch.nn.functional.max_pool2d(quant_tensor, kernel_size=2, stride=2)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "And now the quant weights are valid.\n",
-                "\n",
-                "When we want to add or override an single attribute of a quantizer passed to a layer, defining a whole new quantizer can be too verbose. There is a simpler syntax to achieve the same goal. Let's say we want to have add the `signed` attribute to `MyBinaryQuantizer`, as we just did. We could have also simply done the following:"
+                "For ops that are not invariant to quantization, a `QuantTensor` decays into a floating-point `torch.Tensor`. Example:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 15,
+            "execution_count": 109,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[-0.1000,  0.1000,  0.1000],\n",
-                            "          [-0.1000, -0.1000, -0.1000],\n",
-                            "          [ 0.1000,  0.1000,  0.1000]],\n",
-                            "\n",
-                            "         [[-0.1000,  0.1000, -0.1000],\n",
-                            "          [ 0.1000,  0.1000,  0.1000],\n",
-                            "          [ 0.1000,  0.1000, -0.1000]],\n",
-                            "\n",
-                            "         [[ 0.1000, -0.1000, -0.1000],\n",
-                            "          [ 0.1000, -0.1000, -0.1000],\n",
-                            "          [ 0.1000,  0.1000,  0.1000]]],\n",
-                            "\n",
+                            "tensor([[[[-0.4943, -0.9938, -0.9073,  0.7681],\n",
+                            "          [-0.3262,  0.9186,  0.1786,  0.3659],\n",
+                            "          [ 0.7489,  0.8946, -0.0451, -0.5594],\n",
+                            "          [-0.1346, -0.4943, -0.4770,  0.6951]],\n",
                             "\n",
-                            "        [[[-0.1000,  0.1000,  0.1000],\n",
-                            "          [-0.1000, -0.1000,  0.1000],\n",
-                            "          [ 0.1000, -0.1000, -0.1000]],\n",
+                            "         [[ 0.0676,  0.5111,  0.4943,  0.8459],\n",
+                            "          [-0.8990, -0.9426,  0.0676, -0.7945],\n",
+                            "          [-0.9220,  0.0676, -0.5594,  0.6321],\n",
+                            "          [-0.0676,  0.7772,  0.7177, -0.4414]],\n",
                             "\n",
-                            "         [[ 0.1000,  0.1000, -0.1000],\n",
-                            "          [-0.1000,  0.1000,  0.1000],\n",
-                            "          [ 0.1000,  0.1000, -0.1000]],\n",
-                            "\n",
-                            "         [[ 0.1000,  0.1000, -0.1000],\n",
-                            "          [-0.1000,  0.1000,  0.1000],\n",
-                            "          [ 0.1000, -0.1000,  0.1000]]]], grad_fn=<MulBackward0>), scale=tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
+                            "         [[ 0.4770,  0.2220,  0.0676,  0.5747],\n",
+                            "          [-0.0451, -0.6710, -0.4594, -0.3462],\n",
+                            "          [ 0.9729, -0.7177, -0.5896, -0.5276],\n",
+                            "          [-0.0900,  0.8852,  0.5276, -0.4414]]]], grad_fn=<TanhBackward0>)"
                         ]
                     },
-                    "execution_count": 15,
+                    "execution_count": 109,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "small_scale_quant_conv = QuantConv2d(3, 2, (3,3), weight_quant=MyBinaryWeightQuantizer, weight_signed=True)\n",
-                "small_scale_quant_conv.quant_weight()"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "What we did was to take the name of the attribute `signed`, add the prefix `weight_`, and pass it as a keyword argument to `QuantConv2d`. What happens in the background is that the keyword arguments prefixed with `weight_` are set as attributes of `weight_quant`, possibly overriding any pre-existing value. The same principle applies to `input_`, `output_` and `bias_`.\n",
-                "\n",
-                "This is the reason why, as it was mentioned in the first tutorial, quantized layers can accept arbitrary keyword arguments. It's really just a way to support different styles of syntax when defining a quantizer."
+                "torch.tanh(quant_tensor)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "## Passing a custom quantizer to QuantIdentity\n",
+                "## Input Quantization\n",
                 "\n",
-                "We can do a similar thing with quantized activations:"
+                "We can obtain a valid output `QuantTensor` by making sure that both input and weight of `QuantConv2d` are quantized. To do so, we can set a quantizer for `input_quant`. In this example we pick a *signed 8-bit* quantizer with *per-tensor floating-point scale factor*:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 16,
+            "execution_count": 110,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[ 0.1000,  0.1000,  0.1000, -0.1000],\n",
-                            "        [ 0.1000,  0.1000, -0.1000, -0.1000],\n",
-                            "        [-0.1000, -0.1000, -0.1000, -0.1000],\n",
-                            "        [ 0.1000,  0.1000,  0.1000,  0.1000]], grad_fn=<MulBackward0>), scale=tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
+                            "QuantTensor(value=tensor([[[[ 0.9693, -0.9431,  0.2459],\n",
+                            "          [ 0.5416,  0.9037, -0.5278],\n",
+                            "          [-0.6207, -1.3578, -0.4815]],\n",
+                            "\n",
+                            "         [[ 0.4551, -1.4065,  0.8889],\n",
+                            "          [-0.3393,  0.0803, -0.1748],\n",
+                            "          [-0.0977,  0.6284, -0.7193]],\n",
+                            "\n",
+                            "         [[ 0.3655,  0.7626, -0.2634],\n",
+                            "          [-0.3453,  0.3349,  0.1923],\n",
+                            "          [ 0.5993, -0.9579,  0.3557]]]], grad_fn=<ThnnConv2DBackward0>), scale=tensor([[[[3.2208e-05]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(21.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
-                    "execution_count": 16,
+                    "execution_count": 110,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "from brevitas.proxy import ActQuantProxyFromInjector\n",
-                "from brevitas.nn import QuantIdentity\n",
+                "from brevitas.quant.scaled_int import Int8ActPerTensorFloat\n",
                 "\n",
-                "class MySignedBinaryActQuantizer(MyBinaryQuantizer):\n",
-                "    proxy_class = ActQuantProxyFromInjector\n",
-                "    signed = True\n",
-                "\n",
-                "binary_relu = QuantIdentity(act_quant=MySignedBinaryActQuantizer, return_quant_tensor=True)\n",
-                "binary_relu(torch.randn(4, 4))"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "So there isn't really much difference between a quantizer for weights and a quantizer for activations, they are just wrapped by different proxies. Also, with activations  a prefix is not required when passing keyword arguments. For example, when can override the existing `scaling_init` defined in `MyBinaryQuantizer` with a new value passed in as a keywork argument:"
+                "input_quant_conv = QuantConv2d(\n",
+                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False, \n",
+                "    input_quant=Int8ActPerTensorFloat, return_quant_tensor=True)\n",
+                "out_tensor = input_quant_conv(torch.randn(1, 2, 5, 5))\n",
+                "out_tensor"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 17,
+            "execution_count": 111,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[ 0.0010,  0.0010,  0.0010, -0.0010],\n",
-                            "        [-0.0010,  0.0010, -0.0010,  0.0010],\n",
-                            "        [-0.0010, -0.0010,  0.0010, -0.0010],\n",
-                            "        [-0.0010,  0.0010, -0.0010, -0.0010]], grad_fn=<MulBackward0>), scale=tensor(0.0010, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
+                            "True"
                         ]
                     },
-                    "execution_count": 17,
+                    "execution_count": 111,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "small_scale_binary_identity = QuantIdentity(\n",
-                "    act_quant=MySignedBinaryActQuantizer, scaling_init=0.001, return_quant_tensor=True)\n",
-                "small_scale_binary_identity(torch.randn(4, 4))"
+                "assert out_tensor.is_valid"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "## A Custom Quantizer initialized with Weight Statistics\n",
-                "\n",
-                "So far we have seen use-cases where an `ExtendedInjector` provides, at best, a different kind of syntax to define a quantizer, without any particular other advantage. Let's now make things a bit more complicated to show the sort of situations where it really shines.\n",
-                "\n",
-                "Let's say we want to define a binary weight quantizer where `scaling_impl` is still `ParameterScaling`. However, instead of being user-defined, we want `scaling_init` to be the maximum value found in the weight tensor of the quantized layer.\n",
-                "To support this sort of use cases where the quantizer depends on the layer, a quantized layer automatically passes itself to all its quantizers under the name of `module`.\n",
-                "With only a few lines of code then, we can achieve our goal:"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 18,
-            "metadata": {},
-            "outputs": [],
-            "source": [
-                "from brevitas.inject import value\n",
+                "What happens internally is that the input tensor passed to `input_quant_conv` is being quantized before being passed to the convolution operator. That means we are now computing a convolution between two quantized tensors, which mimplies that the output of the operation is also quantized. As expected then `out_tensor` is marked as valid. \n",
                 "\n",
-                "class ParamFromMaxWeightQuantizer(MySignedBinaryWeightQuantizer):\n",
-                "    \n",
-                "    @value\n",
-                "    def scaling_init(module):\n",
-                "        return module.weight.abs().max()"
+                "Another important thing to notice is how the `bit_width` field of `out_tensor` is relatively high at *21 bits*. In Brevitas, the assumption is always that the output bit-width of an operator reflects the worst-case size of the *accumulator* required by that operation. In other terms, given the *size* of the input and weight tensors and their *bit-widths*, 21 is the bit-width that would be required to represent the largest possible output value that could be generated. This makes sure that the affine quantization invariant is always respected."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Note how we are leveraging the `@value` *decorator* to define a function that is executed at *dependency-injection (DI) time*. This kind of behaviour is similar in spirit to defining a `@property` instead of an *attribute*, with the difference that a `@value` function can depend on other attributes of the Injector, which are automatically passed in as arguments of the function during DI.\n",
-                "\n",
-                "Let's now pass the quantizer to a QuantConv2d and retrieve its quantized weights:"
+                "We could have obtained a similar result by directly passing as input a QuantTensor. In this example we are directly defining a QuantTensor ourselves, but it could also be the output of a previous layer."
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 19,
-            "metadata": {
-                "scrolled": true
-            },
+            "execution_count": 112,
+            "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[ 0.1890,  0.1890,  0.1890],\n",
-                            "          [ 0.1890,  0.1890, -0.1890],\n",
-                            "          [ 0.1890,  0.1890, -0.1890]],\n",
-                            "\n",
-                            "         [[ 0.1890,  0.1890, -0.1890],\n",
-                            "          [-0.1890,  0.1890,  0.1890],\n",
-                            "          [ 0.1890, -0.1890,  0.1890]],\n",
-                            "\n",
-                            "         [[ 0.1890,  0.1890, -0.1890],\n",
-                            "          [-0.1890,  0.1890,  0.1890],\n",
-                            "          [-0.1890, -0.1890, -0.1890]]],\n",
+                            "QuantTensor(value=tensor([[[[ 5.7000e-03,  2.5000e-03, -1.2400e-02, -7.2000e-03,  3.7000e-03],\n",
+                            "          [-2.3000e-03,  7.0000e-04, -1.2700e-02,  5.2000e-03,  4.0000e-04],\n",
+                            "          [-7.9000e-03,  9.5000e-03,  6.6000e-03,  5.4000e-03,  2.5000e-03],\n",
+                            "          [ 1.1100e-02,  2.4000e-03,  1.0000e-02, -3.7000e-03,  7.2000e-03],\n",
+                            "          [-1.1500e-02, -5.8000e-03, -9.3000e-03,  1.0000e-02,  3.5000e-03]],\n",
                             "\n",
-                            "\n",
-                            "        [[[ 0.1890, -0.1890,  0.1890],\n",
-                            "          [ 0.1890, -0.1890,  0.1890],\n",
-                            "          [-0.1890, -0.1890,  0.1890]],\n",
-                            "\n",
-                            "         [[-0.1890,  0.1890, -0.1890],\n",
-                            "          [-0.1890, -0.1890, -0.1890],\n",
-                            "          [-0.1890,  0.1890,  0.1890]],\n",
-                            "\n",
-                            "         [[ 0.1890,  0.1890, -0.1890],\n",
-                            "          [ 0.1890,  0.1890,  0.1890],\n",
-                            "          [-0.1890, -0.1890,  0.1890]]]], grad_fn=<MulBackward0>), scale=tensor(0.1890, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
+                            "         [[-6.8000e-03,  1.1500e-02, -1.0600e-02, -1.5000e-03, -1.9000e-03],\n",
+                            "          [ 2.9000e-03,  9.5000e-03,  7.2000e-03, -3.7000e-03,  7.7000e-03],\n",
+                            "          [-2.4000e-03, -8.9000e-03, -1.2000e-02, -8.1000e-03,  7.2000e-03],\n",
+                            "          [-1.1300e-02, -9.7000e-03, -1.0000e-03,  1.0100e-02,  3.8000e-03],\n",
+                            "          [-1.1900e-02,  6.9000e-03,  8.3000e-03,  1.0000e-04, -6.9000e-03]]]]), scale=tensor(1.0000e-04), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
-                    "execution_count": 19,
+                    "execution_count": 112,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "param_from_max_quant_conv = QuantConv2d(3, 2, (3, 3), weight_quant=ParamFromMaxWeightQuantizer)\n",
-                "param_from_max_quant_conv.quant_weight()"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "Indeed we can verify that `quant_weight_scale()` is equal to `weight.abs().max()`:"
+                "from brevitas.quant_tensor import QuantTensor\n",
+                "\n",
+                "scale = 0.0001\n",
+                "bit_width = 8\n",
+                "zero_point = 0.\n",
+                "int_value = torch.randint(low=- 2 ** (bit_width - 1), high=2 ** (bit_width - 1) - 1, size=(1, 2, 5, 5))\n",
+                "quant_value = (int_value - zero_point) * scale\n",
+                "quant_tensor_input = QuantTensor(\n",
+                "    quant_value, \n",
+                "    scale=torch.tensor(scale), \n",
+                "    zero_point=torch.tensor(zero_point), \n",
+                "    bit_width=torch.tensor(float(bit_width)),\n",
+                "    signed=True,\n",
+                "    training=True)\n",
+                "quant_tensor_input"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 20,
+            "execution_count": 113,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "True"
                         ]
                     },
-                    "execution_count": 20,
+                    "execution_count": 113,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "(param_from_max_quant_conv.quant_weight_scale() == param_from_max_quant_conv.weight.abs().max()).item()"
+                "assert quant_tensor_input.is_valid"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Let's say now that we want to load a pretrained floating-point weight tensor on top of our quantized model. We simuate this scenario by defining a separate `nn.Conv2d` layer with the same weight shape:"
+                "**Note**: how we are explicitly forcing `value`, `scale`, `zero_point` and `bit_width` to be floating-point `torch.Tensor`, as this is expected by Brevitas but it's currently not enforced automatically at initialization time.\n",
+                "\n",
+                "If we now pass in `quant_tensor_input` to `return_quant_conv`, we will see that indeed the output is a valid 21-bit `QuantTensor`:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 21,
+            "execution_count": 114,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "tensor(0.1820, grad_fn=<MaxBackward1>)"
+                            "QuantTensor(value=tensor([[[[ 0.0085,  0.0066,  0.0050],\n",
+                            "          [-0.0038, -0.0009, -0.0115],\n",
+                            "          [-0.0055, -0.0037,  0.0009]],\n",
+                            "\n",
+                            "         [[ 0.0015, -0.0027, -0.0079],\n",
+                            "          [-0.0034, -0.0060,  0.0043],\n",
+                            "          [-0.0008,  0.0052, -0.0033]],\n",
+                            "\n",
+                            "         [[-0.0015,  0.0082, -0.0038],\n",
+                            "          [-0.0021,  0.0004, -0.0054],\n",
+                            "          [-0.0021, -0.0079,  0.0013]]]], grad_fn=<ThnnConv2DBackward0>), scale=tensor([[[[1.8448e-07]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(21.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
-                    "execution_count": 21,
+                    "execution_count": 114,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "from torch import nn\n",
-                "\n",
-                "float_conv = nn.Conv2d(3, 2, (3, 3))\n",
-                "float_conv.weight.abs().max()"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "and then we load it on top of `param_from_max_quant_conv`:"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 22,
-            "metadata": {
-                "tags": [
-                    "raises-exception"
-                ]
-            },
-            "outputs": [
-                {
-                    "ename": "RuntimeError",
-                    "evalue": "Error(s) in loading state_dict for QuantConv2d:\n\tMissing key(s) in state_dict: \"weight_quant.tensor_quant.scaling_impl.value\". ",
-                    "output_type": "error",
-                    "traceback": [
-                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
-                        "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
-                        "\u001b[1;32m<ipython-input-22-5b3646241211>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparam_from_max_quant_conv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat_conv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
-                        "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m-> 1407\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m   1408\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for QuantConv2d:\n\tMissing key(s) in state_dict: \"weight_quant.tensor_quant.scaling_impl.value\". "
-                    ]
-                }
-            ],
-            "source": [
-                "param_from_max_quant_conv.load_state_dict(float_conv.state_dict())"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "Ouch, we get an error. This is because `ParameterScaling` contains a learned `torch.nn.Parameter`, and Pytorch expects all learned parameters of a model to be contained in a `state_dict` that is being loaded.\n",
-                "We can work around the issue by either setting the `IGNORE_MISSING_KEYS` config flag in Brevitas, or by passing `strict=False` to load_state_dict. We go with the former as setting `strict=False` is too forgiving to other kind of problems:"
+                "return_quant_conv = QuantConv2d(\n",
+                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False, return_quant_tensor=True)\n",
+                "out_tensor = return_quant_conv(quant_tensor_input)\n",
+                "out_tensor"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 23,
+            "execution_count": 115,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "<All keys matched successfully>"
+                            "True"
                         ]
                     },
-                    "execution_count": 23,
+                    "execution_count": 115,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "from brevitas import config\n",
-                "config.IGNORE_MISSING_KEYS = True\n",
-                "\n",
-                "param_from_max_quant_conv.load_state_dict(float_conv.state_dict())"
+                "assert out_tensor.is_valid"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Note that we could have also achieve the same goal by setting the *env variable* `BREVITAS_IGNORE_MISSING_KEYS=1`.\n",
-                "\n",
-                "And now if we take a look at the quantized weights again:"
+                "We can also pass in an input `QuantTensor` to a layer that has `input_quant` enabled. In that case, the input gets re-quantized:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 24,
+            "execution_count": 116,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[-0.1820,  0.1820, -0.1820],\n",
-                            "          [ 0.1820, -0.1820,  0.1820],\n",
-                            "          [-0.1820, -0.1820, -0.1820]],\n",
-                            "\n",
-                            "         [[ 0.1820,  0.1820,  0.1820],\n",
-                            "          [-0.1820,  0.1820,  0.1820],\n",
-                            "          [-0.1820, -0.1820,  0.1820]],\n",
-                            "\n",
-                            "         [[ 0.1820, -0.1820,  0.1820],\n",
-                            "          [ 0.1820, -0.1820, -0.1820],\n",
-                            "          [-0.1820, -0.1820, -0.1820]]],\n",
-                            "\n",
+                            "QuantTensor(value=tensor([[[[-0.0035, -0.0037, -0.0050],\n",
+                            "          [ 0.0010, -0.0051, -0.0027],\n",
+                            "          [-0.0010,  0.0047,  0.0017]],\n",
                             "\n",
-                            "        [[[-0.1820, -0.1820,  0.1820],\n",
-                            "          [-0.1820, -0.1820,  0.1820],\n",
-                            "          [ 0.1820, -0.1820,  0.1820]],\n",
+                            "         [[ 0.0021,  0.0002,  0.0027],\n",
+                            "          [ 0.0028,  0.0002, -0.0044],\n",
+                            "          [ 0.0008, -0.0052, -0.0024]],\n",
                             "\n",
-                            "         [[-0.1820,  0.1820,  0.1820],\n",
-                            "          [ 0.1820,  0.1820,  0.1820],\n",
-                            "          [ 0.1820, -0.1820,  0.1820]],\n",
-                            "\n",
-                            "         [[ 0.1820,  0.1820, -0.1820],\n",
-                            "          [ 0.1820, -0.1820,  0.1820],\n",
-                            "          [-0.1820,  0.1820, -0.1820]]]], grad_fn=<MulBackward0>), scale=tensor(0.1820, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
+                            "         [[ 0.0010, -0.0052, -0.0011],\n",
+                            "          [-0.0018,  0.0024,  0.0011],\n",
+                            "          [-0.0001,  0.0039,  0.0035]]]], grad_fn=<ThnnConv2DBackward0>), scale=tensor([[[[1.7410e-07]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(21.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
-                    "execution_count": 24,
+                    "execution_count": 116,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "param_from_max_quant_conv.quant_weight()"
+                "input_quant_conv(quant_tensor_input)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "We see that, as expected, the scale factor has been updated to the new `weight.abs().max()`.\n",
+                "## Output Quantization\n",
                 "\n",
-                "What happens internally is that after `load_state_dict` is called on the layer, `ParamFromMaxWeightQuantizer.tensor_quant` gets called again to re-initialize `BinaryQuant`, and in turn `ParameterScaling` is re-initialized with a new `scaling_init` value computed based on the updated `module.weight` tensor. This whole process wouldn't have been possible without an `ExtendedInjector` behind it. "
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "## Sharing a Quantizer\n",
-                "\n",
-                "There are two ways to share a quantizer between multiple layers, with importance differences.\n",
-                "\n",
-                "The first one, which we have seen so far, is to simply pass the same ExtendedInjector to multiple layers. What that does is sharing the same quantization strategy among different layers. Each layer still gets its own instance of the quantization implementation."
+                "Let's now look at would have happened if we instead enabled output quantization:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 25,
+            "execution_count": 117,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "False"
+                            "QuantTensor(value=tensor([[[[ 0.2111,  0.4060,  0.3654],\n",
+                            "          [-0.7876,  0.8119, -0.9825],\n",
+                            "          [-0.5115,  0.3979, -0.3248]],\n",
+                            "\n",
+                            "         [[ 0.3816,  0.0568, -0.0812],\n",
+                            "          [ 1.0312, -0.7876,  0.8038],\n",
+                            "          [-0.3491, -0.4141,  0.0650]],\n",
+                            "\n",
+                            "         [[-0.5846, -0.4222, -0.0731],\n",
+                            "          [-0.7389,  0.5034, -0.2517],\n",
+                            "          [-0.1624, -0.4385,  0.7308]]]], grad_fn=<MulBackward0>), scale=tensor(0.0081, grad_fn=<DivBackward0>), zero_point=tensor(0.), bit_width=tensor(8.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
-                    "execution_count": 25,
+                    "execution_count": 117,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "quant_conv1 = QuantConv2d(3, 2, (3, 3), weight_quant=MySignedBinaryWeightQuantizer)\n",
-                "quant_conv2 = QuantConv2d(3, 2, (3, 3), weight_quant=MySignedBinaryWeightQuantizer)\n",
+                "from brevitas.quant.scaled_int import Int8ActPerTensorFloat\n",
                 "\n",
-                "quant_conv1.weight_quant is quant_conv2.weight_quant"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "### Sharing a proxy\n",
-                "\n",
-                "The second one, which we are introducing now, allows to share the same quantization instance among multiple layers. This is done by simply sharing the proxy wrapping it. This can be useful in those scenarios where, for example, we want different layers to share the same scale factor. The syntax goes as follows:"
+                "output_quant_conv = QuantConv2d(\n",
+                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=False, \n",
+                "    output_quant=Int8ActPerTensorFloat, return_quant_tensor=True)\n",
+                "out_tensor = output_quant_conv(torch.randn(1, 2, 5, 5))\n",
+                "out_tensor"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 26,
+            "execution_count": 118,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
                             "True"
                         ]
                     },
-                    "execution_count": 26,
+                    "execution_count": 118,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "quant_conv1 = QuantConv2d(3, 2, (3, 3), weight_quant=MySignedBinaryWeightQuantizer)\n",
-                "quant_conv2 = QuantConv2d(3, 2, (3, 3), weight_quant=quant_conv1.weight_quant)\n",
-                "\n",
-                "quant_conv1.weight_quant is quant_conv2.weight_quant"
+                "assert out_tensor.is_valid"
             ]
         },
         {
-            "cell_type": "code",
-            "execution_count": 27,
+            "cell_type": "markdown",
             "metadata": {},
-            "outputs": [
-                {
-                    "data": {
-                        "text/plain": [
-                            "True"
-                        ]
-                    },
-                    "execution_count": 27,
-                    "metadata": {},
-                    "output_type": "execute_result"
-                }
-            ],
             "source": [
-                "(quant_conv1.quant_weight_scale() == quant_conv2.quant_weight_scale()).item()"
+                "We can see again that the output is a valid `QuantTensor`. However, what happened internally is quite different from before.  \n",
+                "Previously, we computed the convolution between two quantized tensors, and got a quantized tensor as output.  \n",
+                "In this case instead, we compute the convolution between a quantized and an unquantized tensor, we take its unquantized output and we quantize it.  \n",
+                "The difference is obvious once we look at the output `bit_width`. In the previous case, we had that the `bit_width` reflected the size of the output accumulator. In this case instead, we have `bit_width=tensor(8.)`, which is what we expected since `output_quant` had been set to an *Int8* quantizer."
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "What happens in background is that the weight quantizer now has access to both `quant_conv1` and `quant_conv2`. \n",
-                "So let's say we want to build a quantizer similar to `ParamFromMaxWeightQuantizer`, but in this case we want the scale factor to be initialized with the average of both weight tensors. When a quantizer has access to multiple parent modules, they are passed in at dependency injection time as a *tuple* under the same name `module` as before. So we can do the following:"
+                "## Bias Quantization\n",
+                "\n",
+                "There is an important scenario where the various options we just saw make a practical difference, and it's quantization of *bias*. In many contexts, such as in the ONNX standard opset and in FINN, bias is assumed to be quantized with scale factor equal to *input scale * weight scale*, which means that we need a valid quantized input somehow. A predefined bias quantizer that reflects that assumption is `brevitas.quant.scaled_int.Int8Bias`. If we simply tried to set it to a `QuantConv2d` without any sort of input quantization, we would get an error:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 28,
-            "metadata": {},
+            "execution_count": 119,
+            "metadata": {
+                "tags": [
+                    "raises-exception"
+                ]
+            },
             "outputs": [
                 {
-                    "data": {
-                        "text/plain": [
-                            "False"
-                        ]
-                    },
-                    "execution_count": 28,
-                    "metadata": {},
-                    "output_type": "execute_result"
+                    "ename": "RuntimeError",
+                    "evalue": "Input scale required",
+                    "output_type": "error",
+                    "traceback": [
+                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+                        "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
+                        "\u001b[0;32m/tmp/ipykernel_48365/2280634207.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     bias_quant=Int8Bias, return_quant_tensor=True)\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbias_quant_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
+                        "\u001b[0;32m/opt/conda/envs/torch_1.10/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+                        "\u001b[0;32m/workspace/scratch/git/fork_brevitas/src/brevitas/nn/quant_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+                        "\u001b[0;32m/workspace/scratch/git/fork_brevitas/src/brevitas/nn/quant_layer.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mquant_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_bit_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_inference_quant_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CachedIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+                        "\u001b[0;32m/opt/conda/envs/torch_1.10/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+                        "\u001b[0;32m/workspace/scratch/git/fork_brevitas/src/brevitas/proxy/parameter_quant.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, input_scale, input_bit_width)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_handler\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_quant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_input_scale\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_scale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input scale required\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_input_bit_width\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_bit_width\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input bit-width required\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+                        "\u001b[0;31mRuntimeError\u001b[0m: Input scale required"
+                    ]
                 }
             ],
             "source": [
-                "class SharedParamFromMeanWeightQuantizer(MySignedBinaryWeightQuantizer):\n",
-                "    \n",
-                "    @value\n",
-                "    def scaling_init(module):\n",
-                "        if isinstance(module, tuple):\n",
-                "            return torch.cat((module[0].weight.view(-1), module[1].weight.view(-1))).abs().mean()\n",
-                "        else:\n",
-                "            return module.weight.abs().mean()\n",
-                "        \n",
-                "quant_conv1 = QuantConv2d(3, 2, (3, 3), weight_quant=SharedParamFromMeanWeightQuantizer)\n",
-                "old_quant_conv1_scale = quant_conv1.quant_weight_scale()\n",
-                "quant_conv2 = QuantConv2d(3, 2, (3, 3), weight_quant=quant_conv1.weight_quant)\n",
-                "new_quant_conv1_scale = quant_conv1.quant_weight_scale()\n",
+                "from brevitas.quant.scaled_int import Int8Bias\n",
                 "\n",
-                "(old_quant_conv1_scale == new_quant_conv1_scale).item()"
+                "bias_quant_conv = QuantConv2d(\n",
+                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
+                "    bias_quant=Int8Bias, return_quant_tensor=True)\n",
+                "bias_quant_conv(torch.randn(1, 2, 5, 5))"
+            ]
+        },
+        {
+            "cell_type": "markdown",
+            "metadata": {},
+            "source": [
+                "We can solve the issue by passing in a valid `QuantTensor`, e.g. the `quant_tensor_input`  we defined above:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 29,
+            "execution_count": 120,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "True"
+                            "QuantTensor(value=tensor([[[[ 0.0005,  0.0043, -0.0004],\n",
+                            "          [ 0.0005,  0.0106,  0.0012],\n",
+                            "          [ 0.0021,  0.0007, -0.0050]],\n",
+                            "\n",
+                            "         [[-0.0067, -0.0035, -0.0059],\n",
+                            "          [-0.0050, -0.0015, -0.0039],\n",
+                            "          [ 0.0015,  0.0028, -0.0008]],\n",
+                            "\n",
+                            "         [[-0.0051, -0.0050,  0.0060],\n",
+                            "          [-0.0015,  0.0037,  0.0071],\n",
+                            "          [ 0.0067,  0.0035, -0.0071]]]], grad_fn=<ThnnConv2DBackward0>), scale=tensor([[[[1.8108e-07]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(22.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
-                    "execution_count": 29,
+                    "execution_count": 120,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "(new_quant_conv1_scale == quant_conv2.quant_weight_scale()).item()"
+                "bias_quant_conv(quant_tensor_input)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Note how, when `quant_conv2` is initialized using the `weight_quant` of `quant_conv1`, weight quantization is re-initialized for both layers such that they end up having the same scale.\n",
-                "\n",
-                "We can see in this example how Brevitas works consistently with Pytorch's eager execution model. When we initialize `quant_conv1` we still don't know that its weight quantizer is going to be shared with `quant_conv2`, and the semantics of Pytorch impose that `quant_conv1` should work correctly both before and after `quant_conv2` is declared. The way we take advantage of dependency injection allows to do so."
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "### Sharing an instance of Activation Quantization\n",
-                "\n",
-                "Sharing an instance of activation quantization is easier because for most scenarios it's enough to simply share the whole layer itself, e.g. calling the same `QuantReLU` from multiple places in the forward pass.\n",
-                "\n",
-                "For those scenarios where sharing the whole layer is not possible, there is something important to keep in mind. Instances of activation quantization include (for performance reasons) the implementation of the non-linear activation itself (if any). So, for example, using a `QuantReLU.act_quant` to initialize a `QuantConv2d.output_quant` should be avoided as we would not share not only the quantizer, but also the relu activation function.  \n",
-                "In general then sharing of instances of activations quantization should be done only between activations of the same *kind*. \n",
-                "\n",
-                "*Note*: we say kind and not type because `input_quant`, `output_quant` and `IdentityQuant` count as being the same kind of activation, even though they belong to different type of layers."
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "## Dealing with Weight Initialization\n",
-                "\n",
-                "There is a type of situation that Brevitas cannot deal with automatically. That is, when the initialization of the quantizer depends on the layer to which it is applied (like with the `ParamFromMaxWeightQuantizer` or `SharedParamFromMeanWeightQuantizer` quantizers), but the layer gets modified after it is initialized. \n",
-                "\n",
-                "The typical example is with weight initialization when training from scratch (so rather than loading from a floating-point state_dict):"
+                "Or by enabling input quantization and then passing in a float a `torch.Tensor` or a `QuantTensor`:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 30,
+            "execution_count": 121,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "False"
+                            "QuantTensor(value=tensor([[[[-0.3825,  0.1371,  0.9135],\n",
+                            "          [-0.2016,  0.7495, -0.4071],\n",
+                            "          [-0.0755,  0.5283,  0.2388]],\n",
+                            "\n",
+                            "         [[ 0.0788, -0.3802, -0.2234],\n",
+                            "          [ 0.8678, -0.5546,  0.4408],\n",
+                            "          [-0.6788,  0.4422,  0.3007]],\n",
+                            "\n",
+                            "         [[ 0.4412, -0.3205,  1.0033],\n",
+                            "          [-0.0083, -0.3295, -0.2076],\n",
+                            "          [ 0.4417, -0.1046, -0.3493]]]], grad_fn=<ThnnConv2DBackward0>), scale=tensor([[[[3.8610e-05]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(22.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
-                    "execution_count": 30,
+                    "execution_count": 121,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "quant_conv_w_init = QuantConv2d(3, 2, (3, 3), weight_quant=ParamFromMaxWeightQuantizer)\n",
-                "torch.nn.init.uniform_(quant_conv_w_init.weight)\n",
-                "\n",
-                "(quant_conv_w_init.weight.abs().max() == quant_conv_w_init.quant_weight_scale()).item()"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "We can see how the scale factor is not initialized correctly anymore. In this case we can simply trigger re-initialization of the weight quantizer manually:"
+                "input_bias_quant_conv = QuantConv2d(\n",
+                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
+                "    input_quant=Int8ActPerTensorFloat, bias_quant=Int8Bias, return_quant_tensor=True)\n",
+                "input_bias_quant_conv(torch.randn(1, 2, 5, 5))"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 31,
+            "execution_count": 122,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "True"
+                            "QuantTensor(value=tensor([[[[ 0.0036,  0.0024, -0.0033],\n",
+                            "          [ 0.0050,  0.0080, -0.0014],\n",
+                            "          [-0.0036, -0.0080, -0.0029]],\n",
+                            "\n",
+                            "         [[ 0.0083, -0.0093,  0.0048],\n",
+                            "          [ 0.0035,  0.0015, -0.0011],\n",
+                            "          [-0.0003,  0.0067,  0.0013]],\n",
+                            "\n",
+                            "         [[-0.0009, -0.0019,  0.0039],\n",
+                            "          [ 0.0010,  0.0056, -0.0037],\n",
+                            "          [ 0.0091, -0.0095,  0.0054]]]], grad_fn=<ThnnConv2DBackward0>), scale=tensor([[[[1.8384e-07]]]], grad_fn=<MulBackward0>), zero_point=tensor(0.), bit_width=tensor(22.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
-                    "execution_count": 31,
+                    "execution_count": 122,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "quant_conv_w_init.weight_quant.init_tensor_quant()\n",
-                "\n",
-                "(quant_conv_w_init.weight.abs().max() == quant_conv_w_init.quant_weight_scale()).item()"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "*Note*: because the way weights are initialized is often the same as to how an optimizer performs the weight update step, there are currently no plans to try to perform re-initialization automatically (as it happens e.g. when a `state_dict` is loaded) since it wouldn't be possible to distinguish between the two scenarios."
+                "input_bias_quant_conv(quant_tensor_input)"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "## Building a Custom Quantization API\n",
-                "\n",
-                "Finally, let's go through an even more complicated example. We are going to look at a scenario that illustrates the differences between a standard `Injector` (implemented in the dependencies library) and our `ExtendedInjector` extension.\n",
+                "Notice how the output `bit_width=tensor(22.)`. This is because, in the worst-case, summing a *21-bit* integer (the size of the accumulator before bias is added) and an *8-bit* integer (the size of quantized bias) gives a *22-bit* integer.\n",
                 "\n",
-                "Let's say we want to build two quantizers for respectively weights and activations and build a simple API on top of them.\n",
-                "In particular, we want to be able to switch between `BinaryQuant` and `ClampedBinaryQuant` (a variant of binary quantization with clamping), and we want to optionally perform *per-channel scaling*.\n",
-                "To do so, we are going to implement the controlling logic through a hierarchy of ExtendedInjector, leaving two boolean flags exposed as arguments of the quantizers, with the idea then that the flags can be set through keyword arguments of the respective quantized layers.\n",
-                "\n",
-                "We can go as follows:"
+                "Let's try now to enable output quantization instead of input quantization. That wouldn't have solved the problem with bias quantization, as output quantization is performed after bias is added:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 32,
-            "metadata": {},
-            "outputs": [],
+            "execution_count": 123,
+            "metadata": {
+                "tags": [
+                    "raises-exception"
+                ]
+            },
+            "outputs": [
+                {
+                    "ename": "RuntimeError",
+                    "evalue": "Input scale required",
+                    "output_type": "error",
+                    "traceback": [
+                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+                        "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
+                        "\u001b[0;32m/tmp/ipykernel_48365/2990591641.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     output_quant=Int8ActPerTensorFloat, bias_quant=Int8Bias, return_quant_tensor=True)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutput_bias_quant_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
+                        "\u001b[0;32m/opt/conda/envs/torch_1.10/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+                        "\u001b[0;32m/workspace/scratch/git/fork_brevitas/src/brevitas/nn/quant_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_weight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+                        "\u001b[0;32m/workspace/scratch/git/fork_brevitas/src/brevitas/nn/quant_layer.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mquant_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_quant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_bit_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_inference_quant_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CachedIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+                        "\u001b[0;32m/opt/conda/envs/torch_1.10/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+                        "\u001b[0;32m/workspace/scratch/git/fork_brevitas/src/brevitas/proxy/parameter_quant.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, input_scale, input_bit_width)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_handler\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_quant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_input_scale\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_scale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input scale required\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_input_bit_width\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_bit_width\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input bit-width required\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+                        "\u001b[0;31mRuntimeError\u001b[0m: Input scale required"
+                    ]
+                }
+            ],
             "source": [
-                "from brevitas.core.quant import ClampedBinaryQuant\n",
-                "from brevitas.proxy import WeightQuantProxyFromInjector, ActQuantProxyFromInjector\n",
-                "from brevitas.inject import this\n",
-                "\n",
-                "\n",
-                "class CommonQuantizer(ExtendedInjector):\n",
-                "    scaling_impl = ParameterScaling\n",
-                "    signed=True\n",
-                "    \n",
-                "    @value\n",
-                "    def tensor_quant(is_clamped):\n",
-                "        # returning a class to auto-wire from a value function\n",
-                "        # wouldn't be allowed in a standard Injector\n",
-                "        if is_clamped:\n",
-                "            return ClampedBinaryQuant\n",
-                "        else:\n",
-                "            return BinaryQuant\n",
-                "    \n",
-                "    @value\n",
-                "    def scaling_shape(scaling_per_output_channel):\n",
-                "        if scaling_per_output_channel:\n",
-                "            # returning this.something from a value function \n",
-                "            # wouldn't be allowed in a standard Injector\n",
-                "            return this.per_channel_broadcastable_shape\n",
-                "        else:\n",
-                "            return ()\n",
-                "        \n",
-                "        \n",
-                "class AdvancedWeightQuantizer(CommonQuantizer):\n",
-                "    proxy_class = WeightQuantProxyFromInjector\n",
-                "        \n",
-                "    @value\n",
-                "    def per_channel_broadcastable_shape(module):\n",
-                "        return (module.weight.shape[0], 1, 1, 1)\n",
-                "    \n",
-                "    @value\n",
-                "    def scaling_init(module, scaling_per_output_channel):\n",
-                "        if scaling_per_output_channel:\n",
-                "            num_ch = module.weight.shape[0]\n",
-                "            return module.weight.abs().view(num_ch, -1).max(dim=1)[0].view(-1, 1, 1, 1)\n",
-                "        else:\n",
-                "            return module.weight.abs().max()\n",
-                "        \n",
-                "        \n",
-                "class AdvancedActQuantizer(CommonQuantizer):\n",
-                "    scaling_init = 0.01\n",
-                "    proxy_class = ActQuantProxyFromInjector"
+                "output_bias_quant_conv = QuantConv2d(\n",
+                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
+                "    output_quant=Int8ActPerTensorFloat, bias_quant=Int8Bias, return_quant_tensor=True)\n",
+                "output_bias_quant_conv(torch.randn(1, 2, 5, 5))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "There are a bunch of things going on here to unpack.  \n",
-                "\n",
-                "The first one is that a `@value` function can return a class to auto-wire and inject, as seen in the definition of `tensor_quant`. This wouldn't normally be possible with a standard `Injector`, but it's possible with an `ExtendedInjector`. This way we can switch between different implementations of `tensor_quant`.\n",
-                "\n",
-                "The second one is the special object `this`. `this` is already present in the *dependencies* library, and it's used as a way to retrieve attributes of the quantizer from within the quantizer itself. However, normally it wouldn't be possible to return a reference to `this` from a `@value` function. Again this is something that only a `ExtendedInjector` supports, and it allows to chain different attributes in a way such that the chained values are computed only when necessary. \n",
-                "\n",
-                "Let's see the quantizers applied to a layer:"
+                "Not all scenarios require bias quantization to depend on the scale factor of the input. In those cases, biases can be quantized the same way weights are quantized, and have their own scale factor. In Brevitas, a predefined quantizer that reflects this other scenario is `Int8BiasPerTensorFloatInternalScaling`. In this case then a valid quantized input is not required:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 33,
+            "execution_count": 124,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[-0.1841, -0.1841, -0.1841],\n",
-                            "          [-0.1841, -0.1841, -0.1841],\n",
-                            "          [-0.1841,  0.1841,  0.1841]],\n",
-                            "\n",
-                            "         [[ 0.1841, -0.1841,  0.1841],\n",
-                            "          [ 0.1841, -0.1841, -0.1841],\n",
-                            "          [ 0.1841,  0.1841, -0.1841]],\n",
+                            "tensor([[[[ 0.2152,  0.8346,  0.0746],\n",
+                            "          [-0.0738, -0.5212,  0.1019],\n",
+                            "          [-0.6004,  0.1500, -0.1453]],\n",
                             "\n",
-                            "         [[-0.1841, -0.1841,  0.1841],\n",
-                            "          [ 0.1841, -0.1841,  0.1841],\n",
-                            "          [ 0.1841,  0.1841, -0.1841]]],\n",
+                            "         [[-1.1551, -1.3458, -0.1312],\n",
+                            "          [ 0.2502, -0.5267,  0.2412],\n",
+                            "          [-0.3556, -0.3289, -0.2276]],\n",
                             "\n",
-                            "\n",
-                            "        [[[ 0.1916, -0.1916,  0.1916],\n",
-                            "          [-0.1916, -0.1916,  0.1916],\n",
-                            "          [ 0.1916, -0.1916, -0.1916]],\n",
-                            "\n",
-                            "         [[-0.1916,  0.1916, -0.1916],\n",
-                            "          [-0.1916, -0.1916,  0.1916],\n",
-                            "          [-0.1916, -0.1916,  0.1916]],\n",
-                            "\n",
-                            "         [[ 0.1916, -0.1916, -0.1916],\n",
-                            "          [-0.1916,  0.1916, -0.1916],\n",
-                            "          [ 0.1916, -0.1916,  0.1916]]]], grad_fn=<MulBackward0>), scale=tensor([[[[0.1841]]],\n",
-                            "\n",
-                            "\n",
-                            "        [[[0.1916]]]], grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
+                            "         [[-0.4599, -0.6094,  0.4682],\n",
+                            "          [-0.5064, -0.6768, -0.6638],\n",
+                            "          [ 0.0066, -0.3581,  0.2359]]]], grad_fn=<ThnnConv2DBackward0>)"
                         ]
                     },
-                    "execution_count": 33,
+                    "execution_count": 124,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "per_channel_quant_conv = QuantConv2d(\n",
-                "    3, 2, (3, 3), \n",
-                "    weight_quant=AdvancedWeightQuantizer, \n",
-                "    weight_is_clamped=False, \n",
-                "    weight_scaling_per_output_channel=True)\n",
-                "per_channel_quant_conv.quant_weight()"
+                "from brevitas.quant.scaled_int import Int8BiasPerTensorFloatInternalScaling\n",
+                "\n",
+                "bias_internal_scale_quant_conv = QuantConv2d(\n",
+                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
+                "    bias_quant=Int8BiasPerTensorFloatInternalScaling, return_quant_tensor=False)\n",
+                "bias_internal_scale_quant_conv(torch.randn(1, 2, 5, 5))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "As expected the weight scale is now a vector. Everything we said so far about quantizers still applies, so for example we can load the floating-point state dict we defined before and observe how it triggers an update of the weight scale:"
+                "There are a couple of situations to be aware of concerning bias quantization that can lead to changes in the output `zero_point`.\n",
+                "\n",
+                "Let's consider the scenario where we compute the convolution between a quantized input tensor and quantized weights. In the first case, we then add an *unquantized* bias on top of the output. In the second one, we add a bias quantized with its own scale factor, e.g. with the `Int8BiasPerTensorFloatInternalScaling` quantizer. In both cases, in order to make sure the output `QuantTensor` is valid (i.e. the affine quantization invariant is respected), the output `zero_point` becomes non-zero:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 34,
+            "execution_count": 125,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[[[-0.1774,  0.1774, -0.1774],\n",
-                            "          [ 0.1774, -0.1774,  0.1774],\n",
-                            "          [-0.1774, -0.1774, -0.1774]],\n",
+                            "QuantTensor(value=tensor([[[[-0.6879, -0.6632, -0.2411],\n",
+                            "          [ 0.2064, -0.7371,  0.3910],\n",
+                            "          [ 0.9533,  0.2994,  0.6546]],\n",
                             "\n",
-                            "         [[ 0.1774,  0.1774,  0.1774],\n",
-                            "          [-0.1774,  0.1774,  0.1774],\n",
-                            "          [-0.1774, -0.1774,  0.1774]],\n",
+                            "         [[-0.4684, -0.4495, -0.5021],\n",
+                            "          [ 0.5738,  0.4199, -0.3380],\n",
+                            "          [ 0.6218, -0.0408, -0.8483]],\n",
                             "\n",
-                            "         [[ 0.1774, -0.1774,  0.1774],\n",
-                            "          [ 0.1774, -0.1774, -0.1774],\n",
-                            "          [-0.1774, -0.1774, -0.1774]]],\n",
+                            "         [[-0.5625,  0.1837, -1.0575],\n",
+                            "          [-1.2816, -0.4993, -0.3409],\n",
+                            "          [ 0.4556, -1.4269,  0.5369]]]], grad_fn=<ThnnConv2DBackward0>), scale=tensor([[[[3.0975e-05]]]], grad_fn=<MulBackward0>), zero_point=tensor([[[[ 1276.0774]],\n",
                             "\n",
+                            "         [[-3152.4585]],\n",
                             "\n",
-                            "        [[[-0.1820, -0.1820,  0.1820],\n",
-                            "          [-0.1820, -0.1820,  0.1820],\n",
-                            "          [ 0.1820, -0.1820,  0.1820]],\n",
-                            "\n",
-                            "         [[-0.1820,  0.1820,  0.1820],\n",
-                            "          [ 0.1820,  0.1820,  0.1820],\n",
-                            "          [ 0.1820, -0.1820,  0.1820]],\n",
-                            "\n",
-                            "         [[ 0.1820,  0.1820, -0.1820],\n",
-                            "          [ 0.1820, -0.1820,  0.1820],\n",
-                            "          [-0.1820,  0.1820, -0.1820]]]], grad_fn=<MulBackward0>), scale=tensor([[[[0.1774]]],\n",
-                            "\n",
-                            "\n",
-                            "        [[[0.1820]]]], grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
+                            "         [[ 7320.2324]]]], grad_fn=<DivBackward0>), bit_width=tensor(21.), signed_t=tensor(True), training_t=tensor(True))"
                         ]
                     },
-                    "execution_count": 34,
+                    "execution_count": 125,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "per_channel_quant_conv.load_state_dict(float_conv.state_dict())\n",
-                "per_channel_quant_conv.quant_weight()"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "In this case we have a per-channel quantizer, so the original floating-point weight tensor is now quantized per channel. \n",
-                "\n",
-                "Similarly, we can apply our custom activation quantizer to e.g. a `QuantIdentity` layer:"
+                "unquant_bias_input_quant_conv = QuantConv2d(\n",
+                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
+                "    input_quant=Int8ActPerTensorFloat, return_quant_tensor=True)\n",
+                "out_tensor = unquant_bias_input_quant_conv(torch.randn(1, 2, 5, 5))\n",
+                "out_tensor"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 35,
+            "execution_count": 126,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "tensor([[ 0.0100, -0.0100, -0.0100,  0.0100],\n",
-                            "        [ 0.0100,  0.0100, -0.0100, -0.0100],\n",
-                            "        [ 0.0100, -0.0100,  0.0100, -0.0100],\n",
-                            "        [ 0.0100,  0.0100, -0.0100,  0.0100]], grad_fn=<MulBackward0>)"
+                            "True"
                         ]
                     },
-                    "execution_count": 35,
+                    "execution_count": 126,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "from brevitas.nn import QuantIdentity\n",
-                "\n",
-                "quant_identity = QuantIdentity(\n",
-                "    act_quant=AdvancedActQuantizer, is_clamped=True, scaling_per_output_channel=False)\n",
-                "quant_identity(torch.randn(4, 4))"
+                "assert out_tensor.is_valid"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "Note how `AdvancedActQuantizer` doesn't define a `per_channel_broadcastable_shape`, yet no errors are triggered. This is because `this.per_channel_broadcastable_shape` is required only when `scaling_per_output_channel` is `True`, while in this case `scaling_per_output_channel` is `False`.\n",
-                "Let' try to set it to `True` then:"
+                "Finally, an important point about `QuantTensor`. With the exception of learned bit-width (which will be the subject of a separate tutorial) and some of the bias quantization scenarios we have just seen, usually returing a `QuantTensor` is not necessary and can create extra complexity. This is why currently `return_quant_tensor` defaults to `False`. We can easily see it in an example:"
             ]
         },
         {
             "cell_type": "code",
-            "execution_count": 36,
-            "metadata": {
-                "tags": [
-                    "raises-exception"
-                ]
-            },
-            "outputs": [
-                {
-                    "ename": "DependencyError",
-                    "evalue": "'AdvancedActQuantizer' can not resolve attribute 'per_channel_broadcastable_shape'",
-                    "output_type": "error",
-                    "traceback": [
-                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
-                        "\u001b[1;31mDependencyError\u001b[0m                           Traceback (most recent call last)",
-                        "\u001b[1;32m<ipython-input-36-b3479e90d1a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m quant_identity = QuantIdentity(\n\u001b[1;32m----> 4\u001b[1;33m     act_quant=AdvancedActQuantizer, is_clamped=True, scaling_per_output_channel=True)\n\u001b[0m",
-                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\nn\\quant_activation.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, act_quant, return_quant_tensor, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[0mact_quant\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mact_quant\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mreturn_quant_tensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_quant_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\nn\\quant_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, act_impl, passthrough_act, input_quant, act_quant, return_quant_tensor, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mpassthrough_act\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mact_quant\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\nn\\mixin\\act.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, act_impl, passthrough_act, act_quant, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[0mproxy_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'act_'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[0mkwargs_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\nn\\mixin\\base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, quant, proxy_protocol, none_quant_injector, proxy_prefix, kwargs_prefix, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mquant_injector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0mquant_injector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquant_injector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfilter_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0mquant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquant_injector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproxy_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquant_injector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxy_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\proxy\\runtime_quant.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, quant_layer, quant_injector)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquant_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquant_injector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mActQuantProxyFromInjector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquant_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquant_injector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_passthrough_act\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_is_passthrough_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquant_injector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\proxy\\quant_proxy.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, quant_layer, quant_injector, export_mode, export_handler)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# Use a normal list and not a ModuleList since this is a pointer to parent modules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtracked_module_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_tracked_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquant_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexport_handler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexport_mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\proxy\\quant_proxy.py\u001b[0m in \u001b[0;36madd_tracked_module\u001b[1;34m(self, module)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtracked_module_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_tracked_modules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_tensor_quant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Trying to add None as a parent module.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "\u001b[1;32mc:\\brevitas_fx\\src\\brevitas\\proxy\\runtime_quant.py\u001b[0m in \u001b[0;36minit_tensor_quant\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_tensor_quant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mtensor_quant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquant_injector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_quant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mact_impl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquant_injector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact_impl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mis_act_enabled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_is_act_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact_impl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_quant\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
-                        "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
-                        "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\pytorch\\lib\\site-packages\\_dependencies\\this.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, __self__)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\".\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mDependencyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                     message = (\n",
-                        "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
-                        "\u001b[1;31mDependencyError\u001b[0m: 'AdvancedActQuantizer' can not resolve attribute 'per_channel_broadcastable_shape'"
-                    ]
-                }
-            ],
-            "source": [
-                "from brevitas.nn import QuantIdentity\n",
-                "\n",
-                "quant_identity = QuantIdentity(\n",
-                "    act_quant=AdvancedActQuantizer, is_clamped=True, scaling_per_output_channel=True)"
-            ]
-        },
-        {
-            "cell_type": "markdown",
-            "metadata": {},
-            "source": [
-                "As expected we get an error saying that the quantizer cannot resolve `per_channel_broadcastable_shape`. If we pass it in then we can get a per-channel quantizer:"
-            ]
-        },
-        {
-            "cell_type": "code",
-            "execution_count": 37,
+            "execution_count": 127,
             "metadata": {},
             "outputs": [
                 {
                     "data": {
                         "text/plain": [
-                            "QuantTensor(value=tensor([[ 0.0100,  0.0100, -0.0100, -0.0100],\n",
-                            "        [-0.0100,  0.0100, -0.0100,  0.0100],\n",
-                            "        [-0.0100, -0.0100, -0.0100,  0.0100],\n",
-                            "        [-0.0100,  0.0100,  0.0100,  0.0100]], grad_fn=<MulBackward0>), scale=tensor([[0.0100],\n",
-                            "        [0.0100],\n",
-                            "        [0.0100],\n",
-                            "        [0.0100]], grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed_t=tensor(True), training_t=tensor(True))"
+                            "tensor([[[[ 0.8357,  0.0733,  0.9527],\n",
+                            "          [ 0.1803,  0.2154,  0.7598],\n",
+                            "          [ 1.1121, -0.8728,  1.0039]],\n",
+                            "\n",
+                            "         [[ 0.7917,  1.0063,  0.6516],\n",
+                            "          [-0.1852, -0.7263,  0.0956],\n",
+                            "          [-0.1876,  0.2747, -0.1617]],\n",
+                            "\n",
+                            "         [[ 0.8299,  0.9934, -0.3821],\n",
+                            "          [ 0.4865,  0.9309, -0.7924],\n",
+                            "          [-0.4201,  0.2343,  0.1532]]]], grad_fn=<ThnnConv2DBackward0>)"
                         ]
                     },
-                    "execution_count": 37,
+                    "execution_count": 127,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
-                "quant_identity = QuantIdentity(\n",
-                "    act_quant=AdvancedActQuantizer, is_clamped=True, scaling_per_output_channel=True,\n",
-                "    per_channel_broadcastable_shape=(4, 1), return_quant_tensor=True)\n",
-                "quant_identity(torch.randn(4, 4))"
+                "bias_input_quant_conv = QuantConv2d(\n",
+                "    in_channels=2, out_channels=3, kernel_size=(3,3), bias=True,\n",
+                "    input_quant=Int8ActPerTensorFloat, bias_quant=Int8Bias)\n",
+                "bias_input_quant_conv(torch.randn(1, 2, 5, 5))"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
-                "We have seen how powerful dependency injection is. In a way, it's even too expressive. For users that are not interesting in building completely custom quantizers, it can be hard to make sense of how the various components available under `brevitas.core` can be assembled together according to best practices."
+                "Altough not obvious, the output is actually implicitly quantized."
             ]
         }
     ],
     "metadata": {
         "kernelspec": {
-            "display_name": "Python 3",
+            "display_name": "torch_1.10",
             "language": "python",
             "name": "python3"
         },
         "language_info": {
             "codemirror_mode": {
                 "name": "ipython",
                 "version": 3
             },
             "file_extension": ".py",
             "mimetype": "text/x-python",
             "name": "python",
             "nbconvert_exporter": "python",
             "pygments_lexer": "ipython3",
-            "version": "3.7.7"
+            "version": "3.7.15"
+        },
+        "vscode": {
+            "interpreter": {
+                "hash": "98d39f4fefefd06bad749c114549705efc94e31e1348b8fb7b25329049b354fa"
+            }
         }
     },
     "nbformat": 4,
     "nbformat_minor": 1
 }
```

### Comparing `brevitas-0.8.0/notebooks/Brevitas_TVMCon2021.ipynb` & `brevitas-0.9.0/notebooks/Brevitas_TVMCon2021.ipynb`

 * *Files identical despite different names*

### Comparing `brevitas-0.8.0/notebooks/ONNX_export_tutorial.ipynb` & `brevitas-0.9.0/notebooks/ONNX_export_tutorial.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9958333333333333%*

 * *Differences: {"'cells'": "{insert: [(1, OrderedDict([('cell_type', 'code'), ('execution_count', None), "*

 * *            "('metadata', OrderedDict()), ('outputs', []), ('source', ['%pip install "*

 * *            "netron'])]))]}"}*

```diff
@@ -17,14 +17,23 @@
                 "Brevitas requires Python 3.7+ and PyTorch 1.5.1+ and can be installed from PyPI with `pip install brevitas`. \n",
                 "\n",
                 "For this notebook, you will also need to install `onnx`, `onnxruntime`, `onnxoptimizer` and `netron` (for visualization of ONNX models).\n",
                 "For this tutorial, PyTorch 1.8.1+ is required."
             ]
         },
         {
+            "cell_type": "code",
+            "execution_count": null,
+            "metadata": {},
+            "outputs": [],
+            "source": [
+                "%pip install netron"
+            ]
+        },
+        {
             "attachments": {},
             "cell_type": "markdown",
             "metadata": {
                 "collapsed": false,
                 "pycharm": {
                     "name": "#%% md\n"
                 }
```

### Comparing `brevitas-0.8.0/notebooks/quantized_recurrent.ipynb` & `brevitas-0.9.0/notebooks/quantized_recurrent.ipynb`

 * *Files identical despite different names*

### Comparing `brevitas-0.8.0/setup.py` & `brevitas-0.9.0/setup.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,60 +1,58 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from setuptools import setup, find_packages
 import os
 
+from setuptools import find_packages
+from setuptools import setup
 
 PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
 REQUIREMENTS_DIR = os.path.join(PROJECT_ROOT, 'requirements')
 
 
 def read(*path):
     return open(os.path.join(*path), encoding='utf8').read()
 
 
 def read_requirements(filename):
     return read(REQUIREMENTS_DIR, filename).splitlines()
 
 
-setup(name="brevitas",
-      use_scm_version=True,
-      setup_requires=read_requirements('requirements-setup.txt'),
-      description="Quantization-aware training in PyTorch",
-      long_description=read(PROJECT_ROOT, 'README.md'),
-      long_description_content_type="text/markdown",
-      author="Alessandro Pappalardo",
-      author_email="alessand@xilinx.com",
-      url="https://github.com/Xilinx/brevitas",
-      python_requires=">=3.7",
-      install_requires=read_requirements('requirements.txt'),
-      extras_require={
-          "notebook": read_requirements('requirements-notebook.txt'),
-          "docs": read_requirements("requirements-docs.txt"),
-          "export": read_requirements('requirements-export.txt'),
-          "hadamard": read_requirements('requirements-hadamard.txt'),
-          "test": read_requirements('requirements-test.txt'),
-          "tts": read_requirements('requirements-tts.txt'),
-          "stt": read_requirements('requirements-stt.txt'),
-          "vision": read_requirements('requirements-vision.txt'),
-          "finn_integration": read_requirements('requirements-finn-integration.txt'),
-          "ort_integration": read_requirements('requirements-ort-integration.txt')
-      },
-      packages=find_packages('src'),
-      package_dir={'': 'src'},
-      zip_safe=False,
-      include_package_data=True,
-      entry_points={
-          'console_scripts': [
-              'brevitas_bnn_pynq_train = brevitas_examples.bnn_pynq.bnn_pynq_train:main',
-              'brevitas_flexml_imagenet_calibration = brevitas_examples.imagenet_classification.flexml_imagenet_calibration:main',
-              'brevitas_imagenet_val = brevitas_examples.imagenet_classification.imagenet_val:main',
-              'brevitas_quartznet_val = brevitas_examples.speech_to_text.quartznet_val:main',
-              'brevitas_melgan_val = brevitas_examples.text_to_speech.melgan_val:main',
-              'brevitas_quartznet_preprocess = brevitas_examples.speech_to_text.get_librispeech_data:main',
-              'brevitas_melgan_preprocess = brevitas_examples.text_to_speech.preprocess_dataset:main'
-          ],
-      })
-
-
+setup(
+    name="brevitas",
+    use_scm_version=True,
+    setup_requires=read_requirements('requirements-setup.txt'),
+    description="Quantization-aware training in PyTorch",
+    long_description=read(PROJECT_ROOT, 'README.md'),
+    long_description_content_type="text/markdown",
+    author="Alessandro Pappalardo",
+    author_email="alessand@xilinx.com",
+    url="https://github.com/Xilinx/brevitas",
+    python_requires=">=3.7",
+    install_requires=read_requirements('requirements.txt'),
+    extras_require={
+        "notebook": read_requirements('requirements-notebook.txt'),
+        "docs": read_requirements("requirements-docs.txt"),
+        "export": read_requirements('requirements-export.txt'),
+        "hadamard": read_requirements('requirements-hadamard.txt'),
+        "test": read_requirements('requirements-test.txt'),
+        "tts": read_requirements('requirements-tts.txt'),
+        "stt": read_requirements('requirements-stt.txt'),
+        "vision": read_requirements('requirements-vision.txt'),
+        "finn_integration": read_requirements('requirements-finn-integration.txt'),
+        "ort_integration": read_requirements('requirements-ort-integration.txt')},
+    packages=find_packages('src'),
+    package_dir={'': 'src'},
+    zip_safe=False,
+    include_package_data=True,
+    entry_points={
+        'console_scripts': [
+            'brevitas_bnn_pynq_train = brevitas_examples.bnn_pynq.bnn_pynq_train:main',
+            'brevitas_qat_imagenet_val = brevitas_examples.imagenet_classification.qat.imagenet_val:main',
+            'brevitas_quartznet_val = brevitas_examples.speech_to_text.quartznet_val:main',
+            'brevitas_melgan_val = brevitas_examples.text_to_speech.melgan_val:main',
+            'brevitas_quartznet_preprocess = brevitas_examples.speech_to_text.get_librispeech_data:main',
+            'brevitas_melgan_preprocess = brevitas_examples.text_to_speech.preprocess_dataset:main',
+            'brevitas_ptq_imagenet_benchmark = brevitas_examples.imagenet_classification.ptq.ptq_benchmark:main',
+            'brevitas_ptq_imagenet_val = brevitas_examples.imagenet_classification.ptq.ptq_evaluate:main'
+        ],})
```

### Comparing `brevitas-0.8.0/src/brevitas/__init__.py` & `brevitas-0.9.0/src/brevitas/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,66 +1,61 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-import os
 import glob
-import warnings
-from packaging import version
-from pkg_resources import get_distribution, DistributionNotFound
+import os
 from typing import List, Optional
+import warnings
 
-from torch.utils import cpp_extension
+from packaging import version
+from pkg_resources import DistributionNotFound
+from pkg_resources import get_distribution
 import torch
 from torch import Tensor
+from torch.utils import cpp_extension
 
-from brevitas import jit as jit
 from brevitas import config
-
+from brevitas import jit as jit
 
 pkg_dir = os.path.dirname(os.path.abspath(__file__))
 
 if torch.__version__.endswith('+cpu'):
     torch_version = version.parse(torch.__version__.rstrip('+cpu'))
 else:
     torch_version = version.parse(torch.__version__)
 
-
 original_cat = torch.cat
 if torch_version < version.parse('1.7.0'):
-    from torch._overrides import has_torch_function, handle_torch_function
+    from torch._overrides import handle_torch_function
+    from torch._overrides import has_torch_function
 
     @torch.jit.ignore
     def unsupported_jit_cat(tensors, dim):
         if not isinstance(tensors, (tuple, list)):
             tensors = tuple(tensors)
             return unsupported_jit_cat(tensors, dim)
         if any(type(t) is not Tensor for t in tensors) and has_torch_function(tensors):
             return handle_torch_function(
                 original_cat, relevant_args=tensors, tensors=tensors, dim=dim)
         else:
             return original_cat(tensors=tensors, dim=dim)
 
-    def cat(
-            tensors: List[Tensor],
-            dim: int = 0) -> Tensor:
+    def cat(tensors: List[Tensor], dim: int = 0) -> Tensor:
         if not torch.jit.is_scripting():
             return unsupported_jit_cat(tensors, dim)
         return original_cat(tensors, dim=dim)
 
     torch.cat = cat
 
-
 try:
     __version__ = get_distribution(__name__).version
 except DistributionNotFound:
     # package is not installed
     pass
 
-
 if config.JIT_ENABLED or config.NATIVE_STE_BACKEND_ENABLED:
     config.NATIVE_STE_BACKEND_ENABLED = True  # for consistency, in case only JIT_ENABLED was true
     extensions_dir = os.path.join(pkg_dir, 'csrc')
     sources = glob.glob(os.path.join(extensions_dir, '*.cpp'))
     sources = [os.path.join(extensions_dir, s) for s in sources]
 
     try:
@@ -70,17 +65,16 @@
             is_python_module=False,
             verbose=config.VERBOSE)
         NATIVE_STE_BACKEND_LOADED = True
     except Exception as e:
         if config.VERBOSE:
             # Warnings calls str on the message argument, can't pass an f-string directly
             error_message = (
-                    f"The Brevitas native STE backend is enabled but couldn't be loaded.\n"
-                    f"Ensure that the \"ninja\" build system is installed (e.g. apt install ninja-build)"
-                    f"\nException: {e}."
-            )
+                f"The Brevitas native STE backend is enabled but couldn't be loaded.\n"
+                f"Ensure that the \"ninja\" build system is installed (e.g. apt install ninja-build)"
+                f"\nException: {e}.")
             warnings.warn(error_message)
         NATIVE_STE_BACKEND_LOADED = False
 else:
     NATIVE_STE_BACKEND_LOADED = False
 
 from brevitas import ops as ops
```

### Comparing `brevitas-0.8.0/src/brevitas/common.py` & `brevitas-0.9.0/src/brevitas/common.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from abc import ABCMeta, abstractmethod
+from abc import ABCMeta
+from abc import abstractmethod
 
 from brevitas import config
 
 
 class ExportMixin(object):
     __metaclass__ = ABCMeta
 
@@ -39,8 +39,8 @@
         elif value and not self.requires_export_handler and self.export_handler is None:
             return  # don't set export mode when it's not required and there is no handler
         elif value and not self._export_mode and self.export_handler is not None:
             self.export_handler.prepare_for_export(self)
             self.export_handler.attach_debug_info(self)
         elif not value and self.export_handler is not None:
             self.export_handler = None
-        self._export_mode = value
+        self._export_mode = value
```

### Comparing `brevitas-0.8.0/src/brevitas/config.py` & `brevitas-0.9.0/src/brevitas/config.py`

 * *Ordering differences only*

 * *Files 3% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-import os
 from distutils.util import strtobool
+import os
+
 try:
     from torch.jit import _enabled
 except ImportError:
     from torch.jit._state import _enabled
 
 
 def env_to_bool(name, default):
     return bool(strtobool(os.environ.get(name, "{}".format(default))))
 
+
 REINIT_ON_STATE_DICT_LOAD = env_to_bool('BREVITAS_REINIT_ON_STATE_DICT_LOAD', True)
 IGNORE_MISSING_KEYS = env_to_bool('BREVITAS_IGNORE_MISSING_KEYS', False)
 # JIT_ENABLED triggers NATIVE_STE_BACKEND_ENABLED to True, but not the other way around
 JIT_ENABLED = env_to_bool('BREVITAS_JIT', False) and _enabled
 NATIVE_STE_BACKEND_ENABLED = env_to_bool('BREVITAS_NATIVE_STE_BACKEND', False)
 VERBOSE = env_to_bool('BREVITAS_VERBOSE', False)
 
 # Internal global variables
 _IS_INSIDE_QUANT_LAYER = None
 _ONGOING_EXPORT = None
-
```

### Comparing `brevitas-0.8.0/src/brevitas/core/bit_width/parameter.py` & `brevitas-0.9.0/src/brevitas/core/bit_width/parameter.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,23 +1,20 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Optional
-
 import torch
 from torch import Tensor
-from torch.nn import Parameter, Module
+from torch.nn import Module
+from torch.nn import Parameter
 
 import brevitas
 import brevitas.config as config
-from brevitas.function import abs_binary_sign_grad
 from brevitas.core.function_wrapper import RoundSte
 from brevitas.core.restrict_val import IntRestrictValue
-
+from brevitas.function import abs_binary_sign_grad
 
 MIN_INT_BIT_WIDTH = 2
 NON_ZERO_EPSILON = 1e-6
 REMOVE_ZERO_BIT_WIDTH = 0.1
 
 
 class BitWidthParameter(brevitas.jit.ScriptModule):
@@ -55,24 +52,27 @@
             bit_width: int,
             min_bit_width: int = MIN_INT_BIT_WIDTH,
             restrict_bit_width_impl: Module = IntRestrictValue(RoundSte()),
             override_pretrained_bit_width: bool = False) -> None:
         super(BitWidthParameter, self).__init__()
 
         if bit_width < MIN_INT_BIT_WIDTH:
-            raise RuntimeError("Int bit width has to be at least {}, instead is {}."
-                            .format(MIN_INT_BIT_WIDTH, bit_width))
+            raise RuntimeError(
+                "Int bit width has to be at least {}, instead is {}.".format(
+                    MIN_INT_BIT_WIDTH, bit_width))
 
         if min_bit_width < MIN_INT_BIT_WIDTH:
-            raise RuntimeError("Min int bit width has to be at least {}, instead is {}."
-                            .format(MIN_INT_BIT_WIDTH, min_bit_width))
+            raise RuntimeError(
+                "Min int bit width has to be at least {}, instead is {}.".format(
+                    MIN_INT_BIT_WIDTH, min_bit_width))
 
         if bit_width < min_bit_width:
-            raise RuntimeError("Int bit width has to be at least {}, instead is {}."
-                            .format(min_bit_width, bit_width))
+            raise RuntimeError(
+                "Int bit width has to be at least {}, instead is {}.".format(
+                    min_bit_width, bit_width))
 
         bit_width = float(int(bit_width))
         min_bit_width = float(int(min_bit_width))
         bit_width_base = restrict_bit_width_impl.restrict_init_float(min_bit_width)
         bit_width = restrict_bit_width_impl.restrict_init_float(bit_width)
         bit_width_offset_init = bit_width - bit_width_base
         self.bit_width_offset = Parameter(torch.tensor(bit_width_offset_init))
@@ -82,34 +82,42 @@
 
     @brevitas.jit.script_method
     def forward(self) -> Tensor:
         bit_width = abs_binary_sign_grad(self.bit_width_offset) + self.bit_width_base
         bit_width = self.restrict_bit_width_impl(bit_width)
         return bit_width
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
+        bit_width_const_key = prefix + 'bit_width'
         bit_width_offset_key = prefix + 'bit_width_offset'
+        if bit_width_const_key in state_dict:
+            assert bit_width_offset_key not in state_dict, "Both should not be true."
+            bit_width = state_dict[bit_width_const_key]
+            state_dict[bit_width_offset_key] = bit_width - self.bit_width_base
+            del state_dict[bit_width_const_key]
+
         if self.override_pretrained and bit_width_offset_key in state_dict:
             del state_dict[bit_width_offset_key]
         super(BitWidthParameter, self)._load_from_state_dict(
-            state_dict, prefix, local_metadata,strict,missing_keys, unexpected_keys, error_msgs)
+            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
         if config.IGNORE_MISSING_KEYS and bit_width_offset_key in missing_keys:
             missing_keys.remove(bit_width_offset_key)
 
 
 class RemoveBitwidthParameter(brevitas.jit.ScriptModule):
     __constants__ = ['non_zero_epsilon', 'override_pretrained']
 
     def __init__(
             self,
             bit_width_to_remove: int,
             override_pretrained_bit_width: bool = False,
             non_zero_epsilon: float = NON_ZERO_EPSILON,
-            remove_zero_bit_width = REMOVE_ZERO_BIT_WIDTH):
+            remove_zero_bit_width=REMOVE_ZERO_BIT_WIDTH):
         super(RemoveBitwidthParameter, self).__init__()
 
         if bit_width_to_remove < 0:
             raise RuntimeError("Bit width to clamp has to be >= 0.".format(bit_width_to_remove))
         elif bit_width_to_remove == 0:
             bit_width_coeff_init = 1 / remove_zero_bit_width
         else:
@@ -119,21 +127,17 @@
         self.override_pretrained = override_pretrained_bit_width
 
     @brevitas.jit.script_method
     def forward(self) -> Tensor:
         bit_width_to_remove = 1.0 / (self.non_zero_epsilon + torch.abs(self.bit_width_coeff))
         return bit_width_to_remove
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         bit_width_coeff_key = prefix + 'bit_width_coeff'
         if self.override_pretrained and bit_width_coeff_key in state_dict:
             del state_dict[bit_width_coeff_key]
         super(RemoveBitwidthParameter, self)._load_from_state_dict(
             state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
         if config.IGNORE_MISSING_KEYS and bit_width_coeff_key in missing_keys:
             missing_keys.remove(bit_width_coeff_key)
-
-
-
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas/core/function_wrapper/clamp.py` & `brevitas-0.9.0/src/brevitas/core/function_wrapper/clamp.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,19 +1,18 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 """
 ScriptModule wrappers for various variants of clamping.
 """
 
-
-import brevitas
 import torch
 from torch import Tensor
+
+import brevitas
 from brevitas.function import tensor_clamp
 
 
 class TensorClamp(brevitas.jit.ScriptModule):
     """
     ScriptModule wrapper for :func:`~brevitas.function.ops.tensor_clamp`.
 
@@ -69,8 +68,8 @@
 
     def __init__(self, min_val: float) -> None:
         super(ClampMin, self).__init__()
         self.min_val = min_val
 
     @brevitas.jit.script_method
     def forward(self, x: Tensor):
-        return x.clamp_min(self.min_val)
+        return x.clamp_min(self.min_val)
```

### Comparing `brevitas-0.8.0/src/brevitas/core/function_wrapper/misc.py` & `brevitas-0.9.0/src/brevitas/core/function_wrapper/misc.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 """
 A collection of miscellaneous ScriptModule used in various quantizers.
 """
 
-import brevitas
 import torch
 
+import brevitas
+
 
 class Identity(brevitas.jit.ScriptModule):
     """
     Identity ScriptModule.
 
     Examples:
         >>> identity = Identity()
@@ -86,13 +86,7 @@
     def __init__(self) -> None:
         super(InplaceLogTwo, self).__init__()
 
     @torch.jit.ignore
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         x.log2_()
         return x
-
-
-
-
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas/core/function_wrapper/ops_ste.py` & `brevitas-0.9.0/src/brevitas/core/function_wrapper/ops_ste.py`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 """
 ScriptModule wrappers of various functions defined in :obj:`~brevitas.function.ops_ste`.
 """
 
-import brevitas
 import torch
+
+import brevitas
 from brevitas.function.ops_ste import *
 
 
 class RoundSte(brevitas.jit.ScriptModule):
     """
     ScriptModule wrapper for :func:`~brevitas.function.ops_ste.round_ste`.
     """
@@ -111,8 +111,8 @@
     """
 
     def __init__(self) -> None:
         super(InplaceTensorClampSte, self).__init__()
 
     @brevitas.jit.script_method
     def forward(self, x: torch.Tensor, min_val: torch.Tensor, max_val: torch.Tensor):
-        return tensor_clamp_ste_(x, min_val, max_val)
+        return tensor_clamp_ste_(x, min_val, max_val)
```

### Comparing `brevitas-0.8.0/src/brevitas/core/function_wrapper/shape.py` & `brevitas-0.9.0/src/brevitas/core/function_wrapper/shape.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 """
 ScriptModule classes to compute the view of a tensor according to various different criteria.
 """
 
 from typing import Optional, Tuple
 
 import torch
 
 import brevitas
-from brevitas.function.shape import over_tensor, over_output_channels, over_batch_over_tensor
-from brevitas.function.shape import over_batch_over_output_channels
 from brevitas.core.function_wrapper import Identity
+from brevitas.function.shape import over_batch_over_output_channels
+from brevitas.function.shape import over_batch_over_tensor
+from brevitas.function.shape import over_output_channels
+from brevitas.function.shape import over_tensor
 
 
 class PermuteDims(brevitas.jit.ScriptModule):
 
     def __init__(self, permute_dims: Tuple[int, ...]) -> None:
         super(PermuteDims, self).__init__()
         self.permute_dims = permute_dims
```

### Comparing `brevitas-0.8.0/src/brevitas/core/quant/binary.py` & `brevitas-0.9.0/src/brevitas/core/quant/binary.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,23 +1,22 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from typing import Tuple
 
 import torch
 from torch import Tensor
 from torch.nn import Module
 
 import brevitas
-from brevitas.function.ops_ste import binary_sign_ste
-from brevitas.core.function_wrapper import TensorClamp
 from brevitas.core.bit_width import BitWidthConst
-from brevitas.core.utils import StatelessBuffer
+from brevitas.core.function_wrapper import TensorClamp
 from brevitas.core.quant.delay import DelayWrapper
+from brevitas.core.utils import StatelessBuffer
+from brevitas.function.ops_ste import binary_sign_ste
 
 
 class BinaryQuant(brevitas.jit.ScriptModule):
     """
     ScriptModule that implements scaled uniform binary quantization of an input tensor.
     Quantization is performed with :func:`~brevitas.function.ops_ste.binary_sign_ste`.
 
@@ -115,11 +114,11 @@
         self.zero_point = StatelessBuffer(torch.tensor(0.0))
         self.delay_wrapper = DelayWrapper(quant_delay_steps)
         self.tensor_clamp_impl = tensor_clamp_impl
 
     @brevitas.jit.script_method
     def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
         scale = self.scaling_impl(x)
-        y = self.tensor_clamp_impl(x, - scale, scale)
+        y = self.tensor_clamp_impl(x, -scale, scale)
         y = binary_sign_ste(y) * scale
         y = self.delay_wrapper(x, y)
         return y, scale, self.zero_point(), self.bit_width()
```

### Comparing `brevitas-0.8.0/src/brevitas/core/quant/delay.py` & `brevitas-0.9.0/src/brevitas/core/quant/delay.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from typing import Optional
 
 from torch import Tensor
 
 import brevitas
 
 
@@ -26,16 +25,17 @@
     def forward(self, x: Tensor, y: Tensor) -> Tensor:
         if self.quant_delay_steps > 0:
             self.quant_delay_steps = self.quant_delay_steps - 1
             return x
         else:
             return y
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         super(_DelayQuant, self)._load_from_state_dict(
             state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
         # Pytorch stores training flag as a buffer with JIT enabled
         training_key = prefix + 'training'
         if training_key in missing_keys:
             missing_keys.remove(training_key)
 
@@ -47,8 +47,8 @@
         if quant_delay_steps is None or quant_delay_steps <= 0:
             self.delay_impl = _NoDelay()
         else:
             self.delay_impl = _DelayQuant(quant_delay_steps)
 
     @brevitas.jit.script_method
     def forward(self, x: Tensor, y: Tensor) -> Tensor:
-        return self.delay_impl(x, y)
+        return self.delay_impl(x, y)
```

### Comparing `brevitas-0.8.0/src/brevitas/core/quant/int.py` & `brevitas-0.9.0/src/brevitas/core/quant/int.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,21 +1,20 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from typing import Optional, Tuple
 
 import torch
 from torch import Tensor
 from torch.nn import Module
 
 import brevitas
-from brevitas.function.ops_ste import round_ste
-from brevitas.core.utils import StatelessBuffer
 from brevitas.core.quant.delay import DelayWrapper
+from brevitas.core.utils import StatelessBuffer
+from brevitas.function.ops_ste import round_ste
 
 
 class PrescaledRestrictIntQuantWithInputBitWidth(brevitas.jit.ScriptModule):
     """
     ScriptModule that wraps around an integer quantization implementation like
     :class:`~brevitas.core.quant.IntQuant`. Zero-point is set to zero, scale is taken as input,
     bit-width is computed from an input bit-width.
@@ -46,49 +45,42 @@
         tensor(0.)
         >>> bit_width
         tensor(4.)
 
     Note:
         Set env variable BREVITAS_JIT=1 to enable TorchScript compilation of this module.
     """
-    def __init__(self,
-                 int_quant: Module,
-                 bit_width_impl: Module):
+
+    def __init__(self, int_quant: Module, bit_width_impl: Module):
         super(PrescaledRestrictIntQuantWithInputBitWidth, self).__init__()
         self.int_quant = int_quant
         self.msb_clamp_bit_width_impl = bit_width_impl
         self.zero_point = StatelessBuffer(torch.tensor(0.0))
 
     @brevitas.jit.script_method
-    def forward(self,
-                x: Tensor,
-                scale: Tensor,
+    def forward(self, x: Tensor, scale: Tensor,
                 input_bit_width: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
         bit_width = self.msb_clamp_bit_width_impl(input_bit_width)
         zero_point = self.zero_point()
         y = self.int_quant(scale, zero_point, bit_width, x)
         return y, scale, zero_point, bit_width
 
 
 class PrescaledRestrictIntQuant(brevitas.jit.ScriptModule):
     """
     """
-    def __init__(self,
-                 int_quant: Module,
-                 bit_width_impl: Module):
+
+    def __init__(self, int_quant: Module, bit_width_impl: Module):
         super(PrescaledRestrictIntQuant, self).__init__()
         self.int_quant = int_quant
         self.msb_clamp_bit_width_impl = bit_width_impl
         self.zero_point = StatelessBuffer(torch.tensor(0.0))
 
-
     @brevitas.jit.script_method
-    def forward(self,
-                x: Tensor,
-                scale: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
+    def forward(self, x: Tensor, scale: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
         msb_clamp_bit_width = self.msb_clamp_bit_width_impl()
         zero_point = self.zero_point()
         y = self.int_quant(scale, zero_point, msb_clamp_bit_width, x)
         return y, scale, zero_point, msb_clamp_bit_width
 
 
 class RescalingIntQuant(brevitas.jit.ScriptModule):
@@ -135,20 +127,22 @@
 
     Note:
         scale = scaling_impl(x) / int_scaling_impl(bit_width)
 
     Note:
         Set env variable BREVITAS_JIT=1 to enable TorchScript compilation of this module.
     """
-    def __init__(self,
-                 int_quant: Module,
-                 scaling_impl: Module,
-                 int_scaling_impl: Module,
-                 zero_point_impl: Module,
-                 bit_width_impl: Module):
+
+    def __init__(
+            self,
+            int_quant: Module,
+            scaling_impl: Module,
+            int_scaling_impl: Module,
+            zero_point_impl: Module,
+            bit_width_impl: Module):
         super(RescalingIntQuant, self).__init__()
         self.int_quant = int_quant
         self.scaling_impl = scaling_impl
         self.int_scaling_impl = int_scaling_impl
         self.zero_point_impl = zero_point_impl
         self.msb_clamp_bit_width_impl = bit_width_impl
 
@@ -161,22 +155,23 @@
         zero_point = self.zero_point_impl(x, scale, bit_width)
         y = self.int_quant(scale, zero_point, bit_width, x)
         return y, scale, zero_point, bit_width
 
 
 class DecoupledRescalingIntQuant(brevitas.jit.ScriptModule):
 
-    def __init__(self,
-                 decoupled_int_quant: Module,
-                 pre_scaling_impl: Module,
-                 scaling_impl: Module,
-                 int_scaling_impl: Module,
-                 pre_zero_point_impl: Module,
-                 zero_point_impl: Module,
-                 bit_width_impl: Module):
+    def __init__(
+            self,
+            decoupled_int_quant: Module,
+            pre_scaling_impl: Module,
+            scaling_impl: Module,
+            int_scaling_impl: Module,
+            pre_zero_point_impl: Module,
+            zero_point_impl: Module,
+            bit_width_impl: Module):
         super(DecoupledRescalingIntQuant, self).__init__()
         self.decoupled_int_quant = decoupled_int_quant
         self.pre_scaling_impl = pre_scaling_impl
         self.scaling_impl = scaling_impl
         self.int_scaling_impl = int_scaling_impl
         self.pre_zero_point_impl = pre_zero_point_impl
         self.zero_point_impl = zero_point_impl
@@ -195,35 +190,68 @@
         y = self.decoupled_int_quant(pre_scale, pre_zero_point, scale, zero_point, bit_width, x)
         return y, scale, zero_point, bit_width, pre_scale, pre_zero_point
 
 
 class TruncIntQuant(brevitas.jit.ScriptModule):
     """
     """
+
     def __init__(
-            self,
-            float_to_int_impl: Module,
-            bit_width_impl: Module,
-            quant_delay_steps: int = 0):
+            self, float_to_int_impl: Module, bit_width_impl: Module, quant_delay_steps: int = 0):
         super(TruncIntQuant, self).__init__()
         self.msb_clamp_bit_width_impl = bit_width_impl
         self.float_to_int_impl = float_to_int_impl
         self.delay_wrapper = DelayWrapper(quant_delay_steps)
 
     @brevitas.jit.script_method
-    def forward(self,
-                x: Tensor,
-                scale: Tensor,
-                zero_point: Tensor,
+    def forward(self, x: Tensor, scale: Tensor, zero_point: Tensor,
                 input_bit_width: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
         y = x / scale
         y = y + zero_point
         y = round_ste(y)  # clean up floating point error
         output_bit_width = self.msb_clamp_bit_width_impl()
         trunc_bit_width = input_bit_width - output_bit_width
         trunc_scale = 2.0 ** trunc_bit_width
         y = y / trunc_scale
         y = self.float_to_int_impl(y)
         y = y - zero_point
         y = y * scale
         y = self.delay_wrapper(x, y)
-        return y, scale, zero_point, output_bit_width
+        return y, scale, zero_point, output_bit_width
+
+
+class DecoupledRescalingIntQuantWithInput(DecoupledRescalingIntQuant):
+
+    def __init__(
+        self,
+        decoupled_int_quant: Module,
+        pre_scaling_impl: Module,
+        scaling_impl: Module,
+        int_scaling_impl: Module,
+        pre_zero_point_impl: Module,
+        zero_point_impl: Module,
+        bit_width_impl: Module,
+    ):
+        super().__init__(
+            decoupled_int_quant,
+            pre_scaling_impl,
+            scaling_impl,
+            int_scaling_impl,
+            pre_zero_point_impl,
+            zero_point_impl,
+            bit_width_impl,
+        )
+        # TODO - check the make sure the pre-scaling module takes the input bit-width and sign
+
+    @brevitas.jit.script_method
+    def forward(self, x: Tensor, input_bit_width: Tensor,
+                input_is_signed: bool) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:
+        bit_width = self.msb_clamp_bit_width_impl()
+        int_threshold = self.int_scaling_impl(bit_width)
+        pre_threshold = self.pre_scaling_impl(x, input_bit_width, input_is_signed)
+        pre_scale = pre_threshold / int_threshold
+        pre_zero_point = self.pre_zero_point_impl(x, pre_scale, bit_width)
+        threshold = self.scaling_impl(x)
+        scale = threshold / int_threshold
+        zero_point = self.zero_point_impl(x, scale, bit_width)
+        y = self.decoupled_int_quant(pre_scale, pre_zero_point, scale, zero_point, bit_width, x)
+        return y, scale, zero_point, bit_width, pre_scale, pre_zero_point
```

### Comparing `brevitas-0.8.0/src/brevitas/core/quant/int_base.py` & `brevitas-0.9.0/src/brevitas/core/quant/int_base.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import torch
 from torch import Tensor
 from torch.nn import Module
 
 import brevitas
-from brevitas.function.ops import max_int, min_int
-from brevitas.core.function_wrapper import RoundSte, TensorClamp
+from brevitas.core.function_wrapper import RoundSte
+from brevitas.core.function_wrapper import TensorClamp
 from brevitas.core.quant.delay import DelayWrapper
+from brevitas.function.ops import max_int
+from brevitas.function.ops import min_int
 
 
 class IntQuant(brevitas.jit.ScriptModule):
     """
     ScriptModule that implements scale, shifted, uniform integer quantization of an input tensor,
     according to an input scale, zero-point and bit-width.
 
@@ -57,20 +58,15 @@
         self.float_to_int_impl = float_to_int_impl
         self.tensor_clamp_impl = tensor_clamp_impl
         self.signed = signed
         self.narrow_range = narrow_range
         self.delay_wrapper = DelayWrapper(quant_delay_steps)
 
     @brevitas.jit.script_method_110_disabled
-    def to_int(
-            self,
-            scale: Tensor,
-            zero_point: Tensor,
-            bit_width: Tensor,
-            x: Tensor) -> Tensor:
+    def to_int(self, scale: Tensor, zero_point: Tensor, bit_width: Tensor, x: Tensor) -> Tensor:
         y = x / scale
         y = y + zero_point
         min_int_val = self.min_int(bit_width)
         max_int_val = self.max_int(bit_width)
         y = self.float_to_int_impl(y)
         y = self.tensor_clamp_impl(y, min_val=min_int_val, max_val=max_int_val)
         return y
@@ -80,20 +76,15 @@
         return min_int(self.signed, self.narrow_range, bit_width)
 
     @brevitas.jit.script_method
     def max_int(self, bit_width):
         return max_int(self.signed, self.narrow_range, bit_width)
 
     @brevitas.jit.script_method
-    def forward(
-            self,
-            scale: Tensor,
-            zero_point: Tensor,
-            bit_width: Tensor,
-            x: Tensor) -> Tensor:
+    def forward(self, scale: Tensor, zero_point: Tensor, bit_width: Tensor, x: Tensor) -> Tensor:
         y_int = self.to_int(scale, zero_point, bit_width, x)
         y = y_int - zero_point
         y = y * scale
         y = self.delay_wrapper(x, y)
         return y
 
 
@@ -141,18 +132,15 @@
         self.tensor_clamp_impl = tensor_clamp_impl
         self.signed = signed
         self.narrow_range = narrow_range
         self.delay_wrapper = DelayWrapper(quant_delay_steps)
 
     @brevitas.jit.script_method_110_disabled
     def to_int(
-            self,
-            pre_scale: Tensor,
-            pre_zero_point: Tensor,
-            bit_width: Tensor,
+            self, pre_scale: Tensor, pre_zero_point: Tensor, bit_width: Tensor,
             x: Tensor) -> Tensor:
         y = x / pre_scale
         y = y + pre_zero_point
         min_int_val = self.min_int(bit_width)
         max_int_val = self.max_int(bit_width)
         y = self.float_to_int_impl(y)
         y = self.tensor_clamp_impl(y, min_val=min_int_val, max_val=max_int_val)
```

### Comparing `brevitas-0.8.0/src/brevitas/core/quant/ternary.py` & `brevitas-0.9.0/src/brevitas/core/quant/ternary.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,22 +1,21 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from typing import Tuple
 
 import torch
 from torch import Tensor
 from torch.nn import Module
 
 import brevitas
-from brevitas.function.ops_ste import ternary_sign_ste
 from brevitas.core.bit_width import BitWidthConst
-from brevitas.core.utils import StatelessBuffer
 from brevitas.core.quant.delay import DelayWrapper
+from brevitas.core.utils import StatelessBuffer
+from brevitas.function.ops_ste import ternary_sign_ste
 
 
 class TernaryQuant(brevitas.jit.ScriptModule):
     """
     ScriptModule that implements scaled uniform ternary quantization of an input tensor.
     Quantization is performed with :func:`~brevitas.function.ops_ste.ternary_sign_ste`.
 
@@ -63,8 +62,8 @@
     @brevitas.jit.script_method
     def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
         scale = self.scaling_impl(x)
         mask = x.abs().gt(self.threshold * scale)
         y = mask.float() * ternary_sign_ste(x)
         y = y * scale
         y = self.delay_wrapper(x, y)
-        return y, scale, self.zero_point(), self.bit_width()
+        return y, scale, self.zero_point(), self.bit_width()
```

### Comparing `brevitas-0.8.0/src/brevitas/core/restrict_val.py` & `brevitas-0.9.0/src/brevitas/core/restrict_val.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,34 +1,34 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Callable, Union, Optional
 import math
+from typing import Callable, Optional, Union
 
 import torch
 from torch import Tensor
 from torch.nn import Module
 
 import brevitas
-from brevitas.inject.enum import RestrictValueType, FloatToIntImplType  # retrocompatibility
-
-from brevitas.core.function_wrapper import Identity, PowerOfTwo, LogTwo, InplaceLogTwo
-from brevitas.core.function_wrapper import ScalarClampMinSte, RoundSte
+from brevitas.core.function_wrapper import Identity
+from brevitas.core.function_wrapper import InplaceLogTwo
+from brevitas.core.function_wrapper import LogTwo
+from brevitas.core.function_wrapper import PowerOfTwo
+from brevitas.core.function_wrapper import RoundSte
+from brevitas.core.function_wrapper import ScalarClampMinSte
+from brevitas.inject.enum import FloatToIntImplType  # retrocompatibility
+from brevitas.inject.enum import RestrictValueType
 
 assert RestrictValueType  # prevent removal of unused import
 assert FloatToIntImplType
 
 
 class _RestrictClampValue(brevitas.jit.ScriptModule):
 
-    def __init__(
-            self,
-            scaling_min_val: Optional[float],
-            restrict_value_impl: Optional[Module]):
+    def __init__(self, scaling_min_val: Optional[float], restrict_value_impl: Optional[Module]):
         super(_RestrictClampValue, self).__init__()
         if scaling_min_val is not None and scaling_min_val != 0:
             self.clamp_min_ste = ScalarClampMinSte(scaling_min_val)
         else:
             self.clamp_min_ste = Identity()
         if restrict_value_impl is not None:
             self.restrict_value_impl = restrict_value_impl
@@ -40,34 +40,30 @@
         x = self.restrict_value_impl(x)
         x = self.clamp_min_ste(x)
         return x
 
 
 class _RestrictValue(brevitas.jit.ScriptModule):
 
-    def __init__(
-            self,
-            restrict_value_impl: Optional[Module]):
+    def __init__(self, restrict_value_impl: Optional[Module]):
         super(_RestrictValue, self).__init__()
         if restrict_value_impl is not None:
             self.restrict_value_impl = restrict_value_impl
         else:
             self.restrict_value_impl = Identity()
 
     @brevitas.jit.script_method
     def forward(self, x: torch.Tensor):
         x = self.restrict_value_impl(x)
         return x
 
 
 class _ClampValue(brevitas.jit.ScriptModule):
 
-    def __init__(
-            self,
-            scaling_min_val: Optional[float]):
+    def __init__(self, scaling_min_val: Optional[float]):
         super(_ClampValue, self).__init__()
         if scaling_min_val is not None and scaling_min_val != 0:
             self.clamp_min_ste = ScalarClampMinSte(scaling_min_val)
         else:
             self.clamp_min_ste = Identity()
         self.min_val = scaling_min_val
```

### Comparing `brevitas-0.8.0/src/brevitas/core/scaling/int_scaling.py` & `brevitas-0.9.0/src/brevitas/core/scaling/int_scaling.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,36 +1,36 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from torch import Tensor
 
 import brevitas
-from brevitas.function.ops import min_int, max_int
+from brevitas.function.ops import max_int
+from brevitas.function.ops import min_int
 
 
 class IntScaling(brevitas.jit.ScriptModule):
     __constants__ = ['signed', 'narrow_range']
 
     def __init__(self, signed: bool, narrow_range: bool):
         super(IntScaling, self).__init__()
         self.signed = signed
         self.narrow_range = narrow_range
 
     @brevitas.jit.script_method
     def forward(self, bit_width: Tensor) -> Tensor:
         if self.signed:
-            return - min_int(self.signed, self.narrow_range, bit_width)
+            return -min_int(self.signed, self.narrow_range, bit_width)
         else:
             return max_int(self.signed, self.narrow_range, bit_width)
 
 
 class PowerOfTwoIntScaling(brevitas.jit.ScriptModule):
     __constants__ = ['signed']
 
     def __init__(self, signed: bool):
         super(PowerOfTwoIntScaling, self).__init__()
         self.signed = signed
 
     @brevitas.jit.script_method
     def forward(self, bit_width: Tensor) -> Tensor:
-        return max_int(self.signed, False, bit_width) + 1
+        return max_int(self.signed, False, bit_width) + 1
```

### Comparing `brevitas-0.8.0/src/brevitas/core/scaling/runtime.py` & `brevitas-0.9.0/src/brevitas/core/scaling/runtime.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,68 +1,76 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Tuple, Optional, List, Union
+from typing import List, Optional, Tuple, Union
 
 import torch
-from torch.nn import Parameter, Module
+from torch.nn import Module
+from torch.nn import Parameter
 
 import brevitas
 import brevitas.config as config
 from brevitas.core.function_wrapper import Identity
-from brevitas.function.ops_ste import abs_binary_sign_grad
-
-from brevitas.core.stats import _ParameterListStats, _RuntimeStats, DEFAULT_MOMENTUM
 from brevitas.core.restrict_val import _RestrictClampValue
+from brevitas.core.stats import _ParameterListStats
+from brevitas.core.stats import _RuntimeStats
+from brevitas.core.stats import DEFAULT_MOMENTUM
+from brevitas.core.utils import ParameterWrapper
+from brevitas.core.utils import StatelessBuffer
+from brevitas.function.ops_ste import abs_binary_sign_grad
 
 
 class StatsFromParameterScaling(brevitas.jit.ScriptModule):
 
     def __init__(
             self,
             scaling_stats_impl: Module,
             scaling_stats_input_view_shape_impl: Module,
             scaling_stats_input_concat_dim: int,
             tracked_parameter_list: List[torch.nn.Parameter],
             restrict_scaling_impl: Module,
             scaling_shape: Tuple[int, ...],
             affine_rescaling: bool = False,
+            affine_shift_scale: bool = False,
             scaling_min_val: Optional[float] = None) -> None:
         super(StatsFromParameterScaling, self).__init__()
         self.parameter_list_stats = _ParameterListStats(
             scaling_stats_impl,
             scaling_shape,
             scaling_stats_input_view_shape_impl,
             scaling_stats_input_concat_dim,
             tracked_parameter_list)
         self.stats_scaling_impl = _StatsScaling(
             restrict_scaling_impl,
             scaling_shape,
             scaling_min_val,
-            affine_rescaling)
+            affine_rescaling,
+            affine_shift_scale)
 
     @brevitas.jit.script_method
     def forward(self, ignored: torch.Tensor) -> torch.Tensor:
         stats = self.parameter_list_stats()
         return self.stats_scaling_impl(stats)
 
 
 class _StatsScaling(brevitas.jit.ScriptModule):
 
     def __init__(
             self,
             restrict_scaling_impl: Module,
             scaling_shape: Tuple[int, ...],
-            scaling_min_val: Optional[float] = None,
-            affine_rescaling: bool = False) -> None:
+            scaling_min_val: Optional[float],
+            affine_rescaling: bool,
+            affine_shift_scale: bool) -> None:
         super(_StatsScaling, self).__init__()
-
+        if affine_shift_scale and not affine_rescaling:
+            raise RuntimeError(
+                "Disabling shifting of the scale requires to enable affine rescaling first.")
         if affine_rescaling:
-            self.affine_rescaling = _AffineRescaling(scaling_shape)
+            self.affine_rescaling = _AffineRescaling(scaling_shape, affine_shift_scale)
         else:
             self.affine_rescaling = Identity()
         self.restrict_clamp_scaling = _RestrictClampValue(scaling_min_val, restrict_scaling_impl)
         self.restrict_scaling_pre = restrict_scaling_impl.restrict_init_module()
 
     @brevitas.jit.script_method
     def forward(self, stats: torch.Tensor) -> torch.Tensor:
@@ -76,52 +84,58 @@
 
     def __init__(
             self,
             scaling_stats_impl: Module,
             scaling_stats_input_view_shape_impl: Module,
             restrict_scaling_impl: Module,
             scaling_shape: Tuple[int, ...],
-            affine_rescaling: bool,
+            affine_rescaling: bool = False,
+            affine_shift_scale: bool = False,
             scaling_stats_momentum: float = DEFAULT_MOMENTUM,
             scaling_min_val: Optional[float] = None) -> None:
         super(RuntimeStatsScaling, self).__init__()
 
         self.runtime_stats = _RuntimeStats(
             scaling_stats_impl,
             scaling_shape,
             scaling_stats_input_view_shape_impl,
             scaling_stats_momentum)
         self.stats_scaling_impl = _StatsScaling(
             restrict_scaling_impl,
             scaling_shape,
             scaling_min_val,
-            affine_rescaling)
+            affine_rescaling,
+            affine_shift_scale)
 
     @brevitas.jit.script_method
     def forward(self, x: torch.Tensor):
         stats = self.runtime_stats(x)
         return self.stats_scaling_impl(stats)
 
 
 class _AffineRescaling(brevitas.jit.ScriptModule):
 
-    def __init__(self, scaling_shape):
+    def __init__(self, scaling_shape, shift_scale):
         super(_AffineRescaling, self).__init__()
         self.affine_weight = Parameter(torch.ones(scaling_shape))
-        self.affine_bias = Parameter(torch.zeros(scaling_shape))
+        if shift_scale:
+            self.affine_bias = ParameterWrapper(torch.zeros(scaling_shape))
+        else:
+            self.affine_bias = StatelessBuffer(torch.tensor(0.))
 
     @brevitas.jit.script_method
     def forward(self, x):
-        out = x * self.affine_weight + self.affine_bias
+        out = x * self.affine_weight + self.affine_bias()
         out = abs_binary_sign_grad(out)
         return out
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         super(_AffineRescaling, self)._load_from_state_dict(
             state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
         affine_weight_key = prefix + 'affine_weight'
         affine_bias_key = prefix + 'affine_bias'
         if config.IGNORE_MISSING_KEYS and affine_weight_key in missing_keys:
             missing_keys.remove(affine_weight_key)
         if config.IGNORE_MISSING_KEYS and affine_bias_key in missing_keys:
-            missing_keys.remove(affine_bias_key)
+            missing_keys.remove(affine_bias_key)
```

### Comparing `brevitas-0.8.0/src/brevitas/core/scaling/standalone.py` & `brevitas-0.9.0/src/brevitas/core/scaling/standalone.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,26 +1,32 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
+from typing import Optional, Tuple, Union
 import warnings
-from typing import Tuple, Optional, Union
 
 import torch
 from torch import Tensor
-from torch.nn import Parameter, Module
+from torch.nn import Module
+from torch.nn import Parameter
 
 import brevitas
 import brevitas.config as config
-from brevitas.function import abs_binary_sign_grad
 from brevitas.core.function_wrapper import Identity
 from brevitas.core.function_wrapper import OverBatchOverTensorView
-from brevitas.core.utils import StatelessBuffer, inplace_momentum_update, inplace_tensor_mul
-from brevitas.core.restrict_val import _RestrictClampValue, _RestrictValue, _ClampValue
-from brevitas.core.stats import _Stats, SCALAR_SHAPE, DEFAULT_MOMENTUM
+from brevitas.core.restrict_val import _ClampValue
+from brevitas.core.restrict_val import _RestrictClampValue
+from brevitas.core.restrict_val import _RestrictValue
+from brevitas.core.stats import _Stats
+from brevitas.core.stats import DEFAULT_MOMENTUM
+from brevitas.core.stats import SCALAR_SHAPE
+from brevitas.core.utils import inplace_momentum_update
+from brevitas.core.utils import inplace_tensor_mul
+from brevitas.core.utils import StatelessBuffer
+from brevitas.function import abs_binary_sign_grad
 
 
 class ConstScaling(brevitas.jit.ScriptModule):
     """
     ScriptModule implementation of a constant scale factor.
 
     Args:
@@ -45,14 +51,15 @@
     Note:
         The forward method accepts a single placeholder argument. This is required by (early versions of)
         TorchScript to be consistent across different scaling implementations.
 
     Note:
         Maps to scaling_impl_type == ScalingImplType.CONST == 'CONST' == 'const' in higher-level APIs.
     """
+
     def __init__(
             self,
             scaling_init: Union[float, Tensor],
             restrict_scaling_impl: Optional[Module] = None,
             scaling_min_val: Optional[float] = None) -> None:
         super(ConstScaling, self).__init__()
         self.restrict_clamp_scaling = _RestrictClampValue(scaling_min_val, restrict_scaling_impl)
@@ -106,26 +113,25 @@
     Note:
         The forward method accepts a single placeholder argument. This is required by (early versions of)
         TorchScript to be consistent across different scaling implementations.
 
     Note:
         Maps to scaling_impl_type == ScalingImplType.PARAMETER == 'PARAMETER' == 'parameter' in higher-level APIs.
     """
+
     def __init__(
             self,
             scaling_init: Union[float, Tensor],
             scaling_shape: Optional[Tuple[int, ...]] = None,
             restrict_scaling_impl: Optional[Module] = None,
             scaling_min_val: Optional[float] = None) -> None:
         super(ParameterScaling, self).__init__()
 
-        if (isinstance(scaling_init, Tensor)
-                and scaling_shape is not None
-                and scaling_init.shape != SCALAR_SHAPE
-                and scaling_init.shape != scaling_shape):
+        if (isinstance(scaling_init, Tensor) and scaling_shape is not None and
+                scaling_init.shape != SCALAR_SHAPE and scaling_init.shape != scaling_shape):
             raise RuntimeError("scaling_init.shape is non-scalar and != from scaling_shape.")
 
         if isinstance(scaling_init, Tensor):
             scaling_init = scaling_init.detach()
         else:
             scaling_init = torch.tensor(scaling_init)
         if restrict_scaling_impl is not None:
@@ -136,16 +142,17 @@
         self.restrict_clamp_scaling = _RestrictClampValue(scaling_min_val, restrict_scaling_impl)
 
     @brevitas.jit.script_method
     def forward(self, placeholder: Tensor) -> Tensor:
         value = abs_binary_sign_grad(self.restrict_clamp_scaling(self.value))
         return value
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         value_key = prefix + 'value'
         retrocomp_value_key = prefix + 'learned_value'
         if retrocomp_value_key in state_dict:  # retrocompatibility
             state_dict[value_key] = state_dict.pop(retrocomp_value_key)
         super(ParameterScaling, self)._load_from_state_dict(
             state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
         if config.IGNORE_MISSING_KEYS and value_key in missing_keys:
@@ -153,15 +160,15 @@
 
 
 class ParameterFromRuntimeStatsScaling(brevitas.jit.ScriptModule):
     """
     ScriptModule implementation of a learned scale factor initialized from runtime statistics.
     The implementation works in two phases. During the first phase, statistics are collected in
     the same fashion as batchnorm, meaning that while the module is in training mode a set of per-batch
-    statistics are computed and returned, while in background an average of them is retained and returned 
+    statistics are computed and returned, while in background an average of them is retained and returned
     in inference mode. During the second phase, the average accumulated during the first
     phase is used to initialize a learned torch.nn.Parameter, and then the behaviour is the same
     as ParameterScaling.
 
     Args:
         collect_stats_steps (int): Number of calls to the forward method in training mode to collect statistics for.
         scaling_stats_impl (Module): Implementation of the statistics computed during the collection phase.
@@ -193,50 +200,50 @@
         Set env variable BREVITAS_IGNORE_MISSING_KEYS=1 to avoid errors when retraining
         from a floating point state dict.
 
     Note:
         Maps to scaling_impl_type == ScalingImplType.PARAMETER_FROM_STATS == 'PARAMETER_FROM_STATS'
         == 'parameter_from_stats' when applied to runtime values (inputs/outputs/activations) in higher-level APIs.
     """
-    __constants__ = ['collect_stats_steps', 'momentum']
+    __constants__ = ['momentum']
 
     def __init__(
             self,
             collect_stats_steps: int,
             scaling_stats_impl: Module,
             scaling_stats_input_view_shape_impl: Module = OverBatchOverTensorView(),
             scaling_shape: Tuple[int, ...] = SCALAR_SHAPE,
             restrict_scaling_impl: Optional[Module] = None,
             scaling_stats_momentum: Optional[float] = DEFAULT_MOMENTUM,
             scaling_min_val: Optional[float] = None) -> None:
         super(ParameterFromRuntimeStatsScaling, self).__init__()
         assert collect_stats_steps > 0, 'Steps should be more than 0'
-        self.collect_stats_steps = collect_stats_steps
+        self.collect_stats_steps: int = brevitas.jit.Attribute(collect_stats_steps, int)
         self.counter: int = brevitas.jit.Attribute(0, int)
         self.stats_input_view_shape_impl = scaling_stats_input_view_shape_impl
         self.stats = _Stats(scaling_stats_impl, scaling_shape)
         self.momentum = scaling_stats_momentum
         self.register_buffer('buffer', torch.full(scaling_shape, 1.0))
         self.value = Parameter(torch.full(scaling_shape, 1.0))
         self.restrict_scaling = _RestrictValue(restrict_scaling_impl)
         self.clamp_scaling = _ClampValue(scaling_min_val)
         if restrict_scaling_impl is not None:
             self.restrict_inplace_preprocess = restrict_scaling_impl.restrict_init_inplace_module()
             self.restrict_preprocess = restrict_scaling_impl.restrict_init_module()
         else:
             self.restrict_inplace_preprocess = Identity()
             self.restrict_preprocess = Identity()
-    
+
     @brevitas.jit.script_method
     def training_forward(self, stats_input: Tensor) -> Tensor:
         if self.counter < self.collect_stats_steps:
             stats_input = self.stats_input_view_shape_impl(stats_input)
             stats = self.stats(stats_input)
             # workaround to avoid find_ununsed_parameter=True in DDP
-            stats = stats + 0. * self.value # stats gradient will change from None to 0.
+            stats = stats + 0. * self.value  # stats gradient will change from None to 0.
             clamped_stats = self.clamp_scaling(stats)
             new_counter = self.counter + 1
             if self.counter == 0:
                 inplace_tensor_mul(self.buffer, clamped_stats.detach())
             else:
                 inplace_momentum_update(
                     self.buffer, clamped_stats.detach(), self.momentum, self.counter, new_counter)
@@ -272,25 +279,27 @@
         if self.counter == 0:
             del output_dict[prefix + 'value']
         # Save buffer into value for any non-zero number of collection steps
         elif self.counter <= self.collect_stats_steps:
             output_dict[prefix + 'value'] = self.restrict_preprocess(self.buffer)
         return output_dict
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
-        super(ParameterFromRuntimeStatsScaling, self)._load_from_state_dict(
-            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
-        value_key = prefix + 'value'
-        # Buffer is supposed to be always missing
-        missing_keys.remove(prefix + 'buffer')
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         # Retrocompatibility with older ParameterScaling, for when scaling impl is switched over
+        value_key = prefix + 'value'
         retrocomp_value_key = prefix + 'learned_value'
         if retrocomp_value_key in state_dict:
             state_dict[value_key] = state_dict.pop(retrocomp_value_key)
+
+        super(ParameterFromRuntimeStatsScaling, self)._load_from_state_dict(
+            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
+        # Buffer is supposed to be always missing
+        missing_keys.remove(prefix + 'buffer')
         # Pytorch stores training flag as a buffer with JIT enabled
         training_key = prefix + 'training'
         if training_key in missing_keys:
             missing_keys.remove(training_key)
         # disable stats collection when a pretrained value is loaded
         if value_key not in missing_keys:
             self.counter = self.collect_stats_steps + 1
```

### Comparing `brevitas-0.8.0/src/brevitas/core/stats/stats_op.py` & `brevitas-0.9.0/src/brevitas/core/stats/stats_op.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,23 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Optional, Tuple
 import math
+from typing import Optional, Tuple
 
 import torch
 from torch import Tensor
 from torch.nn import Parameter
 
 import brevitas
+from brevitas import config
 from brevitas.core.utils import StatelessBuffer
 from brevitas.function.ops import max_int
-from brevitas import config
-from .stats_wrapper import SCALAR_SHAPE
 
+from .stats_wrapper import SCALAR_SHAPE
 
 DEFAULT_STD_DEV_EPSILON = 1e-8
 
 
 class NegativeMinOrZero(brevitas.jit.ScriptModule):
     __constants__ = ['stats_reduce_dim']
 
@@ -37,15 +36,16 @@
             min_val <= self.zero().to(min_val.dtype), min_val, self.zero().to(min_val.dtype))
         return min_val
 
 
 class AbsPercentile(brevitas.jit.ScriptModule):
     __constants__ = ['q', 'stats_reduce_dim']
 
-    def __init__(self, high_percentile_q: float, stats_reduce_dim: Optional[int], percentile_q = None):
+    def __init__(
+            self, high_percentile_q: float, stats_reduce_dim: Optional[int], percentile_q=None):
         super(AbsPercentile, self).__init__()
         if percentile_q is not None:
             raise RuntimeError("percentile_q is deprecated, please pass high_percentile_q.")
         assert high_percentile_q <= 100, "q has to be a percentage"
         self.q = high_percentile_q
         self.stats_reduce_dim = stats_reduce_dim
 
@@ -60,16 +60,16 @@
             assert len(x.size()) == 2, "Only 2-dim input is supported."
             other_dim = abs(self.stats_reduce_dim - 1)
             dim_slice = torch.narrow(x, dim=other_dim, start=0, length=1)
             # k is 1-indexed, so round away from zero
             k = int(math.floor(.01 * self.q * dim_slice.numel() + 0.5))
             result = x.abs().kthvalue(k, dim=self.stats_reduce_dim).values
         return result
-    
-    
+
+
 class NegativePercentileOrZero(brevitas.jit.ScriptModule):
     __constants__ = ['stats_reduce_dim', 'q']
 
     def __init__(self, low_percentile_q, stats_reduce_dim: Optional[int] = None) -> None:
         super(NegativePercentileOrZero, self).__init__()
         self.stats_reduce_dim = stats_reduce_dim
         self.q = low_percentile_q
@@ -88,20 +88,24 @@
             dim_slice = torch.narrow(x, dim=other_dim, start=0, length=1)
             # k is 1-indexed, so round away from zero
             k = int(math.ceil(.01 * self.q * dim_slice.numel()))
             result = x.kthvalue(k, dim=self.stats_reduce_dim).values
         result = torch.where(
             result <= self.zero().to(result.dtype), result, self.zero().to(result.dtype))
         return result
-    
-    
+
+
 class PercentileInterval(brevitas.jit.ScriptModule):
     __constants__ = ['stats_reduce_dim', 'low_q', 'high_q']
 
-    def __init__(self, low_percentile_q, high_percentile_q, stats_reduce_dim: Optional[int] = None) -> None:
+    def __init__(
+            self,
+            low_percentile_q,
+            high_percentile_q,
+            stats_reduce_dim: Optional[int] = None) -> None:
         super(PercentileInterval, self).__init__()
         self.stats_reduce_dim = stats_reduce_dim
         self.low_q = low_percentile_q
         self.high_q = high_percentile_q
 
     @brevitas.jit.script_method
     def forward(self, x: Tensor) -> Tensor:
@@ -260,16 +264,17 @@
 
     @brevitas.jit.script_method
     def forward(self, x: Tensor):
         sigma = self.sigma.view(self.sigma.shape)  # trick to get a tensor type
         out = self.impl(x, sigma)
         return out
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         value_key = prefix + 'sigma'
         retrocomp_value_key = prefix + 'learned_sigma'
         if retrocomp_value_key in state_dict:  # retrocompatibility
             state_dict[value_key] = state_dict.pop(retrocomp_value_key)
         super(MeanLearnedSigmaStd, self)._load_from_state_dict(
             state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
         sigma_key = prefix + 'sigma'
@@ -279,15 +284,15 @@
 
 class KLMinimizerThreshold(torch.nn.Module):
     """
     Based on:
     https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/contrib/quantization.py
     """
 
-    def __init__(self, signed, bit_width_impl, num_bins = 1000 + 1, smoothing_eps=0.0001):
+    def __init__(self, signed, bit_width_impl, num_bins=1000 + 1, smoothing_eps=0.0001):
         super(KLMinimizerThreshold, self).__init__()
         self.num_bins = num_bins
         self.smoothing_eps = smoothing_eps
         self.signed = signed
         self.bit_width_impl = bit_width_impl
         self.absmax_impl = AbsMax()
 
@@ -344,8 +349,44 @@
             q = self.smooth_normalize_distribution(q, self.smoothing_eps)
             if q is None:
                 divergence[i - num_quantized_bins // 2] = float('inf')
             else:
                 divergence[i - num_quantized_bins // 2] = torch.distributions.kl.kl_divergence(p, q)
         min_divergence_idx = torch.argmin(divergence)
         opt_threshold = thresholds[min_divergence_idx]
-        return opt_threshold
+        return opt_threshold
+
+
+class L1Norm(brevitas.jit.ScriptModule):
+    """ScriptModule implementation to collect per-channel L1 normalization stats
+    for weight normalization-based quantization."""
+    __constants__ = ['stats_reduce_dim']
+
+    def __init__(self, stats_reduce_dim: Optional[int] = None) -> None:
+        super(L1Norm, self).__init__()
+        self.stats_reduce_dim = stats_reduce_dim
+
+    @brevitas.jit.script_method
+    def forward(self, x: Tensor):
+        if self.stats_reduce_dim is None:
+            # Need to be able to return the max per-channel L1 norm as a scalar
+            raise NotImplementedError("L1 normalization is not supported per-tensor yet.")
+        else:
+            return x.norm(p=1, dim=self.stats_reduce_dim, keepdim=True)
+
+
+class L2Norm(brevitas.jit.ScriptModule):
+    """ScriptModule implementation to collect per-channel L2 normalization stats
+    for weight normalization-based quantization."""
+    __constants__ = ['stats_reduce_dim']
+
+    def __init__(self, stats_reduce_dim: Optional[int] = None) -> None:
+        super(L2Norm, self).__init__()
+        self.stats_reduce_dim = stats_reduce_dim
+
+    @brevitas.jit.script_method
+    def forward(self, x: Tensor):
+        if self.stats_reduce_dim is None:
+            # Need to be able to return the max per-channel L2 norm as a scalar
+            raise NotImplementedError("L2 normalization is not supported per-tensor yet.")
+        else:
+            return x.norm(p=2, dim=self.stats_reduce_dim, keepdim=True)
```

### Comparing `brevitas-0.8.0/src/brevitas/core/stats/stats_wrapper.py` & `brevitas-0.9.0/src/brevitas/core/stats/stats_wrapper.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,80 +1,80 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Tuple, List, Optional
+from typing import List, Optional, Tuple
 
 import torch
-from torch import nn, Tensor
+from torch import nn
+from torch import Tensor
 
 import brevitas
 import brevitas.config as config
-from .view_wrapper import _ViewCatParameterWrapper, _ViewParameterWrapper
+from brevitas.core.utils import inplace_momentum_update
+from brevitas.core.utils import inplace_tensor_mul
 
+from .view_wrapper import _ViewCatParameterWrapper
+from .view_wrapper import _ViewParameterWrapper
 
 DEFAULT_MOMENTUM = 0.1
 SCALAR_SHAPE = ()
 
 
 class _Stats(brevitas.jit.ScriptModule):
     __constants__ = ['stats_output_shape']
 
-    def __init__(
-            self,
-            stats_impl: nn.Module,
-            stats_output_shape: Tuple[int, ...]) -> None:
+    def __init__(self, stats_impl: nn.Module, stats_output_shape: Tuple[int, ...]) -> None:
         super(_Stats, self).__init__()
         self.stats_output_shape = stats_output_shape
         self.stats_impl = stats_impl
 
     @brevitas.jit.script_method
     def forward(self, input: Tensor) -> Tensor:
         stats = self.stats_impl(input)
         stats = stats.view(self.stats_output_shape)
         return stats
 
 
 class _RuntimeStats(brevitas.jit.ScriptModule):
-    __constants__ = ['stats_input_concat_dim',
-                     'stats_permute_dims',
-                     'momentum']
+    __constants__ = ['stats_input_concat_dim', 'stats_permute_dims', 'momentum']
 
     def __init__(
             self,
             stats_impl: nn.Module,
             stats_output_shape: Tuple[int, ...],
             stats_input_view_shape_impl: nn.Module,
-            stats_buffer_momentum: float = DEFAULT_MOMENTUM) -> None:
+            stats_buffer_momentum: float) -> None:
         super(_RuntimeStats, self).__init__()
-        self.first_batch = brevitas.jit.Attribute(True, bool)
+        self.counter = brevitas.jit.Attribute(0, int)
         self.stats_input_view_shape_impl = stats_input_view_shape_impl
         self.stats = _Stats(stats_impl, stats_output_shape)
         self.momentum = stats_buffer_momentum
         self.register_buffer('running_stats', torch.full(stats_output_shape, 1.0))
 
     @brevitas.jit.script_method
     def forward(self, stats_input) -> Tensor:
         if self.training:
             stats_input = self.stats_input_view_shape_impl(stats_input)
-            out = self.stats(stats_input)
-            if self.first_batch:
-                self.running_stats *= out.detach()
-                self.first_batch = False
+            stats = self.stats(stats_input)
+            new_counter = self.counter + 1
+            if self.counter == 0:
+                inplace_tensor_mul(self.running_stats, stats.detach())
             else:
-                self.running_stats *= (1 - self.momentum)
-                self.running_stats += self.momentum * out.detach()
+                inplace_momentum_update(
+                    self.running_stats, stats.detach(), self.momentum, self.counter, new_counter)
+            self.counter = new_counter
         else:
-            out = self.running_stats
-        return out
+            stats = self.running_stats
+        return stats
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
-        super(_RuntimeStats, self)._load_from_state_dict(state_dict, prefix, local_metadata, strict,
-            missing_keys, unexpected_keys, error_msgs)
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
+        super(_RuntimeStats, self)._load_from_state_dict(
+            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
         running_stats_key = prefix + 'running_stats'
         if config.IGNORE_MISSING_KEYS and running_stats_key in missing_keys:
             missing_keys.remove(running_stats_key)
         # Pytorch stores training flag as a buffer with JIT enabled
         training_key = prefix + 'training'
         if training_key in missing_keys:
             missing_keys.remove(training_key)
@@ -93,25 +93,23 @@
         super(_ParameterListStats, self).__init__()
 
         self.stats_input_concat_dim = stats_input_concat_dim
         self.first_tracked_param = _ViewParameterWrapper(
             tracked_parameter_list[0], stats_input_view_shape_impl)
         if len(tracked_parameter_list) > 1:
             extra_list = [
-                _ViewCatParameterWrapper(param, stats_input_view_shape_impl, stats_input_concat_dim)
-                          for param in tracked_parameter_list[1:]]
+                _ViewCatParameterWrapper(
+                    param, stats_input_view_shape_impl, stats_input_concat_dim)
+                for param in tracked_parameter_list[1:]]
             self.extra_tracked_params_list = torch.nn.ModuleList(extra_list)
         else:
             self.extra_tracked_params_list = None
         self.stats = _Stats(stats_impl, stats_output_shape)
 
     @brevitas.jit.script_method
     def forward(self) -> torch.Tensor:
         stats_input = self.first_tracked_param()
         if self.extra_tracked_params_list is not None:
             for extra_tracked_param in self.extra_tracked_params_list:
                 stats_input = extra_tracked_param(stats_input)
         out = self.stats(stats_input)
         return out
-
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas/core/stats/view_wrapper.py` & `brevitas-0.9.0/src/brevitas/core/stats/view_wrapper.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import torch
 from torch import Tensor
-from torch.nn import Parameter, Module
+from torch.nn import Module
+from torch.nn import Parameter
 
 import brevitas
 from brevitas.core.function_wrapper import StatsInputViewShapeImpl  # retrocomp
 
 
 class _ViewParameterWrapper(brevitas.jit.ScriptModule):
 
@@ -17,16 +17,17 @@
         self.parameter = parameter
         self.view_shape_impl = view_shape_impl
 
     @brevitas.jit.script_method
     def forward(self) -> Tensor:
         return self.view_shape_impl(self.parameter)
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         super(_ViewParameterWrapper, self)._load_from_state_dict(
             state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
         parameter_key = prefix + 'parameter'
         if parameter_key in missing_keys:
             missing_keys.remove(parameter_key)
 
     def state_dict(self, destination=None, prefix='', keep_vars=False):
@@ -45,16 +46,17 @@
         self.view_shape_impl = view_shape_impl
         self.cat_dim = cat_dim
 
     @brevitas.jit.script_method
     def forward(self, x: Tensor) -> Tensor:
         return torch.cat([self.view_shape_impl(self.parameter), x], dim=self.cat_dim)
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         super(_ViewCatParameterWrapper, self)._load_from_state_dict(
             state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
         parameter_key = prefix + 'parameter'
         if parameter_key in missing_keys:
             missing_keys.remove(parameter_key)
 
     def state_dict(self, destination=None, prefix='', keep_vars=False):
```

### Comparing `brevitas-0.8.0/src/brevitas/core/utils.py` & `brevitas-0.9.0/src/brevitas/core/utils.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from typing import Optional
+
 import torch
 
 import brevitas
 
 VALUE_ATTR_NAME = 'value'
 
 
@@ -39,25 +39,48 @@
 
 
 class StatelessBuffer(brevitas.jit.ScriptModule):
 
     def __init__(self, value: torch.Tensor):
         super(StatelessBuffer, self).__init__()
         self.register_buffer(VALUE_ATTR_NAME, value)
-    
+
     @brevitas.jit.script_method
     def forward(self):
         return self.value.detach()
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         super(StatelessBuffer, self)._load_from_state_dict(
             state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
         value_key = prefix + VALUE_ATTR_NAME
         if value_key in missing_keys:
             missing_keys.remove(value_key)
 
     def state_dict(self, destination=None, prefix='', keep_vars=False):
         output_dict = super(StatelessBuffer, self).state_dict(
             destination=destination, prefix=prefix, keep_vars=keep_vars)
         del output_dict[prefix + VALUE_ATTR_NAME]
-        return output_dict
+        return output_dict
+
+
+class SingleArgStatelessBuffer(brevitas.jit.ScriptModule):
+
+    def __init__(self, value: torch.Tensor):
+        super(SingleArgStatelessBuffer, self).__init__()
+        self.const = StatelessBuffer(torch.tensor(value))
+
+    @brevitas.jit.script_method
+    def forward(self, placeholder):
+        return self.const()
+
+
+class ParameterWrapper(brevitas.jit.ScriptModule):
+
+    def __init__(self, value: torch.Tensor):
+        super(ParameterWrapper, self).__init__()
+        self.register_parameter(VALUE_ATTR_NAME, value)
+
+    @brevitas.jit.script_method
+    def forward(self):
+        return self.value
```

### Comparing `brevitas-0.8.0/src/brevitas/core/zero_point.py` & `brevitas-0.9.0/src/brevitas/core/zero_point.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,31 +1,33 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Optional, Tuple, Union, List
+from typing import List, Optional, Tuple, Union
 
 import torch
 from torch import Tensor
-from torch.nn import Module, Parameter
+from torch.nn import Module
+from torch.nn import Parameter
 
 import brevitas
 from brevitas import config
-from brevitas.core.stats import SCALAR_SHAPE, DEFAULT_MOMENTUM, _ParameterListStats
+from brevitas.core.stats import _ParameterListStats
+from brevitas.core.stats import DEFAULT_MOMENTUM
+from brevitas.core.stats import SCALAR_SHAPE
 from brevitas.function import abs_binary_sign_grad
 
-from .utils import StatelessBuffer, inplace_tensor_add, inplace_momentum_update
-
+from .utils import inplace_momentum_update
+from .utils import inplace_tensor_add
+from .utils import StatelessBuffer
 
 __all__ = [
     'ZeroZeroPoint',
-    'MinUintZeroPoint',
-    'ParameterFromRuntimeMinZeroPoint',
-    'ParameterZeroPoint'
-]
+    'StatsFromParameterZeroPoint',
+    'ParameterFromRuntimeZeroPoint',
+    'ParameterZeroPoint']
 
 
 class ZeroZeroPoint(brevitas.jit.ScriptModule):
 
     def __init__(self) -> None:
         super(ZeroZeroPoint, self).__init__()
         self.zero_point = StatelessBuffer(torch.tensor(0.0))
@@ -34,30 +36,29 @@
     def forward(self, x: Tensor, scale: Tensor, bit_width: Tensor) -> Tensor:
         return self.zero_point()
 
 
 class _ScaleShiftZeroPoint(brevitas.jit.ScriptModule):
     __constants__ = ['quantize_zero_point']
 
-    def __init__(
-            self, int_quant: Module, quantize_zero_point: bool) -> None:
+    def __init__(self, int_quant: Module, quantize_zero_point: bool) -> None:
         super(_ScaleShiftZeroPoint, self).__init__()
         self.int_quant = int_quant
         self.quantize_zero_point = quantize_zero_point
 
     @brevitas.jit.script_method
     def forward(self, zero_point: Tensor, scale: Tensor, bit_width: Tensor) -> Tensor:
         min_int = self.int_quant.min_int(bit_width)
         if self.quantize_zero_point:
             out = self.int_quant.to_int(scale, min_int, bit_width, zero_point)
         else:
             out = zero_point / scale + min_int
         return out
-    
-    
+
+
 class StatsFromParameterZeroPoint(brevitas.jit.ScriptModule):
 
     def __init__(
             self,
             int_quant: Module,
             quantize_zero_point: bool,
             zero_point_stats_input_view_shape_impl: Module,
@@ -68,28 +69,24 @@
         super(StatsFromParameterZeroPoint, self).__init__()
         self.parameter_list_stats = _ParameterListStats(
             zero_point_stats_impl,
             zero_point_shape,
             zero_point_stats_input_view_shape_impl,
             zero_point_stats_input_concat_dim,
             tracked_parameter_list)
-        self.scale_shift_zero_point = _ScaleShiftZeroPoint(
-            int_quant, quantize_zero_point)
+        self.scale_shift_zero_point = _ScaleShiftZeroPoint(int_quant, quantize_zero_point)
 
     @brevitas.jit.script_method
     def forward(self, x: Tensor, scale: Tensor, bit_width: Tensor) -> torch.Tensor:
         stats = self.parameter_list_stats()
-        return self.scale_shift_zero_point(- stats, scale, bit_width)
+        return self.scale_shift_zero_point(-stats, scale, bit_width)
 
 
 class ParameterFromRuntimeZeroPoint(brevitas.jit.ScriptModule):
-    __constants__ = ['stats_permute_dims',
-                     'collect_stats_steps',
-                     'zero_point_shape',
-                     'momentum']
+    __constants__ = ['stats_permute_dims', 'collect_stats_steps', 'zero_point_shape', 'momentum']
 
     def __init__(
             self,
             collect_stats_steps: int,
             int_quant: Module,
             quantize_zero_point: bool,
             zero_point_stats_impl: Optional[int],
@@ -102,16 +99,15 @@
         self.counter: int = brevitas.jit.Attribute(0, int)
         self.zero_point_shape = zero_point_shape
         self.stats_input_view_shape_impl = zero_point_stats_input_view_shape_impl
         self.momentum = zero_point_stats_momentum
         self.value = Parameter(torch.full(zero_point_shape, 0.0))
         self.register_buffer('buffer', torch.full(zero_point_shape, 0.0))
         self.zero_point_stats_impl = zero_point_stats_impl
-        self.scale_shift_zero_point = _ScaleShiftZeroPoint(
-            int_quant, quantize_zero_point)
+        self.scale_shift_zero_point = _ScaleShiftZeroPoint(int_quant, quantize_zero_point)
 
     @brevitas.jit.script_method
     def training_forward(self, x) -> Tensor:
         if self.counter < self.collect_stats_steps:
             stats_input = self.stats_input_view_shape_impl(x)
             stats = self.zero_point_stats_impl(stats_input)
             stats = stats.view(self.zero_point_shape)
@@ -142,28 +138,29 @@
             else:
                 out = self.value
         out = abs_binary_sign_grad(out)
         out = self.scale_shift_zero_point(out, scale, bit_width)
         return out
 
     def state_dict(self, destination=None, prefix='', keep_vars=False):
-        output_dict = super(ParameterFromRuntimeZeroPoint, self).state_dict(
-            destination, prefix, keep_vars)        
+        output_dict = super(ParameterFromRuntimeZeroPoint,
+                            self).state_dict(destination, prefix, keep_vars)
         # Avoid saving the buffer
         del output_dict[prefix + 'buffer']
         # Avoid saving the init value
         if self.counter == 0:
             del output_dict[prefix + 'value']
         # Save buffer into value for any non-zero number of collection steps
         elif self.counter <= self.collect_stats_steps:
             output_dict[prefix + 'value'] = self.buffer
         return output_dict
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         super(ParameterFromRuntimeZeroPoint, self)._load_from_state_dict(
             state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
         value_key = prefix + 'value'
         buffer_key = prefix + 'buffer'
         # Buffer is supposed to be always missing
         missing_keys.remove(buffer_key)
         # Pytorch stores training flag as a buffer with JIT enabled
@@ -174,48 +171,44 @@
         if value_key not in missing_keys:
             self.counter = self.collect_stats_steps + 1
         if config.IGNORE_MISSING_KEYS and value_key in missing_keys:
             missing_keys.remove(value_key)
 
 
 class ParameterZeroPoint(brevitas.jit.ScriptModule):
-    __constants__ = ['stats_permute_dims',
-                     'collect_stats_steps',
-                     'momentum']
+    __constants__ = ['stats_permute_dims', 'collect_stats_steps', 'momentum']
 
     def __init__(
             self,
             zero_point_init: Union[float, torch.Tensor],
             int_quant: Module,
             quantize_zero_point: bool,
             zero_point_shape: Tuple[int, ...] = None) -> None:
         super(ParameterZeroPoint, self).__init__()
-        if (isinstance(zero_point_init, Tensor)
-                and zero_point_shape is not None
-                and zero_point_init.shape != SCALAR_SHAPE
-                and zero_point_init.shape != zero_point_shape):
+        if (isinstance(zero_point_init, Tensor) and zero_point_shape is not None and
+                zero_point_init.shape != SCALAR_SHAPE and
+                zero_point_init.shape != zero_point_shape):
             raise RuntimeError("zero_point_init.shape is non-scalar and != from zero_point_shape.")
 
         if isinstance(zero_point_init, Tensor):
             zero_point_init = zero_point_init.detach()
         else:
             zero_point_init = torch.tensor(zero_point_init)
         if zero_point_init.shape == SCALAR_SHAPE and zero_point_shape is not None:
             zero_point_init = torch.full(zero_point_shape, zero_point_init)
         self.value = Parameter(zero_point_init)
-        self.scale_shift_zero_point = _ScaleShiftZeroPoint(
-            int_quant, quantize_zero_point)
+        self.scale_shift_zero_point = _ScaleShiftZeroPoint(int_quant, quantize_zero_point)
 
     @brevitas.jit.script_method
     def forward(self, x: Tensor, scale: Tensor, bit_width: Tensor) -> Tensor:
         out = abs_binary_sign_grad(self.value)
         out = self.scale_shift_zero_point(out, scale, bit_width)
         return out
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         super(ParameterZeroPoint, self)._load_from_state_dict(
             state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
         value_key = prefix + 'value'
         if config.IGNORE_MISSING_KEYS and value_key in missing_keys:
             missing_keys.remove(value_key)
-
```

### Comparing `brevitas-0.8.0/src/brevitas/csrc/autograd_ste_ops.cpp` & `brevitas-0.9.0/src/brevitas/csrc/autograd_ste_ops.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 /* Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
- */ 
+ */
 
 
 #include <ATen/TensorUtils.h>
 #include <torch/extension.h>
 
 using namespace at;
 using torch::Tensor;
@@ -265,8 +265,7 @@
     m.def("ternary_sign_ste_impl", &ternary_sign_ste_impl);
     m.def("ceil_ste_impl", &ceil_ste_impl);
     m.def("floor_ste_impl", &floor_ste_impl);
     m.def("round_to_zero_ste_impl", &round_to_zero_ste_impl);
     m.def("dpu_round_ste_impl", &dpu_round_ste_impl);
     m.def("abs_binary_sign_grad_impl", &abs_binary_sign_grad_impl);
 }
-
```

### Comparing `brevitas-0.8.0/src/brevitas/export/__init__.py` & `brevitas-0.9.0/src/brevitas/export/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,34 +1,33 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from functools import wraps
 
+from .onnx.debug import enable_debug
 from .onnx.finn.manager import FINNManager
 from .onnx.qonnx.manager import QONNXManager
-from .onnx.standard.qoperator.manager import StdQOpONNXManager
 from .onnx.standard.qcdq.manager import StdQCDQONNXManager
+from .onnx.standard.qoperator.manager import StdQOpONNXManager
 from .torch.qcdq.manager import TorchQCDQManager
 from .torch.qoperator.manager import TorchQOpManager
-from .onnx.debug import enable_debug
 
 
 @wraps(FINNManager.export)
 def export_finn_onnx(*args, **kwargs):
     return FINNManager.export(*args, **kwargs)
 
 
 @wraps(QONNXManager.export)
 def export_brevitas_onnx(*args, **kwargs):  # alias for qonnx
     return QONNXManager.export(*args, **kwargs)
 
 
 @wraps(QONNXManager.export)
-def export_qonnx(*args, **kwargs):  
+def export_qonnx(*args, **kwargs):
     return QONNXManager.export(*args, **kwargs)
 
 
 @wraps(StdQOpONNXManager.export)
 def export_onnx_qop(*args, **kwargs):
     return StdQOpONNXManager.export(*args, **kwargs)
 
@@ -41,8 +40,8 @@
 @wraps(TorchQOpManager.export)
 def export_torch_qop(*args, **kwargs):
     return TorchQOpManager.export(*args, **kwargs)
 
 
 @wraps(TorchQCDQManager.export)
 def export_torch_qcdq(*args, **kwargs):
-    return TorchQCDQManager.export(*args, **kwargs)
+    return TorchQCDQManager.export(*args, **kwargs)
```

### Comparing `brevitas-0.8.0/src/brevitas/export/common/handler/base.py` & `brevitas-0.9.0/src/brevitas/export/common/handler/base.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,75 +1,73 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from abc import abstractmethod, ABC
+from abc import ABC
+from abc import abstractmethod
 import math
 
 import torch
-from torch.nn import Module
 from torch import Tensor
+from torch.nn import Module
 
-from brevitas.function.ops import min_int, max_int
+from brevitas.function.ops import max_int
+from brevitas.function.ops import min_int
 
-__all__ = [
-    'BaseHandler',
-    'BitWidthHandlerMixin',
-    'ZeroPointHandlerMixin'
-]
+__all__ = ['BaseHandler', 'BitWidthHandlerMixin', 'ZeroPointHandlerMixin']
 
 
 class BaseHandler(Module, ABC):
-    
+
     def __init__(self) -> None:
         super().__init__()
 
     def attach_debug_info(self, module):
         pass
 
     @abstractmethod
     def prepare_for_export(self, module):
         pass
-    
+
 
 class QuantAxisMixin(ABC):
-    
+
     @classmethod
     def quant_axis(cls, scale):
         for i, s in enumerate(scale.shape):
             if s != 1:
                 return i
         return None
-    
+
 
 class ClipMixin(ABC):
 
     @classmethod
     def int_clip_symbolic_kwargs(cls, narrow, signed, bit_width):
         # equality comparisons among power-of-2 floats are okay
         if narrow or bit_width != 8. and bit_width != 32.:
             if signed and (bit_width < 8. or narrow and bit_width <= 8.):
                 dtype = torch.int8
             elif not signed and (bit_width < 8. or narrow and bit_width <= 8.):
                 dtype = torch.uint8
             elif signed and (bit_width < 32. or narrow and bit_width <= 32.):
                 dtype = torch.int32
             else:
-                raise RuntimeError(f"Sign {signed} and bit width {bit_width} not supported for export.")
+                raise RuntimeError(
+                    f"Sign {signed} and bit width {bit_width} not supported for export.")
             return {
                 'min_val': min_int(signed, narrow, bit_width).to(dtype),
                 'max_val': max_int(signed, narrow, bit_width).to(dtype)}
         else:
             return None
-        
+
     @classmethod
     def float_clip_symbolic_kwargs(cls, narrow, signed, bit_width, scale, zero_point):
         symbolic_kwargs = cls.int_clip_symbolic_kwargs(narrow, signed, bit_width)
         if symbolic_kwargs is not None:
-            symbolic_kwargs['min_val'] = (symbolic_kwargs['min_val'] - zero_point) * scale 
+            symbolic_kwargs['min_val'] = (symbolic_kwargs['min_val'] - zero_point) * scale
             symbolic_kwargs['max_val'] = (symbolic_kwargs['max_val'] - zero_point) * scale
         return symbolic_kwargs
 
 
 class BitWidthHandlerMixin(ABC):
 
     @classmethod
@@ -115,15 +113,15 @@
         if not exponent.is_integer():
             raise RuntimeError("Only power-of-two scale factors are supported.")
         exponent = int(exponent)
         return exponent
 
     @classmethod
     def validate_neg_scalar_int_exponent(cls, scale: Tensor):
-        return - cls.validate_scalar_int_exponent(scale)
+        return -cls.validate_scalar_int_exponent(scale)
 
 
 class ZeroPointHandlerMixin(ABC):
 
     @classmethod
     def zero_point_with_dtype(cls, signed, bit_width, zero_point):
         if not signed:
```

### Comparing `brevitas-0.8.0/src/brevitas/export/common/handler/qcdq.py` & `brevitas-0.9.0/src/brevitas/export/common/handler/qcdq.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,184 +1,177 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from abc import abstractmethod, ABC
+from abc import ABC
+from abc import abstractmethod
 
 import torch
 from torch import Tensor
 
-from brevitas.proxy import WeightQuantProxyFromInjector
-from brevitas.proxy import DecoupledWeightQuantProxyFromInjector
-from brevitas.proxy import BiasQuantProxyFromInjector
+from brevitas.export.common import to_0dim_if_scalar
+from brevitas.export.common import to_item_if_0dim
 from brevitas.proxy import ActQuantProxyFromInjector
+from brevitas.proxy import BiasQuantProxyFromInjector
+from brevitas.proxy import DecoupledWeightQuantProxyFromInjector
+from brevitas.proxy import WeightQuantProxyFromInjector
 from brevitas.proxy.runtime_quant import TruncQuantProxyFromInjector
-from brevitas.export.common import to_0dim_if_scalar, to_item_if_0dim
-from .base import QuantAxisMixin, ClipMixin, ZeroPointHandlerMixin, BitWidthHandlerMixin
+
+from .base import BitWidthHandlerMixin
+from .base import ClipMixin
+from .base import QuantAxisMixin
+from .base import ZeroPointHandlerMixin
 
 
 class DQMixin(ABC):
-    
+
     @abstractmethod
     def validate(self):
         pass
-    
+
     @abstractmethod
     def dequantize_fn(self, x, scale, zero_point, axis):
         pass
-    
+
     @property
     @abstractmethod
     def flatten_dequantize_params(self):
         pass
-    
+
     @property
     @abstractmethod
-    def itemize_scalar_params(self):
+    def itemize_quantize_scalar_params(self):
         pass
-    
+
     def assert_ge_zero(self, *args):
         for a in args:
             bools = a >= 0.
             if isinstance(bools, torch.Tensor):
                 bools = bools.all()
             assert bools
-    
-    
+
+
 class QCDQMixin(DQMixin):
-    
-    @classmethod    
+
+    @classmethod
     @abstractmethod
     def uint8_dtype(cls):
         pass
-    
-    @classmethod    
+
+    @classmethod
     @abstractmethod
     def int8_dtype(cls):
         pass
-    
-    @classmethod    
+
+    @classmethod
     @abstractmethod
     def int32_dtype(cls):
         pass
-    
+
     @property
     @abstractmethod
     def clip_over_integers(self):
         pass
-    
+
     @abstractmethod
     def quantize_fn(self, x, scale, zero_point, dtype, axis):
         pass
-    
+
     @abstractmethod
     def clip_fn(self, x, min_val, max_val):
         pass
-    
-    @classmethod    
+
+    @classmethod
     def signed_dtype(cls, bit_width, is_signed):
         if bit_width is None:
             return None
         if is_signed and bit_width <= 8:
             dtype = cls.int8_dtype()
         elif not is_signed and bit_width <= 8:
             dtype = cls.uint8_dtype()
         elif is_signed and bit_width > 8:
             dtype = cls.int32_dtype()
         else:
-            raise RuntimeError("Unsigned quantization > 8b not supported for export, switch to signed.")
+            raise RuntimeError(
+                "Unsigned quantization > 8b not supported for export, switch to signed.")
         return dtype
 
 
-class QCDQQuantProxyHandlerMixin(
-    QuantAxisMixin, ClipMixin, ZeroPointHandlerMixin, BitWidthHandlerMixin, QCDQMixin, ABC):
-    
+class QCDQQuantProxyHandlerMixin(QuantAxisMixin,
+                                 ClipMixin,
+                                 ZeroPointHandlerMixin,
+                                 BitWidthHandlerMixin,
+                                 QCDQMixin,
+                                 ABC):
+
     def quantize_symbolic_kwargs(cls, scale, zero_point, bit_width, is_signed):
         # compute axis before redefining scale
         # scale can be None for bias quantization
         if scale is not None:
             axis = cls.quant_axis(scale)
             scale = to_0dim_if_scalar(scale.flatten())
         else:
             axis = None
         zp = to_0dim_if_scalar(zero_point.flatten())
         if scale is not None:
             # expand_as must go after 0-dim check
-            zp = zp.expand_as(scale) 
+            zp = zp.expand_as(scale)
         # bit_width can be None for bias quantization
         if bit_width is not None:
             zp = cls.zero_point_with_dtype(is_signed, bit_width, zp)
         # delay itemization of zp whenever scale or bit_width is not there yet
         # which requires a second pass through this function
-        if scale is not None and bit_width is not None and cls.itemize_scalar_params:
+        if scale is not None and bit_width is not None and cls.itemize_quantize_scalar_params:
             scale = to_item_if_0dim(scale)
             zp = to_item_if_0dim(zp)
         dtype = cls.signed_dtype(bit_width, is_signed)
-        return {
-            'scale': scale,
-            'zero_point': zp,
-            'dtype': dtype,
-            'axis': axis}
+        return {'scale': scale, 'zero_point': zp, 'dtype': dtype, 'axis': axis}
 
     def dequantize_symbolic_kwargs(cls, scale, zero_point, bit_width, is_signed):
         # scale can be None for bias quantization
         if scale is not None:
             axis = cls.quant_axis(scale)
             if cls.flatten_dequantize_params:
                 scale = scale.flatten()
-            scale = to_0dim_if_scalar(scale) 
+            scale = to_0dim_if_scalar(scale)
         else:
             axis = None
         if cls.flatten_dequantize_params:
             zero_point = zero_point.flatten()
         zp = to_0dim_if_scalar(zero_point)
-        if scale is not None:  
+        if scale is not None:
             zp = zp.expand_as(scale)
         # bit_width can be None for bias quantization
         if bit_width is not None:
             zp = cls.zero_point_with_dtype(is_signed, bit_width, zp)
-        # delay itemization of zp whenever scale and bit_width are not there yet
-        # which requires a second pass through this function
-        if scale is not None and bit_width is not None and cls.itemize_scalar_params:
-            scale = to_item_if_0dim(scale)
-            zp = to_item_if_0dim(zp)
-        return {
-            'scale': scale,
-            'zero_point': zp,
-            'axis': axis}
+        return {'scale': scale, 'zero_point': zp, 'axis': axis}
 
     def prepare_for_export(self, module):
         if module.is_quant_enabled:
             self.validate(module)
             self.symbolic_kwargs['bit_width'] = module.bit_width()
             self.symbolic_kwargs['quantize_symbolic_kwargs'] = self.quantize_symbolic_kwargs(
                 module.scale(), module.zero_point(), module.bit_width(), module.is_signed)
             self.symbolic_kwargs['dequantize_symbolic_kwargs'] = self.dequantize_symbolic_kwargs(
                 module.scale(), module.zero_point(), module.bit_width(), module.is_signed)
             if self.clip_over_integers:
                 self.symbolic_kwargs['clip_symbolic_kwargs'] = self.int_clip_symbolic_kwargs(
                     module.is_narrow_range, module.is_signed, module.bit_width())
             else:
-                # clip_scale might require a broadcastable shape, while dequant scale might not,
-                # hence we keep them separate 
+                # preserve broadcastable shape if per-channel, scalar item otherwise
                 clip_scale = to_0dim_if_scalar(module.scale())
                 clip_zp = to_0dim_if_scalar(module.zero_point())
-                if self.itemize_scalar_params:
-                    clip_scale = to_item_if_0dim(clip_scale)
-                    clip_zp = to_item_if_0dim(clip_zp)
                 self.symbolic_kwargs['clip_symbolic_kwargs'] = self.float_clip_symbolic_kwargs(
-                    module.is_narrow_range, 
-                    module.is_signed, 
+                    module.is_narrow_range,
+                    module.is_signed,
                     module.bit_width(),
-                    # preserve broadcastable shape if per-channel, scalar item otherwise
-                    clip_scale, 
+                    clip_scale,
                     clip_zp)
         else:
             self.symbolic_kwargs = None
-    
+
     def symbolic_execution(self, x: Tensor):
         assert self.symbolic_kwargs is not None, 'Symbolic execution requires quant to be enabled'
         quantize_symbolic_kwargs = self.symbolic_kwargs['quantize_symbolic_kwargs']
         clip_symbolic_kwargs = self.symbolic_kwargs['clip_symbolic_kwargs']
         dequantize_symbolic_kwargs = self.symbolic_kwargs['dequantize_symbolic_kwargs']
         scale = dequantize_symbolic_kwargs['scale']
         zero_point = dequantize_symbolic_kwargs['zero_point']
@@ -200,15 +193,15 @@
 class QCDQDecoupledWeightQuantProxyHandlerMixin(QCDQWeightQuantProxyHandlerMixin):
     handled_layer = DecoupledWeightQuantProxyFromInjector
 
     def quantize_symbolic_kwargs(cls, module):
         flat_scale = to_0dim_if_scalar(module.pre_scale().flatten())
         zp = to_0dim_if_scalar(module.pre_zero_point().flatten()).expand_as(flat_scale)
         zp = cls.zero_point_with_dtype(module.is_signed, module.bit_width, zp)
-        if cls.itemize_scalar_params:
+        if cls.itemize_quantize_scalar_params:
             flat_scale = to_item_if_0dim(flat_scale)
             zp = to_item_if_0dim(zp)
         return {
             'scale': flat_scale,
             'zero_point': zp,
             'dtype': cls.int8_dtype() if module.is_signed else cls.uint8_dtype(),
             'axis': cls.quant_axis(module.pre_scale())}
@@ -219,31 +212,31 @@
         pre_scale = quantize_symbolic_kwargs['scale']
         pre_zero_point = quantize_symbolic_kwargs['zero_point']
         return out, pre_scale, pre_zero_point, scale, zero_point, bit_width
 
 
 class QCDQActQuantProxyHandlerMixin(QCDQQuantProxyHandlerMixin):
     handled_layer = ActQuantProxyFromInjector
-    
-    
-class QCDQBiasQuantProxyHandlerMixin(
-    DQMixin, QuantAxisMixin, ZeroPointHandlerMixin):
+
+
+class QCDQBiasQuantProxyHandlerMixin(DQMixin, QuantAxisMixin, ZeroPointHandlerMixin):
     handled_layer = BiasQuantProxyFromInjector
-    
+
     def validate(self, module):
-        assert module.bit_width() > 1., 'Binary quant not supported'
+        if module.bit_width() is not None:
+            assert module.bit_width() > 1., 'Binary quant not supported'
         assert module.is_signed, 'Unsigned bias not supported.'
         assert module.rounding_mode == 'ROUND', 'Only round to nearest even supported.'
-    
+
     def prepare_for_export(self, module):
         if module.is_quant_enabled:
             self.validate(module)
             int_biases = {
-                tm.bias.data_ptr():
-                    tm.quant_bias().int(float_datatype=False) for tm in module.tracked_module_list}
+                tm.bias.data_ptr(): tm.quant_bias().int(float_datatype=False)
+                for tm in module.tracked_module_list}
             self.symbolic_kwargs = {
                 'int_biases': int_biases,
                 'scale': module.scale(),
                 'zero_point': module.zero_point(),
                 'bit_width': module.bit_width()}
         else:
             self.symbolic_kwargs = None
@@ -262,38 +255,39 @@
             bit_width = input_bit_width
         quant_axis = self.quant_axis(scale)
         if self.flatten_dequantize_params:
             scale = scale.flatten()
             zero_point = zero_point.flatten()
         scale = to_0dim_if_scalar(scale)
         zero_point = to_0dim_if_scalar(zero_point).expand_as(scale)
-        zero_point = self.zero_point_with_dtype(True, bit_width, zero_point)  # assume signed is True
-        if self.itemize_scalar_params:
-            scale = to_item_if_0dim(scale)
-            zero_point = to_item_if_0dim(zero_point)
-        y = self.dequantize_fn(
-            int_bias, scale, zero_point, quant_axis)
+        zero_point = self.zero_point_with_dtype(
+            True, bit_width, zero_point)  # assume signed is True
+        y = self.dequantize_fn(int_bias, scale, zero_point, quant_axis)
         return y, scale, zero_point, bit_width
 
 
-class QCDQTruncQuantProxyHandlerMixin(QCDQQuantProxyHandlerMixin):
+class QCDQTruncQuantProxyHandlerMixin(QCDQQuantProxyHandlerMixin, ClipMixin):
     handled_layer = TruncQuantProxyFromInjector
 
     def prepare_for_export(self, module: TruncQuantProxyFromInjector):
         if module.is_quant_enabled:
             self.validate(module)
-            self.symbolic_kwargs = {
-                'output_bit_width': module.bit_width()}
+            self.symbolic_kwargs = {'output_bit_width': module.bit_width()}
 
     def symbolic_execution(
-            self, x: Tensor, scale: Tensor, zero_point: Tensor, input_bit_width: Tensor, signed: Tensor):
+            self, x: Tensor, scale: Tensor, zero_point: Tensor, input_bit_width: Tensor,
+            signed: Tensor):
         assert self.symbolic_kwargs is not None, 'Symbolic execution requires quant to be enabled'
         output_bit_width = self.symbolic_kwargs['output_bit_width']
         dtype = self.int8_dtype() if signed else self.uint8_dtype()
+        trunc_scale = 2.0 ** (input_bit_width - output_bit_width)
+        pre_scale = scale * trunc_scale
+        flat_pre_scale = to_0dim_if_scalar(pre_scale.flatten())
         flat_scale = to_0dim_if_scalar(scale.flatten())
         zp = to_0dim_if_scalar(zero_point.flatten()).expand_as(flat_scale)
-        x = self.quantize_fn(x, flat_scale, zp, dtype, self.quant_axis(scale))
-        clip_symbolic_kwargs = self.clip_symbolic_kwargs(signed, False, output_bit_width)
+        x = self.quantize_fn(x, flat_pre_scale, zp, dtype, self.quant_axis(pre_scale))
+        clip_symbolic_kwargs = self.int_clip_symbolic_kwargs(
+            signed=signed, narrow=False, bit_width=output_bit_width)
         if clip_symbolic_kwargs is not None:
             x = self.clip_fn(x, *clip_symbolic_kwargs.values())
         x = self.dequantize_fn(x, flat_scale, zp, self.quant_axis(scale))
-        return x, scale, zero_point, output_bit_width
+        return x, scale, zero_point, output_bit_width
```

### Comparing `brevitas-0.8.0/src/brevitas/export/manager.py` & `brevitas-0.9.0/src/brevitas/export/manager.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,28 +1,31 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Tuple, Union, Optional
-from abc import ABC, abstractmethod
+from abc import ABC
+from abc import abstractmethod
 from contextlib import ExitStack
-from io import BytesIO
 from functools import partial
+from io import BytesIO
+from typing import Optional, Tuple, Union
 
 import torch
-from torch import Tensor, nn
+from torch import nn
+from torch import Tensor
 from torch.nn import Module
 
 from brevitas import config
+from brevitas.nn.mixin.base import _CachedIO
+from brevitas.nn.mixin.base import QuantLayerMixin
+from brevitas.nn.mixin.base import QuantRecurrentLayerMixin
+from brevitas.proxy.quant_proxy import QuantProxyProtocol
 from brevitas.quant_tensor import QuantTensor
+from brevitas.utils.jit_utils import clear_class_registry
 from brevitas.utils.jit_utils import jit_patches_generator
-from brevitas.nn.mixin.base import _CachedIO, QuantLayerMixin, QuantRecurrentLayerMixin
-from brevitas.proxy.quant_proxy import QuantProxyProtocol
 from brevitas.utils.python_utils import patch
-from brevitas.utils.jit_utils import clear_class_registry
 
 
 class _JitTraceExportWrapper(nn.Module):
 
     def __init__(self, model_to_trace):
         super(_JitTraceExportWrapper, self).__init__()
         self.fn_to_trace = lambda *args, **kwargs: model_to_trace(*args, **kwargs)
@@ -115,17 +118,16 @@
 def _set_proxy_export_mode(model: Module, enabled: bool):
     for m in model.modules():
         if isinstance(m, QuantProxyProtocol) and hasattr(m, 'export_mode'):
             m.export_mode = enabled
 
 
 def _set_export_handler(manager_cls, module: Module, instance_type, no_inheritance):
-    if (isinstance(module, instance_type)
-            and hasattr(module, 'export_handler')
-            and module.export_handler is None):
+    if (isinstance(module, instance_type) and hasattr(module, 'export_handler') and
+            module.export_handler is None):
         handler = manager_cls.handler_from_module(module, no_inheritance)
         if handler is None and module.requires_export_handler:
             raise RuntimeError(f"Module {module.__class__} not supported for export.")
         elif handler is None and not module.requires_export_handler:
             pass
         else:
             module.export_handler = handler()
@@ -299,8 +301,8 @@
             if export_path is not None:
                 traced_model.save(export_path)
             # helps fight memory leaks caused by torch.jit.trace
             clear_class_registry()
             _restore_requires_grad(module, requires_grad_backup_dict)
             cls.set_export_mode(module, enabled=False)
             module.train(training_state)
-            return traced_model
+            return traced_model
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/debug.py` & `brevitas-0.9.0/src/brevitas/export/onnx/debug.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,25 +1,23 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import torch
 from torch.autograd import Function
 
 from brevitas.nn.mixin.base import QuantLayerMixin
-from brevitas.quant_tensor import QuantTensor
 from brevitas.proxy.quant_proxy import QuantProxyProtocol
+from brevitas.quant_tensor import QuantTensor
 
 
 class DebugMarkerFunction(Function):
 
     @staticmethod
     def symbolic(g, input, export_debug_name):
-        ret = g.op(
-            'brevitas.onnx::DebugMarker', input, export_debug_name_s=export_debug_name)
+        ret = g.op('brevitas.onnx::DebugMarker', input, export_debug_name_s=export_debug_name)
         return ret
 
     @staticmethod
     def forward(ctx, input, export_debug_name):
         return input
 
 
@@ -42,19 +40,15 @@
         if self.output_enabled:
             self.values[module.export_debug_name + ".output"] = self.unpack(module_out)
 
     def clear(self):
         self.values = {}
 
 
-def enable_debug(
-        model,
-        input_enabled=True,
-        output_enabled=True,
-        proxy_level=False):
+def enable_debug(model, input_enabled=True, output_enabled=True, proxy_level=False):
     base_filter_class = QuantProxyProtocol if proxy_level else QuantLayerMixin
     filter_fn = lambda x: isinstance(x, base_filter_class)
     hook = ONNXDebugHook(input_enabled, output_enabled)
     for name, module in model.named_modules():
         if hasattr(module, "export_debug_name") and filter_fn(module):
             module.export_debug_name = name
             module.export_input_debug = input_enabled
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/finn/function/acc.py` & `brevitas-0.9.0/src/brevitas/export/onnx/finn/function/acc.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import torch
 from torch.autograd import Function
 
 from . import DOMAIN_STRING
 
 
-class QuantAvgPool2dFn(Function):
+class TruncAvgPool2dFn(Function):
 
     @staticmethod
     def symbolic(g, x, out_shape, kernel, stride, signed, ibits, obits, scale, qnt_type):
         if scale is not None:
             x = g.op('{DOMAIN_STRING}::Div', x, scale, activation_qnt_s=qnt_type)
         ret = g.op(
-            f'{DOMAIN_STRING}::QuantAvgPool2d', x,
+            f'{DOMAIN_STRING}::QuantAvgPool2d',
+            x,
             kernel_i=kernel,
             stride_i=stride,
             signed_i=signed,
             ibits_i=ibits,
             obits_i=obits)
         if scale is not None:
             ret = g.op('Mul', ret, scale)
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/finn/function/act.py` & `brevitas-0.9.0/src/brevitas/export/onnx/finn/function/act.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,30 +1,29 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from torch.autograd import Function
 
 from . import DOMAIN_STRING
 
 
 class QuantHardTanhFn(Function):
 
     @staticmethod
     def symbolic(g, input, qnt_type, thres, bias, scale):
         if qnt_type == "BIPOLAR":
             return g.op(
-                f'{DOMAIN_STRING}::MultiThreshold', input, thres,
+                f'{DOMAIN_STRING}::MultiThreshold',
+                input,
+                thres,
                 out_dtype_s=qnt_type,
                 out_scale_f=2.0,
                 out_bias_f=-1.0)
         else:
-            ret = g.op(
-                f'{DOMAIN_STRING}::MultiThreshold', input, thres,
-                out_dtype_s=qnt_type)
+            ret = g.op(f'{DOMAIN_STRING}::MultiThreshold', input, thres, out_dtype_s=qnt_type)
             if bias is not None:
                 ret = g.op('Add', ret, bias)
             if scale is not None:
                 ret = g.op('Mul', ret, scale)
             return ret
 
     @staticmethod
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/finn/function/parameter.py` & `brevitas-0.9.0/src/brevitas/export/onnx/finn/function/parameter.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import torch
 from torch.autograd import Function
 
 from . import DOMAIN_STRING
 
 
 class QuantizedLinearFn(Function):
@@ -30,18 +29,32 @@
         return torch.empty(out_shape, dtype=torch.float, device=x.device)
 
 
 class QuantizedConvNdFn(Function):
 
     @staticmethod
     def symbolic(
-            g, x, W, w_qnt_scale, b_qnt_scale, w_qnt_type, b_qnt_type, out_shape, pads, strides,
-            bias, kernel_shape, groups, dilations):
+            g,
+            x,
+            W,
+            w_qnt_scale,
+            b_qnt_scale,
+            w_qnt_type,
+            b_qnt_type,
+            out_shape,
+            pads,
+            strides,
+            bias,
+            kernel_shape,
+            groups,
+            dilations):
         ret = g.op(
-            f'{DOMAIN_STRING}::Conv', x, W,
+            f'{DOMAIN_STRING}::Conv',
+            x,
+            W,
             weight_qnt_s=w_qnt_type,
             kernel_shape_i=kernel_shape,
             pads_i=pads,
             strides_i=strides,
             group_i=groups,
             dilations_i=dilations)
         if w_qnt_scale is not None:
@@ -54,10 +67,22 @@
                 ret = g.op('Mul', ret, b_qnt_scale)
             else:
                 ret = g.op('Add', ret, bias)
         return ret
 
     @staticmethod
     def forward(
-            ctx, x, W, w_qnt_scale, b_qnt_scale, w_qnt_type, b_qnt_type, out_shape, pads, strides,
-            bias, kernel_shape, groups, dilations):
-        return torch.empty(out_shape, dtype=torch.float, device=x.device)
+            ctx,
+            x,
+            W,
+            w_qnt_scale,
+            b_qnt_scale,
+            w_qnt_type,
+            b_qnt_type,
+            out_shape,
+            pads,
+            strides,
+            bias,
+            kernel_shape,
+            groups,
+            dilations):
+        return torch.empty(out_shape, dtype=torch.float, device=x.device)
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/finn/handler/acc.py` & `brevitas-0.9.0/src/brevitas/export/onnx/finn/handler/acc.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,56 +1,56 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from torch import Tensor
 
-from brevitas.nn import QuantAvgPool2d
+from brevitas.nn import TruncAvgPool2d
+
+from ..function.acc import TruncAvgPool2dFn
 from .base import FINNQuantIOHandler
-from ..function.acc import QuantAvgPool2dFn
 
 
-class FINNQuantAvgPool2dHandler(FINNQuantIOHandler):
-    handled_layer = QuantAvgPool2d
+class FINNTruncAvgPool2dHandler(FINNQuantIOHandler):
+    handled_layer = TruncAvgPool2d
 
     @staticmethod
-    def quant_output_shape(module: QuantAvgPool2d):
+    def quant_output_shape(module: TruncAvgPool2d):
         shape = FINNQuantIOHandler.quant_output_shape(module)
         if shape is None:
-            raise RuntimeError("Caching of output shapes is required to export QuantAvgPool2d")
+            raise RuntimeError("Caching of output shapes is required to export TruncAvgPool2d")
         return shape
 
     @staticmethod
-    def quant_input_bit_width(module: QuantAvgPool2d):
+    def quant_input_bit_width(module: TruncAvgPool2d):
         bit_width = FINNQuantIOHandler.quant_input_bit_width_tensor(module)
         if bit_width is None:
-            raise RuntimeError("Caching of input bit width is required to export QuantAvgPool2d")
+            raise RuntimeError("Caching of input bit width is required to export TruncAvgPool2d")
         return int(bit_width.item())
 
     @staticmethod
-    def quant_output_bit_width(module: QuantAvgPool2d):
+    def quant_output_bit_width(module: TruncAvgPool2d):
         bit_width = FINNQuantIOHandler.quant_output_bit_width_tensor(module)
         if bit_width is None:
-            raise RuntimeError("Caching of output bit width is required to export QuantAvgPool2d")
+            raise RuntimeError("Caching of output bit width is required to export TruncAvgPool2d")
         return int(bit_width.item())
 
     @staticmethod
-    def quant_input_signed(module: QuantAvgPool2d) -> int:
+    def quant_input_signed(module: TruncAvgPool2d) -> int:
         signed = FINNQuantIOHandler.quant_input_signed(module)
         if signed is None:
-            raise RuntimeError("Output sign of QuantAvgPool2d is malformed")
+            raise RuntimeError("Output sign of TruncAvgPool2d is malformed")
         return int(signed)
 
     def prepare_for_export(self, module):
         self.symbolic_kwargs = {
             'out_shape': self.quant_output_shape(module),
             'kernel': module.kernel_size,
             'stride': module.stride,
             'signed': self.quant_input_signed(module),
             'ibits': self.quant_input_bit_width(module),
             'obits': self.quant_output_bit_width(module),
             'scale': self.quant_input_scale(module),
             'qnt_type': self.quant_input_type(module)}
 
     def symbolic_execution(self, inp: Tensor):
-        ret = QuantAvgPool2dFn.apply(inp, *self.symbolic_kwargs.values())
-        return ret
+        ret = TruncAvgPool2dFn.apply(inp, *self.symbolic_kwargs.values())
+        return ret
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/finn/handler/act.py` & `brevitas-0.9.0/src/brevitas/export/onnx/finn/handler/act.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,23 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from typing import Tuple
 
 import torch
 from torch import Tensor
 
-from brevitas.nn import QuantReLU, QuantHardTanh, QuantIdentity
-from .base import FINNQuantInputHandler
-from ..function.act import QuantReLUFn, QuantHardTanhFn
+from brevitas.nn import QuantHardTanh
+from brevitas.nn import QuantIdentity
+from brevitas.nn import QuantReLU
+
+from ..function.act import QuantHardTanhFn
+from ..function.act import QuantReLUFn
 from ..utils import finn_datatype
+from .base import FINNQuantInputHandler
 
 
 class FINNQuantReLUHandler(FINNQuantInputHandler):
     handled_layer = QuantReLU
 
     @staticmethod
     def quant_type(module: QuantReLU):
@@ -68,17 +71,17 @@
     @staticmethod
     def quant_act_bias(module: QuantHardTanh):
         bit_width = int(module.quant_act_bit_width().item())
         if bit_width == 1:
             return torch.tensor(-0.5).type(torch.FloatTensor)
         else:
             if module.is_quant_act_narrow_range:
-                min_non_scaled_val = - (2 ** (bit_width - 1) - 1)
+                min_non_scaled_val = -(2 ** (bit_width - 1) - 1)
             else:
-                min_non_scaled_val = - 2 ** (bit_width - 1)
+                min_non_scaled_val = -2 ** (bit_width - 1)
             return torch.tensor(min_non_scaled_val).type(torch.FloatTensor)
 
     @staticmethod
     def thresholds(module: QuantHardTanh, extend_tensor_to_channels=True):
         bit_width = int(module.quant_act_bit_width().item())
         if bit_width != 1:
             if module.is_quant_act_narrow_range:
@@ -91,15 +94,15 @@
             flat_scale = module.quant_act_scale().view(-1)
             num_scale_channels = flat_scale.shape[0]
             step = torch.abs(flat_scale)
             half_step = step / 2.0
             thresholds = torch.empty(num_scale_channels, num_thresholds)
             # compute the value of the smallest threshold, we'll neg-bias all
             # generated thresholds by this much
-            min_threshold = - half_step - step * ((num_thresholds // 2) - 1)
+            min_threshold = -half_step - step * ((num_thresholds // 2) - 1)
             if not module.is_quant_act_narrow_range:
                 min_threshold -= step
             for c in range(num_scale_channels):
                 for t in range(num_thresholds):
                     thresholds[c][t] = min_threshold[c] + step[c] * t
             if extend_tensor_to_channels:
                 output_channels = module._cached_inp.shape[1]
@@ -133,8 +136,7 @@
     def symbolic_execution(self, inp: Tensor):
         ret = QuantHardTanhFn.apply(inp, *self.symbolic_kwargs.values())
         return ret
 
 
 class FINNQuantIdentityHandler(FINNQuantHardTanhHandler):
     handled_layer = QuantIdentity
-
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/finn/handler/base.py` & `brevitas-0.9.0/src/brevitas/export/onnx/finn/handler/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,23 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from __future__ import annotations
+
 from abc import ABC
-from typing import Tuple, Union, Optional, TYPE_CHECKING
+from typing import Optional, Tuple, TYPE_CHECKING, Union
 
 import torch
 
 if TYPE_CHECKING:
     from brevitas.nn.mixin.base import QuantLayerMixin
     from brevitas.nn.mixin.act import QuantOutputMixin
+
 from brevitas.export.onnx.handler import ONNXBaseHandler
+
 from ..utils import finn_datatype
 
 
 class FINNQuantInputHandler(ONNXBaseHandler, ABC):
 
     @staticmethod
     def quant_input_scale(module: QuantLayerMixin):
@@ -83,8 +85,8 @@
         return bit_width
 
     @staticmethod
     def quant_output_shape(module: Union[QuantLayerMixin, QuantOutputMixin]):
         cached_out = module._cached_out  # TODO add shape property to the module
         if cached_out is not None:
             return cached_out.shape
-        return None
+        return None
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/finn/handler/parameter.py` & `brevitas-0.9.0/src/brevitas/export/onnx/finn/handler/parameter.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,24 +1,27 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from abc import ABC
 from typing import Optional, Union
 
 import torch
 from torch import Tensor
 
-from brevitas.nn import QuantLinear, QuantConv2d, QuantConv1d
+from brevitas.export.onnx.handler import Kernel1dApplHandlerMixin
+from brevitas.export.onnx.handler import Kernel2dApplHandlerMixin
+from brevitas.nn import QuantConv1d
+from brevitas.nn import QuantConv2d
+from brevitas.nn import QuantLinear
 from brevitas.nn.quant_layer import QuantWeightBiasInputOutputLayer as QuantWBIOL
-from brevitas.export.onnx.handler import Kernel2dApplHandlerMixin, Kernel1dApplHandlerMixin
-from .base import FINNQuantIOHandler
-from ..function.parameter import QuantizedLinearFn
+
 from ..function.parameter import QuantizedConvNdFn
+from ..function.parameter import QuantizedLinearFn
 from ..utils import finn_datatype
+from .base import FINNQuantIOHandler
 
 QuantConvNd = Union[QuantConv1d, QuantConv2d]
 
 
 class FINNQuantWBIOLHandler(FINNQuantIOHandler, ABC):
 
     @staticmethod
@@ -123,17 +126,16 @@
             bias = None
         return bias
 
     def prepare_for_export(self, module: QuantConvNd):
         self.validate(module)
         maybe_int_bias = self.maybe_int_bias(module)
         maybe_quant_bias_scale = self.maybe_quant_bias_scale(module)
-        if (maybe_quant_bias_scale is not None
-                and len(maybe_quant_bias_scale.shape) > 0
-                and len(maybe_quant_bias_scale.view(-1)) > 1):
+        if (maybe_quant_bias_scale is not None and len(maybe_quant_bias_scale.shape) > 0 and
+                len(maybe_quant_bias_scale.view(-1)) > 1):
             maybe_quant_bias_scale = maybe_quant_bias_scale.view_as(maybe_int_bias)
         self.symbolic_kwargs = {
             'W': self.int_weight(module),
             'w_qnt_scale': self.quant_weight_scale(module),
             'b_qnt_scale': maybe_quant_bias_scale,
             'w_qnt_type': self.quant_weight_type(module),
             'b_qnt_type': self.maybe_quant_bias_type(module),
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/finn/manager.py` & `brevitas-0.9.0/src/brevitas/export/onnx/finn/manager.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,33 +1,40 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Tuple, Union, Optional
+from typing import Optional, Tuple, Union
 
 import torch
 from torch import Tensor
-from torch.nn import Module, Sequential
 from torch.autograd import Function
+from torch.nn import Module
+from torch.nn import Sequential
 
+from brevitas.export.manager import _set_layer_export_handler
+from brevitas.export.manager import _set_layer_export_mode
 from brevitas.export.onnx.debug import DebugMarkerFunction
-from brevitas.export.onnx.manager import ONNXBaseManager, onnx
-from brevitas.export.manager import _set_layer_export_handler, _set_layer_export_mode
+from brevitas.export.onnx.manager import onnx
+from brevitas.export.onnx.manager import ONNXBaseManager
 from brevitas.quant_tensor import QuantTensor
 
+from .function.acc import TruncAvgPool2dFn
+from .function.act import QuantHardTanhFn
+from .function.act import QuantReLUFn
+from .function.parameter import QuantizedConvNdFn
+from .function.parameter import QuantizedLinearFn
+from .handler.acc import FINNTruncAvgPool2dHandler
+from .handler.act import FINNQuantHardTanhHandler
+from .handler.act import FINNQuantIdentityHandler
+from .handler.act import FINNQuantReLUHandler
+from .handler.parameter import FINNQuantConv1dHandler
+from .handler.parameter import FINNQuantConv2dHandler
+from .handler.parameter import FINNQuantLinearHandler
 from .transform import move_quant_attributes_into_annotations
 from .transform import restore_domain
-from .handler.parameter import FINNQuantConv2dHandler, FINNQuantLinearHandler
-from .handler.parameter import FINNQuantConv1dHandler
-from .handler.act import FINNQuantHardTanhHandler, FINNQuantReLUHandler, FINNQuantIdentityHandler
-from .handler.acc import FINNQuantAvgPool2dHandler
 from .utils import finn_datatype
-from .function.acc import QuantAvgPool2dFn
-from .function.act import QuantHardTanhFn, QuantReLUFn
-from .function.parameter import QuantizedLinearFn, QuantizedConvNdFn
 
 
 class _InputQuantTensorFunction(Function):
     "Account symbolically for scale and zero-point of an input quant tensor"
 
     @staticmethod
     def symbolic(g, x, scale, zero_point):
@@ -78,33 +85,29 @@
     handlers = [
         FINNQuantLinearHandler,
         FINNQuantConv1dHandler,
         FINNQuantConv2dHandler,
         FINNQuantReLUHandler,
         FINNQuantIdentityHandler,
         FINNQuantHardTanhHandler,
-        FINNQuantAvgPool2dHandler]
+        FINNTruncAvgPool2dHandler]
 
     custom_fns = [
         DebugMarkerFunction,
         QuantizedConvNdFn,
         QuantizedLinearFn,
         QuantReLUFn,
         QuantHardTanhFn,
-        QuantAvgPool2dFn
-    ]
+        TruncAvgPool2dFn]
 
-    model_transforms = [
-        move_quant_attributes_into_annotations,
-        restore_domain]
+    model_transforms = [move_quant_attributes_into_annotations, restore_domain]
 
     onnx_passes = [
         # use initializers instead of Constant nodes for fixed params
-        "extract_constant_to_initializer",
-        # remove unused graph inputs & initializers
+        "extract_constant_to_initializer",  # remove unused graph inputs & initializers
         "eliminate_unused_initializer"]
 
     @classmethod
     def set_export_mode(cls, module: Module, enabled: bool):
         _set_layer_export_mode(module, enabled)
 
     @classmethod
@@ -113,24 +116,24 @@
 
     @classmethod
     def export(
             cls,
             module: Module,
             args: Optional[Union[Tensor, QuantTensor, Tuple]] = None,
             export_path: Optional[str] = None,
-            input_shape: Optional[Tuple[int, ...]] = None, # legacy syntax, alternative to args
-            input_t: Optional[Union[Tensor, QuantTensor]] = None,  # legacy syntax, alternative to args
+            input_shape: Optional[Tuple[int, ...]] = None,  # legacy syntax, alternative to args
+            input_t: Optional[Union[Tensor,
+                                    QuantTensor]] = None,  # legacy syntax, alternative to args
             disable_warnings=True,
             **onnx_export_kwargs):
-        if ((input_t is not None and isinstance(input_t, QuantTensor)
-                or args is not None and isinstance(args, QuantTensor))
-                and bool(input_t) != bool(args)):
-            
-            args = args or input_t # If either one is not None, args will be not None
-            input_t = None # Keep only args as not None
+        if ((input_t is not None and isinstance(input_t, QuantTensor) or
+             args is not None and isinstance(args, QuantTensor)) and bool(input_t) != bool(args)):
+
+            args = args or input_t  # If either one is not None, args will be not None
+            input_t = None  # Keep only args as not None
 
             if args.is_not_none:
                 assert args.is_valid, 'Input QuantTensor is not properly quantized'
             training_state = module.training
             preprocessing_module = _InputPreprocessingModule(args.scale, args.zero_point)
             module = Sequential(preprocessing_module, module)
             module.train(training_state)
@@ -144,8 +147,8 @@
                 if 'input_names' in onnx_export_kwargs and onnx_export_kwargs['input_names']:
                     input_name = onnx_export_kwargs['input_names'][0]
                 else:
                     input_name = '0'
                 set_quant_tensor_datatype(onnx_model, input_name, finn_datatype(bit_width, signed))
                 if export_path is not None:
                     onnx.save(onnx_model, export_path)
-        return onnx_model
+        return onnx_model
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/finn/transform.py` & `brevitas-0.9.0/src/brevitas/export/onnx/finn/transform.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import copy
 
 from torch.nn import Module
 
 from ..manager import onnx
 
 
@@ -16,17 +15,15 @@
     model = copy.deepcopy(model)
     qaname = "finn_datatype"
 
     def add_to_annotations(attribute, tensor_name):
         value = attribute.s
         if value != 'FLOAT32':
             tq = onnx.StringStringEntryProto(key=qaname, value=attribute.s)
-            ta = onnx.TensorAnnotation(
-                tensor_name=tensor_name,
-                quant_parameter_tensor_names=[tq])
+            ta = onnx.TensorAnnotation(tensor_name=tensor_name, quant_parameter_tensor_names=[tq])
             model.graph.quantization_annotation.append(ta)
 
     for n in model.graph.node:
         for a in n.attribute:
             mark_for_removal = False
             if a.name == "weight_qnt":
                 # assume second input is weight, make sure it has an initializer
@@ -53,8 +50,7 @@
     if onnx is None:
         raise ModuleNotFoundError("Installation of ONNX is required.")
     model = copy.deepcopy(model)
     for n in model.graph.node:
         if n.op_type in ['MatMul', 'Conv', 'Add', 'Div']:
             n.domain = ''
     return model
-
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/finn/utils.py` & `brevitas-0.9.0/src/brevitas/export/onnx/finn/utils.py`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -7,8 +7,8 @@
     if bit_width == 1 and signed:
         return "BIPOLAR"
     elif bit_width == 1 and not signed:
         return 'BINARY'
     elif bit_width in range(*supported_int_bit_width_range):
         return f"INT{bit_width}" if signed else f"UINT{bit_width}"
     else:
-        raise RuntimeError(f"Unsupported input bit width {bit_width} for export")
+        raise RuntimeError(f"Unsupported input bit width {bit_width} for export")
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/function.py` & `brevitas-0.9.0/src/brevitas/export/onnx/function.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,66 +1,64 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import torch
 from torch.autograd import Function
 
 
 class LSTMCellFn(Function):
 
     @staticmethod
     def symbolic(
             g,
-            inp, 
+            inp,
             weight_i,
             weight_h,
             bias,
             sequence_lens,
-            hidden_state, 
+            hidden_state,
             cell_state,
             direction,
             hidden_size,
             cifg,
             layout,
             output_dir_axes,
             state_dir_axes):
-            outputs, hidden_state, cell_state = g.op(
-                f'LSTM',
-                inp, 
-                weight_i, 
-                weight_h, 
-                bias,
-                sequence_lens,
-                hidden_state, 
-                cell_state,
-                direction_s=direction,
-                hidden_size_i=hidden_size,
-                input_forget_i=cifg,
-                layout_i=layout,
-                outputs=3)
-            outputs = g.op(f'Squeeze', outputs, output_dir_axes)
-            hidden_state = g.op(f'Squeeze', hidden_state, state_dir_axes)
-            cell_state = g.op(f'Squeeze', cell_state, state_dir_axes)
-            return outputs, hidden_state, cell_state
+        outputs, hidden_state, cell_state = g.op(
+            f'LSTM',
+            inp,
+            weight_i,
+            weight_h,
+            bias,
+            sequence_lens,
+            hidden_state,
+            cell_state,
+            direction_s=direction,
+            hidden_size_i=hidden_size,
+            input_forget_i=cifg,
+            layout_i=layout,
+            outputs=3)
+        outputs = g.op(f'Squeeze', outputs, output_dir_axes)
+        hidden_state = g.op(f'Squeeze', hidden_state, state_dir_axes)
+        cell_state = g.op(f'Squeeze', cell_state, state_dir_axes)
+        return outputs, hidden_state, cell_state
 
     @staticmethod
     def forward(
             ctx,
-            inp, 
+            inp,
             weight_i,
             weight_h,
             bias,
             sequence_lens,
-            hidden_state, 
+            hidden_state,
             cell_state,
             direction,
             hidden_size,
             cifg,
             layout,
             output_dir_axes,
             state_dir_axes):
-        outputs = torch.zeros(
-                inp.size(0), inp.size(1), hidden_state.size(1), device=inp.device)
+        outputs = torch.zeros(inp.size(0), inp.size(1), hidden_state.size(1), device=inp.device)
         hidden_state = hidden_state.squeeze(state_dir_axes.item())
         cell_state = cell_state.squeeze(state_dir_axes.item())
-        return outputs, hidden_state, cell_state
+        return outputs, hidden_state, cell_state
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/handler.py` & `brevitas-0.9.0/src/brevitas/export/onnx/handler.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,29 +1,28 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from abc import ABC, abstractmethod
+from abc import ABC
+from abc import abstractmethod
 
 import torch
 from torch import Tensor
 
-from brevitas.nn.quant_rnn import _QuantLSTMLayer
+from brevitas.export.common.handler.base import BaseHandler
 from brevitas.export.onnx import onnx_export_opset
 from brevitas.export.onnx.debug import DebugMarkerFunction
-from brevitas.export.common.handler.base import BaseHandler
+from brevitas.nn.quant_rnn import _QuantLSTMLayer
 
 from .function import LSTMCellFn
 
 __all__ = [
     'Kernel1dApplHandlerMixin',
     'Kernel2dApplHandlerMixin',
     'ONNXBaseHandler',
-    'QuantLSTMLayerHandler'
-]
+    'QuantLSTMLayerHandler']
 
 
 class Kernel1dApplHandlerMixin(ABC):
 
     @staticmethod
     def padding(module):
         if isinstance(module.padding, int):
@@ -104,130 +103,130 @@
 
     def attach_debug_info(self, m):
         self.export_debug_name = m.export_debug_name
         self.debug_input = m.export_input_debug
         self.debug_output = m.export_output_debug
 
     def forward(self, inp: Tensor, *args, **kwargs):
-        debug_fn = lambda x, name:  DebugMarkerFunction.apply(x, self.export_debug_name + name)
+        debug_fn = lambda x, name: DebugMarkerFunction.apply(x, self.export_debug_name + name)
         if self.export_debug_name is not None and self.debug_input:
             inp = debug_fn(inp, '.input')
         out = self.symbolic_execution(inp, *args, **kwargs)
         if self.export_debug_name is not None and self.debug_output:
             if isinstance(out, Tensor):
                 out = debug_fn(out, '.output')
             elif isinstance(out, tuple) and isinstance(out[0], Tensor):
                 out = list(out)
                 out[0] = debug_fn(out[0], '.output')
                 out = tuple(out)
         return out
-    
-    
+
+
 class QuantLSTMLayerHandler(ONNXBaseHandler, ABC):
     handled_layer = _QuantLSTMLayer
 
-    def prepare_for_export(self, module: _QuantLSTMLayer):            
+    def prepare_for_export(self, module: _QuantLSTMLayer):
         self.symbolic_kwargs = {
-            'batch_first': module.cell.batch_first, 
+            'batch_first': module.cell.batch_first,
             'reverse_input': module.cell.reverse_input,
-            'cifg': module.cifg}            
+            'cifg': module.cifg}
         quantizers = [
-            'output', 
-            'cell_state', 
+            'output',
+            'cell_state',
             'input_acc',
-            'forget_acc', 
-            'cell_acc', 
-            'output_acc', 
+            'forget_acc',
+            'cell_acc',
+            'output_acc',
             'input_sigmoid',
             'forget_sigmoid',
             'cell_tanh',
             'output_sigmoid',
             'hidden_state_tanh']
         quantizers = {name: getattr(module.cell, name + '_quant') for name in quantizers}
-        
+
         if all([q.is_quant_enabled for q in quantizers.values()]):
             self.quantized_cell = True
         elif all([not q.is_quant_enabled for q in quantizers.values()]):
             self.quantized_cell = False
         else:
             raise RuntimeError("Export of a partially quantized LSTM cell not supported.")
 
         if self.quantized_cell:
             for (name, quant) in quantizers.items():
-                # ONNX export doesn't handle optional values well, so in case of cifg 
+                # ONNX export doesn't handle optional values well, so in case of cifg
                 # we duplicate the forget ones to the input ones
                 if module.cifg and name == 'input_acc':
                     quant = quantizers['forget_acc']
                 elif module.cifg and name == 'input_sigmoid':
-                    quant = quantizers['forget_sigmoid'] 
+                    quant = quantizers['forget_sigmoid']
                 self.symbolic_kwargs[name + '_scale'] = quant.scale()
                 self.symbolic_kwargs[name + '_zero_point'] = quant.zero_point()
                 self.symbolic_kwargs[name + '_bit_width'] = quant.bit_width()
                 self.symbolic_kwargs[name + '_narrow_range'] = quant.is_narrow_range
                 self.symbolic_kwargs[name + '_signed'] = quant.is_signed
                 self.symbolic_kwargs[name + '_rounding_mode'] = quant.rounding_mode
-    
+
     @abstractmethod
     def quantized_cell_symbolic_execution(
-        self,
-        quant_input, 
-        quant_hidden_state, 
-        quant_cell_state, 
-        quant_weight_ii,
-        quant_weight_if, 
-        quant_weight_ic, 
-        quant_weight_io, 
-        quant_weight_hi,
-        quant_weight_hf, 
-        quant_weight_hc, 
-        quant_weight_ho, 
-        quant_bias_input,
-        quant_bias_forget,
-        quant_bias_cell,
-        quant_bias_output):
+            self,
+            quant_input,
+            quant_hidden_state,
+            quant_cell_state,
+            quant_weight_ii,
+            quant_weight_if,
+            quant_weight_ic,
+            quant_weight_io,
+            quant_weight_hi,
+            quant_weight_hf,
+            quant_weight_hc,
+            quant_weight_ho,
+            quant_bias_input,
+            quant_bias_forget,
+            quant_bias_cell,
+            quant_bias_output):
         pass
-    
+
     def symbolic_execution(
-            self,             
-            quant_input, 
-            quant_hidden_state, 
-            quant_cell_state, 
+            self,
+            quant_input,
+            quant_hidden_state,
+            quant_cell_state,
             quant_weight_ii,
-            quant_weight_if, 
-            quant_weight_ic, 
-            quant_weight_io, 
+            quant_weight_if,
+            quant_weight_ic,
+            quant_weight_io,
             quant_weight_hi,
-            quant_weight_hf, 
-            quant_weight_hc, 
-            quant_weight_ho, 
+            quant_weight_hf,
+            quant_weight_hc,
+            quant_weight_ho,
             quant_bias_input,
             quant_bias_forget,
             quant_bias_cell,
             quant_bias_output):
         if self.quantized_cell:
             return self.quantized_cell_symbolic_execution(
-                quant_input, 
-                quant_hidden_state, 
-                quant_cell_state, 
+                quant_input,
+                quant_hidden_state,
+                quant_cell_state,
                 quant_weight_ii,
-                quant_weight_if, 
-                quant_weight_ic, 
-                quant_weight_io, 
+                quant_weight_if,
+                quant_weight_ic,
+                quant_weight_io,
                 quant_weight_hi,
-                quant_weight_hf, 
-                quant_weight_hc, 
-                quant_weight_ho, 
+                quant_weight_hf,
+                quant_weight_hc,
+                quant_weight_ho,
                 quant_bias_input,
                 quant_bias_forget,
                 quant_bias_cell,
                 quant_bias_output)
         else:
             if onnx_export_opset() < 14:
                 raise RuntimeError("Export of float LSTM cell requires at least opset_version=14.")
-            # The ONNX standard requires parameters to have shape 
+            # The ONNX standard requires parameters to have shape
             # weight_i: [num_directions, 4*hidden_size, input_size]
             # weight_h: [num_directions, 4*hidden_size, hidden_size]
             # bias: [num_directions, 8*hidden_size]
             quant_weight_i = torch.cat(
                 [quant_weight_ii, quant_weight_io, quant_weight_if, quant_weight_ic], dim=0)
             quant_weight_h = torch.cat(
                 [quant_weight_hi, quant_weight_ho, quant_weight_hf, quant_weight_hc], dim=0)
@@ -238,41 +237,42 @@
             # Add a leading dimension for the direction
             quant_weight_i = quant_weight_i.unsqueeze(0)
             quant_weight_h = quant_weight_h.unsqueeze(0)
             quant_bias = quant_bias.unsqueeze(0)
             # Compute relevant dimensions
             seq_len = quant_input.size(int(self.symbolic_kwargs['batch_first']))
             hidden_size = int(quant_hidden_state.size(1))
-            # sSquence_lens is a tensor of dimension batch size with the sequence length 
+            # sSquence_lens is a tensor of dimension batch size with the sequence length
             # of each element in the batch. We don't support variable sequence length yet
-            # so they are set all to the same value 
-            sequence_lens = torch.empty(quant_hidden_state.size(0), dtype=torch.int32).fill_(seq_len)
+            # so they are set all to the same value
+            sequence_lens = torch.empty(
+                quant_hidden_state.size(0), dtype=torch.int32).fill_(seq_len)
             # Initial hidden and cell state have an extra direction dimension and
             #  different shapes depending on whether batch_first is set or not
             quant_hidden_state = quant_hidden_state.unsqueeze(0)
             quant_cell_state = quant_cell_state.unsqueeze(0)
             if self.symbolic_kwargs['batch_first']:
                 quant_hidden_state = quant_hidden_state.permute(1, 0, 2)
                 quant_cell_state = quant_cell_state.permute(1, 0, 2)
             # The ONNX standards requires an extra dimension for the direction,
             # which we don't want here, so we squeeze it. We compute the Squeeze op
             # manually within the function since tracing tend to overcomplicate things
             if self.symbolic_kwargs['batch_first']:
-                output_dir_axes = torch.tensor([1], dtype=torch.int64) 
-                state_dir_axes = torch.tensor([1], dtype=torch.int64) 
+                output_dir_axes = torch.tensor([1], dtype=torch.int64)
+                state_dir_axes = torch.tensor([1], dtype=torch.int64)
             else:
-                output_dir_axes = torch.tensor([2], dtype=torch.int64) 
-                state_dir_axes = torch.tensor([0], dtype=torch.int64) 
+                output_dir_axes = torch.tensor([2], dtype=torch.int64)
+                state_dir_axes = torch.tensor([0], dtype=torch.int64)
             return LSTMCellFn.apply(
-                quant_input, 
-                quant_weight_i, 
-                quant_weight_h, 
-                quant_bias, 
-                sequence_lens, 
-                quant_hidden_state, 
-                quant_cell_state, 
+                quant_input,
+                quant_weight_i,
+                quant_weight_h,
+                quant_bias,
+                sequence_lens,
+                quant_hidden_state,
+                quant_cell_state,
                 'reverse' if self.symbolic_kwargs['reverse_input'] else 'forward',
                 hidden_size,
                 self.symbolic_kwargs['cifg'],
                 self.symbolic_kwargs['batch_first'],
                 output_dir_axes,
-                state_dir_axes)
+                state_dir_axes)
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/manager.py` & `brevitas-0.9.0/src/brevitas/export/onnx/manager.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,34 +1,37 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Tuple, Union, Optional
 from abc import ABC
-from packaging import version
 from contextlib import ExitStack
 from io import BytesIO
+from typing import Optional, Tuple, Union
 import warnings
 
+from packaging import version
+
 try:
     import onnx
     import onnxoptimizer as opt
 except ModuleNotFoundError:
     onnx = None
     opt = None
 
 import torch
-import torch.onnx
 from torch import Tensor
 from torch.nn import Module
+import torch.onnx
 
 from brevitas import torch_version
 from brevitas.quant_tensor import QuantTensor
-from ..manager import BaseManager, ExportContext
-from ..manager import _override_inp_caching_mode, _restore_inp_caching_mode
+
+from ..manager import _override_inp_caching_mode
+from ..manager import _restore_inp_caching_mode
+from ..manager import BaseManager
+from ..manager import ExportContext
 
 
 class ONNXBaseManager(BaseManager, ABC):
 
     model_transforms = []
     onnx_passes = []
     custom_fns = []
@@ -47,17 +50,16 @@
         ka = 'keep_initializers_as_inputs'
         if torch_version >= version.parse('1.3.0') and ka not in export_kwargs:
             export_kwargs[ka] = True
 
     @classmethod
     def solve_enable_onnx_checker(cls, export_kwargs):
         ka = 'enable_onnx_checker'
-        if (torch_version >= version.parse('1.5.0') 
-            and torch_version <= version.parse('1.10.0') 
-            and ka not in export_kwargs):
+        if (torch_version >= version.parse('1.5.0') and torch_version <= version.parse('1.10.0') and
+                ka not in export_kwargs):
             export_kwargs[ka] = False
 
     @classmethod
     def register_custom_fns(cls):
         for fn in cls.custom_fns:
             torch.onnx.register_custom_op_symbolic(
                 f'{cls.target_name}::{fn.__name__}', fn.symbolic, cls.custom_opset)
@@ -148,12 +150,14 @@
 
     @classmethod
     def export(
             cls,
             module: Module,
             args: Optional[Union[Tensor, QuantTensor, Tuple]] = None,
             export_path: Optional[str] = None,
-            input_shape: Optional[Tuple[int, ...]] = None, # legacy syntax, alternative to args
-            input_t: Optional[Union[Tensor, QuantTensor]] = None,  # legacy syntax, alternative to args
+            input_shape: Optional[Tuple[int, ...]] = None,  # legacy syntax, alternative to args
+            input_t: Optional[Union[Tensor,
+                                    QuantTensor]] = None,  # legacy syntax, alternative to args
             disable_warnings=True,
             **onnx_export_kwargs):
-        return cls.export_onnx(module, args, export_path, input_shape, input_t, disable_warnings, **onnx_export_kwargs)
+        return cls.export_onnx(
+            module, args, export_path, input_shape, input_t, disable_warnings, **onnx_export_kwargs)
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/qonnx/function.py` & `brevitas-0.9.0/src/brevitas/export/onnx/qonnx/function.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,67 +1,74 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import torch
 from torch.autograd import Function
 
-from brevitas.function import binary_sign
 from brevitas.core.bit_width import BitWidthConst
-from brevitas.core.quant import IntQuant, TruncIntQuant
-from brevitas.quant.solver.common import solve_float_to_int_impl_from_enum
 from brevitas.core.function_wrapper.clamp import TensorClamp
-
+from brevitas.core.quant import IntQuant
+from brevitas.core.quant import TruncIntQuant
+from brevitas.function import binary_sign
+from brevitas.quant.solver.common import solve_float_to_int_impl_from_enum
 
 DOMAIN_STRING = "onnx.brevitas"
 
 
 class BrevitasBinaryQuantFn(Function):
 
     @staticmethod
     def symbolic(g, x, scale, zero_point, bit_width, narrow_range, signed, rounding_mode):
-        ret = g.op(
-            f'{DOMAIN_STRING}::BipolarQuant',
-            x, scale)
+        ret = g.op(f'{DOMAIN_STRING}::BipolarQuant', x, scale)
         return ret
 
     @staticmethod
     def forward(ctx, x, scale, zero_point, bit_width, narrow_range, signed, rounding_mode):
         y = binary_sign(x) * scale
         return y
 
 
 class BrevitasQuantFn(Function):
 
     @staticmethod
     def symbolic(g, x, scale, zero_point, bit_width, narrow_range, signed, rounding_mode):
         ret = g.op(
             f'{DOMAIN_STRING}::Quant',
-            x, scale, zero_point, bit_width,
+            x,
+            scale,
+            zero_point,
+            bit_width,
             rounding_mode_s=rounding_mode,
             signed_i=int(signed),
             narrow_i=int(narrow_range))
         return ret
 
     @staticmethod
     def forward(ctx, x, scale, zero_point, bit_width, narrow_range, signed, rounding_mode):
         float_to_int_impl = solve_float_to_int_impl_from_enum(rounding_mode)
         quant = IntQuant(
-            float_to_int_impl=float_to_int_impl(),tensor_clamp_impl=TensorClamp(), narrow_range=narrow_range, signed=signed)
+            float_to_int_impl=float_to_int_impl(),
+            tensor_clamp_impl=TensorClamp(),
+            narrow_range=narrow_range,
+            signed=signed)
         y = quant(scale, zero_point, bit_width, x)
         return y
 
 
 class BrevitasTruncFn(Function):
 
     @staticmethod
     def symbolic(g, x, scale, zero_point, input_bit_width, output_bit_width, rounding_mode):
         ret = g.op(
             f'{DOMAIN_STRING}::Trunc',
-            x, scale, zero_point, input_bit_width, output_bit_width,
+            x,
+            scale,
+            zero_point,
+            input_bit_width,
+            output_bit_width,
             rounding_mode_s=rounding_mode)
         return ret
 
     @staticmethod
     def forward(ctx, x, scale, zero_point, input_bit_width, output_bit_width, rounding_mode):
         float_to_int_impl = solve_float_to_int_impl_from_enum(rounding_mode)
         trunc = TruncIntQuant(
@@ -71,318 +78,280 @@
         return y_tuple[0]
 
 
 class BrevitasQuantLSTMCellFn(Function):
 
     @staticmethod
     def symbolic(
-            g, 
-            # args and kwargs passed from _QuantLSTMLayer
-            quant_input, 
-            quant_hidden_state, 
+            g,  # args and kwargs passed from _QuantLSTMLayer
+            quant_input,
+            quant_hidden_state,
             quant_cell_state,
-            quant_weight_ii, 
+            quant_weight_ii,
             quant_weight_if,
             quant_weight_ic,
             quant_weight_io,
-            quant_weight_hi, 
+            quant_weight_hi,
             quant_weight_hf,
             quant_weight_hc,
             quant_weight_ho,
             quant_bias_input,
             quant_bias_forget,
             quant_bias_cell,
-            quant_bias_output,
-            # Symbolic kwargs passed from BrevitasQuantLSTMLayerHandler
+            quant_bias_output,  # Symbolic kwargs passed from BrevitasQuantLSTMLayerHandler
             batch_first,
             reverse_input,
-            cifg,
-            # Output quant
-            output_scale, 
-            output_zero_point, 
-            output_bit_width, 
-            output_narrow_range, 
-            output_signed, 
-            output_rounding_mode,
-            # Cell state quant
-            cell_state_scale, 
-            cell_state_zero_point, 
-            cell_state_bit_width, 
-            cell_state_narrow_range, 
-            cell_state_signed, 
-            cell_state_rounding_mode,
-            # Input gate accumulator quant
-            input_acc_scale, 
+            cifg,  # Output quant
+            output_scale,
+            output_zero_point,
+            output_bit_width,
+            output_narrow_range,
+            output_signed,
+            output_rounding_mode,  # Cell state quant
+            cell_state_scale,
+            cell_state_zero_point,
+            cell_state_bit_width,
+            cell_state_narrow_range,
+            cell_state_signed,
+            cell_state_rounding_mode,  # Input gate accumulator quant
+            input_acc_scale,
             input_acc_zero_point,
-            input_acc_bit_width, 
-            input_acc_narrow_range, 
-            input_acc_signed, 
-            input_acc_rounding_mode,
-            # Forget gate accumulator quant
-            forget_acc_scale, 
-            forget_acc_zero_point, 
-            forget_acc_bit_width, 
-            forget_acc_narrow_range, 
-            forget_acc_signed, 
-            forget_acc_rounding_mode,
-            # Cell gate accumulator quant
-            cell_acc_scale, 
-            cell_acc_zero_point, 
-            cell_acc_bit_width, 
-            cell_acc_narrow_range, 
-            cell_acc_signed, 
-            cell_acc_rounding_mode,
-            # Output gate accumulator quant
-            output_acc_scale, 
-            output_acc_zero_point, 
-            output_acc_bit_width, 
-            output_acc_narrow_range, 
-            output_acc_signed, 
-            output_acc_rounding_mode,
-            # Input gate sigmoid quant
-            input_sigmoid_scale, 
-            input_sigmoid_zero_point, 
-            input_sigmoid_bit_width, 
-            input_sigmoid_narrow_range, 
-            input_sigmoid_signed, 
-            input_sigmoid_rounding_mode,
-            # Forget gate sigmoid quant 
-            forget_sigmoid_scale, 
-            forget_sigmoid_zero_point, 
-            forget_sigmoid_bit_width, 
-            forget_sigmoid_narrow_range, 
-            forget_sigmoid_signed, 
-            forget_sigmoid_rounding_mode,
-            # Cell gate tanh quant
-            cell_tanh_scale, 
-            cell_tanh_zero_point, 
-            cell_tanh_bit_width, 
-            cell_tanh_narrow_range, 
-            cell_tanh_signed, 
-            cell_tanh_rounding_mode,
-            # Output gate sigmoid quant
-            output_sigmoid_scale, 
-            output_sigmoid_zero_point, 
-            output_sigmoid_bit_width, 
-            output_sigmoid_narrow_range, 
-            output_sigmoid_signed, 
-            output_sigmoid_rounding_mode,
-            # Hidden state tanh quant
-            hidden_state_tanh_scale, 
-            hidden_state_tanh_zero_point, 
-            hidden_state_tanh_bit_width, 
-            hidden_state_tanh_narrow_range, 
-            hidden_state_tanh_signed, 
+            input_acc_bit_width,
+            input_acc_narrow_range,
+            input_acc_signed,
+            input_acc_rounding_mode,  # Forget gate accumulator quant
+            forget_acc_scale,
+            forget_acc_zero_point,
+            forget_acc_bit_width,
+            forget_acc_narrow_range,
+            forget_acc_signed,
+            forget_acc_rounding_mode,  # Cell gate accumulator quant
+            cell_acc_scale,
+            cell_acc_zero_point,
+            cell_acc_bit_width,
+            cell_acc_narrow_range,
+            cell_acc_signed,
+            cell_acc_rounding_mode,  # Output gate accumulator quant
+            output_acc_scale,
+            output_acc_zero_point,
+            output_acc_bit_width,
+            output_acc_narrow_range,
+            output_acc_signed,
+            output_acc_rounding_mode,  # Input gate sigmoid quant
+            input_sigmoid_scale,
+            input_sigmoid_zero_point,
+            input_sigmoid_bit_width,
+            input_sigmoid_narrow_range,
+            input_sigmoid_signed,
+            input_sigmoid_rounding_mode,  # Forget gate sigmoid quant
+            forget_sigmoid_scale,
+            forget_sigmoid_zero_point,
+            forget_sigmoid_bit_width,
+            forget_sigmoid_narrow_range,
+            forget_sigmoid_signed,
+            forget_sigmoid_rounding_mode,  # Cell gate tanh quant
+            cell_tanh_scale,
+            cell_tanh_zero_point,
+            cell_tanh_bit_width,
+            cell_tanh_narrow_range,
+            cell_tanh_signed,
+            cell_tanh_rounding_mode,  # Output gate sigmoid quant
+            output_sigmoid_scale,
+            output_sigmoid_zero_point,
+            output_sigmoid_bit_width,
+            output_sigmoid_narrow_range,
+            output_sigmoid_signed,
+            output_sigmoid_rounding_mode,  # Hidden state tanh quant
+            hidden_state_tanh_scale,
+            hidden_state_tanh_zero_point,
+            hidden_state_tanh_bit_width,
+            hidden_state_tanh_narrow_range,
+            hidden_state_tanh_signed,
             hidden_state_tanh_rounding_mode):
-            return g.op(
-                f'{DOMAIN_STRING}::QuantLSTMCell',
-                # Tensors
-                ## Input values
-                quant_input, 
-                quant_hidden_state, 
-                quant_cell_state,
-                quant_weight_ii, 
-                quant_weight_if,
-                quant_weight_ic,
-                quant_weight_io,
-                quant_weight_hi, 
-                quant_weight_hf,
-                quant_weight_hc,
-                quant_weight_ho,
-                quant_bias_input,
-                quant_bias_forget,
-                quant_bias_cell,
-                quant_bias_output,
-                ## Output quant
-                output_scale, 
-                output_zero_point, 
-                output_bit_width, 
-                ## Cell state quant
-                cell_state_scale, 
-                cell_state_zero_point, 
-                cell_state_bit_width, 
-                ## Input gate accumulator quant
-                input_acc_scale, 
-                input_acc_zero_point,
-                input_acc_bit_width, 
-                ## Forget gate accumulator quant
-                forget_acc_scale, 
-                forget_acc_zero_point, 
-                forget_acc_bit_width, 
-                ## Cell gate accumulator quant
-                cell_acc_scale, 
-                cell_acc_zero_point, 
-                cell_acc_bit_width, 
-                ## Output gate accumulator quant
-                output_acc_scale, 
-                output_acc_zero_point, 
-                output_acc_bit_width, 
-                ## Input gate sigmoid quant
-                input_sigmoid_scale, 
-                input_sigmoid_zero_point, 
-                input_sigmoid_bit_width, 
-                ## Forget gate sigmoid quant
-                forget_sigmoid_scale, 
-                forget_sigmoid_zero_point, 
-                forget_sigmoid_bit_width, 
-                ## Cell gate tanh quant
-                cell_tanh_scale, 
-                cell_tanh_zero_point, 
-                cell_tanh_bit_width, 
-                ## Output gate sigmoid quant
-                output_sigmoid_scale, 
-                output_sigmoid_zero_point, 
-                output_sigmoid_bit_width, 
-                ## Hidden state tanh quant
-                hidden_state_tanh_scale, 
-                hidden_state_tanh_zero_point, 
-                hidden_state_tanh_bit_width, 
-                # Attributes
-                batch_first_i=batch_first,
-                reverse_input_i=reverse_input,
-                cifg_i=cifg,
-                output_narrow_i=output_narrow_range, 
-                output_signed_i=output_signed, 
-                output_rounding_mode_s=output_rounding_mode,
-                cell_state_narrow_i=cell_state_narrow_range, 
-                cell_state_signed_i=cell_state_signed, 
-                cell_state_rounding_mode_s=cell_state_rounding_mode,
-                input_acc_narrow_i=input_acc_narrow_range, 
-                input_acc_signed_i=input_acc_signed, 
-                input_acc_rounding_mode_s=input_acc_rounding_mode,
-                forget_acc_narrow_i=forget_acc_narrow_range, 
-                forget_acc_signed_i=forget_acc_signed, 
-                forget_acc_rounding_mode_s=forget_acc_rounding_mode,
-                cell_acc_narrow_i=cell_acc_narrow_range, 
-                cell_acc_signed_i=cell_acc_signed, 
-                cell_acc_rounding_mode_s=cell_acc_rounding_mode,
-                output_acc_narrow_i=output_acc_narrow_range, 
-                output_acc_signed_i=output_acc_signed, 
-                output_acc_rounding_mode_s=output_acc_rounding_mode,
-                input_sigmoid_narrow_i=input_sigmoid_narrow_range, 
-                input_sigmoid_signed_i=input_sigmoid_signed, 
-                input_sigmoid_rounding_mode_s=input_sigmoid_rounding_mode,
-                forget_sigmoid_narrow_i=forget_sigmoid_narrow_range, 
-                forget_sigmoid_signed_i=forget_sigmoid_signed, 
-                forget_sigmoid_rounding_mode_s=forget_sigmoid_rounding_mode,
-                cell_tanh_narrow_i=cell_tanh_narrow_range, 
-                cell_tanh_signed_i=cell_tanh_signed, 
-                cell_tanh_rounding_mode_s=cell_tanh_rounding_mode,
-                output_sigmoid_narrow_range_i=output_sigmoid_narrow_range, 
-                output_sigmoid_signed_i=output_sigmoid_signed, 
-                output_sigmoid_rounding_mode_s=output_sigmoid_rounding_mode,
-                hidden_state_tanh_narrow_i=hidden_state_tanh_narrow_range, 
-                hidden_state_tanh_signed_i=hidden_state_tanh_signed, 
-                hidden_state_tanh_rounding_mode_s=hidden_state_tanh_rounding_mode,
-                # PyTorch requires to specify the number of outputs manually
-                outputs=3)
+        return g.op(
+            f'{DOMAIN_STRING}::QuantLSTMCell',  # Tensors
+            ## Input values
+            quant_input,
+            quant_hidden_state,
+            quant_cell_state,
+            quant_weight_ii,
+            quant_weight_if,
+            quant_weight_ic,
+            quant_weight_io,
+            quant_weight_hi,
+            quant_weight_hf,
+            quant_weight_hc,
+            quant_weight_ho,
+            quant_bias_input,
+            quant_bias_forget,
+            quant_bias_cell,
+            quant_bias_output,  ## Output quant
+            output_scale,
+            output_zero_point,
+            output_bit_width,  ## Cell state quant
+            cell_state_scale,
+            cell_state_zero_point,
+            cell_state_bit_width,  ## Input gate accumulator quant
+            input_acc_scale,
+            input_acc_zero_point,
+            input_acc_bit_width,  ## Forget gate accumulator quant
+            forget_acc_scale,
+            forget_acc_zero_point,
+            forget_acc_bit_width,  ## Cell gate accumulator quant
+            cell_acc_scale,
+            cell_acc_zero_point,
+            cell_acc_bit_width,  ## Output gate accumulator quant
+            output_acc_scale,
+            output_acc_zero_point,
+            output_acc_bit_width,  ## Input gate sigmoid quant
+            input_sigmoid_scale,
+            input_sigmoid_zero_point,
+            input_sigmoid_bit_width,  ## Forget gate sigmoid quant
+            forget_sigmoid_scale,
+            forget_sigmoid_zero_point,
+            forget_sigmoid_bit_width,  ## Cell gate tanh quant
+            cell_tanh_scale,
+            cell_tanh_zero_point,
+            cell_tanh_bit_width,  ## Output gate sigmoid quant
+            output_sigmoid_scale,
+            output_sigmoid_zero_point,
+            output_sigmoid_bit_width,  ## Hidden state tanh quant
+            hidden_state_tanh_scale,
+            hidden_state_tanh_zero_point,
+            hidden_state_tanh_bit_width,
+            # Attributes
+            batch_first_i=batch_first,
+            reverse_input_i=reverse_input,
+            cifg_i=cifg,
+            output_narrow_i=output_narrow_range,
+            output_signed_i=output_signed,
+            output_rounding_mode_s=output_rounding_mode,
+            cell_state_narrow_i=cell_state_narrow_range,
+            cell_state_signed_i=cell_state_signed,
+            cell_state_rounding_mode_s=cell_state_rounding_mode,
+            input_acc_narrow_i=input_acc_narrow_range,
+            input_acc_signed_i=input_acc_signed,
+            input_acc_rounding_mode_s=input_acc_rounding_mode,
+            forget_acc_narrow_i=forget_acc_narrow_range,
+            forget_acc_signed_i=forget_acc_signed,
+            forget_acc_rounding_mode_s=forget_acc_rounding_mode,
+            cell_acc_narrow_i=cell_acc_narrow_range,
+            cell_acc_signed_i=cell_acc_signed,
+            cell_acc_rounding_mode_s=cell_acc_rounding_mode,
+            output_acc_narrow_i=output_acc_narrow_range,
+            output_acc_signed_i=output_acc_signed,
+            output_acc_rounding_mode_s=output_acc_rounding_mode,
+            input_sigmoid_narrow_i=input_sigmoid_narrow_range,
+            input_sigmoid_signed_i=input_sigmoid_signed,
+            input_sigmoid_rounding_mode_s=input_sigmoid_rounding_mode,
+            forget_sigmoid_narrow_i=forget_sigmoid_narrow_range,
+            forget_sigmoid_signed_i=forget_sigmoid_signed,
+            forget_sigmoid_rounding_mode_s=forget_sigmoid_rounding_mode,
+            cell_tanh_narrow_i=cell_tanh_narrow_range,
+            cell_tanh_signed_i=cell_tanh_signed,
+            cell_tanh_rounding_mode_s=cell_tanh_rounding_mode,
+            output_sigmoid_narrow_range_i=output_sigmoid_narrow_range,
+            output_sigmoid_signed_i=output_sigmoid_signed,
+            output_sigmoid_rounding_mode_s=output_sigmoid_rounding_mode,
+            hidden_state_tanh_narrow_i=hidden_state_tanh_narrow_range,
+            hidden_state_tanh_signed_i=hidden_state_tanh_signed,
+            hidden_state_tanh_rounding_mode_s=hidden_state_tanh_rounding_mode,
+            # PyTorch requires to specify the number of outputs manually
+            outputs=3)
 
     @staticmethod
     def forward(
-            ctx,
-            # args and kwargs passed from _QuantLSTMLayer
-            quant_input, 
-            quant_hidden_state, 
+            ctx,  # args and kwargs passed from _QuantLSTMLayer
+            quant_input,
+            quant_hidden_state,
             quant_cell_state,
-            quant_weight_ii, 
+            quant_weight_ii,
             quant_weight_if,
             quant_weight_ic,
             quant_weight_io,
-            quant_weight_hi, 
+            quant_weight_hi,
             quant_weight_hf,
             quant_weight_hc,
             quant_weight_ho,
             quant_bias_input,
             quant_bias_forget,
             quant_bias_cell,
-            quant_bias_output,
-            # Symbolic kwargs passed from BrevitasQuantLSTMLayerHandler
+            quant_bias_output,  # Symbolic kwargs passed from BrevitasQuantLSTMLayerHandler
             batch_first,
             reverse_input,
-            cifg,
-            # Output quant
-            output_scale, 
-            output_zero_point, 
-            output_bit_width, 
-            output_narrow_range, 
-            output_signed, 
-            output_rounding_mode,
-            # Cell state quant
-            cell_state_scale, 
-            cell_state_zero_point, 
-            cell_state_bit_width, 
-            cell_state_narrow_range, 
-            cell_state_signed, 
-            cell_state_rounding_mode,
-            # Input gate accumulator quant
-            input_acc_scale, 
+            cifg,  # Output quant
+            output_scale,
+            output_zero_point,
+            output_bit_width,
+            output_narrow_range,
+            output_signed,
+            output_rounding_mode,  # Cell state quant
+            cell_state_scale,
+            cell_state_zero_point,
+            cell_state_bit_width,
+            cell_state_narrow_range,
+            cell_state_signed,
+            cell_state_rounding_mode,  # Input gate accumulator quant
+            input_acc_scale,
             input_acc_zero_point,
-            input_acc_bit_width, 
-            input_acc_narrow_range, 
-            input_acc_signed, 
-            input_acc_rounding_mode,
-            # Forget gate accumulator quant
-            forget_acc_scale, 
-            forget_acc_zero_point, 
-            forget_acc_bit_width, 
-            forget_acc_narrow_range, 
-            forget_acc_signed, 
-            forget_acc_rounding_mode,
-            # Cell gate accumulator quant
-            cell_acc_scale, 
-            cell_acc_zero_point, 
-            cell_acc_bit_width, 
-            cell_acc_narrow_range, 
-            cell_acc_signed, 
-            cell_acc_rounding_mode,
-            # Output gate accumulator quant
-            output_acc_scale, 
-            output_acc_zero_point, 
-            output_acc_bit_width, 
-            output_acc_narrow_range, 
-            output_acc_signed, 
-            output_acc_rounding_mode,
-            # Input gate sigmoid quant
-            input_sigmoid_scale, 
-            input_sigmoid_zero_point, 
-            input_sigmoid_bit_width, 
-            input_sigmoid_narrow_range, 
-            input_sigmoid_signed, 
-            input_sigmoid_rounding_mode,
-            # Forget gate sigmoid quant 
-            forget_sigmoid_scale, 
-            forget_sigmoid_zero_point, 
-            forget_sigmoid_bit_width, 
-            forget_sigmoid_narrow_range, 
-            forget_sigmoid_signed, 
-            forget_sigmoid_rounding_mode,
-            # Cell gate tanh quant
-            cell_tanh_scale, 
-            cell_tanh_zero_point, 
-            cell_tanh_bit_width, 
-            cell_tanh_narrow_range, 
-            cell_tanh_signed, 
-            cell_tanh_rounding_mode,
-            # Output gate sigmoid quant
-            output_sigmoid_scale, 
-            output_sigmoid_zero_point, 
-            output_sigmoid_bit_width, 
-            output_sigmoid_narrow_range, 
-            output_sigmoid_signed, 
-            output_sigmoid_rounding_mode,
-            # Hidden state tanh quant
-            hidden_state_tanh_scale, 
-            hidden_state_tanh_zero_point, 
-            hidden_state_tanh_bit_width, 
-            hidden_state_tanh_narrow_range, 
-            hidden_state_tanh_signed, 
+            input_acc_bit_width,
+            input_acc_narrow_range,
+            input_acc_signed,
+            input_acc_rounding_mode,  # Forget gate accumulator quant
+            forget_acc_scale,
+            forget_acc_zero_point,
+            forget_acc_bit_width,
+            forget_acc_narrow_range,
+            forget_acc_signed,
+            forget_acc_rounding_mode,  # Cell gate accumulator quant
+            cell_acc_scale,
+            cell_acc_zero_point,
+            cell_acc_bit_width,
+            cell_acc_narrow_range,
+            cell_acc_signed,
+            cell_acc_rounding_mode,  # Output gate accumulator quant
+            output_acc_scale,
+            output_acc_zero_point,
+            output_acc_bit_width,
+            output_acc_narrow_range,
+            output_acc_signed,
+            output_acc_rounding_mode,  # Input gate sigmoid quant
+            input_sigmoid_scale,
+            input_sigmoid_zero_point,
+            input_sigmoid_bit_width,
+            input_sigmoid_narrow_range,
+            input_sigmoid_signed,
+            input_sigmoid_rounding_mode,  # Forget gate sigmoid quant
+            forget_sigmoid_scale,
+            forget_sigmoid_zero_point,
+            forget_sigmoid_bit_width,
+            forget_sigmoid_narrow_range,
+            forget_sigmoid_signed,
+            forget_sigmoid_rounding_mode,  # Cell gate tanh quant
+            cell_tanh_scale,
+            cell_tanh_zero_point,
+            cell_tanh_bit_width,
+            cell_tanh_narrow_range,
+            cell_tanh_signed,
+            cell_tanh_rounding_mode,  # Output gate sigmoid quant
+            output_sigmoid_scale,
+            output_sigmoid_zero_point,
+            output_sigmoid_bit_width,
+            output_sigmoid_narrow_range,
+            output_sigmoid_signed,
+            output_sigmoid_rounding_mode,  # Hidden state tanh quant
+            hidden_state_tanh_scale,
+            hidden_state_tanh_zero_point,
+            hidden_state_tanh_bit_width,
+            hidden_state_tanh_narrow_range,
+            hidden_state_tanh_signed,
             hidden_state_tanh_rounding_mode):
-        # Tp simplify things, here we are returning the outputs 
+        # Tp simplify things, here we are returning the outputs
         # as if they were already concatenated. Scale/zp/bw are avoided too.
         # This preserves output shapes but not values.
         # See _QuantLSTMCell for the actual implementation.
         quant_outputs = torch.zeros(
-            quant_input.size(0), 
-            quant_input.size(1), 
-            quant_hidden_state.size(1), 
+            quant_input.size(0),
+            quant_input.size(1),
+            quant_hidden_state.size(1),
             device=quant_hidden_state.device)
         return quant_outputs, quant_hidden_state, quant_cell_state
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/qonnx/handler.py` & `brevitas-0.9.0/src/brevitas/export/onnx/qonnx/handler.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,29 +1,28 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from abc import ABC
 from copy import copy
 
 import torch
 from torch import Tensor
 
 from brevitas.export.onnx.handler import ONNXBaseHandler
-from brevitas.proxy import WeightQuantProxyFromInjector
-from brevitas.proxy import DecoupledWeightQuantProxyFromInjector
-from brevitas.proxy import BiasQuantProxyFromInjector
+from brevitas.export.onnx.handler import QuantLSTMLayerHandler
 from brevitas.proxy import ActQuantProxyFromInjector
+from brevitas.proxy import BiasQuantProxyFromInjector
+from brevitas.proxy import DecoupledWeightQuantProxyFromInjector
+from brevitas.proxy import WeightQuantProxyFromInjector
 from brevitas.proxy.runtime_quant import TruncQuantProxyFromInjector
-from brevitas.export.onnx.handler import QuantLSTMLayerHandler
 
-from .function import BrevitasQuantFn
 from .function import BrevitasBinaryQuantFn
-from .function import BrevitasTruncFn
+from .function import BrevitasQuantFn
 from .function import BrevitasQuantLSTMCellFn
+from .function import BrevitasTruncFn
 
 
 class BrevitasQuantProxyHandler(ONNXBaseHandler, ABC):
 
     def validate(self, module):
         if module.bit_width() == 1:
             assert module.zero_point() == 0, "Zero-point not supported for binary quant."
@@ -104,65 +103,62 @@
         zero_point = symbolic_kwargs.pop('zero_point')
         if scale is None:
             assert input_scale is not None, 'Input scale required for bias export'
             scale = input_scale
         if bit_width is None:
             assert input_bit_width is not None, 'Input bit_width required for bias export'
             bit_width = input_bit_width
-        y = BrevitasQuantFn.apply(
-            x, scale, zero_point, bit_width, *symbolic_kwargs.values())
+        y = BrevitasQuantFn.apply(x, scale, zero_point, bit_width, *symbolic_kwargs.values())
         return y, scale, zero_point, bit_width
 
 
 class BrevitasTruncQuantProxyHandler(ONNXBaseHandler):
     handled_layer = TruncQuantProxyFromInjector
 
     def prepare_for_export(self, module: TruncQuantProxyFromInjector):
         self.symbolic_kwargs = {
-                'output_bit_width': module.bit_width(),
-                'rounding_mode': module.rounding_mode}
+            'output_bit_width': module.bit_width(), 'rounding_mode': module.rounding_mode}
 
     def symbolic_execution(
-            self, x: Tensor, scale: Tensor, zero_point: Tensor, input_bit_width: Tensor, signed: Tensor):
+            self, x: Tensor, scale: Tensor, zero_point: Tensor, input_bit_width: Tensor,
+            signed: Tensor):
         y = BrevitasTruncFn.apply(
             x, scale, zero_point, input_bit_width, *self.symbolic_kwargs.values())
         return y, scale, zero_point, self.symbolic_kwargs['output_bit_width']
 
 
 class BrevitasQuantLSTMLayerHandler(QuantLSTMLayerHandler):
-    
+
     def quantized_cell_symbolic_execution(
-        self,
-        quant_input, 
-        quant_hidden_state, 
-        quant_cell_state, 
-        quant_weight_ii,
-        quant_weight_if, 
-        quant_weight_ic, 
-        quant_weight_io, 
-        quant_weight_hi,
-        quant_weight_hf, 
-        quant_weight_hc, 
-        quant_weight_ho, 
-        quant_bias_input,
-        quant_bias_forget,
-        quant_bias_cell,
-        quant_bias_output):
+            self,
+            quant_input,
+            quant_hidden_state,
+            quant_cell_state,
+            quant_weight_ii,
+            quant_weight_if,
+            quant_weight_ic,
+            quant_weight_io,
+            quant_weight_hi,
+            quant_weight_hf,
+            quant_weight_hc,
+            quant_weight_ho,
+            quant_bias_input,
+            quant_bias_forget,
+            quant_bias_cell,
+            quant_bias_output):
         return BrevitasQuantLSTMCellFn.apply(
-            quant_input, 
-            quant_hidden_state, 
-            quant_cell_state, 
+            quant_input,
+            quant_hidden_state,
+            quant_cell_state,
             quant_weight_ii,
-            quant_weight_if, 
-            quant_weight_ic, 
-            quant_weight_io, 
+            quant_weight_if,
+            quant_weight_ic,
+            quant_weight_io,
             quant_weight_hi,
-            quant_weight_hf, 
-            quant_weight_hc, 
-            quant_weight_ho, 
+            quant_weight_hf,
+            quant_weight_hc,
+            quant_weight_ho,
             quant_bias_input,
             quant_bias_forget,
             quant_bias_cell,
-            quant_bias_output, 
+            quant_bias_output,
             *self.symbolic_kwargs.values())
-    
-
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/qonnx/manager.py` & `brevitas-0.9.0/src/brevitas/export/onnx/qonnx/manager.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,61 +1,57 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
+from typing import Optional, Tuple, Union
 
-from typing import Tuple, Union, Optional
-from torch.nn import Module
 from torch import Tensor
+from torch.nn import Module
 
-from brevitas.export.onnx.debug import DebugMarkerFunction
-from brevitas.export.onnx.manager import ONNXBaseManager
 from brevitas.export.manager import _set_proxy_export_handler
-from brevitas.export.manager import _set_recurrent_layer_export_handler
 from brevitas.export.manager import _set_proxy_export_mode
+from brevitas.export.manager import _set_recurrent_layer_export_handler
 from brevitas.export.manager import _set_recurrent_layer_export_mode
+from brevitas.export.onnx.debug import DebugMarkerFunction
+from brevitas.export.onnx.manager import ONNXBaseManager
 
+from .function import BrevitasBinaryQuantFn
+from .function import BrevitasQuantFn
+from .function import BrevitasQuantLSTMCellFn
+from .function import BrevitasTruncFn
 from .handler import BrevitasActQuantProxyHandler
 from .handler import BrevitasBiasQuantProxyHandler
-from .handler import BrevitasWeightQuantProxyHandler
-from .handler import BrevitasTruncQuantProxyHandler
 from .handler import BrevitasDecoupledWeightQuantProxyHandler
 from .handler import BrevitasQuantLSTMLayerHandler
-
-from .function import BrevitasQuantFn
-from .function import BrevitasTruncFn
-from .function import BrevitasBinaryQuantFn
-from .function import BrevitasQuantLSTMCellFn
+from .handler import BrevitasTruncQuantProxyHandler
+from .handler import BrevitasWeightQuantProxyHandler
 
 
 class QONNXManager(ONNXBaseManager):
     target_name = 'brevitas'
     dequantize_tracing_input = False
 
     onnx_passes = [
         # use initializers instead of Constant nodes for fixed params
-        "extract_constant_to_initializer",
-        # remove unused graph inputs & initializers
+        "extract_constant_to_initializer",  # remove unused graph inputs & initializers
         "eliminate_unused_initializer"]
 
     handlers = [
         BrevitasActQuantProxyHandler,
         BrevitasBiasQuantProxyHandler,
         BrevitasWeightQuantProxyHandler,
         BrevitasDecoupledWeightQuantProxyHandler,
         BrevitasTruncQuantProxyHandler,
-        BrevitasQuantLSTMLayerHandler
-    ]
+        BrevitasQuantLSTMLayerHandler]
 
     custom_fns = [
         DebugMarkerFunction,
         BrevitasQuantFn,
         BrevitasBinaryQuantFn,
         BrevitasTruncFn,
-        BrevitasQuantLSTMCellFn
-    ]
+        BrevitasQuantLSTMCellFn]
 
     @classmethod
     def set_export_mode(cls, model: Module, enabled: bool):
         _set_proxy_export_mode(model, enabled)
         _set_recurrent_layer_export_mode(model, enabled)
 
     @classmethod
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/standard/function.py` & `brevitas-0.9.0/src/brevitas/export/onnx/standard/function.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,98 +1,58 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from torch.autograd import Function
+
 from brevitas.export.onnx import onnx_export_opset
 
 AXIS_OPSET = 13
 
 
 class DequantizeLinearFn(Function):
 
     @staticmethod
-    def symbolic(
-            g, x,
-            input_scale,
-            input_zero_point,
-            input_axis):
+    def symbolic(g, x, input_scale, input_zero_point, input_axis):
         opset_version = onnx_export_opset()
-        
+
         if input_axis is not None and opset_version < AXIS_OPSET:
             raise RuntimeError('ONNX Opset 13 is required for per-channel quantization')
         elif input_axis is not None and opset_version >= AXIS_OPSET:
-            ret = g.op(
-                'DequantizeLinear', x,
-                input_scale,
-                input_zero_point,
-                axis_i=input_axis)
+            ret = g.op('DequantizeLinear', x, input_scale, input_zero_point, axis_i=input_axis)
         else:
-            ret = g.op(
-                'DequantizeLinear', x,
-                input_scale,
-                input_zero_point)
+            ret = g.op('DequantizeLinear', x, input_scale, input_zero_point)
         return ret
 
     @staticmethod
-    def forward(
-            ctx, int_x,
-            input_scale,
-            input_zero_point,
-            input_axis):
+    def forward(ctx, int_x, input_scale, input_zero_point, input_axis):
         return int_x.float()
 
 
 class IntClipFn(Function):
 
     @staticmethod
-    def symbolic(
-            g, int_x,
-            min_int_val,
-            max_int_val):
-        ret = g.op(
-            'Clip', int_x, min_int_val, max_int_val)
+    def symbolic(g, int_x, min_int_val, max_int_val):
+        ret = g.op('Clip', int_x, min_int_val, max_int_val)
         return ret
 
     @staticmethod
-    def forward(
-            ctx, int_x,
-            min_int_val,
-            max_int_val):
+    def forward(ctx, int_x, min_int_val, max_int_val):
         return int_x
 
 
 class QuantizeLinearFn(Function):
 
     @staticmethod
-    def symbolic(
-            g, x,
-            output_scale,
-            ouput_zero_point,
-            output_dtype,
-            output_axis):
+    def symbolic(g, x, output_scale, ouput_zero_point, output_dtype, output_axis):
         opset_version = onnx_export_opset()
-        
+
         if output_axis is not None and opset_version < AXIS_OPSET:
             raise RuntimeError('ONNX Opset 13 is required for per-channel quantization')
         elif output_axis is not None and opset_version >= AXIS_OPSET:
-            ret = g.op(
-                'QuantizeLinear', x,
-                output_scale,
-                ouput_zero_point,
-                axis_i=output_axis)
+            ret = g.op('QuantizeLinear', x, output_scale, ouput_zero_point, axis_i=output_axis)
         else:
-            ret = g.op(
-                'QuantizeLinear', x,
-                output_scale,
-                ouput_zero_point)
+            ret = g.op('QuantizeLinear', x, output_scale, ouput_zero_point)
         return ret
 
     @staticmethod
-    def forward(
-            ctx, x,
-            output_scale,
-            ouput_zero_point,
-            output_dtype,
-            output_axis):
+    def forward(ctx, x, output_scale, ouput_zero_point, output_dtype, output_axis):
         return x.type(output_dtype)
-
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/standard/manager.py` & `brevitas-0.9.0/src/brevitas/export/onnx/standard/manager.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,21 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-import warnings
-from typing import Tuple, Union, Optional
 from abc import ABC
-from packaging import version
+from typing import Optional, Tuple, Union
+import warnings
 
-from torch.nn import Module
+from packaging import version
 from torch import Tensor
+from torch.nn import Module
 
 from brevitas import torch_version
-from brevitas.quant_tensor import QuantTensor
 from brevitas.export.onnx.manager import ONNXBaseManager
+from brevitas.quant_tensor import QuantTensor
 
 DEFAULT_OPSET = 13
 
 
 class StdONNXBaseManager(ONNXBaseManager, ABC):
 
     @classmethod
@@ -25,17 +24,16 @@
         if ka not in export_kwargs:
             export_kwargs[ka] = DEFAULT_OPSET
             warnings.warn(f"ONNX opset version set to {DEFAULT_OPSET}, override with {ka}=")
 
     @classmethod
     def solve_enable_onnx_checker(cls, export_kwargs):
         ka = 'enable_onnx_checker'
-        if (torch_version >= version.parse('1.5.0') 
-            and torch_version <= version.parse('1.10.0') 
-            and ka not in export_kwargs):
+        if (torch_version >= version.parse('1.5.0') and torch_version <= version.parse('1.10.0') and
+                ka not in export_kwargs):
             export_kwargs[ka] = True
 
     @classmethod
     def export_onnx(
             cls,
             module: Module,
             args: Optional[Union[Tensor, QuantTensor, Tuple]],
@@ -43,8 +41,8 @@
             input_shape: Optional[Tuple[int, ...]],
             input_t: Optional[Union[Tensor, QuantTensor]],
             disable_warnings,
             **onnx_export_kwargs):
         cls.solve_onnx_opset(onnx_export_kwargs)
         output = super().export_onnx(
             module, args, export_path, input_shape, input_t, disable_warnings, **onnx_export_kwargs)
-        return output
+        return output
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/standard/qcdq/handler.py` & `brevitas-0.9.0/src/brevitas/export/onnx/standard/qcdq/handler.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,117 +1,122 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from abc import ABC
 from copy import copy
 
 import torch
 from torch import Tensor
 
-from brevitas.export.onnx.handler import ONNXBaseHandler, QuantLSTMLayerHandler
-from brevitas.export.common.handler.qcdq import (
-    DQMixin,
-    QCDQMixin,
-    ZeroPointHandlerMixin,
-    QCDQWeightQuantProxyHandlerMixin,
-    QCDQDecoupledWeightQuantProxyHandlerMixin,
-    QCDQActQuantProxyHandlerMixin,
-    QCDQBiasQuantProxyHandlerMixin,
-    QCDQTruncQuantProxyHandlerMixin)
 from brevitas.export.common.handler.base import QuantAxisMixin
-
-from ..function import QuantizeLinearFn, DequantizeLinearFn, IntClipFn
+from brevitas.export.common.handler.qcdq import DQMixin
+from brevitas.export.common.handler.qcdq import QCDQActQuantProxyHandlerMixin
+from brevitas.export.common.handler.qcdq import QCDQBiasQuantProxyHandlerMixin
+from brevitas.export.common.handler.qcdq import QCDQDecoupledWeightQuantProxyHandlerMixin
+from brevitas.export.common.handler.qcdq import QCDQMixin
+from brevitas.export.common.handler.qcdq import QCDQTruncQuantProxyHandlerMixin
+from brevitas.export.common.handler.qcdq import QCDQWeightQuantProxyHandlerMixin
+from brevitas.export.common.handler.qcdq import ZeroPointHandlerMixin
+from brevitas.export.onnx.handler import ONNXBaseHandler
+from brevitas.export.onnx.handler import QuantLSTMLayerHandler
+
+from ..function import DequantizeLinearFn
+from ..function import IntClipFn
+from ..function import QuantizeLinearFn
 
 
 class StdDQONNXMixin(DQMixin, ABC):
-    
+
     def dequantize_fn(self, x, scale, zero_point, axis):
         return DequantizeLinearFn.apply(x, scale, zero_point, axis)
-    
+
     @property
     def flatten_dequantize_params(self):
         return True
 
     @property
-    def itemize_scalar_params(self):
+    def itemize_quantize_scalar_params(self):
         return False
 
 
 class StdQCDQONNXMixin(QCDQMixin, StdDQONNXMixin, ABC):
-    
+
     @property
     def clip_over_integers(self):
         return True
-    
-    @classmethod    
+
+    @classmethod
     def int8_dtype(cls):
         return torch.int8
 
-    @classmethod    
+    @classmethod
     def uint8_dtype(cls):
         return torch.uint8
-    
-    @classmethod    
+
+    @classmethod
     def int32_dtype(cls):
         return torch.int32
 
     def validate(self, module):
         self.validate_8b_bit_width(module.bit_width(), le_then=True)
         assert module.bit_width() > 1., 'Binary quant not supported'
-        assert module.rounding_mode == 'ROUND', 'Only round to nearest even supported'
+        assert module.rounding_mode.upper() == 'ROUND', 'Only round to nearest even supported'
 
     def quantize_fn(self, x, scale, zero_point, dtype, axis):
         return QuantizeLinearFn.apply(x, scale, zero_point, dtype, axis)
-    
+
     def clip_fn(self, x, min_val, max_val):
         return IntClipFn.apply(x, min_val, max_val)
 
 
-class StdQCDQONNXWeightQuantProxyHandler(
-    StdQCDQONNXMixin, QCDQWeightQuantProxyHandlerMixin, ONNXBaseHandler):
+class StdQCDQONNXWeightQuantProxyHandler(StdQCDQONNXMixin,
+                                         QCDQWeightQuantProxyHandlerMixin,
+                                         ONNXBaseHandler):
     pass
 
 
-class StdQCDQONNXDecoupledWeightQuantProxyHandler(
-    StdQCDQONNXMixin, QCDQDecoupledWeightQuantProxyHandlerMixin, ONNXBaseHandler):
+class StdQCDQONNXDecoupledWeightQuantProxyHandler(StdQCDQONNXMixin,
+                                                  QCDQDecoupledWeightQuantProxyHandlerMixin,
+                                                  ONNXBaseHandler):
     pass
 
 
-class StdQCDQONNXActQuantProxyHandler(
-    StdQCDQONNXMixin, QCDQActQuantProxyHandlerMixin, ONNXBaseHandler):
+class StdQCDQONNXActQuantProxyHandler(StdQCDQONNXMixin,
+                                      QCDQActQuantProxyHandlerMixin,
+                                      ONNXBaseHandler):
     pass
 
 
-class StdQCDQONNXBiasQuantProxyHandler(
-    StdDQONNXMixin, QCDQBiasQuantProxyHandlerMixin, ONNXBaseHandler):
+class StdQCDQONNXBiasQuantProxyHandler(StdDQONNXMixin,
+                                       QCDQBiasQuantProxyHandlerMixin,
+                                       ONNXBaseHandler):
     pass
 
 
-class StdQCDQONNXTruncQuantProxyHandler(
-    StdQCDQONNXMixin, QCDQTruncQuantProxyHandlerMixin, ONNXBaseHandler):
+class StdQCDQONNXTruncQuantProxyHandler(StdQCDQONNXMixin,
+                                        QCDQTruncQuantProxyHandlerMixin,
+                                        ONNXBaseHandler):
     pass
 
 
 class StdQCDQONNXQuantLSTMLayerHandler(QuantLSTMLayerHandler):
-    
+
     def quantized_cell_symbolic_execution(
-        self,
-        quant_input, 
-        quant_hidden_state, 
-        quant_cell_state, 
-        quant_weight_ii,
-        quant_weight_if, 
-        quant_weight_ic, 
-        quant_weight_io, 
-        quant_weight_hi,
-        quant_weight_hf, 
-        quant_weight_hc, 
-        quant_weight_ho, 
-        quant_bias_input,
-        quant_bias_forget,
-        quant_bias_cell,
-        quant_bias_output):
+            self,
+            quant_input,
+            quant_hidden_state,
+            quant_cell_state,
+            quant_weight_ii,
+            quant_weight_if,
+            quant_weight_ic,
+            quant_weight_io,
+            quant_weight_hi,
+            quant_weight_hf,
+            quant_weight_hc,
+            quant_weight_ho,
+            quant_bias_input,
+            quant_bias_forget,
+            quant_bias_cell,
+            quant_bias_output):
         raise RuntimeError(
-            "Quantized LSTM cell is not supported for ONNX QCDQ " 
+            "Quantized LSTM cell is not supported for ONNX QCDQ "
             "(weights only quantization is). Use export_qonnx.")
-
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/standard/qcdq/manager.py` & `brevitas-0.9.0/src/brevitas/export/onnx/standard/qcdq/manager.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,61 +1,57 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from torch.nn import Module
 
-from brevitas.export.onnx.debug import DebugMarkerFunction
 from brevitas.export.manager import _set_proxy_export_handler
-from brevitas.export.manager import _set_recurrent_layer_export_handler
 from brevitas.export.manager import _set_proxy_export_mode
+from brevitas.export.manager import _set_recurrent_layer_export_handler
 from brevitas.export.manager import _set_recurrent_layer_export_mode
+from brevitas.export.onnx.debug import DebugMarkerFunction
 from brevitas.export.onnx.function import LSTMCellFn
 
-from .handler import (
-    StdQCDQONNXWeightQuantProxyHandler,
-    StdQCDQONNXBiasQuantProxyHandler,
-    StdQCDQONNXActQuantProxyHandler,
-    StdQCDQONNXDecoupledWeightQuantProxyHandler,
-    StdQCDQONNXTruncQuantProxyHandler,
-    StdQCDQONNXQuantLSTMLayerHandler)
-
-from ..function import QuantizeLinearFn, DequantizeLinearFn, IntClipFn
+from ..function import DequantizeLinearFn
+from ..function import IntClipFn
+from ..function import QuantizeLinearFn
 from ..manager import StdONNXBaseManager
+from .handler import StdQCDQONNXActQuantProxyHandler
+from .handler import StdQCDQONNXBiasQuantProxyHandler
+from .handler import StdQCDQONNXDecoupledWeightQuantProxyHandler
+from .handler import StdQCDQONNXQuantLSTMLayerHandler
+from .handler import StdQCDQONNXTruncQuantProxyHandler
+from .handler import StdQCDQONNXWeightQuantProxyHandler
 
 
 class StdQCDQONNXManager(StdONNXBaseManager):
     target_name = 'StdQCDQONNX'
     dequantize_tracing_input = False
 
     onnx_passes = [
         # use initializers instead of Constant nodes for fixed params
-        "extract_constant_to_initializer",
-        # remove unused graph inputs & initializers
+        "extract_constant_to_initializer",  # remove unused graph inputs & initializers
         "eliminate_unused_initializer"]
 
     handlers = [
         StdQCDQONNXWeightQuantProxyHandler,
         StdQCDQONNXBiasQuantProxyHandler,
         StdQCDQONNXActQuantProxyHandler,
         StdQCDQONNXDecoupledWeightQuantProxyHandler,
         StdQCDQONNXTruncQuantProxyHandler,
-        StdQCDQONNXQuantLSTMLayerHandler
-    ]
+        StdQCDQONNXQuantLSTMLayerHandler]
 
     custom_fns = [
         DebugMarkerFunction,
         QuantizeLinearFn,
         DequantizeLinearFn,
         IntClipFn,
-        LSTMCellFn,
-    ]
+        LSTMCellFn,]
 
     @classmethod
     def set_export_mode(cls, model: Module, enabled: bool):
         _set_proxy_export_mode(model, enabled)
         _set_recurrent_layer_export_mode(model, enabled)
 
     @classmethod
     def set_export_handler(cls, module: Module):
         _set_proxy_export_handler(cls, module)
-        _set_recurrent_layer_export_handler(cls, module)
+        _set_recurrent_layer_export_handler(cls, module)
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/standard/qoperator/function.py` & `brevitas-0.9.0/src/brevitas/export/onnx/standard/qoperator/function.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import torch
 from torch.autograd import Function
 
 
 class QLinearConvFn(Function):
 
     @staticmethod
     def symbolic(
-            g, int_x,
+            g,
+            int_x,
             input_scale,
             input_zero_point,
             int_weight,
             weight_scale,
             weight_zero_point,
             output_scale,
             ouput_zero_point,
@@ -24,15 +24,16 @@
             kernel_size,
             padding,
             stride,
             groups,
             dilation):
         if int_bias is not None:
             ret = g.op(
-                'QLinearConv', int_x,
+                'QLinearConv',
+                int_x,
                 input_scale,
                 input_zero_point,
                 int_weight,
                 weight_scale,
                 weight_zero_point,
                 output_scale,
                 ouput_zero_point,
@@ -40,15 +41,16 @@
                 kernel_shape_i=kernel_size,
                 pads_i=padding,
                 strides_i=stride,
                 group_i=groups,
                 dilations_i=dilation)
         else:
             ret = g.op(
-                'QLinearConv', int_x,
+                'QLinearConv',
+                int_x,
                 input_scale,
                 input_zero_point,
                 int_weight,
                 weight_scale,
                 weight_zero_point,
                 output_scale,
                 ouput_zero_point,
@@ -57,15 +59,16 @@
                 strides_i=stride,
                 group_i=groups,
                 dilations_i=dilation)
         return ret
 
     @staticmethod
     def forward(
-            ctx, int_x,
+            ctx,
+            int_x,
             input_scale,
             input_zero_point,
             int_weight,
             weight_scale,
             weight_zero_point,
             output_scale,
             output_zero_point,
@@ -80,41 +83,44 @@
         return torch.empty(out_shape, dtype=output_dtype, device=int_x.device)
 
 
 class QLinearMatMulFn(Function):
 
     @staticmethod
     def symbolic(
-            g, int_x,
+            g,
+            int_x,
             input_scale,
             input_zero_point,
             int_weight,
             weight_scale,
             weight_zero_point,
             output_scale,
             ouput_zero_point,
             output_dtype,
             out_shape):
         ret = g.op(
-            'QLinearMatMul', int_x,
+            'QLinearMatMul',
+            int_x,
             input_scale,
             input_zero_point,
             int_weight,
             weight_scale,
             weight_zero_point,
             output_scale,
             ouput_zero_point)
         return ret
 
     @staticmethod
     def forward(
-            ctx, int_x,
+            ctx,
+            int_x,
             input_scale,
             input_zero_point,
             int_weight,
             weight_scale,
             weight_zero_point,
             output_scale,
             output_zero_point,
             output_dtype,
             out_shape):
-        return torch.empty(out_shape, dtype=output_dtype, device=int_x.device)
+        return torch.empty(out_shape, dtype=output_dtype, device=int_x.device)
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/standard/qoperator/handler/act.py` & `brevitas-0.9.0/src/brevitas/export/onnx/standard/qoperator/handler/act.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,15 +1,22 @@
 from abc import ABC
 
 import torch
 from torch import Tensor
 
-from brevitas.nn import QuantReLU, QuantIdentity, QuantHardTanh, QuantTanh, QuantSigmoid
+from brevitas.export.onnx.standard.function import DequantizeLinearFn
+from brevitas.export.onnx.standard.function import IntClipFn
+from brevitas.export.onnx.standard.function import QuantizeLinearFn
+from brevitas.nn import QuantHardTanh
+from brevitas.nn import QuantIdentity
+from brevitas.nn import QuantReLU
+from brevitas.nn import QuantSigmoid
+from brevitas.nn import QuantTanh
 from brevitas.nn.quant_layer import QuantNonLinearActLayer as QuantNLAL
-from brevitas.export.onnx.standard.function import QuantizeLinearFn, DequantizeLinearFn, IntClipFn
+
 from .base import StdQOpONNXQuantLayerHandler
 
 
 class StdQOpONNXQuantNLALHandler(StdQOpONNXQuantLayerHandler, ABC):
 
     @classmethod
     def validate(cls, module: QuantNLAL):
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/standard/qoperator/handler/base.py` & `brevitas-0.9.0/src/brevitas/export/onnx/standard/qoperator/handler/base.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,27 +1,31 @@
-from abc import ABC, abstractmethod
+from abc import ABC
+from abc import abstractmethod
 
 import torch
 from torch import Tensor
 
-
-from brevitas.export.onnx.standard.function import QuantizeLinearFn, DequantizeLinearFn, IntClipFn
-from brevitas.export.common.handler.base import (
-    QuantAxisMixin, ScaleHandlerMixin, BitWidthHandlerMixin, ZeroPointHandlerMixin, ClipMixin)
+from brevitas.export.common.handler.base import BitWidthHandlerMixin
+from brevitas.export.common.handler.base import ClipMixin
+from brevitas.export.common.handler.base import QuantAxisMixin
+from brevitas.export.common.handler.base import ScaleHandlerMixin
+from brevitas.export.common.handler.base import ZeroPointHandlerMixin
 from brevitas.export.onnx.handler import ONNXBaseHandler
+from brevitas.export.onnx.standard.function import DequantizeLinearFn
+from brevitas.export.onnx.standard.function import IntClipFn
+from brevitas.export.onnx.standard.function import QuantizeLinearFn
 
 
-class StdQOpONNXQuantLayerHandler(
-    ONNXBaseHandler, 
-    QuantAxisMixin, 
-    ScaleHandlerMixin, 
-    ClipMixin,
-    BitWidthHandlerMixin, 
-    ZeroPointHandlerMixin, 
-    ABC):
+class StdQOpONNXQuantLayerHandler(ONNXBaseHandler,
+                                  QuantAxisMixin,
+                                  ScaleHandlerMixin,
+                                  ClipMixin,
+                                  BitWidthHandlerMixin,
+                                  ZeroPointHandlerMixin,
+                                  ABC):
 
     @abstractmethod
     def op_symbolic_execution(self, inp: Tensor):
         pass
 
     @abstractmethod
     def input_symbolic_execution(self, inp: Tensor):
@@ -105,34 +109,40 @@
         else:
             return None
 
     @classmethod
     def dequant_symbolic_kwargs_from_cached_io(cls, cached_io):
         cls.validate_8b_bit_width(cached_io.bit_width, le_then=True)
         return {
-            'input_scale': cached_io.scale,
-            'input_zero_point': cls.zero_point_with_dtype(
-                cached_io.signed, cached_io.bit_width, cached_io.zero_point),
-            'input_axis': cls.quant_axis(cached_io.scale)}
+            'input_scale':
+                cached_io.scale,
+            'input_zero_point':
+                cls.zero_point_with_dtype(
+                    cached_io.signed, cached_io.bit_width, cached_io.zero_point),
+            'input_axis':
+                cls.quant_axis(cached_io.scale)}
 
     @classmethod
     def quant_symbolic_kwargs_from_cached_io(cls, cached_io):
         cls.validate_8b_bit_width(cached_io.bit_width, le_then=True)
         q_kwargs = {
-            'output_scale': cached_io.scale,
-            'output_zero_point': cls.zero_point_with_dtype(
-                cached_io.signed, cached_io.bit_width, cached_io.zero_point),
-            'output_dtype': cls.torch_8b_dtype(cached_io.signed),
-            'output_axis': cls.quant_axis(cached_io.scale)}
+            'output_scale':
+                cached_io.scale,
+            'output_zero_point':
+                cls.zero_point_with_dtype(
+                    cached_io.signed, cached_io.bit_width, cached_io.zero_point),
+            'output_dtype':
+                cls.torch_8b_dtype(cached_io.signed),
+            'output_axis':
+                cls.quant_axis(cached_io.scale)}
         # TODO support narrow caching
         # Assume narrow is False since we are preventing it everywhere else
         int_clip_kwargs = cls.clip_symbolic_kwargs(False, cached_io.signed, cached_io.bit_width)
         return q_kwargs, int_clip_kwargs
 
-
     def symbolic_execution(self, inp: Tensor):
         inp = self.input_symbolic_execution(inp)
         out = self.op_symbolic_execution(inp)
         ret = self.output_symbolic_execution(out)
         return ret
 
 
@@ -169,8 +179,7 @@
         output_quant_symbolic_kwargs = self.symbolic_kwargs['output_quant_symbolic_kwargs']
         output_clip_symbolic_kwargs = self.symbolic_kwargs['output_clip_symbolic_kwargs']
         if output_quant_symbolic_kwargs is not None:
             out = QuantizeLinearFn.apply(out, *output_quant_symbolic_kwargs.values())
             if output_clip_symbolic_kwargs is not None:
                 out = IntClipFn.apply(out, *output_clip_symbolic_kwargs.values())
         return out
-
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/standard/qoperator/handler/parameter.py` & `brevitas-0.9.0/src/brevitas/export/onnx/standard/qoperator/handler/parameter.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,20 +1,28 @@
+from abc import ABC
+from abc import abstractmethod
 from typing import Union
-from abc import ABC, abstractmethod
 
 import torch
 from torch import Tensor
 
+from brevitas.export.common import to_0dim_if_scalar
+from brevitas.export.onnx.handler import Kernel1dApplHandlerMixin
+from brevitas.export.onnx.handler import Kernel2dApplHandlerMixin
+from brevitas.export.onnx.standard.function import DequantizeLinearFn
+from brevitas.export.onnx.standard.function import IntClipFn
+from brevitas.export.onnx.standard.function import QuantizeLinearFn
+from brevitas.nn import QuantConv1d
+from brevitas.nn import QuantConv2d
+from brevitas.nn import QuantLinear
 from brevitas.nn.quant_layer import QuantWeightBiasInputOutputLayer as QuantWBIOL
-from brevitas.nn import QuantConv2d, QuantConv1d, QuantLinear
-from brevitas.export.onnx.handler import Kernel2dApplHandlerMixin, Kernel1dApplHandlerMixin
-from brevitas.export.onnx.standard.function import QuantizeLinearFn, DequantizeLinearFn, IntClipFn
-from ..function import QLinearConvFn, QLinearMatMulFn
+
+from ..function import QLinearConvFn
+from ..function import QLinearMatMulFn
 from .base import StdQOpONNXQuantLayerHandler
-from  brevitas.export.common import to_0dim_if_scalar
 
 
 class StdQOpONNXQuantWBIOLHandler(StdQOpONNXQuantLayerHandler, ABC):
 
     @staticmethod
     def int_weight(module: QuantWBIOL):
         int_weight = module.int_weight(float_datatype=False).detach()
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/standard/qoperator/handler/pool.py` & `brevitas-0.9.0/src/brevitas/export/onnx/standard/qoperator/handler/pool.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 from abc import ABC
 from typing import Union
 
 import torch
 from torch import Tensor
-
 # early import of max_pool to avoid being affected by monkeypatching
-from torch.nn.functional import max_pool1d, max_pool2d
+from torch.nn.functional import max_pool1d
+from torch.nn.functional import max_pool2d
+
+from brevitas.nn import QuantMaxPool1d
+from brevitas.nn import QuantMaxPool2d
 
-from brevitas.nn import QuantMaxPool1d, QuantMaxPool2d
 from .base import StdQOpONNXQuantWrapperHandler
 
 
 class StdQOpONNXQuantMaxPoolNd(StdQOpONNXQuantWrapperHandler, ABC):
 
-
     @classmethod
     def op_symbolic_kwargs(cls, module: Union[QuantMaxPool1d, QuantMaxPool2d]):
         return {
             'kernel_size': module.kernel_size,
             'stride': module.stride,
             'padding': module.padding,
             'dilation': module.dilation,
@@ -34,8 +35,8 @@
 
 
 class StdQOpONNXQuantMaxPool2d(StdQOpONNXQuantMaxPoolNd):
     handled_layer = QuantMaxPool2d
 
     def op_symbolic_execution(self, inp: Tensor):
         op_symbolic_kwargs = self.symbolic_kwargs['op_symbolic_kwargs']
-        return max_pool2d(inp, *op_symbolic_kwargs.values())
+        return max_pool2d(inp, *op_symbolic_kwargs.values())
```

### Comparing `brevitas-0.8.0/src/brevitas/export/onnx/standard/qoperator/manager.py` & `brevitas-0.9.0/src/brevitas/export/onnx/standard/qoperator/manager.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,52 +1,53 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
+from typing import Optional, Tuple, Union
 
-from typing import Tuple, Optional, Union
 from packaging import version
-
 from torch import Tensor
 from torch.nn import functional as F
 from torch.nn import Module
 
 from brevitas import torch_version
-from brevitas.quant_tensor import QuantTensor
+from brevitas.export.manager import _set_layer_export_handler
+from brevitas.export.manager import _set_layer_export_mode
 from brevitas.export.onnx.manager import ONNXBaseManager
-from brevitas.export.manager import _set_layer_export_handler, _set_layer_export_mode
+from brevitas.quant_tensor import QuantTensor
 
-from .handler.base import StdQOpONNXQuantLayerHandler
-from .handler.parameter import StdQOpONNXQuantConv2dHandler
-from .handler.parameter import StdQOpONNXQuantConv1dHandler
-from .handler.parameter import StdQOpONNXQuantLinearHandler
-from .handler.act import StdQOpONNXQuantReLUHandler
+from ..function import DequantizeLinearFn
+from ..function import IntClipFn
+from ..function import QuantizeLinearFn
+from ..manager import StdONNXBaseManager
 from .handler.act import StdQOpONNXQuantHardTanhHandler
 from .handler.act import StdQOpONNXQuantIdentityHandler
-from .handler.act import StdQOpONNXQuantTanhHandler
+from .handler.act import StdQOpONNXQuantReLUHandler
 from .handler.act import StdQOpONNXQuantSigmoidHandler
+from .handler.act import StdQOpONNXQuantTanhHandler
+from .handler.base import StdQOpONNXQuantLayerHandler
+from .handler.parameter import StdQOpONNXQuantConv1dHandler
+from .handler.parameter import StdQOpONNXQuantConv2dHandler
+from .handler.parameter import StdQOpONNXQuantLinearHandler
 from .handler.pool import StdQOpONNXQuantMaxPool1d
 from .handler.pool import StdQOpONNXQuantMaxPool2d
-from ..function import QuantizeLinearFn, DequantizeLinearFn, IntClipFn
-from ..manager import StdONNXBaseManager
 
 
 class StdQOpONNXManager(StdONNXBaseManager):
     target_name = 'StdQOpONNX'
 
     _fn_to_cache = [
         F.relu,
         F.relu6,
         F.hardtanh,
         F.max_pool1d,
         F.max_pool2d,
         F.max_pool3d,
         F.adaptive_max_pool1d,
         F.adaptive_max_pool2d,
-        F.adaptive_max_pool3d,
-    ]
+        F.adaptive_max_pool3d,]
 
     handlers = [
         StdQOpONNXQuantConv1dHandler,
         StdQOpONNXQuantConv2dHandler,
         StdQOpONNXQuantLinearHandler,
         StdQOpONNXQuantReLUHandler,
         StdQOpONNXQuantHardTanhHandler,
@@ -84,8 +85,7 @@
     @classmethod
     def set_export_mode(cls, module: Module, enabled: bool):
         _set_layer_export_mode(module, enabled)
 
     @classmethod
     def set_export_handler(cls, module: Module):
         _set_layer_export_handler(cls, module)
-
```

### Comparing `brevitas-0.8.0/src/brevitas/export/torch/qcdq/handler.py` & `brevitas-0.9.0/src/brevitas/export/torch/qcdq/handler.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,127 +1,127 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from abc import ABC
 
 import torch
 
-from brevitas.proxy import BiasQuantProxyFromInjector
+from brevitas.export.common import to_0dim_if_scalar
+from brevitas.export.common import to_item_if_0dim
 from brevitas.export.common.handler.base import BaseHandler
-from brevitas.export.common.handler.qcdq import (
-    DQMixin,
-    QCDQMixin,
-    QCDQWeightQuantProxyHandlerMixin,
-    QCDQQuantProxyHandlerMixin,
-    QCDQActQuantProxyHandlerMixin,
-    QCDQBiasQuantProxyHandlerMixin,
-    QCDQTruncQuantProxyHandlerMixin)
-from brevitas.export.common import to_0dim_if_scalar, to_item_if_0dim
+from brevitas.export.common.handler.qcdq import DQMixin
+from brevitas.export.common.handler.qcdq import QCDQActQuantProxyHandlerMixin
+from brevitas.export.common.handler.qcdq import QCDQBiasQuantProxyHandlerMixin
+from brevitas.export.common.handler.qcdq import QCDQMixin
+from brevitas.export.common.handler.qcdq import QCDQQuantProxyHandlerMixin
+from brevitas.export.common.handler.qcdq import QCDQTruncQuantProxyHandlerMixin
+from brevitas.export.common.handler.qcdq import QCDQWeightQuantProxyHandlerMixin
+from brevitas.proxy import BiasQuantProxyFromInjector
 
 
 def _itemize_clip_bounds(clip_args):
     if clip_args is not None:
         clip_args['min_val'] = clip_args['min_val'].item()
         clip_args['max_val'] = clip_args['max_val'].item()
     return clip_args
 
 
 class TorchDQMixin(DQMixin, ABC):
-    
+
     def dequantize_fn(self, x, scale, zero_point, axis):
-        # cast zero_point to float, otherwise if both x 
+        # cast zero_point to float, otherwise if both x
         # and zero_point are uint (as in asym quant)
         # uint - uint can lead to errors. Don't cast x to float
         # as the main float datatype might not be float32 (e.g float16)
         if isinstance(zero_point, torch.Tensor):
             zero_point = zero_point.to(torch.float)
         else:
             zero_point = float(zero_point)
         return (x - zero_point) * scale
-    
+
     @property
     def flatten_dequantize_params(self):
         return False
 
     @property
-    def itemize_scalar_params(self):
+    def itemize_quantize_scalar_params(self):
         return True
 
 
-class TorchQCDQMixin(
-    TorchDQMixin, QCDQMixin, ABC):
-    
+class TorchQCDQMixin(TorchDQMixin, QCDQMixin, ABC):
+
     def __init__(self) -> None:
         super().__init__()
         self.symbolic_kwargs = {}
 
     @property
     def clip_over_integers(self):
         return True
 
-    @classmethod    
+    @classmethod
     def int8_dtype(cls):
         return torch.qint8
 
-    @classmethod    
+    @classmethod
     def uint8_dtype(cls):
         return torch.quint8
-    
-    @classmethod    
+
+    @classmethod
     def int32_dtype(cls):
         return torch.qint32
 
     def validate(self, module):
         assert module.bit_width() > 1., 'Binary quant not supported'
-        assert module.rounding_mode == 'ROUND', 'Only round to nearest even supported'
-        
+        assert module.rounding_mode.upper() == 'ROUND', 'Only round to nearest even supported'
+
     def quantize_fn(self, x, scale, zero_point, dtype, axis):
         if axis is None:
             y = torch.quantize_per_tensor(x, scale, zero_point, dtype)
         else:
             y = torch.quantize_per_channel(x, scale, zero_point, axis, dtype)
         return y.int_repr()
-    
+
     def clip_fn(self, x, min_val, max_val):
         return torch.clamp(x, min_val, max_val)
-    
+
     def forward(self, *args, **kwargs):
         return self.symbolic_execution(*args, **kwargs)
-    
+
 
 class TorchQCDQHandler(BaseHandler):
-    
+
     def forward(self, *args, **kwargs):
         return self.symbolic_execution(*args, **kwargs)
 
 
-class TorchQCDQWeightQuantProxyHandler(
-    TorchQCDQMixin, QCDQWeightQuantProxyHandlerMixin, TorchQCDQHandler):
-    
+class TorchQCDQWeightQuantProxyHandler(TorchQCDQMixin,
+                                       QCDQWeightQuantProxyHandlerMixin,
+                                       TorchQCDQHandler):
+
     @classmethod
     def int_clip_symbolic_kwargs(cls, narrow, signed, bit_width):
         clip_args = super().int_clip_symbolic_kwargs(narrow, signed, bit_width)
         return _itemize_clip_bounds(clip_args)
 
-    
-class TorchQCDQActQuantProxyHandler(
-    TorchQCDQMixin, QCDQActQuantProxyHandlerMixin, TorchQCDQHandler):
-    
+
+class TorchQCDQActQuantProxyHandler(TorchQCDQMixin, QCDQActQuantProxyHandlerMixin,
+                                    TorchQCDQHandler):
+
     @classmethod
     def int_clip_symbolic_kwargs(cls, narrow, signed, bit_width):
         clip_args = super().int_clip_symbolic_kwargs(narrow, signed, bit_width)
         return _itemize_clip_bounds(clip_args)
 
 
-class TorchQCDQBiasQuantProxyHandler(
-    TorchDQMixin, QCDQBiasQuantProxyHandlerMixin, TorchQCDQHandler):    
+class TorchQCDQBiasQuantProxyHandler(TorchDQMixin, QCDQBiasQuantProxyHandlerMixin,
+                                     TorchQCDQHandler):
     pass
 
 
-class TorchQCDQTruncQuantProxyHandler(
-    TorchQCDQMixin, QCDQTruncQuantProxyHandlerMixin, TorchQCDQHandler):
+class TorchQCDQTruncQuantProxyHandler(TorchQCDQMixin,
+                                      QCDQTruncQuantProxyHandlerMixin,
+                                      TorchQCDQHandler):
 
     @classmethod
     def int_clip_symbolic_kwargs(cls, narrow, signed, bit_width):
         clip_args = super().int_clip_symbolic_kwargs(narrow, signed, bit_width)
-        return _itemize_clip_bounds(clip_args)
+        return _itemize_clip_bounds(clip_args)
```

### Comparing `brevitas-0.8.0/src/brevitas/export/torch/qcdq/manager.py` & `brevitas-0.9.0/src/brevitas/export/torch/qcdq/manager.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,28 +1,26 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from typing import Optional, Tuple, Union
 
 from torch import Tensor
 from torch.nn import Module
 
 from brevitas.export.manager import _set_proxy_export_handler
 from brevitas.export.manager import _set_proxy_export_mode
-from brevitas.export.manager import BaseManager, ExportContext
+from brevitas.export.manager import BaseManager
+from brevitas.export.manager import ExportContext
+
+from .handler import TorchQCDQActQuantProxyHandler
+from .handler import TorchQCDQBiasQuantProxyHandler
+from .handler import TorchQCDQTruncQuantProxyHandler
+from .handler import TorchQCDQWeightQuantProxyHandler
 
 
-from .handler import (
-    TorchQCDQWeightQuantProxyHandler,
-    TorchQCDQActQuantProxyHandler,
-    TorchQCDQBiasQuantProxyHandler,
-    TorchQCDQTruncQuantProxyHandler)
-     
-        
 class TorchQCDQManager(BaseManager):
     target_name = 'torch'
 
     handlers = [
         TorchQCDQWeightQuantProxyHandler,
         TorchQCDQActQuantProxyHandler,
         TorchQCDQBiasQuantProxyHandler,
@@ -33,15 +31,11 @@
         _set_proxy_export_mode(model, enabled)
 
     @classmethod
     def set_export_handler(cls, module: Module):
         _set_proxy_export_handler(cls, module)
 
     @classmethod
-    def export(
-            cls,
-            module: Module,
-            args,
-            export_path: Optional[str] = None):
+    def export(cls, module: Module, args, export_path: Optional[str] = None):
         with ExportContext(cls):
             traced_module = cls.jit_inference_trace(module, args, export_path)
-        return traced_module
+        return traced_module
```

### Comparing `brevitas-0.8.0/src/brevitas/export/torch/qoperator/handler/act.py` & `brevitas-0.9.0/src/brevitas/export/torch/qoperator/handler/act.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from abc import ABC
 
 import torch
 from torch import Tensor
 
-from brevitas.nn import QuantReLU, QuantIdentity, QuantHardTanh
+from brevitas.nn import QuantHardTanh
+from brevitas.nn import QuantIdentity
+from brevitas.nn import QuantReLU
 from brevitas.nn.quant_layer import QuantNonLinearActLayer as QuantNLAL
 
-from .base import PytorchQuantLayerHandler
 from . import qF
+from .base import PytorchQuantLayerHandler
 
 
 class PytorchQuantNLALHandler(PytorchQuantLayerHandler, ABC):
 
     @classmethod
     def explicit_output_dtype(cls) -> bool:
         return True
@@ -58,8 +59,8 @@
 
     @classmethod
     def prepare_qf(cls, module):
         return None, None
 
 
 class PytorchQuantHardTanhHandler(PytorchQuantIdentityHandler):
-    handled_layer = QuantHardTanh
+    handled_layer = QuantHardTanh
```

### Comparing `brevitas-0.8.0/src/brevitas/export/torch/qoperator/handler/base.py` & `brevitas-0.9.0/src/brevitas/export/torch/qoperator/handler/base.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,16 +1,19 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from abc import ABC, abstractmethod
+from abc import ABC
+from abc import abstractmethod
 
 import torch
 from torch import Tensor
-from brevitas.export.common.handler.base import BaseHandler, BitWidthHandlerMixin, ZeroPointHandlerMixin
+
+from brevitas.export.common.handler.base import BaseHandler
+from brevitas.export.common.handler.base import BitWidthHandlerMixin
+from brevitas.export.common.handler.base import ZeroPointHandlerMixin
 
 SCALAR_SHAPE = ()
 
 
 def _is_scalar(x: Tensor):
     return x.shape == SCALAR_SHAPE
 
@@ -61,8 +64,8 @@
     @classmethod
     def prepare_output_quant(cls, module):
         scale = module.quant_output_scale()
         zero_point = cls.quant_output_zero_point(module)
         signed = module.is_quant_output_signed
         incl_dtype = cls.explicit_output_dtype()
         quant_impl, quant_kwargs = cls.gen_quant_impl_kwargs(scale, zero_point, signed, incl_dtype)
-        return quant_impl, quant_kwargs
+        return quant_impl, quant_kwargs
```

### Comparing `brevitas-0.8.0/src/brevitas/export/torch/qoperator/handler/parameter.py` & `brevitas-0.9.0/src/brevitas/export/torch/qoperator/handler/parameter.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-import warnings
-from typing import Union
 from abc import ABC
+from typing import Union
+import warnings
 
 import torch
 from torch import Tensor
 
-from brevitas.nn import QuantConv2d, QuantConv1d, QuantLinear
+from brevitas.nn import QuantConv1d
+from brevitas.nn import QuantConv2d
+from brevitas.nn import QuantLinear
 from brevitas.nn.quant_layer import QuantWeightBiasInputOutputLayer as QuantWBIOL
 
 from .base import PytorchQuantLayerHandler
 
 
 class PytorchQuantWBIOLHandler(PytorchQuantLayerHandler):
 
@@ -124,9 +125,7 @@
     @classmethod
     def explicit_output_dtype(cls):
         return False
 
     @classmethod
     def prepare_qf(cls, module: QuantLinear):
         return torch.nn.quantized.functional.linear, {'bias': cls.prepare_bias(module)}
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas/export/torch/qoperator/handler/pool.py` & `brevitas-0.9.0/src/brevitas/export/torch/qoperator/handler/pool.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from abc import ABC
 from typing import Union
 
-from brevitas.nn import QuantMaxPool1d, QuantMaxPool2d
-from .base import PytorchQuantLayerHandler
+from brevitas.nn import QuantMaxPool1d
+from brevitas.nn import QuantMaxPool2d
+
 from . import qF
+from .base import PytorchQuantLayerHandler
 
 
 class PytorchQuantMaxPoolNd(PytorchQuantLayerHandler, ABC):
 
     @classmethod
     def validate(cls, module):
         # nothing to do here, pytorch's quant max pool is standard max pool
@@ -52,8 +53,8 @@
 
 
 class PytorchQuantMaxPool2d(PytorchQuantMaxPoolNd):
     handled_layer = QuantMaxPool2d
 
     @classmethod
     def prepare_qf(cls, module: QuantMaxPool1d):
-        return qF.max_pool2d, cls.prepare_qf_kwargs(module)
+        return qF.max_pool2d, cls.prepare_qf_kwargs(module)
```

### Comparing `brevitas-0.8.0/src/brevitas/export/torch/qoperator/manager.py` & `brevitas-0.9.0/src/brevitas/export/torch/qoperator/manager.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,28 +1,31 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Union, Optional, Tuple
+from typing import Optional, Tuple, Union
 
 import torch
 from torch import Tensor
 from torch.nn import Module
 
+from brevitas.export.manager import _set_layer_export_handler
+from brevitas.export.manager import _set_layer_export_mode
+from brevitas.export.manager import BaseManager
+from brevitas.export.manager import ExportContext
 from brevitas.quant_tensor import QuantTensor
-from brevitas.export.manager import BaseManager, ExportContext
-from brevitas.export.manager import _set_layer_export_handler, _set_layer_export_mode
-from .handler.parameter import PytorchQuantConv2dHandler
-from .handler.parameter import PytorchQuantConv1dHandler
-from .handler.parameter import PytorchQuantLinearHandler
+
+from .handler import qF
+from .handler.act import PytorchQuantHardTanhHandler
 from .handler.act import PytorchQuantIdentityHandler
 from .handler.act import PytorchQuantReLUHandler
-from .handler.act import PytorchQuantHardTanhHandler
-from .handler.pool import PytorchQuantMaxPool1d, PytorchQuantMaxPool2d
-from .handler import qF
+from .handler.parameter import PytorchQuantConv1dHandler
+from .handler.parameter import PytorchQuantConv2dHandler
+from .handler.parameter import PytorchQuantLinearHandler
+from .handler.pool import PytorchQuantMaxPool1d
+from .handler.pool import PytorchQuantMaxPool2d
 
 
 class TorchQOpManager(BaseManager):
     target_name = 'torch'
 
     handlers = [
         PytorchQuantMaxPool1d,
@@ -57,8 +60,8 @@
             raise RuntimeError("Export accepts either an input shape or an input tensor, not both")
         if input_t is None and input_shape is not None:
             input_t = torch.empty(*input_shape)
         with ExportContext(cls):
             traced_module = cls.jit_inference_trace(module, input_t)
         if export_path is not None:
             traced_module.save(export_path)
-        return traced_module
+        return traced_module
```

### Comparing `brevitas-0.8.0/src/brevitas/function/ops.py` & `brevitas-0.9.0/src/brevitas/function/ops.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 """
 Implementation of various core operations often performed as part of quantization.
 The implemented functions adheres to the restriction imposed by Pytorch 1.1.0's TorchScript compiler.
 """
 
 import torch
 from torch import Tensor
@@ -179,13 +178,13 @@
         tensor(0)
         >>> min_int(signed=True, narrow_range=False, bit_width=torch.tensor(8))
         tensor(-128)
         >>> min_int(signed=False, narrow_range=False, bit_width=torch.tensor(8))
         tensor(0)
     """
     if signed and narrow_range:
-        value = - (2 ** (bit_width - 1)) + 1
+        value = -(2 ** (bit_width - 1)) + 1
     elif signed and not narrow_range:
-        value = - (2 ** (bit_width - 1))
+        value = -(2 ** (bit_width - 1))
     else:
         value = 0 * bit_width
     return value
```

### Comparing `brevitas-0.8.0/src/brevitas/function/ops_ste.py` & `brevitas-0.9.0/src/brevitas/function/ops_ste.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,46 +1,46 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 """
 Implementation of various functions with a straight-through gradient estimators, dispatched to
 either a native just-in-time compiled backend (when env ``BREVITAS_JIT=1``) or to an autograd
 Function implemented in :obj:`~brevitas.ops.autograd_ste_ops` (when env ``BREVITAS_JIT=0``).
 
 The native backend is enabled when ``BREVITAS_JIT`` is enabled to allow for end-to-end compilation
 of the built-in quantizers, since as of Pytorch 1.8.1 a torch.autograd.Function is not supported by
 the compiler.
 """
 
 import torch
 from torch import Tensor
 
 import brevitas
-from brevitas.function.ops import dpu_round, tensor_clamp
-from brevitas.function.ops import tensor_clamp_, binary_sign, round_to_zero
+from brevitas.function.ops import binary_sign
+from brevitas.function.ops import dpu_round
+from brevitas.function.ops import round_to_zero
+from brevitas.function.ops import tensor_clamp
+from brevitas.function.ops import tensor_clamp_
 
 __all__ = [
     'round_ste',
     'ceil_ste',
     'floor_ste',
     'tensor_clamp_ste',
     'tensor_clamp_ste_',
     'scalar_clamp_ste',
     'scalar_clamp_min_ste',
     'binary_sign_ste',
     'ternary_sign_ste',
     'round_to_zero_ste',
     'dpu_round_ste',
-    'abs_binary_sign_grad'
-]
-
+    'abs_binary_sign_grad']
 
 if brevitas.NATIVE_STE_BACKEND_LOADED:
-    fn_prefix = torch 
+    fn_prefix = torch
     script_flag = brevitas.jit.script
 else:
     fn_prefix = brevitas
     script_flag = torch.jit.ignore
 
 
 @script_flag
@@ -117,17 +117,17 @@
 
 @script_flag
 def tensor_clamp_ste(x: Tensor, min_val: Tensor, max_val: Tensor) -> Tensor:
     """
     Function that implements :func:`~brevitas.function.ops.tensor_clamp` with a straight-through
     gradient estimator for the gradient of y w.r.t. to x, while the gradient of y w.r.t. to min_val
     and max_val is always None.
-    
+
     Notes:
-        Wrapper for either :func:`~brevitas.ops.autograd_ste_ops.tensor_clamp_ste_impl` (with 
+        Wrapper for either :func:`~brevitas.ops.autograd_ste_ops.tensor_clamp_ste_impl` (with
         env ``BREVITAS_JIT=0``) or its native just-in-time compiled variant (with
         ``BREVITAS_JIT=1``).
 
     Examples:
         >>> x = torch.tensor([1.5, 0.4, -1.5], requires_grad=True)
         >>> y = tensor_clamp_ste(x, torch.tensor([-1.0, -0.5, -0.5]), torch.tensor([1.0, 0.5, 0.5]))
         >>> y
@@ -348,23 +348,23 @@
 def abs_binary_sign_grad(x: Tensor) -> Tensor:
     """
     Function that implements :func:`torch.abs` with a binary-sign backward, in order to
     have subgradient 1 in 0. Compare with :func:`torch.abs`' subgradient of 0 in 0.
 
     Notes:
         Wrapper for either :func:`~brevitas.ops.autograd_ste_ops.abs_binary_sign_grad_impl`
-        (with env ``BREVITAS_JIT=0``) or its native just-in-time compiled variant (with 
+        (with env ``BREVITAS_JIT=0``) or its native just-in-time compiled variant (with
         ``BREVITAS_JIT=1``).
 
     Examples:
         >>> x = torch.tensor([0.0], requires_grad=True)
         >>> y = abs_binary_sign_grad(x)
         >>> y
         tensor([0.], grad_fn=<AbsBinarySignGradFnBackward>)
         >>> grad = torch.tensor([0.1])
         >>> y.backward(grad)
         >>> (x.grad == grad).all().item()
         True
     """
     if torch._C._get_tracing_state():
         return torch.abs(x)
-    return fn_prefix.ops.autograd_ste_ops.abs_binary_sign_grad_impl(x)
+    return fn_prefix.ops.autograd_ste_ops.abs_binary_sign_grad_impl(x)
```

### Comparing `brevitas-0.8.0/src/brevitas/function/shape.py` & `brevitas-0.9.0/src/brevitas/function/shape.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 """
 Implementation of various functions to compute shapes that induce flattening along certain
 dimensions of a tensor.
 """
 
 from typing import Tuple
 
@@ -14,16 +13,15 @@
 
 import brevitas
 
 __all__ = [
     'over_tensor',
     'over_output_channels',
     'over_batch_over_tensor',
-    'over_batch_over_output_channels'
-]
+    'over_batch_over_output_channels']
 
 
 @brevitas.jit.script
 def over_tensor(x: Tensor) -> int:
     """
     Computes the shape s such that x.view(s) is a flat tensor.
 
@@ -90,8 +88,8 @@
     Returns:
         A tuple containing the 3-dim shape.
 
     Examples:
         >>> over_batch_over_output_channels(torch.randn([2, 3, 4, 3]))
         (2, 3, -1)
     """
-    return x.shape[0], x.shape[1], -1
+    return x.shape[0], x.shape[1], -1
```

### Comparing `brevitas-0.8.0/src/brevitas/fx/backport/graph.py` & `brevitas-0.9.0/src/brevitas/fx/backport/graph.py`

 * *Files 2% similar despite different names*

```diff
@@ -37,29 +37,36 @@
 CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 POSSIBILITY OF SUCH DAMAGE.
 
 Forked as-is from PyTorch 1.8.1
 """
 
-from .node import Node, Argument, Target, map_arg
-
-from typing import Callable, Any, List, Dict, Optional, Tuple, Set
 import builtins
-import torch
-import types
 import keyword
 import re
+import types
+from typing import Any, Callable, Dict, List, Optional, Set, Tuple
+
+import torch
+
+from .node import Argument
+from .node import map_arg
+from .node import Node
+from .node import Target
+
 
 def _shadows_builtin_name(name: str) -> bool:
     return name in builtins.__dict__ or name in keyword.kwlist or name in {'inf', 'nan', 'NoneType'}
 
+
 def _is_magic(x: str) -> bool:
     return x.startswith('__') and x.endswith('__')
 
+
 def _snake_case(s: str) -> str:
     """
     Transforms the given string ``s`` to a Python-style variable name
 
     Examples:
         ``mod.snake_case`` -> ``mod.snake_case``
         ``mod.pascalCase``-> ``mod.pascal_case``
@@ -70,51 +77,57 @@
     for c in s:
         if prev_lower and c.isupper():
             chars.append('_')
         chars.append(c.lower())
         prev_lower = c.islower()
     return ''.join(chars)
 
+
 def get_qualified_name(func: Callable[..., Any]) -> str:
     # things like getattr just appear in builtins
     if getattr(builtins, func.__name__, None) is func:
         return func.__name__
     name = func.__name__
     module = _find_module_of_method(func)
-    module = module.replace('torch._ops', 'torch.ops')  # WAR for bug in how torch.ops assigns module
+    module = module.replace(
+        'torch._ops', 'torch.ops')  # WAR for bug in how torch.ops assigns module
     return f'{module}.{name}'
 
+
 # this is fixed on master, WAR for 1.5
 def _find_module_of_method(orig_method: Callable[..., Any]) -> str:
     name = orig_method.__name__
     module = orig_method.__module__
     if module is not None:
         return module
     for guess in [torch, torch.nn.functional]:
         if getattr(guess, name, None) is orig_method:
             return guess.__name__
     raise RuntimeError(f'cannot find module for {orig_method}')
 
+
 def _format_args(args: Tuple[Argument, ...], kwargs: Dict[str, Argument]) -> str:
     args_s = ', '.join(repr(a) for a in args)
     kwargs_s = ', '.join(f'{k} = {repr(v)}' for k, v in kwargs.items())
     if args_s and kwargs_s:
         return f'{args_s}, {kwargs_s}'
     return args_s or kwargs_s
 
+
 def _format_target(base: str, target: str) -> str:
     elems = target.split('.')
     r = base
     for e in elems:
         if not e.isidentifier():
             r = f'getattr({r}, "{e}")'
         else:
             r = f'{r}.{e}'
     return r
 
+
 # Borrowed from CPython typing module
 # https://github.com/python/cpython/blob/f90dc36c15d7fee0efaf6d39e97be0bdf2683e93/Lib/typing.py#L156
 def _type_repr(obj):
     """Return the repr() of an object, special-casing types (internal helper).
     If obj is a type, we return a shorter version than the default
     type.__repr__, based on the module and qualified name, which is
     typically enough to uniquely identify a type.  For everything
@@ -125,31 +138,35 @@
     # all type aliases to fall through to ``repr``, so if we have a type that is
     # in the module typing, don't go down this path.
     if isinstance(obj, type) and obj.__module__ != 'typing':
         if obj.__module__ == 'builtins':
             return obj.__qualname__
         return f'{obj.__module__}.{obj.__qualname__}'
     if obj is ...:
-        return('...')
+        return ('...')
     if isinstance(obj, types.FunctionType):
         return obj.__name__
     return repr(obj)
 
+
 class _InsertPoint:
+
     def __init__(self, graph, new_insert):
         self.graph = graph
         self.orig_insert, graph._insert = graph._insert, new_insert
 
     def __enter__(self):
         pass
 
     def __exit__(self, type, value, tb):
         self.graph._insert = self.orig_insert
 
+
 class _node_list:
+
     def __init__(self, graph: 'Graph', direction: str = '_next'):
         assert direction in ['_next', '_prev']
         self.graph = graph
         self.direction = direction
 
     def __len__(self):
         return self.graph._len
@@ -161,14 +178,15 @@
             if not cur._erased:
                 yield cur
             cur = getattr(cur, direction)
 
     def __reversed__(self):
         return _node_list(self.graph, '_next' if self.direction == '_prev' else '_prev')
 
+
 class Graph:
     """
     ``Graph`` is the main data structure used in the FX Intermediate Representation.
     It consists of a series of ``Node`` s, each representing callsites (or other
     syntactic constructs). The list of ``Node`` s, taken together, constitute a
     valid Python function.
 
@@ -204,20 +222,21 @@
             %relu_1 : [#users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})
             %sum_1 : [#users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})
             %topk_1 : [#users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})
             return topk_1
 
     For the semantics of operations represented in the ``Graph``, please see :class:`Node`.
     """
+
     def __init__(self):
         """
         Construct an empty Graph.
         """
-        self._root : Node = Node(self, '', 'root', '', (), {})
-        self._used_names : Dict[str, int] = {}  # base name -> number
+        self._root: Node = Node(self, '', 'root', '', (), {})
+        self._used_names: Dict[str, int] = {}  # base name -> number
         self._insert = self._root.prepend
         self._len = 0
 
     @property
     def nodes(self) -> _node_list:
         """
         Get the list of Nodes that constitute this Graph.
@@ -228,15 +247,15 @@
         Returns:
 
             A doubly-linked list of Nodes. Note that ``reversed`` can be called on
             this list to switch iteration order.
         """
         return _node_list(self)
 
-    def graph_copy(self, g : 'Graph', val_map : Dict[Node, Node]) -> 'Optional[Argument]':
+    def graph_copy(self, g: 'Graph', val_map: Dict[Node, Node]) -> 'Optional[Argument]':
         """
         Copy all nodes from a given graph into ``self``.
 
         Args:
 
             g (Graph): The source graph from which to copy Nodes.
 
@@ -251,15 +270,15 @@
         """
         for node in g.nodes:
             if node in val_map:
                 continue
             if node.op == 'output':
                 rv = map_arg(node.args[0], lambda n: val_map[n])
                 return rv
-            val_map[node] = self.node_copy(node, lambda n : val_map[n])
+            val_map[node] = self.node_copy(node, lambda n: val_map[n])
         return None
 
     def __deepcopy__(self, memo=None) -> 'Graph':
         """
         Explicitly implement __deepcopy__ to prevent excessive recursion depth
         from the default implementation. This uses graph_copy to copy the nodes
         in an iterative way, rather than recursive. It also populates the
@@ -268,19 +287,22 @@
         """
         memo = memo if memo else {}
         g = Graph()
         output_val = g.graph_copy(self, val_map=memo)
         g.output(output_val)
         return g
 
-    def create_node(self, op: str, target: 'Target',
-                    args: Optional[Tuple['Argument', ...]] = None,
-                    kwargs: Optional[Dict[str, 'Argument']] = None,
-                    name: Optional[str] = None,
-                    type_expr: Optional[Any] = None) -> Node:
+    def create_node(
+            self,
+            op: str,
+            target: 'Target',
+            args: Optional[Tuple['Argument', ...]] = None,
+            kwargs: Optional[Dict[str, 'Argument']] = None,
+            name: Optional[str] = None,
+            type_expr: Optional[Any] = None) -> Node:
         """
         Create a ``Node`` and add it to the ``Graph`` at the current insert-point.
         Note that the current insert-point can be set via :meth:`Graph.inserting_before`
         and :meth:`Graph.inserting_after`.
 
         Args:
             op (str): the opcode for this Node. One of 'call_function', 'call_method', 'get_attr',
@@ -298,37 +320,40 @@
             type_expr (Optional[Any]): an optional type annotation representing the
                 Python type the output of this node will have.
 
         Returns:
 
             The newly-created and inserted node.
         """
-        assert op in ('call_function', 'call_method', 'get_attr', 'call_module', 'placeholder', 'output')
+        assert op in (
+            'call_function', 'call_method', 'get_attr', 'call_module', 'placeholder', 'output')
         args = () if args is None else args
         kwargs = {} if kwargs is None else kwargs
         assert isinstance(args, tuple), "args must be a tuple"
         assert isinstance(kwargs, dict), "kwargs must be a dict"
-        unique_name = self._create_unique_name(name if name is not None else self._target_to_str(target))
+        unique_name = self._create_unique_name(
+            name if name is not None else self._target_to_str(target))
         n = Node(self, unique_name, op, target, args, kwargs, type_expr)
         self._insert(n)
         self._len += 1
         return n
 
-    def erase_node(self, to_erase : Node) -> None:
+    def erase_node(self, to_erase: Node) -> None:
         """
         Erases a ``Node`` from the ``Graph``. Throws an exception if
         there are still users of that node in the ``Graph``.
 
         Args:
 
             to_erase (Node): The ``Node`` to erase from the ``Graph``.
         """
         if len(to_erase.users) > 0:
-            raise RuntimeError(f'Tried to erase Node {to_erase} but it still had {len(to_erase.users)} '
-                               f'users in the graph: {to_erase.users}!')
+            raise RuntimeError(
+                f'Tried to erase Node {to_erase} but it still had {len(to_erase.users)} '
+                f'users in the graph: {to_erase.users}!')
 
         to_erase._remove_from_list()
         to_erase._erased = True  # iterators may retain handles to erased nodes
         self._len -= 1
 
         # Null out this Node's argument nodes so that the Nodes referred to
         # can update their ``users`` accordingly
@@ -427,19 +452,20 @@
 
         .. note::
             The same insertion point and type expression rules apply for this method
             as ``Graph.create_node``.
         """
         return self.create_node('get_attr', qualified_name, type_expr=type_expr)
 
-    def call_module(self,
-                    module_name: str,
-                    args: Optional[Tuple['Argument', ...]] = None,
-                    kwargs: Optional[Dict[str, 'Argument']] = None,
-                    type_expr: Optional[Any] = None) -> Node:
+    def call_module(
+            self,
+            module_name: str,
+            args: Optional[Tuple['Argument', ...]] = None,
+            kwargs: Optional[Dict[str, 'Argument']] = None,
+            type_expr: Optional[Any] = None) -> Node:
         """
         Insert a ``call_module`` ``Node`` into the ``Graph``. A ``call_module`` node
         represents a call to the forward() function of a ``Module`` in the ``Module``
         hierarchy.
 
         Args:
 
@@ -464,19 +490,20 @@
 
         .. note::
             The same insertion point and type expression rules apply for this method
             as :meth:`Graph.create_node`.
         """
         return self.create_node('call_module', module_name, args, kwargs, type_expr=type_expr)
 
-    def call_method(self,
-                    method_name: str,
-                    args: Optional[Tuple['Argument', ...]] = None,
-                    kwargs: Optional[Dict[str, 'Argument']] = None,
-                    type_expr: Optional[Any] = None) -> Node:
+    def call_method(
+            self,
+            method_name: str,
+            args: Optional[Tuple['Argument', ...]] = None,
+            kwargs: Optional[Dict[str, 'Argument']] = None,
+            type_expr: Optional[Any] = None) -> Node:
         """
         Insert a ``call_method`` ``Node`` into the ``Graph``. A ``call_method`` node
         represents a call to a given method on the 0th element of ``args``.
 
         Args:
 
             method_name (str): The name of the method to apply to the self argument.
@@ -498,19 +525,20 @@
 
         .. note::
             The same insertion point and type expression rules apply for this method
             as :meth:`Graph.create_node`.
         """
         return self.create_node('call_method', method_name, args, kwargs, type_expr=type_expr)
 
-    def call_function(self,
-                      the_function: Callable[..., Any],
-                      args: Optional[Tuple['Argument', ...]] = None,
-                      kwargs: Optional[Dict[str, 'Argument']] = None,
-                      type_expr: Optional[Any] = None) -> Node:
+    def call_function(
+            self,
+            the_function: Callable[..., Any],
+            args: Optional[Tuple['Argument', ...]] = None,
+            kwargs: Optional[Dict[str, 'Argument']] = None,
+            type_expr: Optional[Any] = None) -> Node:
         """
         Insert a ``call_function`` ``Node`` into the ``Graph``. A ``call_function`` node
         represents a call to a Python callable, specified by ``the_function``. ``the_function``
         can be
 
         Args:
 
@@ -533,15 +561,16 @@
 
         .. note::
             The same insertion point and type expression rules apply for this method
             as :meth:`Graph.create_node`.
         """
         return self.create_node('call_function', the_function, args, kwargs, type_expr=type_expr)
 
-    def node_copy(self, node: Node, arg_transform: Callable[[Node], 'Argument'] = lambda x: x) -> Node:
+    def node_copy(
+            self, node: Node, arg_transform: Callable[[Node], 'Argument'] = lambda x: x) -> Node:
         """
         Copy a node from one graph into another. ``arg_transform`` needs to transform arguments from
         the graph of node to the graph of self. Example::
 
             # Copying all the nodes in `g` into `new_graph`
             g : torch.fx.Graph = ...
             new_graph = torch.fx.graph()
@@ -581,32 +610,32 @@
         .. note::
 
             The same insertion point and type expression rules apply for this method
             as ``Graph.create_node``.
         """
         return self.create_node(op='output', target='output', args=(result,), type_expr=type_expr)
 
-    def _target_to_str(self, target : Target) -> str:
+    def _target_to_str(self, target: Target) -> str:
         if callable(target):
             op = target.__name__
         else:
             assert isinstance(target, str)
             op = target
             if _is_magic(op):
                 op = op[2:-2]
         op = _snake_case(op)
         return op
 
-    def _create_unique_name(self, candidate : str) -> str:
+    def _create_unique_name(self, candidate: str) -> str:
         # delete all characters that are illegal in a Python identifier
         candidate = re.sub('[^0-9a-zA-Z_]+', '_', candidate)
         if candidate[0].isdigit():
             candidate = f'_{candidate}'
 
-        def illegal_shadowing_name(name : str) -> bool:
+        def illegal_shadowing_name(name: str) -> bool:
             return hasattr(torch, name) or \
                 hasattr(torch.nn.functional, name) or \
                 hasattr(torch.nn, name) or \
                 _shadows_builtin_name(name)
 
         while candidate in self._used_names or illegal_shadowing_name(candidate):
             match = re.match(r"(.*)_(\d+)$", candidate)
@@ -629,55 +658,54 @@
                 qualified name targets. This is usually 'self'.
 
         Returns:
 
             The string source code generated from this ``Graph``.
         """
         free_vars: List[str] = []
-        modules_used : Set[str] = set()
+        modules_used: Set[str] = set()
         body: List[str] = []
 
         # Wrap string in list to pass by reference
-        maybe_return_annotation : List[str] = ['']
+        maybe_return_annotation: List[str] = ['']
 
-        def register_modules_used(qualified_name : str):
+        def register_modules_used(qualified_name: str):
             if '.' in qualified_name:
                 module_name = qualified_name.split('.', maxsplit=1)[0]
                 modules_used.add(module_name)
 
-        def type_repr(o : Any):
+        def type_repr(o: Any):
             typename = _type_repr(o)
             if all(x.isidentifier() for x in typename.split('.')):
                 register_modules_used(typename)
             else:
                 # this is a constructor type, e.g. typing.List[torch.Tensor]
                 modules_used.add(o.__module__)
                 for sub_type in o.__args__:
                     # make sure we have torch.Tensor
                     type_repr(sub_type)
             return typename
 
-
         # Run through reverse nodes and record the first instance of a use
         # of a given node. This represents the *last* use of the node in the
         # execution order of the program, which we will use to free unused
         # values
-        node_to_last_use : Dict[Node, Node] = {}
-        user_to_last_uses : Dict[Node, List[Node]] = {}
+        node_to_last_use: Dict[Node, Node] = {}
+        user_to_last_uses: Dict[Node, List[Node]] = {}
 
-        def register_last_uses(n : Node, user : Node):
+        def register_last_uses(n: Node, user: Node):
             if n not in node_to_last_use:
                 node_to_last_use[n] = user
                 user_to_last_uses.setdefault(user, []).append(n)
 
         for node in reversed(self.nodes):
             map_arg(node.args, lambda n: register_last_uses(n, node))
             map_arg(node.kwargs, lambda n: register_last_uses(n, node))
 
-        def delete_unused_values(user : Node):
+        def delete_unused_values(user: Node):
             """
             Delete values after their last use. This ensures that values that are
             not used in the remainder of the code are freed and the memory usage
             of the code is optimal.
             """
             if user.op == 'placeholder':
                 return
@@ -687,15 +715,15 @@
             nodes_to_delete = user_to_last_uses.get(user, [])
             if len(nodes_to_delete):
                 to_delete_str = ' = '.join([n.name for n in nodes_to_delete] + ['None'])
                 body.append(f';  {to_delete_str}\n')
             else:
                 body.append('\n')
 
-        def emit_node(node : Node):
+        def emit_node(node: Node):
             if node.op == 'placeholder':
                 assert isinstance(node.target, str)
                 maybe_type_annotation = '' if node.type is None else f' : {type_repr(node.type)}'
                 maybe_default_arg = '' if not node.args else f' = {repr(node.args[0])}'
                 free_vars.append(f'{node.target}{maybe_type_annotation}{maybe_default_arg}')
                 raw_name = node.target.replace('*', '')
                 if raw_name != node.name:
@@ -708,30 +736,35 @@
                     f'({_format_args(node.args[1:], node.kwargs)})')
                 return
             elif node.op == 'call_function':
                 assert callable(node.target)
                 # pretty print operators
                 if node.target.__module__ == '_operator' and node.target.__name__ in magic_methods:
                     assert isinstance(node.args, tuple)
-                    body.append(f'{node.name} = {magic_methods[node.target.__name__].format(*(repr(a) for a in node.args))}')
+                    body.append(
+                        f'{node.name} = {magic_methods[node.target.__name__].format(*(repr(a) for a in node.args))}'
+                    )
                     return
                 qualified_name = get_qualified_name(node.target)
                 register_modules_used(qualified_name)
                 if qualified_name == 'getattr' and \
                    isinstance(node.args, tuple) and \
                    isinstance(node.args[1], str) and \
                    node.args[1].isidentifier():
                     # pretty print attribute access
                     body.append(f'{node.name} = {_format_target(repr(node.args[0]), node.args[1])}')
                     return
-                body.append(f'{node.name} = {qualified_name}({_format_args(node.args, node.kwargs)})')
+                body.append(
+                    f'{node.name} = {qualified_name}({_format_args(node.args, node.kwargs)})')
                 return
             elif node.op == 'call_module':
                 assert isinstance(node.target, str)
-                body.append(f'{node.name} = {_format_target(root_module, node.target)}({_format_args(node.args, node.kwargs)})')
+                body.append(
+                    f'{node.name} = {_format_target(root_module, node.target)}({_format_args(node.args, node.kwargs)})'
+                )
                 return
             elif node.op == 'get_attr':
                 assert isinstance(node.target, str)
                 body.append(f'{node.name} = {_format_target(root_module, node.target)}')
                 return
             elif node.op == 'output':
                 if node.type is not None:
@@ -767,18 +800,18 @@
         return fn_code
 
     def __str__(self) -> str:
         """
         Print a human-readable (not machine-readable) string representation
         of this Graph
         """
-        placeholder_names : List[str] = []
+        placeholder_names: List[str] = []
         # This is a one-element array just so ``format_node`` can modify the closed
         # over value
-        maybe_return_typename : List[str] = ['']
+        maybe_return_typename: List[str] = ['']
 
         def format_arg(arg) -> str:
             if isinstance(arg, list):
                 items = ', '.join(format_arg(a) for a in arg)
                 return f'[{items}]'
             elif isinstance(arg, tuple):
                 items = ', '.join(format_arg(a) for a in arg)
@@ -811,15 +844,15 @@
                     return get_qualified_name(target)
                 if target.__module__ == 'builtins':
                     return f'builtins.{target.__name__}'
                 elif target.__module__ == '_operator':
                     return f'operator.{target.__name__}'
             return get_qualified_name(target)
 
-        def format_node(n : Node) -> Optional[str]:
+        def format_node(n: Node) -> Optional[str]:
             if n.op == 'placeholder':
                 assert isinstance(n.target, str)
                 arg_str = n.target
                 arg_str += arg_str + f': {_type_repr(n.type)}' if n.type is not None else ''
                 placeholder_names.append(arg_str)
                 return None
             elif n.op == 'get_attr':
@@ -830,15 +863,14 @@
                     maybe_return_typename[0] = f' -> {_type_repr(n.type)}'
                 return f'return {n.args[0]}'
             else:
                 maybe_typename = f'{_type_repr(n.type)} ' if n.type is not None else ''
                 return f'%{n.name} : {maybe_typename}[#users={len(n.users)}] = {n.op}[target={pretty_print_target(n.target)}](' \
                        f'args = {format_arg(n.args)}, kwargs = {format_arg(n.kwargs)})'
 
-
         node_strs = [format_node(node) for node in self.nodes]
         param_str = ', '.join(placeholder_names)
         s = f'graph({param_str}){maybe_return_typename[0]}:'
         for node_str in node_strs:
             if node_str:
                 s += '\n    ' + node_str
         return s
@@ -847,23 +879,22 @@
         """
         Prints the intermediate representation of the graph in tabular
         format.
         """
         try:
             from tabulate import tabulate
         except ImportError:
-            print("`print_tabular` relies on the library `tabulate`, "
-                  "which could not be found on this machine. Run `pip "
-                  "install tabulate` to install the library.")
-        node_specs = [[n.op, n.name, n.target, n.args, n.kwargs]
-                      for n in self.nodes]
-        print(tabulate(node_specs,
-              headers=['opcode', 'name', 'target', 'args', 'kwargs']))
+            print(
+                "`print_tabular` relies on the library `tabulate`, "
+                "which could not be found on this machine. Run `pip "
+                "install tabulate` to install the library.")
+        node_specs = [[n.op, n.name, n.target, n.args, n.kwargs] for n in self.nodes]
+        print(tabulate(node_specs, headers=['opcode', 'name', 'target', 'args', 'kwargs']))
 
-    def lint(self, root : Optional[torch.nn.Module] = None):
+    def lint(self, root: Optional[torch.nn.Module] = None):
         """
         Runs various checks on this Graph to make sure it is well-formed. In
         particular:
         - Checks Nodes have correct ownership (owned by this graph)
         - Checks Nodes appear in topological order
         - If ``root`` is provided, checks that targets exist in ``root``
 
@@ -871,28 +902,36 @@
 
             root (Optional[torch.nn.Module]): The root module with which to check
                 for targets. This is equivalent to the ``root`` argument that is
                 passed when constructing a ``GraphModule``.
         """
 
         # Check topo order
-        def check_arg(arg : Node, n : Optional[Node] = None) -> None:
+        def check_arg(arg: Node, n: Optional[Node] = None) -> None:
             context_str = f' of Node \'{n}\' ' if n else ' '
             if arg.graph is not self:
-                raise RuntimeError(f'Argument \'{arg}\'{context_str}does not belong to this Graph, '
-                                   f'but was used as an argument! If you are copying nodes from another graph, make '
-                                   f'sure to use ``arg_transform`` on node_copy() to remap values\n{self}')
+                raise RuntimeError(
+                    f'Argument \'{arg}\'{context_str}does not belong to this Graph, '
+                    f'but was used as an argument! If you are copying nodes from another graph, make '
+                    f'sure to use ``arg_transform`` on node_copy() to remap values\n{self}')
             if arg not in seen_values:
-                raise RuntimeError(f'Argument \'{arg}\'{context_str}was used before it has been '
-                                   f'defined! Please check that Nodes in the graph are topologically ordered\n{self}')
+                raise RuntimeError(
+                    f'Argument \'{arg}\'{context_str}was used before it has been '
+                    f'defined! Please check that Nodes in the graph are topologically ordered\n{self}'
+                )
 
-        seen_names : Set[str] = set()
-        seen_values : Set[Node] = set()
+        seen_names: Set[str] = set()
+        seen_values: Set[Node] = set()
         for node in self.nodes:
-            if node.op not in ['placeholder', 'call_method', 'call_module', 'call_function', 'get_attr', 'output']:
+            if node.op not in ['placeholder',
+                               'call_method',
+                               'call_module',
+                               'call_function',
+                               'get_attr',
+                               'output']:
                 raise RuntimeError(f'Node {node} had unknown opcode {node.op}!')
             if node.graph is not self:
                 raise RuntimeError(f'Node \'{node}\' does not belong to this Graph!')
             map_arg(node.args, lambda arg: check_arg(arg, node))
             map_arg(node.kwargs, lambda arg: check_arg(arg, node))
             seen_values.add(node)
 
@@ -907,16 +946,17 @@
                     assert isinstance(node.target, str)
                     target_atoms = node.target.split('.')
                     m_itr = root
                     for i, atom in enumerate(target_atoms):
                         m_itr = getattr(m_itr, atom, None)
                         if m_itr is None:
                             seen_qualname = '.'.join(target_atoms[:i])
-                            raise RuntimeError(f'Node {node} target {node.target} references nonexistent attribute '
-                                               f'{atom} of {seen_qualname}')
+                            raise RuntimeError(
+                                f'Node {node} target {node.target} references nonexistent attribute '
+                                f'{atom} of {seen_qualname}')
 
 
 reflectable_magic_methods = {
     'add': '{} + {}',
     'sub': '{} - {}',
     'mul': '{} * {}',
     'floordiv': '{} // {}',
@@ -925,20 +965,20 @@
     'mod': '{} % {}',
     'pow': '{} ** {}',
     'lshift': '{} << {}',
     'rshift': '{} >> {}',
     'and': '{} & {}',
     'or': '{} | {}',
     'xor': '{} ^ {}',
-    'getitem': '{}[{}]'
-}
+    'getitem': '{}[{}]'}
 
 magic_methods = dict({
     'eq': '{} == {}',
     'ne': '{} != {}',
     'lt': '{} < {}',
     'gt': '{} > {}',
     'le': '{} <= {}',
     'ge': '{} >= {}',
     'pos': '+{}',
     'neg': '-{}',
-    'invert': '~{}'}, **reflectable_magic_methods)
+    'invert': '~{}'},
+                     **reflectable_magic_methods)
```

### Comparing `brevitas-0.8.0/src/brevitas/fx/backport/graph_module.py` & `brevitas-0.9.0/src/brevitas/fx/backport/graph_module.py`

 * *Files 0% similar despite different names*

```diff
@@ -37,27 +37,30 @@
 CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 POSSIBILITY OF SUCH DAMAGE.
 
 Forked as-is from PyTorch 1.8.1
 """
 
-import torch
-import torch.nn as nn
-import linecache
-from typing import Type, Dict, List, Any, Union, Optional
-from .graph import Graph
 import copy
-import sys
-import traceback
+import linecache
 import math
-from pathlib import Path
 import os
+from pathlib import Path
+import sys
+import traceback
+from typing import Any, Dict, List, Optional, Type, Union
 import warnings
 
+import torch
+import torch.nn as nn
+
+from .graph import Graph
+
+
 # Forked from torch/nn/modules/module.py
 def _addindent(s_, numSpaces):
     s = s_.split('\n')
     # don't do anything for single-line stuff
     if len(s) == 1:
         return s_
     first = s.pop(0)
@@ -68,48 +71,58 @@
 
 
 # normal exec loses the source code, however we can patch
 # the linecache module to still recover it.
 # using exec_with_source will add it to our local cache
 # and then tools like TorchScript will be able to get source info.
 _next_id = 0
+
+
 def exec_with_source(src: str, globals: Dict[str, Any]):
     global _next_id
     key = f'<eval_with_key_{_next_id}>'
     _next_id += 1
     _eval_cache[key] = [line + '\n' for line in src.splitlines()]
     exec(compile(src, key, 'exec'), globals)
 
+
 # patch linecache so that any code we exec using exec_with_source
 # works with inspect
-_eval_cache : Dict[str, List[str]] = {}
+_eval_cache: Dict[str, List[str]] = {}
 _orig_getlines = linecache.getlines
+
+
 def patched_getline(*args, **kwargs):
     if args[0] in _eval_cache:
         return _eval_cache[args[0]]
     return _orig_getlines(*args, **kwargs)
+
+
 linecache.getlines = patched_getline
 
-def _forward_from_src(src : str):
+
+def _forward_from_src(src: str):
     # If you add more globals here, remember to add their names to fx.graph._shadows_builtin_name!
-    gbls: Dict[str, Any] = {'inf': math.inf, 'nan': math.nan, 'NoneType' : type(None)}
+    gbls: Dict[str, Any] = {'inf': math.inf, 'nan': math.nan, 'NoneType': type(None)}
     exec_with_source(src, gbls)
     return gbls['forward']
 
 
-def deserialize_graphmodule(body : dict) -> torch.nn.Module:
+def deserialize_graphmodule(body: dict) -> torch.nn.Module:
     """
     Deserialize a GraphModule given the dictionary of the original module,
     using the code to reconstruct the graph. We delete the actual graph before
     saving the dictionary so that changes to the in-memory graph format do not
     get serialized.
     """
+
     # We create a dummy class here because symbolic_trace pulls the forward()
     # function off of the class, rather than the instance
     class CodeOnlyModule(torch.nn.Module):
+
         def __init__(self, body):
             super().__init__()
             self.__dict__ = body
 
     try:
         CodeOnlyModule.forward = _forward_from_src(body['_code'])
     except KeyError:
@@ -118,20 +131,22 @@
         CodeOnlyModule.forward = _forward_from_src(body['code'])
 
     from .symbolic_trace import Tracer
 
     # we shouldn't trace into any of the submodules, they were not
     # because they were not traced in the original GraphModule
     class KeepModules(Tracer):
+
         def is_leaf_module(self, _: torch.nn.Module, __: str) -> bool:
             return True
 
     com = CodeOnlyModule(body)
     return GraphModule(com, KeepModules().trace(com))
 
+
 # copy an attribute value with qualified name 'target' from 'from_module' to 'to_module'
 # This installs empty Modules where none exist yet if they are subpaths of target
 def _copy_attr(from_module: torch.nn.Module, to_module: torch.nn.Module, target: str):
     *prefix, field = target.split('.')
     for item in prefix:
         f = getattr(from_module, item)
         t = getattr(to_module, item, None)
@@ -166,39 +181,46 @@
         if t is None:
             t = torch.nn.Module()
             setattr(to_module, item, t)
         to_module = t
 
     setattr(to_module, field, from_obj)
 
+
 class GraphModule(torch.nn.Module):
     """
     GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a
     ``graph`` attribute, as well as ``code`` and ``forward`` attributes generated
     from that ``graph``.
 
     .. warning::
 
         When ``graph`` is reassigned, ``code`` and ``forward`` will be automatically
         regenerated. However, if you edit the contents of the ``graph`` without reassigning
         the ``graph`` attribute itself, you must call ``recompile()`` to update the generated
         code.
 
     """
+
     def __new__(cls: 'Type[GraphModule]', *args, **kwargs):
         # each instance of a graph module needs its own forward method
         # so create a new singleton class for each instance.
         # it is a subclass of the user-defined class, the only difference
         # is an extra layer to install the forward method
 
         class GraphModuleImpl(cls):  # type: ignore
             pass
+
         return super().__new__(GraphModuleImpl)
 
-    def __init__(self, root: Union[torch.nn.Module, Dict[str, Any]], graph: Graph, class_name: str = 'GraphModule'):
+    def __init__(
+            self,
+            root: Union[torch.nn.Module, Dict[str, Any]],
+            graph: Graph,
+            class_name: str = 'GraphModule'):
         """
         Construct a GraphModule.
 
         Args:
 
             root (Union[torch.nn.Module, Dict[str, Any]):
                 ``root`` can either be an nn.Module instance or a Dict mapping strings to any attribute type.
@@ -227,16 +249,17 @@
                     _copy_attr(root, self, node.target)
         elif isinstance(root, dict):
             targets_to_copy = []
             for node in graph.nodes:
                 if node.op in ['get_attr', 'call_module']:
                     assert isinstance(node.target, str)
                     if node.target not in root:
-                        raise RuntimeError('Node ' + str(node) + ' referenced target ' + node.target +
-                                           ' but that target was not provided in ``root``!')
+                        raise RuntimeError(
+                            'Node ' + str(node) + ' referenced target ' + node.target +
+                            ' but that target was not provided in ``root``!')
                     targets_to_copy.append(node.target)
             # Sort targets in ascending order of the # of atoms.
             # This will ensure that less deeply nested attributes are assigned
             # before more deeply nested attributes. For example, foo.bar
             # will be assigned before foo.bar.baz. Otherwise, we might assign
             # the user-provided ``foo.bar`` and wipe out the previously-assigned
             # ``foo.bar.baz``
@@ -266,15 +289,15 @@
         Set the underlying ``Graph`` for this ``GraphModule``. This will internally
         recompile the ``GraphModule`` so that the generated ``forward()`` function
         corresponds to ``g``
         """
         self._graph = g
         self.recompile()
 
-    def to_folder(self, folder: Union[str, os.PathLike], module_name : str = "FxModule"):
+    def to_folder(self, folder: Union[str, os.PathLike], module_name: str = "FxModule"):
         """Dumps out module to ``folder`` with ``module_name`` so that it can be
         imported with ``from <folder> import <module_name>``
 
         Args:
 
             folder (Union[str, os.PathLike]): The folder to write the code out to
 
@@ -290,15 +313,22 @@
 from torch.nn import *
 class {module_name}(torch.nn.Module):
     def __init__(self):
         super().__init__()
 """
 
         def _gen_model_repr(module_name: str, module: torch.nn.Module) -> Optional[str]:
-            safe_reprs = [nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d]
+            safe_reprs = [
+                nn.Linear,
+                nn.Conv1d,
+                nn.Conv2d,
+                nn.Conv3d,
+                nn.BatchNorm1d,
+                nn.BatchNorm2d,
+                nn.BatchNorm3d]
             if type(module) in safe_reprs:
                 return f"{module.__repr__()}"
             else:
                 return None
 
         blobified_modules = []
         for module_name, module in self.named_children():
@@ -323,16 +353,17 @@
         module_file = folder / 'module.py'
         module_file.write_text(model_str)
 
         init_file = folder / '__init__.py'
         init_file.write_text('from .module import *')
 
         if len(blobified_modules) > 0:
-            warnings.warn("Was not able to save the following children modules as reprs -"
-                          f"saved as pickled files instead: {blobified_modules}")
+            warnings.warn(
+                "Was not able to save the following children modules as reprs -"
+                f"saved as pickled files instead: {blobified_modules}")
 
     @property
     def code(self) -> str:
         """
         Return the Python code generated from the ``Graph`` underlying this
         ``GraphModule``.
         """
@@ -358,14 +389,15 @@
         def wrapped_call(self, *args, **kwargs):
             old_excepthook = sys.excepthook
             try:
                 sys.excepthook = print_full_traceback
                 return cls_call(self, *args, **kwargs)
             finally:
                 sys.excepthook = old_excepthook
+
         cls.__call__ = wrapped_call
 
     def __reduce__(self):
         """
         Serialization of GraphModule. We serialize only the generated code, not
         the underlying ``Graph``. This is because ``Graph`` does not have on-disk
         backward-compatibility guarantees, whereas Python source code does.
@@ -387,14 +419,15 @@
     def __copy__(self):
         return GraphModule(self, self.graph)
 
     def __str__(self) -> str:
         orig_str = super().__str__()
         return '\n'.join([orig_str, self._code])
 
+
 # workarounds for issues in __torch_function__
 
 # WAR for __torch_function__ not handling tensor lists,
 # fix is in https://github.com/pytorch/pytorch/pull/34725
 # orig_cat = torch.cat
 # def patched_cat(*args, **kwargs):
 #     tensors = args[0]
```

### Comparing `brevitas-0.8.0/src/brevitas/fx/backport/immutable_collections.py` & `brevitas-0.9.0/src/brevitas/fx/backport/immutable_collections.py`

 * *Files 8% similar despite different names*

```diff
@@ -44,23 +44,38 @@
 _help_mutation = """\
 If you are attempting to modify the kwargs or args of a torch.fx.Node object,
 instead create a new copy of it and assign the copy to the node:
     new_args = ... # copy and mutate args
     node.args = new_args
 """
 
+
 def _no_mutation(self, *args, **kwargs):
-    raise NotImplementedError(f"'{type(self).__name__}' object does not support mutation. {_help_mutation}")
+    raise NotImplementedError(
+        f"'{type(self).__name__}' object does not support mutation. {_help_mutation}")
+
 
 def _create_immutable_container(base, mutable_functions):
     container = type('immutable_' + base.__name__, (base,), {})
     for attr in mutable_functions:
         setattr(container, attr, _no_mutation)
     return container
 
-immutable_list = _create_immutable_container(list,
-                                             ['__delitem__', '__iadd__', '__imul__', '__setitem__', 'append',
-                                              'clear', 'extend', 'insert', 'pop', 'remove'])
+
+immutable_list = _create_immutable_container(
+    list,
+    [
+        '__delitem__',
+        '__iadd__',
+        '__imul__',
+        '__setitem__',
+        'append',
+        'clear',
+        'extend',
+        'insert',
+        'pop',
+        'remove'])
 immutable_list.__reduce__ = lambda self: (immutable_list, (tuple(iter(self)),))
 
-immutable_dict = _create_immutable_container(dict, ['__delitem__', '__setitem__', 'clear', 'pop', 'popitem', 'update'])
+immutable_dict = _create_immutable_container(
+    dict, ['__delitem__', '__setitem__', 'clear', 'pop', 'popitem', 'update'])
 immutable_dict.__reduce__ = lambda self: (immutable_dict, (iter(self.items()),))
```

### Comparing `brevitas-0.8.0/src/brevitas/fx/backport/interpreter.py` & `brevitas-0.9.0/src/brevitas/fx/backport/interpreter.py`

 * *Files 2% similar despite different names*

```diff
@@ -37,20 +37,25 @@
 CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 POSSIBILITY OF SUCH DAMAGE.
 
 Forked as-is from PyTorch 1.8.1
 """
 
-from .graph_module import GraphModule
+from typing import Any, Dict, Iterator, Optional, Tuple
+
 from .graph import Graph
-from .node import Argument, Node, Target, map_arg
+from .graph_module import GraphModule
+from .node import Argument
+from .node import map_arg
+from .node import Node
+from .node import Target
 from .proxy import Proxy
 from .symbolic_trace import Tracer
-from typing import Any, Dict, Iterator, Optional, Tuple
+
 
 class Interpreter:
     """
     An Interpreter executes an FX graph Node-by-Node. This pattern
     can be useful for many things, including writing code
     transformations as well as analysis passes.
 
@@ -94,21 +99,22 @@
             input = torch.randn(3, 4)
             result = NegSigmSwapInterpreter(gm).run(input)
             torch.testing.assert_allclose(result, torch.neg(input).sigmoid())
 
     Args:
         module (GraphModule): The module to be executed
     """
-    def __init__(self, module : GraphModule):
+
+    def __init__(self, module: GraphModule):
         assert isinstance(module, GraphModule)
         self.module = module
         self.submodules = dict(self.module.named_modules())
-        self.env : Dict[Node, Any] = {}
+        self.env: Dict[Node, Any] = {}
 
-    def run(self, *args, initial_env : Optional[Dict[Node, Any]] = None) -> Any:
+    def run(self, *args, initial_env: Optional[Dict[Node, Any]] = None) -> Any:
         """
         Run `module` via interpretation and return the result.
 
         Args:
             *args: The arguments to the Module to run, in positional order
             initial_env (Optional[Dict[Node, Any]]): An optional starting environment for execution.
                 This is a dict mapping `Node` to any value. This can be used, for example, to
@@ -119,15 +125,15 @@
             Any: The value returned from executing the Module
         """
         self.env = initial_env if initial_env else {}
 
         # Positional function args are consumed left-to-right by
         # `placeholder` nodes. Use an iterator to keep track of
         # position and extract those values.
-        self.args_iter : Iterator[Any] = iter(args)
+        self.args_iter: Iterator[Any] = iter(args)
 
         for node in self.module.graph.nodes:
             if node in self.env:
                 # Short circuit if we have this value. This could
                 # be used, for example, for partial evaluation
                 # where the caller has pre-populated `env` with
                 # values for a subset of the program.
@@ -135,15 +141,15 @@
 
             self.env[node] = self.run_node(node)
 
             if node.op == 'output':
                 output_val = self.env[node]
                 return output_val
 
-    def run_node(self, n : Node) -> Any:
+    def run_node(self, n: Node) -> Any:
         """
         Run a specific node ``n`` and return the result.
         Calls into placeholder, get_attr, call_function,
         call_method, call_module, or output depending
         on ``node.op``
 
         Args:
@@ -155,15 +161,16 @@
         args, kwargs = self.fetch_args_kwargs_from_env(n)
         assert isinstance(args, tuple)
         assert isinstance(kwargs, dict)
         return getattr(self, n.op)(n.target, args, kwargs)
 
     # Main Node running APIs
 
-    def placeholder(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:
+    def placeholder(
+            self, target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) -> Any:
         """
         Execute a ``placeholder`` node. Note that this is stateful:
         ``Interpreter`` maintains an internal iterator over
         arguments passed to ``run`` and this method returns
         next() on that iterator.
 
         Args:
@@ -180,15 +187,15 @@
         if target.startswith('*'):
             # For a starred parameter e.g. `*args`, retrieve all
             # remaining values from the args list.
             return list(self.args_iter)
         else:
             return next(self.args_iter)
 
-    def get_attr(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:
+    def get_attr(self, target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) -> Any:
         """
         Execute a ``get_attr`` node. Will retrieve an attribute
         value from the ``Module`` hierarchy of ``self.module``.
 
         Args:
             target (Target): The call target for this node. See
                 `Node <https://pytorch.org/docs/master/fx.html#torch.fx.Node>`__ for
@@ -198,15 +205,16 @@
 
         Return:
             Any: The value of the attribute that was retrieved
         """
         assert isinstance(target, str)
         return self.fetch_attr(target)
 
-    def call_function(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:
+    def call_function(
+            self, target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) -> Any:
         """
         Execute a ``call_function`` node and return the result.
 
         Args:
             target (Target): The call target for this node. See
                 `Node <https://pytorch.org/docs/master/fx.html#torch.fx.Node>`__ for
                 details on semantics
@@ -217,15 +225,16 @@
             Any: The value returned by the function invocation
         """
         assert not isinstance(target, str)
 
         # Execute the function and return the result
         return target(*args, **kwargs)
 
-    def call_method(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:
+    def call_method(
+            self, target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) -> Any:
         """
         Execute a ``call_method`` node and return the result.
 
         Args:
             target (Target): The call target for this node. See
                 `Node <https://pytorch.org/docs/master/fx.html#torch.fx.Node>`__ for
                 details on semantics
@@ -238,15 +247,16 @@
         # args[0] is the `self` object for this method call
         self_obj, *args_tail = args  # type: ignore
 
         # Execute the method and return the result
         assert isinstance(target, str)
         return getattr(self_obj, target)(*args_tail, **kwargs)
 
-    def call_module(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:
+    def call_module(
+            self, target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) -> Any:
         """
         Execute a ``call_module`` node and return the result.
 
         Args:
             target (Target): The call target for this node. See
                 `Node <https://pytorch.org/docs/master/fx.html#torch.fx.Node>`__ for
                 details on semantics
@@ -260,15 +270,15 @@
 
         # Execute the method and return the result
         assert isinstance(target, str)
         submod = self.fetch_attr(target)
 
         return submod(*args, **kwargs)
 
-    def output(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:
+    def output(self, target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) -> Any:
         """
         Execute an ``output`` node. This really just retrieves
         the value referenced by the ``output`` node and returns it.
 
         Args:
             target (Target): The call target for this node. See
                 `Node <https://pytorch.org/docs/master/fx.html#torch.fx.Node>`__ for
@@ -279,33 +289,34 @@
         Return:
             Any: The return value referenced by the output node
         """
         return args[0]
 
     # Helper methods
 
-    def fetch_attr(self, target : str):
+    def fetch_attr(self, target: str):
         """
         Fetch an attribute from the ``Module`` hierarchy of ``self.module``.
 
         Args:
             target (str): The fully-qualfiied name of the attribute to fetch
 
         Return:
             Any: The value of the attribute.
         """
         target_atoms = target.split('.')
         attr_itr = self.module
         for i, atom in enumerate(target_atoms):
             if not hasattr(attr_itr, atom):
-                raise RuntimeError(f"Node referenced nonexistent target {'.'.join(target_atoms[:i])}")
+                raise RuntimeError(
+                    f"Node referenced nonexistent target {'.'.join(target_atoms[:i])}")
             attr_itr = getattr(attr_itr, atom)
         return attr_itr
 
-    def fetch_args_kwargs_from_env(self, n : Node) -> Tuple[Tuple, Dict]:
+    def fetch_args_kwargs_from_env(self, n: Node) -> Tuple[Tuple, Dict]:
         """
         Fetch the concrete values of ``args`` and ``kwargs`` of node ``n``
         from the current execution environment.
 
         Args:
             n (Node): The node for which ``args`` and ``kwargs`` should be fetched.
 
@@ -314,31 +325,35 @@
         """
         args = self.map_nodes_to_values(n.args, n)
         assert isinstance(args, tuple)
         kwargs = self.map_nodes_to_values(n.kwargs, n)
         assert isinstance(kwargs, dict)
         return args, kwargs
 
-    def map_nodes_to_values(self, args : Argument, n : Node) -> Argument:
+    def map_nodes_to_values(self, args: Argument, n: Node) -> Argument:
         """
         Recursively descend through ``args`` and look up the concrete value
         for each ``Node`` in the current execution environment.
 
         Args:
             args (Argument): Data structure within which to look up concrete values
 
             n (Node): Node to which ``args`` belongs. This is only used for error reporting.
         """
-        def load_arg(n_arg : Node) -> Any:
+
+        def load_arg(n_arg: Node) -> Any:
             if n_arg not in self.env:
-                raise RuntimeError(f'Node {n} referenced nonexistent value {n_arg}! Run Graph.lint() '
-                                   f'to diagnose such issues')
+                raise RuntimeError(
+                    f'Node {n} referenced nonexistent value {n_arg}! Run Graph.lint() '
+                    f'to diagnose such issues')
             return self.env[n_arg]
+
         return map_arg(args, load_arg)
 
+
 class Transformer(Interpreter):
     """
     ``Transformer`` is a special type of interpreter that produces a
     new ``Module``. It exposes a ``transform()`` method that returns
     the transformed ``Module``. ``Transformer`` does not require
     arguments to run, as ``Interpreter`` does. ``Transformer`` works
     entirely symbolically.
@@ -369,29 +384,33 @@
             transformed : torch.nn.Module = NegSigmSwapXformer(gm).transform()
             input = torch.randn(3, 4)
             torch.testing.assert_allclose(transformed(input), torch.neg(input).sigmoid())
 
     Args:
         module (GraphModule): The ``Module`` to be transformed.
     """
+
     def __init__(self, module):
         super().__init__(module)
         self.new_graph = Graph()
 
         class TransformerTracer(Tracer):
+
             def __init__(self, graph: Graph):
                 super().__init__()
                 self.graph = graph
 
             def is_leaf_module(self, _, __) -> bool:
                 return True
+
         self.tracer = TransformerTracer(self.new_graph)
         self.tracer.root = module
 
-    def placeholder(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Proxy:
+    def placeholder(
+            self, target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) -> Proxy:
         """
         Execute a ``placeholder`` node. In ``Transformer``, this is
         overridden to insert a new ``placeholder`` into the output
         graph.
 
         Args:
             target (Target): The call target for this node. See
@@ -399,15 +418,16 @@
                 details on semantics
             args (Tuple): Tuple of positional args for this invocation
             kwargs (Dict): Dict of keyword arguments for this invocation
         """
         assert isinstance(target, str)
         return Proxy(self.new_graph.placeholder(target), self.tracer)
 
-    def get_attr(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Proxy:
+    def get_attr(
+            self, target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) -> Proxy:
         """
         Execute a ``get_attr`` node. In ``Transformer``, this is
         overridden to insert a new ``get_attr`` node into the output
         graph.
 
         Args:
             target (Target): The call target for this node. See
@@ -415,15 +435,16 @@
                 details on semantics
             args (Tuple): Tuple of positional args for this invocation
             kwargs (Dict): Dict of keyword arguments for this invocation
         """
         assert isinstance(target, str)
         return Proxy(self.new_graph.get_attr(target), self.tracer)
 
-    def call_module(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:
+    def call_module(
+            self, target: 'Target', args: Tuple[Argument, ...], kwargs: Dict[str, Any]) -> Any:
         # Override so that the leaf module policy from `self.tracer` is respected.
         assert isinstance(target, str)
         submod = self.fetch_attr(target)
         return self.tracer.call_module(submod, submod.forward, args, kwargs)
 
     def transform(self) -> GraphModule:
         """
```

### Comparing `brevitas-0.8.0/src/brevitas/fx/backport/node.py` & `brevitas-0.9.0/src/brevitas/fx/backport/node.py`

 * *Files 1% similar despite different names*

```diff
@@ -38,34 +38,37 @@
 ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 POSSIBILITY OF SUCH DAMAGE.
 
 Forked as-is from PyTorch 1.8.1
 """
 
 # Nodes represent a definition of a value in our graph of operators.
-from typing import TYPE_CHECKING, Union, Callable, Any, Tuple, List, Optional, Dict
-from .immutable_collections import immutable_dict, immutable_list
+from typing import Any, Callable, Dict, List, Optional, Tuple, TYPE_CHECKING, Union
+
 import torch
 
+from .immutable_collections import immutable_dict
+from .immutable_collections import immutable_list
+
 if TYPE_CHECKING:
     from .graph import Graph
 
 BaseArgumentTypes = Union[str, int, float, bool, torch.dtype, torch.Tensor]
 base_types = BaseArgumentTypes.__args__  # type: ignore
 
 Target = Union[Callable[..., Any], str]
 
 Argument = Optional[Union[
     Tuple[Any, ...],  # actually Argument, but mypy can't represent recursive types
     List[Any],  # actually Argument
     Dict[str, Any],  # actually Argument
     slice,  # Slice[Argument, Argument, Argument], but slice is not a templated type in typing
     'Node',
-    BaseArgumentTypes
-]]
+    BaseArgumentTypes]]
+
 
 class Node:
     """
     ``Node`` is the data structure that represents individual operations within
     a ``Graph``. For the most part, Nodes represent callsites to various entities,
     such as operators, methods, and Modules (some exceptions include nodes that
     specify function inputs and outputs). Each ``Node`` has a function specified
@@ -86,49 +89,64 @@
       ``args`` and ``kwargs`` represent the arguments to invoke the module on, *including the self argument*.
     - ``call_method`` calls a method on a value. ``name`` is as similar. ``target`` is the string name of the method
       to apply to the ``self`` argument. ``args`` and ``kwargs`` represent the arguments to invoke the module on,
       *including the self argument*
     - ``output`` contains the output of the traced function in its ``args[0]`` attribute. This corresponds to the "return" statement
       in the Graph printout.
     """
-    def __init__(self, graph: 'Graph', name: str, op: str, target: 'Target',
-                 args: Tuple['Argument', ...], kwargs: Dict[str, 'Argument'],
-                 type : Optional[Any] = None) -> None:
+
+    def __init__(
+            self,
+            graph: 'Graph',
+            name: str,
+            op: str,
+            target: 'Target',
+            args: Tuple['Argument', ...],
+            kwargs: Dict[str, 'Argument'],
+            type: Optional[Any] = None) -> None:
         self.graph = graph
         self.name = name  # unique name of value being created
-        assert op in ['placeholder', 'call_method', 'call_module', 'call_function', 'get_attr', 'output', 'root']
+        assert op in [
+            'placeholder',
+            'call_method',
+            'call_module',
+            'call_function',
+            'get_attr',
+            'output',
+            'root']
         self.op = op  # the kind of operation = placeholder|call_method|call_module|call_function|get_attr
         if op in ['call_method', 'call_module']:
             assert isinstance(target, str)
         self.target = target  # for method/module/function, the name of the method/module/function/attr
         # being invoked, e.g add, layer1, or torch.add
 
         # All `Node`-valued inputs. Key is the Node, value is don't-care.
         # The public API for this is `all_input_nodes`, this private attribute
         # should not be accessed directly.
-        self._input_nodes : Dict[Node, None] = {}
-        self.__update_args_kwargs(map_arg(args, lambda x: x), map_arg(kwargs, lambda x: x))  # type: ignore
+        self._input_nodes: Dict[Node, None] = {}
+        self.__update_args_kwargs(
+            map_arg(args, lambda x: x), map_arg(kwargs, lambda x: x))  # type: ignore
 
         # All of the nodes that use the value produced by this Node
         # Note one user may correspond to several uses, e.g. the node fo ``x + x``
         # would appear once here, but represents two uses.
         #
         # Is a dict to act as an "ordered set". Keys are significant, value dont-care
-        self.users : Dict['Node', None] = {}
+        self.users: Dict['Node', None] = {}
         # Type expression representing the output value of this node.
         # This should contain the same class of Type objects that would appear
         # as type annotations for function inputs/outputs.
         #
         # For placeholder nodes, this value will be used to type-annotate the
         # generated function parameters.
         # For the return ndoe, this value will be used to type-annotate the
         # generated function return type. (Note this is a special case. ``return``
         # does not produce a value, it's more of a notation. Thus, this value
         # describes the type of args[0] in the ``return`` node.
-        self.type : Optional[Any] = type
+        self.type: Optional[Any] = type
         self._prev = self
         self._next = self
         self._erased = False
 
     @property
     def next(self) -> 'Node':
         """
@@ -192,15 +210,15 @@
 
         Assignment to this property is allowed. All accounting of uses and users
         is updated automatically on assignment.
         """
         return self._args
 
     @args.setter
-    def args(self, a : Tuple[Argument, ...]):
+    def args(self, a: Tuple[Argument, ...]):
         """
         Set the tuple of arguments to this Node. The interpretation of arguments
         depends on the node's opcode. See the ``fx.Graph`` docstring for more
         information.
         """
         # DO NOT CALL `__update_args_kwargs` directly. The correct way to
         # set `args` is via direct assignment, i.e. `node.args = new_args`
@@ -215,15 +233,15 @@
 
         Assignment to this property is allowed. All accounting of uses and users
         is updated automatically on assignment.
         """
         return self._kwargs
 
     @kwargs.setter
-    def kwargs(self, k : Dict[str, Argument]):
+    def kwargs(self, k: Dict[str, Argument]):
         """
         Set the dict of kwargs to this Node. The interpretation of arguments
         depends on the node's opcode. See the ``fx.Graph`` docstring for more
         information.
         """
         # DO NOT CALL `__update_args_kwargs` directly. The correct way to
         # set `args` is via direct assignment, i.e. `node.kwargs = new_kwargs`
@@ -239,15 +257,16 @@
         Returns:
 
             List of ``Nodes`` that appear in the ``args`` and ``kwargs`` of this
             ``Node``, in that order.
         """
         return list(self._input_nodes.keys())
 
-    def __update_args_kwargs(self, new_args : Tuple['Argument', ...], new_kwargs : Dict[str, 'Argument']):
+    def __update_args_kwargs(
+            self, new_args: Tuple['Argument', ...], new_kwargs: Dict[str, 'Argument']):
         """
         This API is internal. Do *not* call it directly.
         """
         self._args = new_args
         self._kwargs = new_kwargs
 
         for old_use in self._input_nodes.keys():
@@ -259,52 +278,56 @@
 
         for new_use in self._input_nodes.keys():
             new_use.users.setdefault(self)
 
     def __repr__(self) -> str:
         return self.name
 
-    def replace_all_uses_with(self, replace_with : 'Node') -> List['Node']:
+    def replace_all_uses_with(self, replace_with: 'Node') -> List['Node']:
         """
         Replace all uses of ``self`` in the Graph with the Node ``replace_with``.
 
         Args:
 
             replace_with (Node): The node to replace all uses of ``self`` with.
 
         Returns:
 
             The list of Nodes on which this change was made.
         """
         to_process = list(self.users)
         for use_node in to_process:
-            def maybe_replace_node(n : Node) -> Node:
+
+            def maybe_replace_node(n: Node) -> Node:
                 if n == self:
                     return replace_with
                 else:
                     return n
 
             new_args = map_arg(use_node.args, maybe_replace_node)
             new_kwargs = map_arg(use_node.kwargs, maybe_replace_node)
             assert isinstance(new_args, tuple)
             assert isinstance(new_kwargs, dict)
             use_node.__update_args_kwargs(new_args, new_kwargs)
 
         assert len(self.users) == 0
         return to_process
 
+
 def map_arg(a: Argument, fn: Callable[[Node], Argument]) -> Argument:
     """ Apply fn to each Node appearing arg. arg may be a list, tuple, slice, or dict with string keys. """
     return map_aggregate(a, lambda x: fn(x) if isinstance(x, Node) else x)
 
+
 def map_aggregate(a: Argument, fn: Callable[[Argument], Argument]) -> Argument:
     """ Apply fn to each Node appearing arg. arg may be a list, tuple, slice, or dict with string keys. """
     if isinstance(a, tuple):
         return tuple(map_aggregate(elem, fn) for elem in a)
     elif isinstance(a, list):
         return immutable_list(map_aggregate(elem, fn) for elem in a)
     elif isinstance(a, dict):
         return immutable_dict((k, map_aggregate(v, fn)) for k, v in a.items())
     elif isinstance(a, slice):
-        return slice(map_aggregate(a.start, fn), map_aggregate(a.stop, fn), map_aggregate(a.step, fn))
+        return slice(
+            map_aggregate(a.start, fn), map_aggregate(a.stop, fn), map_aggregate(a.step, fn))
     else:
         return fn(a)
```

### Comparing `brevitas-0.8.0/src/brevitas/fx/backport/proxy.py` & `brevitas-0.9.0/src/brevitas/fx/backport/proxy.py`

 * *Files 8% similar despite different names*

```diff
@@ -38,43 +38,62 @@
 ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 POSSIBILITY OF SUCH DAMAGE.
 
 Forked as-is from PyTorch 1.8.1
 """
 
 import dis
-import torch
 import inspect
 import operator
+from typing import Any, Dict, Iterable, Iterator, Optional, Tuple
+
+import torch
 
-from .graph import magic_methods, reflectable_magic_methods, Graph
-from typing import Tuple, Dict, Optional, Iterable, Any, Iterator
-from .node import Target, Node, Argument, base_types, map_aggregate
+from .graph import Graph
+from .graph import magic_methods
+from .graph import reflectable_magic_methods
+from .node import Argument
+from .node import base_types
+from .node import map_aggregate
+from .node import Node
+from .node import Target
 from .torch_function._overrides import is_tensor_method_or_property
 
+
 class TracerBase:
     graph: Graph
 
-    def create_node(self, kind : str, target : Target,
-                    args : Tuple[Argument, ...], kwargs : Dict[str, Argument], name : Optional[str] = None,
-                    type_expr : Optional[Any] = None) -> Node:
+    def create_node(
+            self,
+            kind: str,
+            target: Target,
+            args: Tuple[Argument, ...],
+            kwargs: Dict[str, Argument],
+            name: Optional[str] = None,
+            type_expr: Optional[Any] = None) -> Node:
         """
         Inserts a graph node given target, args, kwargs, and name.
 
         This method can be overridden to do extra checking, validation, or
         modification of values used in node creation. For example, one might
         want to disallow in-place operations from being recorded.
         """
         return self.graph.create_node(kind, target, args, kwargs, name, type_expr)
 
     def proxy(self, node: Node) -> 'Proxy':
         return Proxy(node, self)
 
-    def create_proxy(self, kind: str, target: Target, args: Tuple[Any, ...], kwargs: Dict[str, Any],
-                     name: Optional[str] = None, type_expr : Optional[Any] = None):
+    def create_proxy(
+            self,
+            kind: str,
+            target: Target,
+            args: Tuple[Any, ...],
+            kwargs: Dict[str, Any],
+            name: Optional[str] = None,
+            type_expr: Optional[Any] = None):
         '''
         Create a Node from the given arguments, then return the Node
         wrapped in a Proxy object.
 
         If kind = 'placeholder', then we're creating a Node that
         represents the parameter of a function. If we need to encode
         a default parameter, we use the ``args`` tuple. ``args`` is
@@ -108,16 +127,18 @@
                 # Check for invalid dict keys. We do not want a Proxy to appear
                 # anywhere within the key. Since keys can be collection types,
                 # we iterate through the key with map_aggregate
                 k = self.create_arg(k)
 
                 def no_node(arg):
                     if isinstance(arg, Node):
-                        raise RuntimeError("Keys for dictionaries used as an argument cannot contain a "
-                                           "Node. Got key: {k}")
+                        raise RuntimeError(
+                            "Keys for dictionaries used as an argument cannot contain a "
+                            "Node. Got key: {k}")
+
                 map_aggregate(k, no_node)
 
                 r[k] = self.create_arg(v)
             return r
         elif isinstance(a, slice):
             return slice(self.create_arg(a.start), self.create_arg(a.stop), self.create_arg(a.step))
 
@@ -139,31 +160,35 @@
 
     def iter(self, obj: 'Proxy') -> Iterator:
         """Called when a proxy object is being iterated over, such as
         when used in control flow.  Normally we don't know what to do because
         we don't know the value of the proxy, but a custom tracer can attach more
         information to the graph node using create_node and can choose to return an iterator.
         """
-        raise TraceError('Proxy object cannot be iterated. '
-                         'This can be attempted when used in a for loop or as a *args or **kwargs function argument.')
+        raise TraceError(
+            'Proxy object cannot be iterated. '
+            'This can be attempted when used in a for loop or as a *args or **kwargs function argument.'
+        )
 
     def keys(self, obj: 'Proxy') -> Any:
         """Called when a proxy object is has the keys() method called.
         This is what happens when ** is called on a proxy. This should return an
         iterator it ** is suppose to work in your custom tracer.
         """
         return Attribute(obj, 'keys')()
 
 
 # used in Proxy object when just appending to the graph while not tracing.
 class GraphAppendingTracer(TracerBase):
+
     def __init__(self, graph: Graph):
         super().__init__()
         self.graph = graph
 
+
 class TraceError(ValueError):
     pass
 
 
 class Proxy:
     """
     ``Proxy`` objects are ``Node`` wrappers that flow through the
@@ -171,14 +196,15 @@
     (``torch`` function calls, method calls, operators) that they touch
     into the growing FX Graph.
 
     If you're doing graph transforms, you can wrap your own ``Proxy``
     method around a raw ``Node`` so that you can use the overloaded
     operators to add additional things to a ``Graph``.
     """
+
     def __init__(self, node: Node, tracer: 'Optional[TracerBase]' = None):
         if tracer is None:
             # This allows you to create a Proxy object around a raw Node
             tracer = GraphAppendingTracer(node.graph)
         self.tracer = tracer
         self.node = node
 
@@ -207,61 +233,77 @@
     def __bool__(self) -> bool:
         return self.tracer.to_bool(self)
 
     def keys(self):
         return self.tracer.keys(self)
 
     def __len__(self):
-        raise RuntimeError("'len' is not supported in symbolic tracing by default. If you want "
-                           "this call to be recorded, please call torch.fx.wrap('len') at "
-                           "module scope")
+        raise RuntimeError(
+            "'len' is not supported in symbolic tracing by default. If you want "
+            "this call to be recorded, please call torch.fx.wrap('len') at "
+            "module scope")
 
     def __torch_function__(self, orig_method, types, args=None, kwargs=None):
         args = args if args else ()
         kwargs = kwargs if kwargs else {}
         if is_tensor_method_or_property(orig_method):
             return self.tracer.create_proxy('call_method', orig_method.__name__, args, kwargs)
         else:
-            return self.tracer.create_proxy('call_function', orig_method, args, kwargs,
-                                            name=self.tracer.graph._target_to_str(orig_method.__name__))
+            return self.tracer.create_proxy(
+                'call_function',
+                orig_method,
+                args,
+                kwargs,
+                name=self.tracer.graph._target_to_str(orig_method.__name__))
+
 
 class Attribute(Proxy):
+
     def __init__(self, root: Proxy, attr: str):
         self.root = root
         self.attr = attr
         self.tracer = root.tracer
         self._node: Optional[Node] = None
 
     @property
     def node(self):
         # the node for attributes is added lazily, since most will just be method calls
         # which do not rely on the getitem call
         if self._node is None:
-            self._node = self.tracer.create_proxy('call_function', getattr, (self.root, self.attr), {}).node
+            self._node = self.tracer.create_proxy(
+                'call_function', getattr, (self.root, self.attr), {}).node
         return self._node
 
     def __call__(self, *args, **kwargs):
         return self.tracer.create_proxy('call_method', self.attr, (self.root,) + args, kwargs)
 
+
 for method in magic_methods:
+
     def scope(method):
+
         def impl(*args, **kwargs):
             tracer = args[0].tracer
             target = getattr(operator, method)
             return tracer.create_proxy('call_function', target, args, kwargs)
+
         impl.__name__ = method
         as_magic = f'__{method}__'
         setattr(Proxy, as_magic, impl)
+
     scope(method)
 
+
 def _define_reflectable(orig_method_name):
     method_name = f'__r{orig_method_name}__'
 
     def impl(self, rhs):
         target = getattr(operator, orig_method_name)
         return self.tracer.create_proxy('call_function', target, (rhs, self), {})
+
     impl.__name__ = method_name
     impl.__qualname__ = method_name
     setattr(Proxy, method_name, impl)
 
+
 for orig_method_name in reflectable_magic_methods:
     _define_reflectable(orig_method_name)
```

### Comparing `brevitas-0.8.0/src/brevitas/fx/backport/subgraph_rewriter.py` & `brevitas-0.9.0/src/brevitas/fx/backport/subgraph_rewriter.py`

 * *Files 4% similar despite different names*

```diff
@@ -37,98 +37,100 @@
 CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 POSSIBILITY OF SUCH DAMAGE.
 
 Forked as-is from PyTorch 1.8.1
 """
 
-from .graph_module import GraphModule
+import copy
+from typing import Callable, Dict, List, NamedTuple, Set
+
 from .graph import Graph
+from .graph_module import GraphModule
 from .node import Node
 from .symbolic_trace import symbolic_trace
 
-import copy
-from typing import Callable, Dict, List, NamedTuple, Set
 
 class Match(NamedTuple):
     # Node from which the match was found
     anchor: Node
     # Maps nodes in the pattern subgraph to nodes in the larger graph
     nodes_map: Dict[Node, Node]
 
+
 class SubgraphMatcher:
-    def __init__(self, pattern : Graph) -> None:
+
+    def __init__(self, pattern: Graph) -> None:
         self.pattern = pattern
         if len(pattern.nodes) == 0:
             raise ValueError("SubgraphMatcher cannot be initialized with an "
                              "empty pattern")
         # `self.pattern_anchor` is the output Node in `pattern`
         self.pattern_anchor = next(iter(reversed(pattern.nodes)))
         # Ensure that there is only a single output value in the pattern
         # since we don't support multiple outputs
         assert len(self.pattern_anchor.all_input_nodes) == 1, \
             "Pattern matching on multiple outputs is not supported"
         # Maps nodes in the pattern subgraph to nodes in the larger graph
         self.nodes_map: Dict[Node, Node] = {}
 
-    def matches_subgraph_from_anchor(self, anchor : Node) -> bool:
+    def matches_subgraph_from_anchor(self, anchor: Node) -> bool:
         """
         Checks if the whole pattern can be matched starting from
         ``anchor`` in the larger graph.
 
         Pattern matching is done by recursively comparing the pattern
         node's use-def relationships against the graph node's.
         """
         self.nodes_map = {}
         return self._match_nodes(self.pattern_anchor, anchor)
 
     # Compare the pattern node `pn` against the graph node `gn`
-    def _match_nodes(self, pn : Node, gn : Node) -> bool:
+    def _match_nodes(self, pn: Node, gn: Node) -> bool:
 
         # Check if we've already matched these nodes in the current
         # traversal
         if pn in self.nodes_map:
             return self.nodes_map[pn] == gn
 
-        def attributes_are_equal(pn : Node, gn : Node) -> bool:
+        def attributes_are_equal(pn: Node, gn: Node) -> bool:
             # Use placeholder and output nodes as wildcards. The
             # only exception is that an output node can't match
             # a placeholder
-            if (pn.op == "placeholder"
-                    or (pn.op == "output" and gn.op != "placeholder")):
+            if (pn.op == "placeholder" or (pn.op == "output" and gn.op != "placeholder")):
                 return True
             return pn.op == gn.op and pn.target == gn.target
 
         # Terminate early if the node attributes are not equal
         if not attributes_are_equal(pn, gn):
             return False
 
         # Optimistically mark `pn` as a match for `gn`
         self.nodes_map[pn] = gn
 
         # Traverse the use-def relationships to ensure that `pn` is a true
         # match for `gn`
-        if (pn.op != "output"
-                and len(pn.all_input_nodes) != len(gn.all_input_nodes)):
+        if (pn.op != "output" and len(pn.all_input_nodes) != len(gn.all_input_nodes)):
             return False
         if pn.op == "output":
-            match_found = any(self._match_nodes(pn.all_input_nodes[0], gn_)
-                              for gn_ in gn.all_input_nodes)
+            match_found = any(
+                self._match_nodes(pn.all_input_nodes[0], gn_) for gn_ in gn.all_input_nodes)
         else:
-            match_found = (len(pn.all_input_nodes) == len(gn.all_input_nodes)
-                           and all(self._match_nodes(pn_, gn_) for pn_, gn_
-                                   in zip(pn.all_input_nodes, gn.all_input_nodes)))
+            match_found = (
+                len(pn.all_input_nodes) == len(gn.all_input_nodes) and all(
+                    self._match_nodes(pn_, gn_) for pn_,
+                    gn_ in zip(pn.all_input_nodes, gn.all_input_nodes)))
         if not match_found:
             self.nodes_map.pop(pn)
             return False
 
         return True
 
 
-def replace_pattern(gm : GraphModule, pattern : Callable, replacement : Callable) -> List[Match]:
+def replace_pattern(gm: GraphModule, pattern: Callable, replacement: Callable) -> List[Match]:
     """
     Matches all possible non-overlapping sets of operators and their
     data dependencies (``pattern``) in the Graph of a GraphModule
     (``gm``), then replaces each of these matched subgraphs with another
     subgraph (``replacement``).
 
     Args:
@@ -250,35 +252,34 @@
     matches: List[Match] = []
 
     # Consider each node as an "anchor" (deepest matching graph node)
     for anchor in original_graph.nodes:
 
         if matcher.matches_subgraph_from_anchor(anchor):
 
-            def pattern_is_contained(nodes_map : Dict[Node, Node]) -> bool:
+            def pattern_is_contained(nodes_map: Dict[Node, Node]) -> bool:
                 # `lookup` represents all the nodes in `original_graph`
                 # that are part of `pattern`
-                lookup: Dict[Node, Node] = {v : k for k, v
-                                            in nodes_map.items()}
+                lookup: Dict[Node, Node] = {v: k for k, v in nodes_map.items()}
                 for n in lookup.keys():
 
                     # Nodes that can "leak"...
 
                     # Placeholders (by definition)
                     if n.op == "placeholder":
                         continue
                     # Pattern output (acts as a container)
                     if lookup[n].op == "output":
                         continue
                     # Result contained by pattern output (what we'll
                     # hook in to the new Graph, thus what we'll
                     # potentially use in other areas of the Graph as
                     # an input Node)
-                    if (len(lookup[n].users) == 1
-                            and list(lookup[n].users.keys())[0].op == "output"):
+                    if (len(lookup[n].users) == 1 and
+                            list(lookup[n].users.keys())[0].op == "output"):
                         continue
 
                     for user in n.users:
                         # If this node has users that were not in
                         # `lookup`, then it must leak out of the
                         # pattern subgraph
                         if user not in lookup:
@@ -286,52 +287,48 @@
                 return True
 
             # It's not a match if the pattern leaks out into the rest
             # of the graph
             if pattern_is_contained(matcher.nodes_map):
                 for k, v in matcher.nodes_map.items():
                     # Shallow copy nodes_map
-                    matches.append(Match(anchor=anchor,
-                                   nodes_map=copy.copy(matcher.nodes_map)))
+                    matches.append(Match(anchor=anchor, nodes_map=copy.copy(matcher.nodes_map)))
 
     # The set of all nodes in `original_graph` that we've seen thus far
     # as part of a pattern match
     replaced_nodes: Set[Node] = set()
 
     # Return True if one of the nodes in the current match has already
     # been used as part of another match
-    def overlaps_with_prev_match(match : Match) -> bool:
+    def overlaps_with_prev_match(match: Match) -> bool:
         for n in match.nodes_map.values():
             if n in replaced_nodes and n.op != "placeholder":
                 return True
         return False
 
     for match in matches:
 
         # Skip overlapping matches
         if overlaps_with_prev_match(match):
             continue
 
         # Map replacement graph nodes to their copy in `original_graph`
         val_map: Dict[Node, Node] = {}
 
-        pattern_placeholders = [n for n in pattern_graph.nodes
-                                if n.op == "placeholder"]
+        pattern_placeholders = [n for n in pattern_graph.nodes if n.op == "placeholder"]
         assert len(pattern_placeholders)
-        replacement_placeholders = [n for n in replacement_graph.nodes
-                                    if n.op == "placeholder"]
+        replacement_placeholders = [n for n in replacement_graph.nodes if n.op == "placeholder"]
         assert len(pattern_placeholders) == len(replacement_placeholders)
-        placeholder_map = {r : p for r, p
-                           in zip(replacement_placeholders, pattern_placeholders)}
+        placeholder_map = {r: p for r, p in zip(replacement_placeholders, pattern_placeholders)}
 
         # node from `original_graph` that matched with the output node
         # in `pattern`
         subgraph_output: Node = match.anchor
 
-        def mark_node_as_replaced(n : Node) -> None:
+        def mark_node_as_replaced(n: Node) -> None:
             if n not in match.nodes_map.values():
                 return
             for n_ in n.all_input_nodes:
                 mark_node_as_replaced(n_)
             replaced_nodes.add(n)
 
         mark_node_as_replaced(subgraph_output)
@@ -344,16 +341,15 @@
             pattern_node = placeholder_map[replacement_node]
             original_graph_node = match.nodes_map[pattern_node]
             # Populate `val_map`
             val_map[replacement_node] = original_graph_node
 
         # Copy the replacement graph over
         with original_graph.inserting_before(subgraph_output):
-            copied_output = original_graph.graph_copy(replacement_graph,
-                                                      val_map)
+            copied_output = original_graph.graph_copy(replacement_graph, val_map)
 
         # Hook the output Node of the replacement subgraph in to the
         # original Graph at the correct location
 
         # CASE 1: We need to hook the replacement subgraph in somewhere
         # in the middle of the graph. We replace the Node in the
         # original graph that corresponds to the end of the pattern
@@ -365,16 +361,15 @@
             # the Node that was originally matched as part of
             # `pattern` (i.e. a Node from the original graph). We can
             # figure this out by looking in `match.nodes_map`. The map
             # was created before `replacement_subgraph` was spliced in,
             # so we know that, if a Node is in `match.nodes_map.values`,
             # it must have come from the original graph
             for n in subgraph_output.all_input_nodes:
-                if (n.op != "placeholder"
-                        and n in match.nodes_map.values()):
+                if (n.op != "placeholder" and n in match.nodes_map.values()):
                     subgraph_output = n
                     break
             assert subgraph_output.op != "output"
         # CASE 2: The pattern subgraph match extends to the end of the
         # original graph, so we need to change the current graph's
         # output Node to reflect the insertion of the replacement graph.
         # We'll keep the current output Node, but update its args and
```

### Comparing `brevitas-0.8.0/src/brevitas/fx/backport/symbolic_trace.py` & `brevitas-0.9.0/src/brevitas/fx/backport/symbolic_trace.py`

 * *Files 4% similar despite different names*

```diff
@@ -39,63 +39,86 @@
 ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 POSSIBILITY OF SUCH DAMAGE.
 
 Forked from PyTorch 1.8.1
 """
 
 import builtins
+from contextlib import ExitStack
 import functools
 import inspect
+from itertools import chain
 import math
 import os
-from types import CodeType, FunctionType, ModuleType
-from typing import Any, Dict, NamedTuple, Optional, Set, Tuple, List, Callable, Union
-from itertools import chain
-from contextlib import ExitStack
+from types import CodeType
+from types import FunctionType
+from types import ModuleType
+from typing import Any, Callable, Dict, List, NamedTuple, Optional, Set, Tuple, Union
 
 import torch
 
 try:
     from torch._C import ScriptObject  # type: ignore
 except:
     ScriptObject = None
 
-from .node import Argument, map_aggregate
 from .graph import Graph
 from .graph_module import GraphModule
-from .proxy import TracerBase, Proxy
+from .node import Argument
+from .node import map_aggregate
+from .proxy import Proxy
+from .proxy import TracerBase
 from .torch_function import gen_patches
 
 HAS_VARSTUFF = inspect.CO_VARARGS | inspect.CO_VARKEYWORDS
 
 # These need to run in global scope to handle nested calls correctly
-_orig_module_call : Callable = torch.nn.Module.__call__
-_orig_module_getattr : Callable = torch.nn.Module.__getattr__
+_orig_module_call: Callable = torch.nn.Module.__call__
+_orig_module_getattr: Callable = torch.nn.Module.__getattr__
 
 
 def _patch_function(fn: FunctionType, nargs: int) -> FunctionType:
     co = fn.__code__
     co_flags = co.co_flags & ~HAS_VARSTUFF
-    co_args : tuple
+    co_args: tuple
     if hasattr(co, "co_posonlyargcount"):
         co_args = (
-            nargs, 0,
-            0, co.co_nlocals, co.co_stacksize,
-            co_flags, co.co_code, co.co_consts, co.co_names,
-            co.co_varnames, co.co_filename, co.co_name,
-            co.co_firstlineno, co.co_lnotab, co.co_freevars,
-            co.co_cellvars
-        )
+            nargs,
+            0,
+            0,
+            co.co_nlocals,
+            co.co_stacksize,
+            co_flags,
+            co.co_code,
+            co.co_consts,
+            co.co_names,
+            co.co_varnames,
+            co.co_filename,
+            co.co_name,
+            co.co_firstlineno,
+            co.co_lnotab,
+            co.co_freevars,
+            co.co_cellvars)
     else:
         co_args = (
-            nargs, 0, co.co_nlocals,
-            co.co_stacksize, co_flags, co.co_code, co.co_consts,
-            co.co_names, co.co_varnames, co.co_filename,
-            co.co_name, co.co_firstlineno, co.co_lnotab,
-            co.co_freevars, co.co_cellvars)
+            nargs,
+            0,
+            co.co_nlocals,
+            co.co_stacksize,
+            co_flags,
+            co.co_code,
+            co.co_consts,
+            co.co_names,
+            co.co_varnames,
+            co.co_filename,
+            co.co_name,
+            co.co_firstlineno,
+            co.co_lnotab,
+            co.co_freevars,
+            co.co_cellvars)
     new_code = CodeType(*co_args)  # type: ignore
     return FunctionType(new_code, fn.__globals__, fn.__name__, fn.__defaults__, fn.__closure__)
 
     # we need to insert placeholder nodes for *args and **kwargs
     # we can't call this function normally, otherwise it would try to unpack them
     # instead, let's make python think that args and kwargs are normal variables
 
@@ -106,15 +129,16 @@
     of ``torch.fx.symbolic_trace``. A call to ``symbolic_trace(m)`` is equivalent
     to ``Tracer().trace(m)``.
 
     Tracer can be subclassed to override various behaviors of the tracing
     process. The different behaviors that can be overridden are described
     in the docstrings of the methods on this class.
     """
-    def __init__(self, autowrap_modules : Tuple[ModuleType] = (math, )):
+
+    def __init__(self, autowrap_modules: Tuple[ModuleType] = (math,)):
         """
         Construct a Tracer object.
 
         Args:
 
             autowrap_modules (List[ModuleType]): defaults to `[math]`,
                 Python modules whose functions should be wrapped automatically
@@ -122,15 +146,17 @@
         """
 
         super().__init__()
 
         # Functions we will eagerly wrap when we see them while tracing
         # this captures both `math.sqrt()` and `from math import sqrt` automatically
         self._autowrap_function_ids: Set[int] = {
-            id(value) for name, value in chain(*[m.__dict__.items() for m in autowrap_modules])
+            id(value)
+            for name,
+            value in chain(*[m.__dict__.items() for m in autowrap_modules])
             if not name.startswith("_") and callable(value)}
 
         # Python modules to apply autowrap to at the start, in addition to
         # modules we see while tracing
         self._autowrap_search: List[ModuleType] = list(autowrap_modules)
 
     def create_arg(self, a: Any) -> 'Argument':
@@ -184,15 +210,15 @@
         # constructed (and we probably don't want to rely on that, either), so
         # for any constant Tensor values we encounter, first search for if they
         # are an attribute of some module in the module hierarchy. If so, emit
         # a get_attr to retrieve that tensor. Otherwise, we'll store away the
         # tensor value into a special attribute on the Module s.t. we can
         # retrieve it with a get_attr.
         if isinstance(a, tuple(i for i in [torch.Tensor, ScriptObject] if i is not None)):
-            qualname : Optional[str] = self.tensor_attrs.get(a)
+            qualname: Optional[str] = self.tensor_attrs.get(a)
 
             # Tensor was not found in the Module hierarchy, stow it away in a
             # special attribute and set the qualname to refer to that
             if not qualname:
                 i = 0
                 while True:
                     qualname = f'_tensor_constant{i}'
@@ -200,15 +226,15 @@
                         break
                     i += 1
                 setattr(self.root, qualname, a)
 
             return self.create_node('get_attr', qualname, (), {})
         return super().create_arg(a)
 
-    def is_leaf_module(self, m: torch.nn.Module, module_qualified_name : str) -> bool:
+    def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool:
         """
         A method to specify whether a given ``nn.Module`` is a "leaf" module.
 
         Leaf modules are the atomic units that appear in
         the IR, referenced by ``call_module`` calls. By default,
         Modules in the PyTorch standard library namespace (torch.nn)
         are leaf modules. All other modules are traced through and
@@ -221,15 +247,15 @@
             module_qualified_name (str): The path to root of this module. For example,
                 if you have a module hierarchy where submodule ``foo`` contains
                 submodule ``bar``, which contains submodule ``baz``, that module will
                 appear with the qualified name ``foo.bar.baz`` here.
         """
         return m.__module__.startswith('torch.nn') and not isinstance(m, torch.nn.Sequential)
 
-    def path_of_module(self, mod : torch.nn.Module) -> str:
+    def path_of_module(self, mod: torch.nn.Module) -> str:
         """
         Helper method to find the qualified name of ``mod`` in the Module hierarchy
         of ``root``. For example, if ``root`` has a submodule named ``foo``, which has
         a submodule named ``bar``, passing ``bar`` into this function will return
         the string "foo.bar".
 
         Args:
@@ -237,15 +263,20 @@
             mod (str): The ``Module`` to retrieve the qualified name for.
         """
         for n, p in self.root.named_modules():
             if mod is p:
                 return n
         raise NameError('module is not installed as a submodule')
 
-    def call_module(self, m: torch.nn.Module, forward: Callable[..., Any], args : Tuple[Any, ...], kwargs : Dict[str, Any]) -> Any:
+    def call_module(
+            self,
+            m: torch.nn.Module,
+            forward: Callable[..., Any],
+            args: Tuple[Any, ...],
+            kwargs: Dict[str, Any]) -> Any:
         """
         Method that specifies the behavior of this ``Tracer`` when it encounters
         a call to an ``nn.Module`` instance.
 
         By default, the behavior is to check if the called module is a leaf module
         via ``is_leaf_module``. If it is, emit a ``call_module`` node referring to
         ``m`` in the ``Graph``. Otherwise, call the ``Module`` normally, tracing through
@@ -283,49 +314,56 @@
         # defined via ``functools.wraps``. In this case, the outer code object
         # will likely not contain the actual parameters we care about, so unwrap
         # the function to get to the innermost callable.
         fn_for_analysis = inspect.unwrap(root_fn)
         co = fn_for_analysis.__code__
         total_args = co.co_argcount + co.co_kwonlyargcount
         names_iter = iter(co.co_varnames)
-        args : List[Any] = []
+        args: List[Any] = []
         skip_arg_idx = 0
         if is_module:
             if total_args == 0:
                 raise RuntimeError('``self`` argument cannot be part of *args expansion!')
             skip_arg_idx = 1
             next(names_iter)  # skip self
             args.append(self.root)
 
         sig = inspect.signature(fn_for_analysis)
 
         def proxy_placeholder(name: str):
             if concrete_args is not None and name in concrete_args:
                 return concrete_args[name]
             if name[0] == '*':
-                default = ()    # type: ignore
+                default = ()  # type: ignore
             else:
                 param = sig.parameters[name]
-                default = () if param.default is inspect.Parameter.empty else (param.default,)  # type: ignore
-            return self.create_proxy('placeholder', name, default, {},
-                                     type_expr=fn_for_analysis.__annotations__.get(name, None))
+                default = () if param.default is inspect.Parameter.empty else (
+                    param.default,)  # type: ignore
+            return self.create_proxy(
+                'placeholder',
+                name,
+                default, {},
+                type_expr=fn_for_analysis.__annotations__.get(name, None))
 
         args.extend(proxy_placeholder(next(names_iter)) for _ in range(skip_arg_idx, total_args))
 
         if co.co_kwonlyargcount > 0 or co.co_flags & HAS_VARSTUFF:
             # TODO: type annotations for *args and **kwargs
             if co.co_flags & inspect.CO_VARARGS:
                 args.append(proxy_placeholder('*' + next(names_iter)))
             if co.co_flags & inspect.CO_VARKEYWORDS:
                 args.append(proxy_placeholder('**' + next(names_iter)))
             root_fn = _patch_function(root_fn, len(args))
 
         return root_fn, args
 
-    def trace(self, root: Union[torch.nn.Module, Callable], concrete_args: Optional[Dict[str, Any]] = None) -> Graph:
+    def trace(
+            self,
+            root: Union[torch.nn.Module, Callable],
+            concrete_args: Optional[Dict[str, Any]] = None) -> Graph:
         """
         Trace ``root`` and return the corresponding FX ``Graph`` representation. ``root``
         can either be an ``nn.Module`` instance or a Python callable.
 
         Note that after this call, ``self.root`` may be different from the ``root`` passed
         in here. For example, when a free function is passed to ``trace()``, we will
         create an ``nn.Module`` instance to use as the root and add embedded constants
@@ -349,31 +387,31 @@
             fn = root
         self.graph = Graph()
 
         # When we encounter a Tensor value that's not a parameter, we look if it
         # is some other attribute on the model. Construct a dict mapping Tensor
         # values to the qualified name here for efficiency. This is used downstream
         # in create_arg
-        self.tensor_attrs : Dict[torch.Tensor, str] = {}
+        self.tensor_attrs: Dict[torch.Tensor, str] = {}
 
-        def collect_tensor_attrs(m : torch.nn.Module, prefix_atoms : List[str]):
+        def collect_tensor_attrs(m: torch.nn.Module, prefix_atoms: List[str]):
             for k, v in m.__dict__.items():
                 if isinstance(v, tuple(i for i in [torch.Tensor, ScriptObject] if i is not None)):
                     self.tensor_attrs[v] = '.'.join(prefix_atoms + [k])
             for k, v in m.named_children():
                 collect_tensor_attrs(v, prefix_atoms + [k])
 
         collect_tensor_attrs(self.root, [])
 
         assert isinstance(fn, FunctionType)
 
         fn_globals = fn.__globals__  # run before it gets patched
         fn, args = self.create_args_for_root(fn, isinstance(root, torch.nn.Module), concrete_args)
 
-        parameter_proxy_cache : Dict[str, Proxy] = {}  # Reduce number of get_attr calls
+        parameter_proxy_cache: Dict[str, Proxy] = {}  # Reduce number of get_attr calls
 
         # Method dispatch on parameters is not recorded unless it's directly used.
         # Thus, we need to insert a proxy when __getattr__ requests a parameter.
         @functools.wraps(_orig_module_getattr)
         def module_getattr_wrapper(mod, attr):
             attr_val = _orig_module_getattr(mod, attr)
             if isinstance(attr_val, torch.nn.Parameter):
@@ -382,42 +420,47 @@
                         if n not in parameter_proxy_cache:
                             parameter_proxy_cache[n] = self.create_proxy('get_attr', n, (), {})
                         return parameter_proxy_cache[n]
             return attr_val
 
         @functools.wraps(_orig_module_call)
         def module_call_wrapper(mod, *args, **kwargs):
+
             def forward(*args, **kwargs):
                 return _orig_module_call(mod, *args, **kwargs)
 
-            _autowrap_check(patcher, getattr(getattr(mod, "forward", mod), "__globals__", {}),
-                            self._autowrap_function_ids)
+            _autowrap_check(
+                patcher,
+                getattr(getattr(mod, "forward", mod), "__globals__", {}),
+                self._autowrap_function_ids)
             return self.call_module(mod, forward, args, kwargs)
 
         with _Patcher() as patcher:
             # allow duplicate patches to support the case of nested calls
-            patcher.patch_method(torch.nn.Module, "__getattr__", module_getattr_wrapper, deduplicate=False)
-            patcher.patch_method(torch.nn.Module, "__call__", module_call_wrapper, deduplicate=False)
+            patcher.patch_method(
+                torch.nn.Module, "__getattr__", module_getattr_wrapper, deduplicate=False)
+            patcher.patch_method(
+                torch.nn.Module, "__call__", module_call_wrapper, deduplicate=False)
             _patch_wrapped_functions(patcher)
             _autowrap_check(patcher, fn_globals, self._autowrap_function_ids)
             for module in self._autowrap_search:
                 _autowrap_check(patcher, module.__dict__, self._autowrap_function_ids)
 
             self.create_node('output', 'output', (self.create_arg(fn(*args)),), {})
 
         return self.graph
 
 
 # List of pairs of (global dict, function name) functions
 # to patch for the purposes of the wrap() API.
-_wrapped_fns_to_patch : List[Tuple[dict, str]] = []
+_wrapped_fns_to_patch: List[Tuple[dict, str]] = []
 
 # List of methods on classes to wrap (class type, function name)
 # this currently only works for Tensor.* methods that aren't traced properly
-_wrapped_methods_to_patch : List[Tuple[type, str]] = []
+_wrapped_methods_to_patch: List[Tuple[type, str]] = []
 
 if os.environ.get("FX_PATCH_GETITEM") == "1":
     # This change is needed to trace models like PositionalEmbedding from BERT:
     # https://github.com/pytorch/benchmark/blob/master/torchbenchmark/models/BERT_pytorch/bert_pytorch/model/embedding/position.py  # noqa
     # but causes issues in quantization documented here:
     # https://github.com/pytorch/pytorch/issues/50710
     # once that is fixed we can make this the default behavior.
@@ -437,14 +480,15 @@
             proxy = x
 
     map_aggregate(objects_to_search, find_proxy)
     return proxy
 
 
 def _create_wrapped_func(orig_fn):
+
     @functools.wraps(orig_fn)
     def wrapped(*args, **kwargs):
         """
         Given an closed-over ``orig_function`` to invoke, search the args and kwargs for
         a Proxy object. If there is one, emit a ``call_function`` node to preserve the
         call to this leaf function directly. Otherwise, just return the results of
         this function call, as this function is not being traced.
@@ -473,59 +517,66 @@
             return proxy.tracer.create_proxy('call_method', name, args, kwargs)
         return orig_fn(*args, **kwargs)
 
     return wrapped
 
 
 class _PatchedFn(NamedTuple):
-    frame_dict : Any
-    fn_name : str
-    orig_fn : Any
+    frame_dict: Any
+    fn_name: str
+    orig_fn: Any
 
     def revert(self):
         raise NotImplementedError()
 
 
 class _PatchedFnSetItem(_PatchedFn):
+
     def revert(self):
         self.frame_dict[self.fn_name] = self.orig_fn
 
 
 class _PatchedFnDel(_PatchedFn):
+
     def revert(self):
         del self.frame_dict[self.fn_name]
 
 
 class _PatchedFnSetAttr(_PatchedFn):
+
     def revert(self):
         setattr(self.frame_dict, self.fn_name, self.orig_fn)
 
 
 class _Patcher(object):
+
     def __init__(self):
         super(_Patcher, self).__init__()
-        self.patches_made : List[_PatchedFn] = []
-        self.visited : Set[int] = set()
+        self.patches_made: List[_PatchedFn] = []
+        self.visited: Set[int] = set()
 
-    def patch(self, frame_dict : Dict[str, Any], name : str, new_fn : Callable,
-              deduplicate : bool = True):
+    def patch(
+            self,
+            frame_dict: Dict[str, Any],
+            name: str,
+            new_fn: Callable,
+            deduplicate: bool = True):
         """
         Replace frame_dict[name] with new_fn until we exit the context manager.
         """
         new_fn.__fx_already_patched = deduplicate  # type: ignore
         if name not in frame_dict and hasattr(builtins, name):
             self.patches_made.append(_PatchedFnDel(frame_dict, name, None))
         elif getattr(frame_dict[name], "__fx_already_patched", False):
             return  # already patched, no need to do it again
         else:
             self.patches_made.append(_PatchedFnSetItem(frame_dict, name, frame_dict[name]))
         frame_dict[name] = new_fn
 
-    def patch_method(self, cls: type, name : str, new_fn : Callable,
-                     deduplicate : bool = True):
+    def patch_method(self, cls: type, name: str, new_fn: Callable, deduplicate: bool = True):
         """
         Replace object_or_dict.name with new_fn until we exit the context manager.
         """
         new_fn.__fx_already_patched = deduplicate  # type: ignore
         orig_fn = getattr(cls, name)
         if getattr(orig_fn, "__fx_already_patched", False):
             return  # already patched, no need to do it again
@@ -549,15 +600,15 @@
         """
         while self.patches_made:
             # unpatch in reverse order to handle duplicates correctly
             self.patches_made.pop().revert()
         self.visited.clear()
 
 
-def _patch_wrapped_functions(patcher : _Patcher):
+def _patch_wrapped_functions(patcher: _Patcher):
     """
     Go through ``_wrapped_fn_patch_table`` and, for each frame object, wrap
     the listed global functions in the `_create_wrapped_func` wrapper.
     """
     for frame_dict, name in _wrapped_fns_to_patch:
         if name not in frame_dict and hasattr(builtins, name):
             orig_fn = getattr(builtins, name)
@@ -565,26 +616,26 @@
             orig_fn = frame_dict[name]
         patcher.patch(frame_dict, name, _create_wrapped_func(orig_fn))
 
     for cls, name in _wrapped_methods_to_patch:
         patcher.patch_method(cls, name, _create_wrapped_method(cls, name))
 
 
-def _autowrap_check(patcher : _Patcher, frame_dict : Dict[str, Any], function_ids : Set[int]):
+def _autowrap_check(patcher: _Patcher, frame_dict: Dict[str, Any], function_ids: Set[int]):
     """
     Some methods, like `math.sqrt` are common enough we want to automatically wrap them as we see them.
     This method searches a scope for them and patches them if found.
     """
     if patcher.visit_once(frame_dict):
         for name, value in frame_dict.items():
             if not name.startswith("_") and callable(value) and id(value) in function_ids:
                 patcher.patch(frame_dict, name, _create_wrapped_func(value))
 
 
-def wrap(fn_or_name : Union[str, Callable]):
+def wrap(fn_or_name: Union[str, Callable]):
     """
     This function can be called at module-level scope to register fn_or_name as a "leaf function".
     A "leaf function" will be preserved as a CallFunction node in the FX trace instead of being
     traced through::
 
         # foo/bar/baz.py
         def my_custom_function(x, y):
@@ -610,16 +661,17 @@
 
     Args:
 
         fn_or_name (Union[str, Callable]): The function or name of the global function to insert into the
             graph when it's called
     """
     if not callable(fn_or_name) and not isinstance(fn_or_name, str):
-        raise RuntimeError('Unsupported type for global function! Must be either a callable or '
-                           'string name')
+        raise RuntimeError(
+            'Unsupported type for global function! Must be either a callable or '
+            'string name')
 
     if hasattr(fn_or_name, '__code__'):
         assert not isinstance(fn_or_name, str)  # to make mypy happy
         fn_name = fn_or_name.__code__.co_name
     else:
         assert isinstance(fn_or_name, str), "fn_or_name must be a global function or string name"
         fn_name = fn_or_name
@@ -633,15 +685,17 @@
 
     # consider implementing Callable version of this via _autowrap_function_ids / _autowrap_search
     # semantics would be slightly different, but would add support `from x import wrapped_function`
     _wrapped_fns_to_patch.append((f.f_globals, fn_name))
     return fn_or_name
 
 
-def symbolic_trace(root : Union[torch.nn.Module, Callable], concrete_args: Optional[Dict[str, Any]] = None) -> GraphModule:
+def symbolic_trace(
+        root: Union[torch.nn.Module, Callable],
+        concrete_args: Optional[Dict[str, Any]] = None) -> GraphModule:
     """Symbolic tracing API
 
     Given an ``nn.Module`` or function instance ``root``, this function will return a ``GraphModule``
     constructed by recording operations seen while tracing through ``root``.
 
     Args:
         root (Union[torch.nn.Module, Callable]): Module or function to be traced and converted
```

### Comparing `brevitas-0.8.0/src/brevitas/fx/backport/torch_function/_overrides.py` & `brevitas-0.9.0/src/brevitas/fx/backport/torch_function/_overrides.py`

 * *Files 1% similar despite different names*

```diff
@@ -56,28 +56,30 @@
 NOTE: heavily inspired by NumPy's ``__array_function__`` (see:
 https://github.com/pytorch/pytorch/issues/24015 and
 https://www.numpy.org/neps/nep-0018-array-function-protocol.html
 )
 
 """
 
-import functools
-from inspect import getfullargspec, getcallargs
 import collections
+import functools
+from inspect import getcallargs
+from inspect import getfullargspec
 
 from .signatures import get_tensor_overrides
 
 ArgSpec = collections.namedtuple('ArgSpec', 'args varargs keywords defaults')
 
+
 def getargspec(func):
     spec = getfullargspec(func)
     return ArgSpec(spec.args, spec.varargs, spec.varkw, spec.defaults)
 
-from torch import Tensor
 
+from torch import Tensor
 
 _TENSOR_ONLY = [Tensor]
 
 
 def _get_overloaded_types_and_args(relevant_args):
     """Returns a list of arguments on which to call __torch_function__.
 
@@ -137,16 +139,15 @@
             else:
                 overloaded_types = [arg_type]
                 overloaded_args = [arg]
 
     return overloaded_types, overloaded_args
 
 
-def _implement_torch_function(
-        implementation, relevant_args, args, kwargs):
+def _implement_torch_function(implementation, relevant_args, args, kwargs):
     # Check for __torch_function__ methods.
     types, overloaded_args = _get_overloaded_types_and_args(relevant_args)
     # Short-cut for common cases: no overload or only Tensor overload
     # (directly or with subclasses that do not override __torch_function__).
     if not overloaded_args or types == _TENSOR_ONLY:
         return implementation(*args, **kwargs)
 
@@ -154,28 +155,28 @@
     for overloaded_arg in overloaded_args:
         result = overloaded_arg.__torch_function__(implementation, types, args, kwargs)
 
         if result is not NotImplemented:
             return result
 
     func_name = '{}.{}'.format(implementation.__module__, implementation.__name__)
-    raise TypeError("no implementation found for '{}' on types that implement "
-                    '__torch_function__: {}'
-                    .format(func_name, list(map(type, overloaded_args))))
+    raise TypeError(
+        "no implementation found for '{}' on types that implement "
+        '__torch_function__: {}'.format(func_name, list(map(type, overloaded_args))))
 
 
 def torch_function_dispatch(dispatcher):
 
     def decorator(implementation):
 
         @functools.wraps(implementation)
         def wrapper(*args, **kwargs):
             relevant_args = dispatcher(*args, **kwargs)
             return _implement_torch_function(implementation, relevant_args, args, kwargs)
+
         return wrapper
 
     return decorator
 
 
-
 def is_tensor_method_or_property(func) -> bool:
-    return func in get_tensor_overrides() or func.__name__ == "__get__"
+    return func in get_tensor_overrides() or func.__name__ == "__get__"
```

### Comparing `brevitas-0.8.0/src/brevitas/fx/backport/torch_function/patch.py` & `brevitas-0.9.0/src/brevitas/fx/backport/torch_function/patch.py`

 * *Ordering differences only*

 * *Files 4% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from unittest import mock
 import inspect
-from packaging import version
+from unittest import mock
 
+from packaging import version
 import torch
 
 import brevitas
 from brevitas.utils.python_utils import patch
-from .signatures import get_torch_overrides
-from .signatures import get_nn_functional_overrides
-from ._overrides import torch_function_dispatch
+
 from ._overrides import _implement_torch_function
+from ._overrides import torch_function_dispatch
+from .signatures import get_nn_functional_overrides
 from .signatures import get_testing_overrides
+from .signatures import get_torch_overrides
 
 
 def make_above_16_patches():
 
     original_torch_cat = torch.cat
     original_torch_stack = torch.stack
```

### Comparing `brevitas-0.8.0/src/brevitas/fx/brevitas_tracer.py` & `brevitas-0.9.0/src/brevitas/fx/brevitas_tracer.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,34 +1,34 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Callable, Optional, Dict, Any, Union
-
 from contextlib import ExitStack
-from torch.nn import Sequential, Module
+from typing import Any, Callable, Dict, Optional, Union
 
-from . import GraphModule, Tracer
-from .value_tracer import ValueTracer
+from torch.nn import Module
+from torch.nn import Sequential
+
+from . import GraphModule
+from . import Tracer
 from .backport.torch_function import gen_patches
+from .value_tracer import ValueTracer
 
 
 def _is_brevitas_leaf_module(m, fully_qualified_name):
     is_torch_nn = m.__module__.startswith('torch.nn')
     is_brevitas_nn = m.__module__.startswith('brevitas.nn')
     is_brevitas_core = m.__module__.startswith('brevitas.core')
     is_brevitas_proxy = m.__module__.startswith('brevitas.proxy')
     is_seq = isinstance(m, Sequential)
     is_brevitas = is_brevitas_nn or is_brevitas_proxy or is_brevitas_core
     return is_torch_nn and not is_seq or is_brevitas
 
 
 def _trace_with_backport(
-        tracer: Tracer,
-        root : Union[Module, Callable],
+        tracer: Tracer, root: Union[Module, Callable],
         concrete_args: Optional[Dict[str, Any]]) -> GraphModule:
     patches = gen_patches()
     if patches:
         with ExitStack() as stack:
             for patch in patches:
                 stack.enter_context(patch)
             graph = tracer.trace(root, concrete_args)
@@ -36,36 +36,31 @@
         graph = tracer.trace(root, concrete_args)
     name = root.__class__.__name__ if isinstance(root, Module) else root.__name__
     return GraphModule(tracer.root, graph, name)
 
 
 class BrevitasValueTracer(ValueTracer):
 
-    def is_leaf_module(self, m: Module, module_qualified_name : str) -> bool:
+    def is_leaf_module(self, m: Module, module_qualified_name: str) -> bool:
         return _is_brevitas_leaf_module(m, module_qualified_name)
 
 
 class BrevitasSymbolicTracer(Tracer):
 
-    def is_leaf_module(self, m: Module, module_qualified_name : str) -> bool:
+    def is_leaf_module(self, m: Module, module_qualified_name: str) -> bool:
         return _is_brevitas_leaf_module(m, module_qualified_name)
 
 
-def symbolic_trace(root, concrete_args = None):
+def symbolic_trace(root, concrete_args=None):
     return _trace_with_backport(Tracer(), root, concrete_args)
 
 
-def value_trace(root, concrete_args = None):
+def value_trace(root, concrete_args=None):
     return _trace_with_backport(ValueTracer(), root, concrete_args)
 
 
-def brevitas_symbolic_trace(root, concrete_args = None):
+def brevitas_symbolic_trace(root, concrete_args=None):
     return _trace_with_backport(BrevitasSymbolicTracer(), root, concrete_args)
 
 
-def brevitas_value_trace(root, concrete_args = None):
+def brevitas_value_trace(root, concrete_args=None):
     return _trace_with_backport(BrevitasValueTracer(), root, concrete_args)
-
-
-
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas/fx/value_tracer.py` & `brevitas-0.9.0/src/brevitas/fx/value_tracer.py`

 * *Files 1% similar despite different names*

```diff
@@ -36,47 +36,53 @@
 SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 POSSIBILITY OF SUCH DAMAGE.
 """
 
-
-from typing import Union, Callable, Optional, Dict, Any, List, Tuple
-from types import FunctionType, ModuleType
+import builtins
 import functools
 import inspect
-import builtins
-import operator
 import math
+import operator
+from types import FunctionType
+from types import ModuleType
+from typing import Any, Callable, Dict, List, Optional, Tuple, Union
 
 try:
     from torch._C import ScriptObject
 except:
     ScriptObject = None
 
 from torch.nn import Module
 
 from brevitas.quant_tensor import QuantTensorBase
-from . import _Patcher, _patch_function, _orig_module_call, _orig_module_getattr
-from . import _find_proxy, _wrapped_methods_to_patch, _wrapped_fns_to_patch
-from . import _autowrap_check
+
 from . import *
+from . import _autowrap_check
+from . import _find_proxy
+from . import _orig_module_call
+from . import _orig_module_getattr
+from . import _patch_function
+from . import _Patcher
+from . import _wrapped_fns_to_patch
+from . import _wrapped_methods_to_patch
 
 _UNSET = object()
 extended_base_types = base_types + (QuantTensorBase,)
 
 
 class UnsetValueException(Exception):
     pass
 
 
 class ValueProxy(Proxy):
 
-    def __init__(self, node: Node, value, tracer = None):
+    def __init__(self, node: Node, value, tracer=None):
         super(ValueProxy, self).__init__(node, tracer)
         if isinstance(value, Proxy):
             raise RuntimeError("Value of a proxy can't be a proxy.")
         self.value = value
 
     @property
     def value(self):
@@ -139,14 +145,15 @@
                 value = _UNSET
             name = self.tracer.graph._target_to_str(orig_method.__name__)
             return self.tracer.create_proxy(
                 'call_function', orig_method, args, kwargs, name, value=value)
 
 
 class ValueAttribute(ValueProxy):
+
     def __init__(self, root: Proxy, attr: str, value: Any):
         self.root = root
         self.attr = attr
         self.value = value
         self.tracer = root.tracer
         self._node: Optional[Node] = None
 
@@ -165,60 +172,64 @@
         except UnsetValueException:
             value = _UNSET
         return self.tracer.create_proxy(
             'call_method', self.attr, (self.root,) + args, kwargs, value=value)
 
 
 for method in magic_methods:
+
     def scope(method):
+
         def impl(*args, **kwargs):
             tracer = args[0].tracer
             target = getattr(operator, method)
             try:
                 value = target(*tracer.unpack_arg(args), **tracer.unpack_arg(kwargs))
             except UnsetValueException:
                 value = _UNSET
             return tracer.create_proxy('call_function', target, args, kwargs, value=value)
+
         impl.__name__ = method
         as_magic = f'__{method}__'
         setattr(ValueProxy, as_magic, impl)
+
     scope(method)
 
 
 def _define_reflectable(orig_method_name):
     method_name = f'__r{orig_method_name}__'
 
     def impl(self, rhs):
         target = getattr(operator, orig_method_name)
         try:
             value = target(self.tracer.unpack_arg(rhs), self.value)
         except UnsetValueException:
             value = _UNSET
         return self.tracer.create_proxy('call_function', target, (rhs, self), {}, value=value)
+
     impl.__name__ = method_name
     impl.__qualname__ = method_name
     setattr(ValueProxy, method_name, impl)
 
 
 for orig_method_name in reflectable_magic_methods:
     _define_reflectable(orig_method_name)
 
 
 class ValueTracer(Tracer):
 
-    def __init__(self, autowrap_modules : Tuple[ModuleType] = (math, )):
+    def __init__(self, autowrap_modules: Tuple[ModuleType] = (math,)):
         super(ValueTracer, self).__init__(autowrap_modules)
         self.concrete_mode = False
 
     def to_bool(self, obj: 'Proxy') -> bool:
         return obj.value.__bool__()
 
     def iter(self, obj: 'Proxy'):
-        return self.create_proxy(
-            'call_function', iter, (obj,), {}, value=obj.value.__iter__())
+        return self.create_proxy('call_function', iter, (obj,), {}, value=obj.value.__iter__())
 
     # TODO deal with keys and values
     def keys(self, obj: 'Proxy') -> Any:
         """Called when a proxy object is has the keys() method called.
         This is what happens when ** is called on a proxy. This should return an
         iterator it ** is suppose to work in your custom tracer.
         """
@@ -245,14 +256,15 @@
                 k = self.unpack_arg(k)
 
                 def no_node(arg):
                     if isinstance(arg, Node):
                         raise RuntimeError(
                             "Keys for dictionaries used as an argument cannot contain a Node. "
                             "Got key: {k}")
+
                 map_aggregate(k, no_node)
 
                 r[k] = self.unpack_arg(v)
             return r
         elif isinstance(a, slice):
             return slice(self.unpack_arg(a.start), self.unpack_arg(a.stop), self.unpack_arg(a.step))
 
@@ -260,17 +272,23 @@
             # base case: we unwrap the Proxy object
             return a.value
         elif isinstance(a, extended_base_types) or a is None or a is ...:
             return a
 
         raise NotImplementedError(f"argument of type: {type(a)}")
 
-    def create_proxy(self, kind: str, target: Target, args: Tuple[Any, ...],
-                     kwargs: Dict[str, Any], name: Optional[str] = None,
-                     type_expr : Optional[Any] = None, value: Any = _UNSET):
+    def create_proxy(
+            self,
+            kind: str,
+            target: Target,
+            args: Tuple[Any, ...],
+            kwargs: Dict[str, Any],
+            name: Optional[str] = None,
+            type_expr: Optional[Any] = None,
+            value: Any = _UNSET):
         '''
         Create a Node from the given arguments, then return the Node
         wrapped in a Proxy object.
 
         If kind = 'placeholder', then we're creating a Node that
         represents the parameter of a function. If we need to encode
         a default parameter, we use the ``args`` tuple. ``args`` is
@@ -279,15 +297,20 @@
         args_ = self.create_arg(args)
         kwargs_ = self.create_arg(kwargs)
         assert isinstance(args_, tuple)
         assert isinstance(kwargs_, dict)
         node = self.create_node(kind, target, args_, kwargs_, name, type_expr)
         return self.proxy(node, value)
 
-    def call_module(self, m: torch.nn.Module, forward: Callable[..., Any], args : Tuple[Any, ...], kwargs : Dict[str, Any]) -> Any:
+    def call_module(
+            self,
+            m: torch.nn.Module,
+            forward: Callable[..., Any],
+            args: Tuple[Any, ...],
+            kwargs: Dict[str, Any]) -> Any:
         """
         Method that specifies the behavior of this ``Tracer`` when it encounters
         a call to an ``nn.Module`` instance.
 
         By default, the behavior is to check if the called module is a leaf module
         via ``is_leaf_module``. If it is, emit a ``call_module`` node referring to
         ``m`` in the ``Graph``. Otherwise, call the ``Module`` normally, tracing through
@@ -334,15 +357,15 @@
         # defined via ``functools.wraps``. In this case, the outer code object
         # will likely not contain the actual parameters we care about, so unwrap
         # the function to get to the innermost callable.
         fn_for_analysis = inspect.unwrap(root_fn)
         co = fn_for_analysis.__code__
         total_args = co.co_argcount + co.co_kwonlyargcount
         names_iter = iter(co.co_varnames)
-        args : List[Any] = []
+        args: List[Any] = []
         skip_arg_idx = 0
         if is_module:
             if total_args == 0:
                 raise RuntimeError('``self`` argument cannot be part of *args expansion!')
             skip_arg_idx = 1
             next(names_iter)  # skip self
             args.append(self.root)
@@ -350,16 +373,15 @@
         def proxy_placeholder(name: str):
             if concrete_args is not None and name in concrete_args:
                 value = concrete_args[name]
             else:
                 param = inspect.signature(fn_for_analysis).parameters[name]
                 value = _UNSET if param.default is inspect.Parameter.empty else param.default
             type_expr = fn_for_analysis.__annotations__.get(name, None)
-            return self.create_proxy(
-                'placeholder', name, (), {}, type_expr=type_expr, value=value)
+            return self.create_proxy('placeholder', name, (), {}, type_expr=type_expr, value=value)
 
         args.extend(proxy_placeholder(next(names_iter)) for _ in range(skip_arg_idx, total_args))
 
         # TODO values for *args and **kwargs
         if co.co_kwonlyargcount > 0 or co.co_flags & HAS_VARSTUFF:
             # TODO: type annotations for *args and **kwargs
             if co.co_flags & inspect.CO_VARARGS:
@@ -396,16 +418,15 @@
                 collect_tensor_attrs(v, prefix_atoms + [k])
 
         collect_tensor_attrs(self.root, [])
 
         assert isinstance(fn, FunctionType)
 
         fn_globals = fn.__globals__  # run before it gets patched
-        fn, args = self.create_args_for_root(
-            fn, isinstance(root, torch.nn.Module), concrete_args)
+        fn, args = self.create_args_for_root(fn, isinstance(root, torch.nn.Module), concrete_args)
 
         parameter_proxy_cache: Dict[str, Proxy] = {}  # Reduce number of get_attr calls
 
         # Method dispatch on parameters is not recorded unless it's directly used.
         # Thus, we need to insert a proxy when __getattr__ requests a parameter.
         @functools.wraps(_orig_module_getattr)
         def module_getattr_wrapper(mod, attr):
@@ -417,41 +438,45 @@
                             parameter_proxy_cache[n] = self.create_proxy(
                                 'get_attr', n, (), {}, value=p)
                         return parameter_proxy_cache[n]
             return attr_val
 
         @functools.wraps(_orig_module_call)
         def module_call_wrapper(mod, *args, **kwargs):
+
             def forward(*args, **kwargs):
                 return _orig_module_call(mod, *args, **kwargs)
 
-            _autowrap_check(patcher, getattr(getattr(mod, "forward", mod), "__globals__", {}),
-                            self._autowrap_function_ids)
+            _autowrap_check(
+                patcher,
+                getattr(getattr(mod, "forward", mod), "__globals__", {}),
+                self._autowrap_function_ids)
             if self.concrete_mode:
                 return forward(*args, **kwargs)
             else:
                 return self.call_module(mod, forward, args, kwargs)
 
         with _Patcher() as patcher:
             # allow duplicate patches to support the case of nested calls
-            patcher.patch_method(torch.nn.Module, "__getattr__", module_getattr_wrapper,
-                                 deduplicate=False)
-            patcher.patch_method(torch.nn.Module, "__call__", module_call_wrapper,
-                                 deduplicate=False)
+            patcher.patch_method(
+                torch.nn.Module, "__getattr__", module_getattr_wrapper, deduplicate=False)
+            patcher.patch_method(
+                torch.nn.Module, "__call__", module_call_wrapper, deduplicate=False)
             _patch_wrapped_value_functions(patcher)
             _autowrap_check(patcher, fn_globals, self._autowrap_function_ids)
             for module in self._autowrap_search:
                 _autowrap_check(patcher, module.__dict__, self._autowrap_function_ids)
 
             self.create_node('output', 'output', (self.create_arg(fn(*args)),), {})
 
         return self.graph
 
 
 def _create_wrapped_value_func(orig_fn):
+
     @functools.wraps(orig_fn)
     def wrapped(*args, **kwargs):
         """
         Given an closed-over ``orig_function`` to invoke, search the args and kwargs for
         a Proxy object. If there is one, emit a ``call_function`` node to preserve the
         call to this leaf function directly. Otherwise, just return the results of
         this function call, as this function is not being traced.
@@ -491,23 +516,21 @@
         else:
             value = orig_fn(*args, **kwargs)
         return value
 
     return wrapped
 
 
-def _patch_wrapped_value_functions(patcher : _Patcher):
+def _patch_wrapped_value_functions(patcher: _Patcher):
     """
     Go through ``_wrapped_fn_patch_table`` and, for each frame object, wrap
     the listed global functions in the `_create_wrapped_func` wrapper.
     """
     for frame_dict, name in _wrapped_fns_to_patch:
         if name not in frame_dict and hasattr(builtins, name):
             orig_fn = getattr(builtins, name)
         else:
             orig_fn = frame_dict[name]
         patcher.patch(frame_dict, name, _create_wrapped_value_func(orig_fn))
 
     for cls, name in _wrapped_methods_to_patch:
         patcher.patch_method(cls, name, _create_wrapped_value_method(cls, name))
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas/graph/base.py` & `brevitas-0.9.0/src/brevitas/graph/base.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,37 +1,38 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
+from abc import ABC
+from abc import abstractmethod
 from inspect import getcallargs
-from abc import abstractmethod, ABC
 
 import torch
 from torch.nn import Module
 
-from brevitas.fx import GraphModule, Node
-from brevitas.fx import immutable_dict, get_testing_overrides
+from brevitas.fx import get_testing_overrides
+from brevitas.fx import GraphModule
+from brevitas.fx import immutable_dict
+from brevitas.fx import Node
 from brevitas.graph.utils import *
+from brevitas.utils.python_utils import islambda
 
 __all__ = [
     'Transform',
     'PerInputTrasform',
     'GraphTransform',
     'PerInputModuleToModuleByHook',
     'ModuleToModule',
     'InsertModuleCallAfter',
     'ModuleToModuleByName',
     'ModuleToModuleByClass',
     'ModuleInstanceToModuleInstance',
     'ModuleToModuleByInstance',
     'MethodToModule',
     'FnToModule',
-    'CallableToModule'
-]
-
+    'CallableToModule']
 
 _TORCH_TESTING_DICT = get_testing_overrides()
 
 
 class Transform(ABC):
 
     @abstractmethod
@@ -100,92 +101,93 @@
         self.replace_modules(model)
         self.cleanup()
         return model
 
 
 class ModuleToModule(GraphTransform, ABC):
 
-    def __init__(
-            self,
-            new_module_class,
-            **kwargs):
+    def __init__(self, new_module_class, **kwargs):
         super().__init__()
         self.new_module_class = new_module_class
         self.new_module_kwargs = kwargs
 
     def _map_origin_vars(self, vars: dict):
         return {k: v is not None if k == 'bias' else v for k, v in vars.items()}
 
     def _module_attributes(self, module):
         attrs = vars(module)
         # workaround since bias doesn't show up on vars of Linear
         if hasattr(module, 'bias'):
             attrs['bias'] = module.bias
         return attrs
 
+    def _evaluate_new_kwargs(self, new_kwargs, old_module):
+        update_dict = dict()
+        for k, v in self.new_module_kwargs.items():
+            if islambda(v):
+                v = v(old_module)
+            update_dict[k] = v
+        new_kwargs.update(update_dict)
+        return new_kwargs
+
     def _init_new_module(self, old_module: Module):
         # get attributes of original module
         new_kwargs = self._module_attributes(old_module)
         # transforms attribute of original module, e.g. bias Parameter -> bool
         new_kwargs = self._map_origin_vars(new_kwargs)
         # restrict to only values that are in the init of the new module
         new_module_signature_keys = signature_keys(self.new_module_class)
         new_kwargs = {k: v for k, v in new_kwargs.items() if k in new_module_signature_keys}
         # update with kwargs passed to the rewriter
-        new_kwargs.update(self.new_module_kwargs)
+        new_kwargs = self._evaluate_new_kwargs(new_kwargs, self.new_module_kwargs)
         # init the new module
         new_module = self.new_module_class(**new_kwargs)
         return new_module
 
     def _replace_old_module(self, model, old_module, new_module, load_state_dict=True):
         replace_module(model, old_module, new_module)
         if load_state_dict:
             new_module.load_state_dict(old_module.state_dict())
 
 
 class InsertModuleCallAfter(GraphTransform):
 
-    def __init__(self, module_name, node):
+    def __init__(self, module_name, node, node_to_exclude=()):
         self.module_name = module_name
         self.node = node
+        self.node_to_exclude = node_to_exclude
 
     def apply(self, graph_model: GraphModule) -> GraphModule:
         with graph_model.graph.inserting_after(self.node):
             quant_identity_node = graph_model.graph.call_module(self.module_name, args=(self.node,))
-        replace_all_uses_except(self.node, quant_identity_node, [quant_identity_node])
+        replace_all_uses_except(
+            self.node, quant_identity_node, [quant_identity_node] + list(self.node_to_exclude))
         graph_model.recompile()
         graph_model.graph.lint()
         return graph_model
 
 
 class ModuleInstanceToModuleInstance(Transform):
 
-    def __init__(
-            self,
-            old_module_instance,
-            new_module_instance):
+    def __init__(self, old_module_instance, new_module_instance):
         self.old_module_instance = old_module_instance
         self.new_module_instance = new_module_instance
 
     def apply(self, model: GraphModule) -> GraphModule:
         for old_module in model.modules():
             if old_module is self.old_module_instance:
                 # init the new module based on the old one
                 replace_module(model, old_module, self.new_module_instance)
                 break
         return model
 
 
 class ModuleToModuleByName(ModuleToModule):
 
-    def __init__(
-            self,
-            old_module_name,
-            new_module_class,
-            **kwargs):
+    def __init__(self, old_module_name, new_module_class, **kwargs):
         super().__init__(new_module_class, **kwargs)
         self.old_module_name = old_module_name
 
     def apply(self, model: GraphModule) -> GraphModule:
         for name, old_module in model.named_modules():
             if name == self.old_module_name:
                 # init the new module based on the old one
@@ -193,19 +195,15 @@
                 self._replace_old_module(model, old_module, new_module)
                 break
         return model
 
 
 class ModuleToModuleByInstance(ModuleToModule):
 
-    def __init__(
-            self,
-            old_module_instance,
-            new_module_class,
-            **kwargs):
+    def __init__(self, old_module_instance, new_module_class, **kwargs):
         super().__init__(new_module_class, **kwargs)
         self.old_module_instance = old_module_instance
 
     def apply(self, model: GraphModule) -> GraphModule:
         for old_module in model.modules():
             if old_module is self.old_module_instance:
                 # init the new module based on the old one
@@ -213,19 +211,15 @@
                 self._replace_old_module(model, old_module, new_module)
                 break
         return model
 
 
 class ModuleToModuleByClass(ModuleToModule):
 
-    def __init__(
-            self,
-            old_module_class,
-            new_module_class,
-            **kwargs):
+    def __init__(self, old_module_class, new_module_class, **kwargs):
         super().__init__(new_module_class, **kwargs)
         self.old_module_class = old_module_class
 
     def apply(self, model: GraphModule) -> GraphModule:
         old_new_module_dict = {}
         for old_module in model.modules():
             # check for equality, not inheritance
@@ -238,19 +232,15 @@
         for old_module, new_module in old_new_module_dict.items():
             self._replace_old_module(model, old_module, new_module)
         return model
 
 
 class CallableToModule(GraphTransform, ABC):
 
-    def __init__(
-            self,
-            old_callable,
-            new_module_class,
-            **kwargs):
+    def __init__(self, old_callable, new_module_class, **kwargs):
         super().__init__()
         self.old_callable = old_callable
         self.new_module_class = new_module_class
         self.new_module_kwargs = kwargs
 
     @abstractmethod
     def match_node(self, node: Node):
```

### Comparing `brevitas-0.8.0/src/brevitas/graph/calibrate.py` & `brevitas-0.9.0/src/brevitas/graph/calibrate.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,75 +1,83 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from functools import partial
 from abc import ABC
+from functools import partial
+import sys
 
 from torch import nn
+import torch.nn.functional as F
 
-from brevitas.proxy.parameter_quant import WeightQuantProxyFromInjector
+from brevitas.nn import QuantHardTanh
+from brevitas.nn import QuantLinear
+from brevitas.nn.quant_layer import QuantWeightBiasInputOutputLayer as QuantWBIOL
+from brevitas.nn.utils import compute_channel_view_shape
 from brevitas.proxy.parameter_quant import BiasQuantProxyFromInjector
+from brevitas.proxy.parameter_quant import WeightQuantProxyFromInjector
 from brevitas.proxy.runtime_quant import ActQuantProxyFromInjector
-from brevitas.proxy.runtime_quant import TruncQuantProxyFromInjector
 from brevitas.proxy.runtime_quant import ClampQuantProxyFromInjector
-from brevitas.nn.quant_layer import QuantWeightBiasInputOutputLayer as QuantWBIOL
-from brevitas.nn import QuantLinear, QuantHardTanh
-from brevitas.nn.utils import compute_channel_view_shape
+from brevitas.proxy.runtime_quant import TruncQuantProxyFromInjector
 from brevitas.quant_tensor import QuantTensor
-import torch.nn.functional as F
+
 from .base import Transform
 
 __all__ = [
-    'ClipFloatWeights',
-    'DisableEnableQuantization',
-    'bias_correction_mode',
-    'calibration_mode'
-]
-
-_PARAM_PROXIES = (
-    WeightQuantProxyFromInjector,
-    BiasQuantProxyFromInjector)
-
-_ACC_PROXIES = (
-    TruncQuantProxyFromInjector,
-    ClampQuantProxyFromInjector)
+    'ClipFloatWeights', 'DisableEnableQuantization', 'bias_correction_mode', 'calibration_mode']
+
+_PARAM_PROXIES = (WeightQuantProxyFromInjector, BiasQuantProxyFromInjector)
+
+_ACC_PROXIES = (TruncQuantProxyFromInjector, ClampQuantProxyFromInjector)
 
 _LAYERS_TO_CLIP = (
     nn.Conv1d,
     nn.Conv2d,
     nn.Conv3d,
     nn.Linear,
     nn.ConvTranspose1d,
     nn.ConvTranspose2d,
     nn.ConvTranspose3d)
 
 
+def extend_collect_stats_steps(module):
+    if hasattr(module, 'collect_stats_steps'):
+        # We extend the collect steps in PTQ to match potentially long calibrations
+        module.collect_stats_steps = sys.maxsize
+
+
 def finalize_collect_stats(module):
     if hasattr(module, 'collect_stats_steps') and hasattr(module, 'counter'):
-        module.counter = module.collect_stats_steps
+        # If the counter has already reached collect_stats_steps, we do not want to reset it
+        # otherwise the restrict_preprocess might be applied twice: during calibration
+        # (that happens in training mode) and then when the model is evaluated
+        module.counter = max(module.collect_stats_steps, module.counter)
 
 
 class calibration_mode:
+
     def __init__(self, model, enabled=True):
         self.model = model
         self.previous_training_state = model.training
         self.disable_quant_inference = DisableEnableQuantization()
-        self.enabled=enabled
+        self.enabled = enabled
 
     def __enter__(self):
         if self.enabled:
-            self.disable_quant_inference.apply(self.model, is_training=True, quantization_enabled=False)
+            self.model.apply(extend_collect_stats_steps)
+            self.disable_quant_inference.apply(
+                self.model, is_training=True, quantization_enabled=False)
 
     def __exit__(self, type, value, traceback):
         self.model.apply(finalize_collect_stats)
-        self.disable_quant_inference.apply(self.model, is_training=self.previous_training_state, quantization_enabled=True)
+        self.disable_quant_inference.apply(
+            self.model, is_training=self.previous_training_state, quantization_enabled=True)
 
 
 class bias_correction_mode:
+
     def __init__(self, model, enabled=True):
         self.model = model
         self.bias_correction = _BiasCorrection()
         self.enabled = enabled
         self.hooks = []
 
     def __enter__(self):
@@ -88,20 +96,20 @@
         super(ClipFloatWeights, self).__init__()
         self.threshold = threshold
         self.layers_to_clip = layers_to_clip
 
     def apply(self, model):
         for module in model.modules():
             if isinstance(module, self.layers_to_clip):
-                module.weight.data.clamp_(- self.threshold, self.threshold)
+                module.weight.data.clamp_(-self.threshold, self.threshold)
         return model
-        
+
 
 class DisableEnableQuantization(Transform):
-    
+
     def __init__(self):
         super(DisableEnableQuantization, self).__init__()
         self.disable_act_quant_hooks = []
 
     def unpack_input(self, inp):
         if isinstance(inp, tuple):
             inp = inp[0]
@@ -117,15 +125,15 @@
         # as this is what would happen with a shared act_quant
         # this gets called both during (empty) input_quant and act_quant
         # but for HardTanh it's not an issue
         if isinstance(module.tracked_module_list[0], QuantHardTanh):
             inp = F.hardtanh(
                 inp, min_val=module.quant_injector.min_val, max_val=module.quant_injector.max_val)
         return QuantTensor(value=inp, training=module.training)
-    
+
     def disable_act_quantization(self, model, is_training):
         for module in model.modules():
             if isinstance(module, ActQuantProxyFromInjector):
                 hook = module.register_forward_hook(self.disable_act_quant_hook)
                 module.train(is_training)
                 self.disable_act_quant_hooks.append(hook)
             elif isinstance(module, _ACC_PROXIES):
@@ -133,15 +141,15 @@
                 module.disable_quant = True
 
     def disable_param_quantization(self, model, is_training):
         for module in model.modules():
             if isinstance(module, _PARAM_PROXIES):
                 module.train(is_training)
                 module.disable_quant = True
-    
+
     def enable_act_quantization(self, model, is_training):
         for module in model.modules():
             if isinstance(module, _ACC_PROXIES):
                 module.train(is_training)
                 module.disable_quant = False
             elif isinstance(module, ActQuantProxyFromInjector):
                 module.train(is_training)
@@ -204,25 +212,27 @@
     def apply_correction(self, model):
         for name, module in model.named_modules():
             if name in self.correction_map.keys():
                 correction = self.correction_map[name] / self.iterations[name]
                 if module.bias is not None:
                     module.bias.data += correction
                 else:
-                    module.register_parameter('bias', nn.Parameter(correction).to(module.weight.device))
+                    module.register_parameter(
+                        'bias', nn.Parameter(correction).to(module.weight.device))
 
     def correct_bias_hook(self, module, inp, name, parent_module):
         inp = self.unpack_input(inp)
         if name in self.float_mean_map.keys():
             transpose_dim = self.channel_dim(inp, parent_module)
             quant_mean = self.compute_mean(inp, transpose_dim)
             error = self.float_mean_map[name] - quant_mean
             self.update_correction(name, error)
             del self.float_mean_map[name]
-            inp_broadcast_shape = compute_channel_view_shape(inp, channel_dim=self.channel_dim(inp, parent_module))
+            inp_broadcast_shape = compute_channel_view_shape(
+                inp, channel_dim=self.channel_dim(inp, parent_module))
             return inp + error.reshape(inp_broadcast_shape)
 
     def register_collect_float_mean_hook(self, module, name):
         hook_fn = partial(self.collect_float_mean_hook, name=name, parent_module=module)
         hook = module.output_quant.register_forward_pre_hook(hook_fn)
         self.collect_float_mean_hooks.append(hook)
 
@@ -263,15 +273,15 @@
         and the error between the two.
 
         We do not return the original Quant output, but its "corrected version", i.e., the FP version
         """
         self.disable_act_quantization(module, is_training=False)
         self.disable_param_quantization(module, is_training=False)
         self.register_collect_float_mean_hook(module, name)
-        module.forward(*inp) # Required to avoid infinite recursion
+        module.forward(*inp)  # Required to avoid infinite recursion
         self.float_mean_hooks_cleanup()
         self.enable_act_quantization(module, is_training=False)
         self.enable_param_quantization(module, is_training=False)
         self.register_correct_bias_hook(module, name)
-        out = module.forward(*inp) # Required to avoid infinite recursion
+        out = module.forward(*inp)  # Required to avoid infinite recursion
         self.iterations[name] += 1
-        return out
+        return out
```

### Comparing `brevitas-0.8.0/src/brevitas/graph/fixed_point.py` & `brevitas-0.9.0/src/brevitas/graph/fixed_point.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,30 +1,28 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import torch
 from torch import nn
 
+from brevitas.fx import GraphModule
+from brevitas.fx import immutable_dict
+from brevitas.fx import immutable_list
 import brevitas.nn as qnn
-from brevitas.fx import GraphModule, immutable_dict, immutable_list
 from brevitas.nn.utils import merge_bn
+
 from .base import UntilFixedPointGraphTransform
 from .utils import del_module
-from .utils import replace_all_uses_except
 from .utils import get_module
-from .utils import get_output_channels
 from .utils import get_output_channel_dim
+from .utils import get_output_channels
 from .utils import matches_module_pattern
+from .utils import replace_all_uses_except
 
-__all__ = [
-    'MoveSplitBatchNormBeforeCat',
-    'MergeBatchNorm',
-    'CollapseConsecutiveConcats'
-]
+__all__ = ['MoveSplitBatchNormBeforeCat', 'MergeBatchNorm', 'CollapseConsecutiveConcats']
 
 
 class MoveSplitBatchNormBeforeCat(UntilFixedPointGraphTransform):
 
     DEFAULT_BEFORE_MODULES_TYPES = (
         nn.BatchNorm1d,
         nn.BatchNorm2d,
@@ -43,35 +41,33 @@
 
     def __init__(self, before_modules_types=DEFAULT_BEFORE_MODULES_TYPES):
         super(MoveSplitBatchNormBeforeCat, self).__init__()
         self.before_modules_types = before_modules_types
 
     def is_converged(self, graph_model: GraphModule) -> bool:
         for cat_node in graph_model.graph.nodes:
-            if (cat_node.target is torch.cat
-                    and len(cat_node.users) == 1
-                    and cat_node.kwargs['dim'] == 1
-                    and list(cat_node.users)[0].op == 'call_module'):
+            if (cat_node.target is torch.cat and len(cat_node.users) == 1 and
+                    cat_node.kwargs['dim'] == 1 and list(cat_node.users)[0].op == 'call_module'):
                 bn_node = list(cat_node.users)[0]
                 module = get_module(graph_model, bn_node.target)
                 if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):
                     inp_nodes = cat_node.all_input_nodes
-                    if all([inp_node.op == 'call_module' 
-                        and len(inp_node.users) == 1 for inp_node in inp_nodes]):
+                    if all([inp_node.op == 'call_module' and len(inp_node.users) == 1
+                            for inp_node in inp_nodes]):
                         before_mods = [
                             get_module(graph_model, inp_node.target) for inp_node in inp_nodes]
                         if all(isinstance(mod, self.before_modules_types) for mod in before_mods):
                             assert inp_nodes == cat_node.kwargs['tensors']
                             num_features_list = [get_output_channels(mod) for mod in before_mods]
                             chunk_bn_list = [type(module)(n) for n in num_features_list]
                             for i, chunk_bn in enumerate(chunk_bn_list):
                                 chunk_bn_name = f'{bn_node.name}_{i}'
                                 graph_model.add_module(chunk_bn_name, chunk_bn)
                                 start = sum(num_features_list[:i])
-                                end = sum(num_features_list[:i+1])
+                                end = sum(num_features_list[:i + 1])
                                 chunk_bn.weight.data = module.weight.data[start:end]
                                 chunk_bn.bias.data = module.bias.data[start:end]
                                 chunk_bn.running_mean = module.running_mean.data[start:end]
                                 chunk_bn.running_var = module.running_var.data[start:end]
                                 inp_node = cat_node.kwargs['tensors'][i]
                                 with graph_model.graph.inserting_after(inp_node):
                                     chunk_bn_node = graph_model.graph.call_module(
@@ -84,32 +80,25 @@
                             graph_model.recompile()
                             return False
         return True
 
 
 class MergeBatchNorm(UntilFixedPointGraphTransform):
 
-    DEFAULT_PATTERNS = (
-        (nn.BatchNorm1d, nn.BatchNorm1d),
-        (nn.BatchNorm2d, nn.BatchNorm2d),
-        (nn.BatchNorm3d, nn.BatchNorm3d),
-        (nn.Linear, nn.BatchNorm1d),
-        (nn.Conv1d, nn.BatchNorm1d),
-        (nn.Conv2d, nn.BatchNorm2d),
-        (nn.Conv3d, nn.BatchNorm3d),
-        (nn.ConvTranspose1d, nn.BatchNorm1d),
-        (nn.ConvTranspose2d, nn.BatchNorm2d),
-        (nn.ConvTranspose3d, nn.BatchNorm3d),
-        (qnn.BatchNorm1dToQuantScaleBias, nn.BatchNorm1d),
-        (qnn.BatchNorm2dToQuantScaleBias, nn.BatchNorm2d),
-        (qnn.QuantLinear, nn.BatchNorm1d),
-        (qnn.QuantConv1d, nn.BatchNorm1d),
-        (qnn.QuantConv2d, nn.BatchNorm2d),
-        (qnn.QuantConvTranspose1d, nn.BatchNorm1d),
-        (qnn.QuantConvTranspose2d, nn.BatchNorm2d))
+    DEFAULT_PATTERNS = ((nn.BatchNorm1d, nn.BatchNorm1d), (nn.BatchNorm2d, nn.BatchNorm2d),
+                        (nn.BatchNorm3d, nn.BatchNorm3d), (nn.Linear, nn.BatchNorm1d),
+                        (nn.Conv1d, nn.BatchNorm1d), (nn.Conv2d, nn.BatchNorm2d),
+                        (nn.Conv3d, nn.BatchNorm3d), (nn.ConvTranspose1d, nn.BatchNorm1d),
+                        (nn.ConvTranspose2d, nn.BatchNorm2d), (nn.ConvTranspose3d, nn.BatchNorm3d),
+                        (qnn.BatchNorm1dToQuantScaleBias,
+                         nn.BatchNorm1d), (qnn.BatchNorm2dToQuantScaleBias,
+                                           nn.BatchNorm2d), (qnn.QuantLinear, nn.BatchNorm1d),
+                        (qnn.QuantConv1d, nn.BatchNorm1d), (qnn.QuantConv2d, nn.BatchNorm2d),
+                        (qnn.QuantConvTranspose1d,
+                         nn.BatchNorm1d), (qnn.QuantConvTranspose2d, nn.BatchNorm2d))
 
     def __init__(self, patterns=DEFAULT_PATTERNS):
         super(MergeBatchNorm, self).__init__()
         self.patterns = list(patterns)
 
     def is_converged(self, graph_model: GraphModule):
         named_modules = dict(graph_model.named_modules())
@@ -132,28 +121,29 @@
 class CollapseConsecutiveConcats(UntilFixedPointGraphTransform):
 
     def merge_tensor_kwargs(self, node_to_extract, node_to_merge_in):
         cat_tensors1 = list(node_to_extract.kwargs['tensors'])
         cat_tensors2 = node_to_merge_in.kwargs['tensors']
         if not isinstance(cat_tensors2, (list, tuple)):
             cat_tensors2 = [cat_tensors2]
+        index_for_insertion = cat_tensors2.index(node_to_extract)
         cat_tensors2 = [t for t in cat_tensors2 if t is not node_to_extract]
+        cat_tensors2[index_for_insertion:index_for_insertion] = cat_tensors1
         kwargs = dict(node_to_merge_in.kwargs)
-        kwargs['tensors'] = (cat_tensors1 + cat_tensors2)
+        kwargs['tensors'] = cat_tensors2
         node_to_merge_in.kwargs = immutable_dict(kwargs)
 
     def is_converged(self, graph_model):
         for i, node in enumerate(graph_model.graph.nodes):
             if node.op == 'call_function' and node.target is torch.cat:
                 for inp_node in node.all_input_nodes:
-                    if (inp_node.op == 'call_function'
-                            and inp_node.target is torch.cat
-                            and node.kwargs['dim'] == inp_node.kwargs['dim']
-                            and len(inp_node.users) == 1):
-                        self.merge_tensor_args(inp_node, node)
+                    if (inp_node.op == 'call_function' and inp_node.target is torch.cat and
+                            node.kwargs['dim'] == inp_node.kwargs['dim'] and
+                            len(inp_node.users) == 1):
+                        self.merge_tensor_kwargs(inp_node, node)
                         graph_model.graph.erase_node(inp_node)
                         graph_model.graph.lint()
                         graph_model.recompile()
                         return False
         return True
 
     def move_args_to_kwargs(self, graph_model):
```

### Comparing `brevitas-0.8.0/src/brevitas/graph/per_input.py` & `brevitas-0.9.0/src/brevitas/graph/per_input.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,62 +1,60 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from functools import reduce
 from operator import mul
 
 import torch
 from torch import nn
 
 from brevitas.graph.utils import replace_module
-from brevitas.nn import QuantConv1d, QuantConv2d
+from brevitas.nn import QuantConv1d
+from brevitas.nn import QuantConv2d
+
 from .base import PerInputModuleToModuleByHook
 
-__all__ = [
-    'AdaptiveAvgPoolToAvgPool',
-    'AvgPoolToQuantDepthwiseConv'
-]
+__all__ = ['AdaptiveAvgPoolToAvgPool', 'AvgPoolToQuantDepthwiseConv']
 
 
 class AdaptiveAvgPoolToAvgPool(PerInputModuleToModuleByHook):
 
-    SUPPORTED_LAYERS = (
-        nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d)
+    SUPPORTED_LAYERS = (nn.AdaptiveAvgPool1d, nn.AdaptiveAvgPool2d, nn.AdaptiveAvgPool3d)
 
     def register_hooks(self, model):
         for module in model.modules():
             if isinstance(module, self.SUPPORTED_LAYERS):
                 hook_handler = module.register_forward_pre_hook(self.hook_fn)
                 self.hook_handlers.append(hook_handler)
-    
+
     def get_adaptive_output_size(self, adaptive_avgpool):
         output_size = adaptive_avgpool.output_size
         if isinstance(output_size, tuple):
             return output_size
         else:
             assert isinstance(output_size, int)
             if isinstance(adaptive_avgpool, nn.AdaptiveAvgPool1d):
                 return (output_size,)
             elif isinstance(adaptive_avgpool, nn.AdaptiveAvgPool2d):
                 return (output_size, output_size)
             else:
                 assert isinstance(adaptive_avgpool, nn.AdaptiveAvgPool3d)
                 return (output_size, output_size, output_size)
-    
+
     def replace_modules(self, model, global_avgpool_unit_stride=True):
         for adaptive_avgpool, size in self.input_size_map.items():
             output_size = self.get_adaptive_output_size(adaptive_avgpool)
             input_size = size[-len(output_size):]
             mod = [input_size[i] % output_size[i] for i in range(0, len(output_size))]
             if mod == [0] * len(output_size):
                 # Reference https://stackoverflow.com/a/63603993/16744139
                 s = tuple(int(input_size[i] / output_size[i]) for i in range(0, len(output_size)))
-                k = tuple(input_size[i] - s[i] * (output_size[i] - 1) for i in range(0, len(output_size)))
-                # Set stride 1 whenever the adaptive avg pool is global 
+                k = tuple(
+                    input_size[i] - s[i] * (output_size[i] - 1) for i in range(0, len(output_size)))
+                # Set stride 1 whenever the adaptive avg pool is global
                 if global_avgpool_unit_stride and all(os == 1 for os in output_size):
                     s = tuple([1] * len(s))
                 kwargs = {'kernel_size': k, 'stride': s}
                 if isinstance(adaptive_avgpool, nn.AdaptiveAvgPool1d):
                     avgpool = nn.AvgPool1d(**kwargs)
                 elif isinstance(adaptive_avgpool, nn.AdaptiveAvgPool2d):
                     avgpool = nn.AvgPool2d(**kwargs)
@@ -64,16 +62,15 @@
                     assert isinstance(adaptive_avgpool, nn.AdaptiveAvgPool3d)
                     avgpool = nn.AvgPool3d(**kwargs)
                 replace_module(model, adaptive_avgpool, avgpool)
 
 
 class AvgPoolToQuantDepthwiseConv(PerInputModuleToModuleByHook):
 
-    SUPPORTED_LAYERS = (
-        nn.AvgPool1d, nn.AvgPool2d, nn.AvgPool3d)
+    SUPPORTED_LAYERS = (nn.AvgPool1d, nn.AvgPool2d, nn.AvgPool3d)
 
     def __init__(self, **conv_kwargs):
         super().__init__()
         self.conv_kwargs = conv_kwargs
 
     def register_hooks(self, model):
         for module in model.modules():
@@ -96,18 +93,19 @@
                 dw_conv = QuantConv1d(**kwargs)
             elif isinstance(avgpool, nn.AvgPool2d):
                 dw_conv = QuantConv2d(**kwargs)
             else:
                 assert isinstance(avgpool, nn.AvgPool3d)
                 raise RuntimeError("QuantConv3d not supported yet.")
             kernel_value = 1. / reduce(mul, dw_conv.kernel_size)
-            dw_conv.register_parameter('scalar_weight', torch.nn.Parameter(torch.tensor(kernel_value)))
+            dw_conv.register_parameter(
+                'scalar_weight', torch.nn.Parameter(torch.tensor(kernel_value)))
             weight_shape = dw_conv.weight.shape
             del dw_conv.weight
             # Attach property to instance by dynamically subclassing and assigning the subclass to the instance
             # Reference https://gist.github.com/Wilfred/49b0409c6489f1bdf5a5c98a488b31b5
             class_name = dw_conv.__class__.__name__ + 'FromAvgPool'
             child_class = type(
                 class_name, (dw_conv.__class__,),
                 {'weight': property(lambda self: self.scalar_weight.expand(weight_shape))})
             dw_conv.__class__ = child_class
-            replace_module(model, avgpool, dw_conv)
+            replace_module(model, avgpool, dw_conv)
```

### Comparing `brevitas-0.8.0/src/brevitas/graph/standardize.py` & `brevitas-0.9.0/src/brevitas/graph/standardize.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,27 +1,32 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from copy import deepcopy
 from typing import Dict
 
 from torch import nn
 import torch.nn.functional as F
 
-from brevitas.fx import GraphModule, Node, immutable_dict
-from .base import GraphTransform, MethodToModule, FnToModule
-from .utils import set_module, replace_all_uses_except, get_module
+from brevitas.fx import GraphModule
+from brevitas.fx import immutable_dict
+from brevitas.fx import Node
+
+from .base import FnToModule
+from .base import GraphTransform
+from .base import MethodToModule
+from .utils import get_module
+from .utils import replace_all_uses_except
+from .utils import set_module
 
 __all__ = [
     'DuplicateSharedStatelessModule',
     'MeanMethodToAdaptiveAvgPool2d',
     'TorchFunctionalToModule',
-    'DisableLastReturnQuantTensor'
-]
+    'DisableLastReturnQuantTensor']
 
 
 class DuplicateSharedStatelessModule(GraphTransform):
 
     def apply(self, graph_model: GraphModule):
         named_mods = graph_model.named_modules()  # duplicates are returned only once
         dup_mod_dict: Dict[str, int] = {}
@@ -43,25 +48,21 @@
         return graph_model
 
 
 class MeanMethodToAdaptiveAvgPool2d(MethodToModule):
 
     def __init__(self):
         super(MeanMethodToAdaptiveAvgPool2d, self).__init__(
-            old_callable='mean',
-            new_module_class=nn.AdaptiveAvgPool2d,
-            output_size=(1, 1))
+            old_callable='mean', new_module_class=nn.AdaptiveAvgPool2d, output_size=(1, 1))
 
     def match_node(self, node: Node) -> bool:
         spr = super(MeanMethodToAdaptiveAvgPool2d, self).match_node(node)
-        is_adaptive_2d_mean = (
-                (2, 3) in node.args
-                or [2, 3] in node.args
-                or 'dim' in node.kwargs
-                and (node.kwargs['dim'] == (2, 3) or node.kwargs['dim'] == [2, 3]))
+        is_adaptive_2d_mean = ((2, 3) in node.args or [2, 3] in node.args or
+                               'dim' in node.kwargs and
+                               (node.kwargs['dim'] == (2, 3) or node.kwargs['dim'] == [2, 3]))
         return spr and is_adaptive_2d_mean
 
     def move_node_args_to_kwargs(self, node: Node):
         if 'dim' in node.kwargs:
             node.kwargs = immutable_dict(dict(node.kwargs).pop('dim'))
         elif (2, 3) in node.args or [2, 3] in node.args:
             node.args = tuple([a for a in node.args if a != (2, 3) and a != [2, 3]])
@@ -73,47 +74,59 @@
             batch_size_node = graph_model.graph.call_method('size', args=(node, 0))
         with graph_model.graph.inserting_after(batch_size_node):
             squeeze_node = graph_model.graph.call_method(
                 'reshape', args=(node, (batch_size_node, -1)))
         replace_all_uses_except(node, squeeze_node, [squeeze_node, batch_size_node])
 
 
+class RemoveStochasticModules(GraphTransform):
+
+    def apply(self, graph_model: GraphModule) -> GraphModule:
+        for node in graph_model.graph.nodes:
+            if 'stochastic_depth' in node.name:
+                previous_nodes = node.all_input_nodes
+                next_node = list(node.users.keys())[0]
+                next_node_args = list(next_node.args)
+                index_for_insertion = next_node_args.index(node)
+                next_node_args = [t for t in next_node_args if t is not node]
+                next_node_args[index_for_insertion:index_for_insertion] = previous_nodes
+                next_node.args = tuple(next_node_args)
+                graph_model.graph.erase_node(node)
+                graph_model.graph.lint()
+                graph_model.recompile()
+        return graph_model
+
+
 class TorchFunctionalToModule(GraphTransform):
 
-    FN_TO_MODULE_MAP = (
-        (F.relu, nn.ReLU),
-        (F.relu_, nn.ReLU),
-        (F.relu6, nn.ReLU6),
-        (F.hardtanh, nn.Hardtanh),
-        (F.hardtanh_, nn.Hardtanh),
-        (F.leaky_relu, nn.LeakyReLU),
-        (F.leaky_relu_, nn.LeakyReLU),
-        (F.max_pool1d, nn.MaxPool1d),
-        (F.max_pool2d, nn.MaxPool2d),
-        (F.max_pool3d, nn.MaxPool3d),
-        (F.avg_pool1d, nn.AvgPool1d),
-        (F.avg_pool2d, nn.AvgPool2d),
-        (F.avg_pool3d, nn.AvgPool3d),
-        (F.adaptive_avg_pool1d, nn.AdaptiveAvgPool1d),
-        (F.adaptive_avg_pool2d, nn.AdaptiveAvgPool2d),
-        (F.adaptive_avg_pool3d, nn.AdaptiveAvgPool3d))
+    FN_TO_MODULE_MAP = ((F.relu, nn.ReLU), (F.relu_, nn.ReLU), (F.relu6, nn.ReLU6),
+                        (F.hardtanh, nn.Hardtanh), (F.hardtanh_,
+                                                    nn.Hardtanh), (F.leaky_relu, nn.LeakyReLU),
+                        (F.leaky_relu_, nn.LeakyReLU), (F.max_pool1d,
+                                                        nn.MaxPool1d), (F.max_pool2d, nn.MaxPool2d),
+                        (F.max_pool3d, nn.MaxPool3d), (F.avg_pool1d,
+                                                       nn.AvgPool1d), (F.avg_pool2d, nn.AvgPool2d),
+                        (F.avg_pool3d, nn.AvgPool3d), (F.adaptive_avg_pool1d, nn.AdaptiveAvgPool1d),
+                        (F.adaptive_avg_pool2d,
+                         nn.AdaptiveAvgPool2d), (F.adaptive_avg_pool3d, nn.AdaptiveAvgPool3d))
 
     def __init__(self, fn_to_module_map=FN_TO_MODULE_MAP):
         super().__init__()
         self.rewriter_list = [FnToModule(fn, mclass) for (fn, mclass) in fn_to_module_map]
 
     def apply(self, model: GraphModule) -> GraphModule:
         for rewriter in self.rewriter_list:
             model = rewriter.apply(model)
         return model
 
 
 class DisableLastReturnQuantTensor(GraphTransform):
 
     def apply(self, graph_model: GraphModule):
-        for node in graph_model.graph.nodes:
+        for node in reversed(graph_model.graph.nodes):
             if node.op == 'call_module':
                 module = get_module(graph_model, node.target)
-                if hasattr(module, 'return_quant_tensor') and module.return_quant_tensor:
-                    if len(node.users) == 1 and list(node.users)[0].op == 'output':
-                        module.return_quant_tensor = False
-        return graph_model
+                if len(node.users) == 1 and node.op == 'call_module' and \
+                    hasattr(module, 'return_quant_tensor') and module.return_quant_tensor:
+                    module.return_quant_tensor = False
+                    break
+        return graph_model
```

### Comparing `brevitas-0.8.0/src/brevitas/graph/target/flexml.py` & `brevitas-0.9.0/src/brevitas/graph/quantize_impl.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,413 +1,433 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
-
-
 import operator
 
 import torch
-from torch import nn
+import torch.nn as nn
 
 import brevitas
-from brevitas import config
-import brevitas.nn as qnn
-from brevitas.quant import Int16Bias
-from brevitas.quant import Int8WeightPerTensorFixedPoint
-from brevitas.quant import Int8ActPerTensorFixedPoint
-from brevitas.quant import Uint8ActPerTensorFixedPoint
-from brevitas.quant import Uint8ActPerTensorFixedPointMaxInit
-from brevitas.graph.base import ModuleToModuleByClass, ModuleToModuleByInstance
-from brevitas.graph.base import ModuleInstanceToModuleInstance
 from brevitas.graph.base import InsertModuleCallAfter
-from brevitas.graph.standardize import TorchFunctionalToModule
-from brevitas.graph.standardize import DuplicateSharedStatelessModule
-from brevitas.graph.standardize import MeanMethodToAdaptiveAvgPool2d
-from brevitas.graph.standardize import DisableLastReturnQuantTensor
-from brevitas.graph.per_input import AdaptiveAvgPoolToAvgPool
-from brevitas.graph.per_input import AvgPoolToQuantDepthwiseConv
-from brevitas.graph.fixed_point import MergeBatchNorm
-from brevitas.graph.fixed_point import MoveSplitBatchNormBeforeCat
-from brevitas.graph.fixed_point import CollapseConsecutiveConcats
-from brevitas.graph.equalize import EqualizeGraph
-from brevitas.fx import value_trace
+from brevitas.graph.base import ModuleInstanceToModuleInstance
+from brevitas.graph.base import ModuleToModuleByInstance
 from brevitas.graph.utils import get_module
 
-
 ADD_FNS = [torch.add, operator.add, operator.iadd]
 
 ADD_METHODS = ['add', 'add_']
 CAT = brevitas.original_cat
 
-
-QUANT_WBIOL_MAP = {
-    nn.Conv1d: qnn.QuantConv1d,
-    nn.Conv2d: qnn.QuantConv2d,
-    nn.ConvTranspose1d: qnn.QuantConvTranspose1d,
-    nn.ConvTranspose2d: qnn.QuantConvTranspose2d,
-    nn.BatchNorm1d: qnn.BatchNorm1dToQuantScaleBias,
-    nn.BatchNorm2d: qnn.BatchNorm2dToQuantScaleBias,
-    nn.Linear: qnn.QuantLinear}
-
-
-QUANT_AVGPOOL_MAP = {
-    nn.AvgPool2d: qnn.flexml.FlexMLQuantAvgPool2d
-}
-
-
-SIGN_PRESERVING_MODULES = [
+SIGN_PRESERVING_MODULES = (
+    nn.Dropout,
+    nn.Dropout2d,
+    nn.Dropout3d,
     nn.MaxPool1d,
     nn.MaxPool2d,
     nn.MaxPool3d,
     nn.AvgPool1d,
     nn.AvgPool2d,
     nn.AvgPool3d,
     nn.AdaptiveAvgPool1d,
     nn.AdaptiveAvgPool2d,
-    nn.AdaptiveAvgPool3d]
+    nn.AdaptiveAvgPool3d)
+
+PRECISION_PRESERVING_MODULES = (
+    nn.Dropout, nn.Dropout2d, nn.Dropout3d, nn.MaxPool1d, nn.MaxPool2d, nn.MaxPool3d)
 
 
-def flexml_inp_placeholder_handler(model):
+def inp_placeholder_handler(model, input_quantizer):
+    """
+    Add Quantization step at the input of the network.
+    """
     rewriters = []
+    if input_quantizer is None:
+        return model
     for node in model.graph.nodes:
         if node.op == 'placeholder':
-            inp_quant = qnn.QuantIdentity(Int8ActPerTensorFixedPoint, return_quant_tensor=True)
+            act_quant, kwargs_act_quant = input_quantizer
+            inp_quant = act_quant(**kwargs_act_quant)
             name = node.name + '_quant'
             model.add_module(name, inp_quant)
             rewriters.append(InsertModuleCallAfter(name, node))
     for rewriter in rewriters:
         model = rewriter.apply(model)
     return model
 
 
-def are_inputs_unsigned(model, node, is_unsigned_list):
+def are_inputs_unsigned(model, node, is_unsigned_list, quant_act_map, unsigned_act_tuple):
     for inp_node in node.all_input_nodes:
         if inp_node.op == 'call_module':
             inp_module = get_module(model, inp_node.target)
-            if isinstance(inp_module, (nn.ReLU, nn.ReLU6)):
+            if isinstance(inp_module, tuple(quant_act_map.keys())) and isinstance(
+                    inp_module, unsigned_act_tuple):
                 is_unsigned_list.append(True)
             elif isinstance(inp_module, tuple(SIGN_PRESERVING_MODULES)):
-                are_inputs_unsigned(model, inp_node, is_unsigned_list)
-            elif isinstance(inp_module, (qnn.QuantReLU, qnn.QuantIdentity, qnn.QuantHardTanh)):
+                are_inputs_unsigned(
+                    model, inp_node, is_unsigned_list, quant_act_map, unsigned_act_tuple)
+            elif hasattr(inp_module, 'is_quant_act_signed'):
                 is_unsigned_list.append(not inp_module.is_quant_act_signed)
             else:
                 is_unsigned_list.append(False)
         elif inp_node.op == 'call_function':
             if inp_node.target in [torch.reshape, torch.flatten, torch.transpose, CAT] + ADD_FNS:
-                are_inputs_unsigned(model, inp_node, is_unsigned_list)
+                are_inputs_unsigned(
+                    model, inp_node, is_unsigned_list, quant_act_map, unsigned_act_tuple)
             else:
                 is_unsigned_list.append(False)
         elif inp_node.op == 'call_method':
             if inp_node.target in ['view', 'reshape', 'flatten', 't', 'permute'] + ADD_METHODS:
-                are_inputs_unsigned(model, inp_node, is_unsigned_list)
+                are_inputs_unsigned(
+                    model, inp_node, is_unsigned_list, quant_act_map, unsigned_act_tuple)
             else:
                 is_unsigned_list.append(False)
     return all(is_unsigned_list)
 
 
-def _tensor_quant_in_list(tensor_quant, module_list, same_sign):
+def _tensor_quant_in_list(act_quant, module_list, same_sign):
+    tq = act_quant.fused_activation_quant_proxy.tensor_quant
     for m in module_list:
         if m is None:
             continue
-        if same_sign and m is tensor_quant:
+        m_tq = m.fused_activation_quant_proxy.tensor_quant
+        if same_sign and m_tq is tq:
             return True
-        elif not same_sign and m.scaling_impl is tensor_quant.scaling_impl and m.int_scaling_impl is tensor_quant.int_scaling_impl:
+        elif not same_sign and m_tq.scaling_impl is tq.scaling_impl and m_tq.int_scaling_impl is tq.int_scaling_impl:
             return True
     return False
 
 
-def are_inputs_quantized(model, node, quantized_modules_list, same_sign):
+def are_inputs_quantized_and_aligned(model, node, quantized_modules_list, quant_act_map, same_sign):
+    """
+    Check if the inputs to `node` are quantized and aligned.
+    If same_sign=True, aligned means that the inputs should have same sign and scale factor.
+    Otherwise, they need to have only the same scale factors.
+    If none of the previous conditions are met (e.g., FP input, or not aligned scales), the function
+    returns False.
+    """
     for inp_node in node.all_input_nodes:
         if inp_node.op == 'call_module':
             inp_module = get_module(model, inp_node.target)
-            if isinstance(inp_module, (nn.ReLU, nn.ReLU6)):
+            if isinstance(inp_module, tuple(quant_act_map.keys())):
                 quantized_modules_list.append(None)
-            elif isinstance(inp_module, tuple(SIGN_PRESERVING_MODULES)):
-                are_inputs_quantized(model, inp_node, quantized_modules_list, same_sign)
-            elif isinstance(inp_module, (qnn.QuantReLU, qnn.QuantIdentity, qnn.QuantHardTanh)):
-                tq = inp_module.act_quant.fused_activation_quant_proxy.tensor_quant
-                if _tensor_quant_in_list(tq, quantized_modules_list, same_sign):
-                    continue
-                quantized_modules_list.append(tq)
-            elif isinstance(inp_module, qnn.flexml.FlexMLQuantLeakyReLU):
-                tq = inp_module.output_quant.act_quant.fused_activation_quant_proxy.tensor_quant
-                if _tensor_quant_in_list(tq, quantized_modules_list, same_sign):
+            elif isinstance(inp_module, tuple(PRECISION_PRESERVING_MODULES)) and (
+                    not same_sign or
+                (same_sign and isinstance(inp_module, tuple(SIGN_PRESERVING_MODULES)))):
+                are_inputs_quantized_and_aligned(
+                    model, inp_node, quantized_modules_list, quant_act_map, same_sign)
+            elif hasattr(inp_module, 'act_quant'):
+                aq = inp_module.act_quant
+                if _tensor_quant_in_list(aq, quantized_modules_list, same_sign):
                     continue
-                quantized_modules_list.append(tq)
+                quantized_modules_list.append(aq)
             else:
                 quantized_modules_list.append(None)
         elif inp_node.op == 'call_function':
             if inp_node.target in [torch.reshape, torch.flatten, torch.transpose]:
-                are_inputs_quantized(model, inp_node, quantized_modules_list, same_sign)
+                are_inputs_quantized_and_aligned(
+                    model, inp_node, quantized_modules_list, quant_act_map, same_sign)
             elif inp_node.target is CAT:
-                are_inputs_quantized(model, inp_node, quantized_modules_list, True)
+                are_inputs_quantized_and_aligned(
+                    model, inp_node, quantized_modules_list, quant_act_map, True)
             elif inp_node.target in ADD_FNS:
-                are_inputs_quantized(model, inp_node, quantized_modules_list, False)
+                are_inputs_quantized_and_aligned(
+                    model, inp_node, quantized_modules_list, quant_act_map, False)
             else:
                 quantized_modules_list.append(None)
         elif inp_node.op == 'call_method':
             if inp_node.target in ['view', 'reshape', 'flatten', 't', 'permute']:
-                are_inputs_quantized(model, inp_node, quantized_modules_list, same_sign)
+                are_inputs_quantized_and_aligned(
+                    model, inp_node, quantized_modules_list, quant_act_map, same_sign)
             elif inp_node.target in ADD_METHODS:
-                are_inputs_quantized(model, inp_node, quantized_modules_list, False)
+                are_inputs_quantized_and_aligned(
+                    model, inp_node, quantized_modules_list, quant_act_map, False)
             else:
                 quantized_modules_list.append(None)
     if None in quantized_modules_list:
         return False
     elif len(quantized_modules_list) > 1:
         return False
     else:
         return True
 
 
-def output_quant_handler(model, node, rewriters, is_sign_preserving):
+def output_quant_handler(
+        model,
+        node,
+        rewriters,
+        is_sign_preserving,
+        quant_identity_map,
+        quant_act_map=None,
+        unsigned_act_tuple=None):
+    """
+    Starting from `node`, check if any of the users requires requantization (i.e., it does not have
+    an act_quant attribute). In that case, the functions adds a requantization step to which all the
+    branches are connected. If another branch has its own requantization step, there will be two
+    consecutive for that branch.
+    """
+    if is_sign_preserving and (quant_act_map is None or unsigned_act_tuple is None):
+        raise RuntimeError("Missing information for output_quant_handler")
     quant_module = None
     quant_module_name = None
     for user in node.users:
         output_quant = True
         if user.op == 'call_module':
             user_module = get_module(model, user.target)
-            if isinstance(user_module, (qnn.QuantReLU, qnn.QuantIdentity, qnn.flexml.FlexMLQuantLeakyReLU)):
+            if hasattr(user_module, 'act_quant'):
                 output_quant = False
         if output_quant:
             if quant_module_name is None and quant_module is None:
-                if is_sign_preserving and are_inputs_unsigned(model, node, []):
-                    quant_module = qnn.QuantIdentity(
-                        act_quant=Uint8ActPerTensorFixedPoint, return_quant_tensor=True)
+                if is_sign_preserving and are_inputs_unsigned(
+                        model, node, [], quant_act_map, unsigned_act_tuple):
+                    quant_module_class, quant_module_kwargs = quant_identity_map['unsigned']
                 else:
-                    quant_module = qnn.QuantIdentity(
-                        act_quant=Int8ActPerTensorFixedPoint, return_quant_tensor=True)
+                    quant_module_class, quant_module_kwargs = quant_identity_map['signed']
+                quant_module = quant_module_class(**quant_module_kwargs)
                 quant_module_name = node.name + '_output_quant'
                 model.add_module(quant_module_name, quant_module)
-            rewriters.append(InsertModuleCallAfter(quant_module_name, node))
+                rewriters.append(InsertModuleCallAfter(quant_module_name, node))
 
 
-def cat_input_handler(model, node, quant_identity_name, quant_identity, rewriters):
+def recursive_input_handler(
+        model,
+        node,
+        shared_quant_identity_name,
+        shared_quant_identity,
+        rewriters,
+        quant_identity_map,
+        align_input_quant_fn,
+        align_sign):
+    """
+    For a given CAT or ADD node, iterate through its inputs to make sure they are correctly aligned.
+    """
     for inp_node in node.all_input_nodes:
         if inp_node.op == 'call_module':
             module = get_module(model, inp_node.target)
-            if isinstance(module, tuple(SIGN_PRESERVING_MODULES)):
-                cat_input_handler(model, inp_node, quant_identity_name, quant_identity, rewriters)
-            elif isinstance(module, qnn.QuantReLU):
-                rewriter = ModuleToModuleByInstance(
-                    module, qnn.QuantReLU,
-                    # WORKAROUND
-                    # TODO act_quant=quant_identity.act_quant is currently broken
-                    # because it overrides act_impl even though it shouldn't
-                    signed=quant_identity.act_quant.is_signed,
-                    narrow_range=quant_identity.act_quant.is_narrow_range,
-                    tensor_quant=quant_identity.act_quant.fused_activation_quant_proxy.tensor_quant)
-                rewriters.append(rewriter)
-            elif isinstance(module, qnn.QuantIdentity):
-                rewriter = ModuleInstanceToModuleInstance(
-                    module, quant_identity)
-                rewriters.append(rewriter)
-            elif isinstance(module, qnn.flexml.FlexMLQuantLeakyReLU):
-                rewriter = ModuleToModuleByInstance(
-                    module, qnn.flexml.FlexMLQuantLeakyReLU, output_quant=quant_identity)
-                rewriters.append(rewriter)
+            # Precision preserving modules can be safely traversed
+            # In case align_sign is True, the modules should also be sign preserving
+            if isinstance(module, tuple(PRECISION_PRESERVING_MODULES)) and (
+                    not align_sign or
+                (align_sign and isinstance(module, tuple(SIGN_PRESERVING_MODULES)))):
+                recursive_input_handler(
+                    model,
+                    inp_node,
+                    shared_quant_identity_name,
+                    shared_quant_identity,
+                    rewriters,
+                    quant_identity_map,
+                    align_input_quant_fn,
+                    align_sign)
             else:
-                rewriters.append(InsertModuleCallAfter(quant_identity_name, inp_node))
-        elif inp_node.op == 'call_function' and inp_node.target in [torch.flatten, torch.reshape, torch.transpose]:
-            cat_input_handler(model, inp_node, quant_identity_name, quant_identity, rewriters)
-        elif inp_node.op == 'call_function' and inp_node.target is CAT:
-            cat_input_handler(model, inp_node, quant_identity_name, quant_identity, rewriters)
-        elif inp_node.op == 'call_method' and inp_node.target in ['view', 'reshape', 'flatten', 'transpose']:
-            cat_input_handler(model, inp_node, quant_identity_name, quant_identity, rewriters)
-        else:
-            rewriters.append(InsertModuleCallAfter(quant_identity_name, inp_node))
-
-
-def add_input_handler(model, node, quant_identity_name, quant_identity, rewriters):
-    for inp_node in node.all_input_nodes:
-        if inp_node.op == 'call_module':
-            module = get_module(model, inp_node.target)
-            if isinstance(module, tuple(SIGN_PRESERVING_MODULES)):
-                add_input_handler(model, inp_node, quant_identity_name, quant_identity, rewriters)
-            elif isinstance(module, qnn.QuantReLU):
-                rewriter = ModuleToModuleByInstance(
-                    module, qnn.QuantReLU,
-                    act_quant=Uint8ActPerTensorFixedPoint,
-                    scaling_impl=quant_identity.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl,
-                    int_scaling_impl=quant_identity.act_quant.fused_activation_quant_proxy.tensor_quant.int_scaling_impl,
-                    return_quant_tensor=True)
-                rewriters.append(rewriter)
-            elif isinstance(module, qnn.QuantIdentity):
-                if module.is_quant_act_signed == quant_identity.is_quant_act_signed:
-                    rewriters.append(ModuleInstanceToModuleInstance(module, quant_identity))
-                else:
-                    assert not module.is_quant_act_signed and quant_identity.is_quant_act_signed
+                # Based on the current module, generate an align_output object
+                align_output = align_input_quant_fn(
+                    module,
+                    shared_quant_identity,
+                    shared_quant_identity_name,
+                    quant_identity_map,
+                    align_sign)
+                # If it is a tuple, it is a combination of QuantAct and its configuration that
+                # will replace the current inp_node module
+                if isinstance(align_output, tuple):
+                    quant_module_class, quant_module_kwargs = align_output
                     rewriter = ModuleToModuleByInstance(
-                        module, qnn.QuantIdentity,
-                        act_quant=Uint8ActPerTensorFixedPoint,
-                        scaling_impl=quant_identity.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl,
-                        int_scaling_impl=quant_identity.act_quant.fused_activation_quant_proxy.tensor_quant.int_scaling_impl,
-                        return_quant_tensor=True)
+                        module, quant_module_class, **quant_module_kwargs)
                     rewriters.append(rewriter)
-            elif isinstance(module, qnn.flexml.FlexMLQuantLeakyReLU):
-                rewriter = ModuleToModuleByInstance(
-                    module, qnn.flexml.FlexMLQuantLeakyReLU, output_quant=quant_identity)
-                rewriters.append(rewriter)
-            else:
-                rewriters.append(InsertModuleCallAfter(quant_identity_name, inp_node))
-        elif inp_node.op == 'call_function' and inp_node.target in [torch.flatten, torch.reshape, torch.transpose]:
-            add_input_handler(model, inp_node, quant_identity_name, quant_identity, rewriters)
+                # If it is a nn.Module, it is an already instatiated QuantAct that will replace
+                # the current inp_node module
+                elif isinstance(align_output, torch.nn.Module):
+                    rewriter = ModuleInstanceToModuleInstance(module, align_output)
+                    rewriters.append(rewriter)
+                # If it is a string, we simply add a requantization activation after the current
+                # inp_node module
+                elif isinstance(align_output, str):
+                    rewriters.append(InsertModuleCallAfter(shared_quant_identity_name, inp_node))
+                else:
+                    assert align_output is None, f"align_output {str(align_output)} not supported."
+        elif inp_node.op == 'call_function' and inp_node.target in [
+                torch.flatten, torch.reshape, torch.transpose, operator.getitem,
+                operator.__getitem__]:
+            recursive_input_handler(
+                model,
+                inp_node,
+                shared_quant_identity_name,
+                shared_quant_identity,
+                rewriters,
+                quant_identity_map,
+                align_input_quant_fn,
+                align_sign)
         elif inp_node.op == 'call_function' and inp_node.target is CAT:
-            cat_input_handler(model, inp_node, quant_identity_name, quant_identity, rewriters)
-        elif inp_node.op == 'call_method' and inp_node.target in ['view', 'reshape', 'flatten', 'transpose']:
-            add_input_handler(model, inp_node, quant_identity_name, quant_identity, rewriters)
+            recursive_input_handler(
+                model,
+                inp_node,
+                shared_quant_identity_name,
+                shared_quant_identity,
+                rewriters,
+                quant_identity_map,
+                align_input_quant_fn,
+                align_sign=True)
+        elif inp_node.op == 'call_method' and inp_node.target in [
+                'view', 'reshape', 'flatten', 'transpose']:
+            recursive_input_handler(
+                model,
+                inp_node,
+                shared_quant_identity_name,
+                shared_quant_identity,
+                rewriters,
+                quant_identity_map,
+                align_input_quant_fn,
+                align_sign)
         else:
-            rewriters.append(InsertModuleCallAfter(quant_identity_name, inp_node))
+            rewriters.append(InsertModuleCallAfter(shared_quant_identity_name, inp_node))
 
 
-def flexml_act_handler(model):
-    rewriters = []
-    for node in model.graph.nodes:
-        if node.op == 'call_module':
-            module = get_module(model, node.target)
-            if isinstance(module, nn.ReLU):
-                rewriter = ModuleToModuleByInstance(
-                    module, qnn.QuantReLU,
-                    act_quant=Uint8ActPerTensorFixedPoint, return_quant_tensor=True)
-                rewriters.append(rewriter)
-            elif isinstance(module, nn.ReLU6):
-                rewriter = ModuleToModuleByInstance(
-                    module, qnn.QuantReLU,
-                    act_quant=Uint8ActPerTensorFixedPointMaxInit,
-                    max_val=6., return_quant_tensor=True)
-                rewriters.append(rewriter)
-            elif isinstance(module, nn.LeakyReLU):
-                rewriter = ModuleToModuleByInstance(
-                    module, qnn.flexml.FlexMLQuantLeakyReLU)
-                rewriters.append(rewriter)
-    for rewriter in rewriters:
-        model = rewriter.apply(model)
-    return model
-
-
-def _get_quant_module(model, node):
-    if are_inputs_unsigned(model, node, []):
-        quant_module = qnn.QuantIdentity(Uint8ActPerTensorFixedPoint, return_quant_tensor=True)
+def _get_quant_module(model, node, quant_identity_map, quant_act_map, unsigned_act_tuple):
+    """
+    Generate a QuantIdentity node that will be used for requantization step around a node.
+    If all inputs to that node are unsigned, the QuantIdentity will also be unsigned, otherwise it
+    is signed.
+    """
+    if are_inputs_unsigned(model, node, [], quant_act_map, unsigned_act_tuple):
+        quant_module_class, quant_module_kwargs = quant_identity_map['unsigned']
     else:
-        quant_module = qnn.QuantIdentity(Int8ActPerTensorFixedPoint, return_quant_tensor=True)
+        quant_module_class, quant_module_kwargs = quant_identity_map['signed']
+    quant_module = quant_module_class(**quant_module_kwargs)
     quant_module_name = node.name + '_quant'
     model.add_module(quant_module_name, quant_module)
     return quant_module, quant_module_name
 
 
-def flexml_residual_handler(model):
+def residual_handler(
+        model, quant_identity_map, quant_act_map, unsigned_act_tuple, align_input_quant_fn):
 
     def is_converged(model):
 
         for node in model.graph.nodes:
-            if (node.op == 'call_function' and node.target in ADD_FNS + [CAT]
-                    or node.op == 'call_method' and node.target in ADD_METHODS):
+            if (node.op == 'call_function' and node.target in ADD_FNS + [CAT] or
+                    node.op == 'call_method' and node.target in ADD_METHODS):
                 rewriters = []
-                if node.target is CAT:
-                    if are_inputs_quantized(model, node, [], True):
-                        continue
-                    quant_module, quant_module_name = _get_quant_module(model, node)
-                    cat_input_handler(model, node, quant_module_name, quant_module, rewriters)
-                else:
-                    if are_inputs_quantized(model, node, [], False):
-                        continue
-                    quant_module, quant_module_name = _get_quant_module(model, node)
-                    add_input_handler(model, node, quant_module_name, quant_module, rewriters)
+                # If the op is CAT, check that inputs have same sign, and in recursive_input_handler
+                # force that the sign is aligned
+                same_sign = node.target is CAT
+
+                # If input to the CAT or ADD node are quantized and aligned correctly, continue to
+                # the next node
+                if are_inputs_quantized_and_aligned(model,
+                                                    node, [],
+                                                    quant_act_map,
+                                                    same_sign=same_sign):
+                    continue
+
+                # Generate a QuantIdentity module to use for alignement of the inputs
+                shared_quant_identity, shared_quant_identity_name = _get_quant_module(
+                    model, node, quant_identity_map, quant_act_map, unsigned_act_tuple)
+
+                # Recursively, for every input node, traverse the graph to determine how to quantize
+                # and align that input node.
+                recursive_input_handler(
+                    model,
+                    node,
+                    shared_quant_identity_name,
+                    shared_quant_identity,
+                    rewriters,
+                    quant_identity_map,
+                    align_input_quant_fn,
+                    align_sign=same_sign)
                 for rewriter in rewriters:
                     model = rewriter.apply(model)
                 model.graph.lint()
                 model.recompile()
                 return False
         return True
 
     while not is_converged(model):
         continue
 
     return model
 
 
-def flexml_add_output_quant_handler(model):
-    rewriters = []
-    for node in model.graph.nodes:
-        if (node.op == 'call_function' and node.target in ADD_FNS
-                or node.op == 'call_method' and node.target in ADD_METHODS):
-            output_quant_handler(model, node, rewriters, is_sign_preserving=True)
-    for rewriter in rewriters:
-        model = rewriter.apply(model)
-    return model
-
-
-def flexml_wbiol_handler(model):
+def add_output_quant_handler(model, quant_identity_map, quant_act_map, unsigned_act_tuple):
+    """
+    Check the output of every add node, to decide whether it needs to be requantized or not, based
+    on the logic of output_quant_handler.
+    """
     rewriters = []
     for node in model.graph.nodes:
-        if node.op == 'call_module':
-            module = get_module(model, node.target)
-            if isinstance(module, tuple(QUANT_WBIOL_MAP.keys())):
-                output_quant_handler(model, node, rewriters, is_sign_preserving=False)
-                rewriter = ModuleToModuleByInstance(
-                    module, QUANT_WBIOL_MAP[type(module)],
-                    weight_quant=Int8WeightPerTensorFixedPoint,
-                    bias_quant=Int16Bias,
-                    return_quant_tensor=True)
-                rewriters.append(rewriter)
+        if (node.op == 'call_function' and node.target in ADD_FNS or
+                node.op == 'call_method' and node.target in ADD_METHODS):
+            output_quant_handler(
+                model,
+                node,
+                rewriters,
+                is_sign_preserving=True,
+                quant_identity_map=quant_identity_map,
+                quant_act_map=quant_act_map,
+                unsigned_act_tuple=unsigned_act_tuple)
     for rewriter in rewriters:
         model = rewriter.apply(model)
     return model
 
 
-def flexml_avgpool_handler(model, *model_args, avgpool_to_depthwise_conv=False, **model_kwargs):
-    rewriters = []
+def layer_handler(
+    model,
+    layer_map,
+    requantize_output,
+    quant_identity_map=dict(),
+    quant_act_map=dict(),
+    unsigned_act_tuple=dict()):
+    """
+    Replace FP weight layers with their corresponding quantized version
+    """
+    if requantize_output and (len(quant_identity_map) == 0 or len(quant_act_map) == 0 or
+                              len(unsigned_act_tuple) == 0):
+        raise RuntimeError("Missing information to requantize output")
     for node in model.graph.nodes:
+        rewriters = []
         if node.op == 'call_module':
             module = get_module(model, node.target)
-            if isinstance(module, tuple(QUANT_AVGPOOL_MAP.keys())):
-                output_quant_handler(model, node, rewriters, is_sign_preserving=True)
-                if avgpool_to_depthwise_conv:
-                    rewriter = AvgPoolToQuantDepthwiseConv(
-                        weight_quant=Int8WeightPerTensorFixedPoint,
-                        bias_quant=Int16Bias,
-                        return_quant_tensor=True)
-                else:
+            if isinstance(module, tuple(layer_map.keys())):
+                if requantize_output:
+                    if len(node.users) > 1 and all(['getitem' in n.name for n in node.users]):
+                        for n in node.users:
+                            if len(n.users) > 0:
+                                output_quant_handler(
+                                    model,
+                                    n,
+                                    rewriters,
+                                    is_sign_preserving=isinstance(module, SIGN_PRESERVING_MODULES),
+                                    quant_identity_map=quant_identity_map,
+                                    quant_act_map=quant_act_map,
+                                    unsigned_act_tuple=unsigned_act_tuple)
+                    else:
+                        output_quant_handler(
+                            model,
+                            node,
+                            rewriters,
+                            is_sign_preserving=isinstance(module, SIGN_PRESERVING_MODULES),
+                            quant_identity_map=quant_identity_map,
+                            quant_act_map=quant_act_map,
+                            unsigned_act_tuple=unsigned_act_tuple)
+                if layer_map[type(module)] is not None:
+                    quant_module_class, quant_module_kwargs = layer_map[type(module)]
+                    # Quantize the input if is not quantized, input_quant is not specified,
+                    # and the quant_identity_map is provided.
+                    # The last requirement is needed to avoid requantizing the input to activations
+                    if not are_inputs_quantized_and_aligned(
+                            model, node, [], quant_act_map, same_sign=False
+                    ) and not 'input_quant' in quant_module_kwargs and len(quant_identity_map) > 0:
+                        # Define the source node where to add the requantization step
+                        previous_node = node.all_input_nodes[0]
+                        # Exclude all the other possible users
+                        previous_node_users = list(previous_node.users.keys())
+                        previous_node_users.remove(node)
+
+                        act_quant, kwargs_act_quant = quant_identity_map['signed']
+                        inp_quant = act_quant(**kwargs_act_quant)
+                        name = node.name + '_input_quant'
+                        model.add_module(name, inp_quant)
+                        rewriter = InsertModuleCallAfter(
+                            name, previous_node, tuple(previous_node_users))
+                        rewriters.append(rewriter)
                     rewriter = ModuleToModuleByInstance(
-                        module, QUANT_AVGPOOL_MAP[type(module)],
-                        return_quant_tensor=True)
-                rewriters.append(rewriter)
-    for rewriter in rewriters:
-        if isinstance(rewriter, AvgPoolToQuantDepthwiseConv):
-            model = rewriter.apply(model, *model_args, **model_kwargs)
-        else:
+                        module, quant_module_class, **quant_module_kwargs)
+                    rewriters.append(rewriter)
+        for rewriter in rewriters:
             model = rewriter.apply(model)
     return model
-
-
-def preprocess_flexml(model, *model_args, equalization_iters = 0, **model_kwargs):
-    training_state = model.training
-    model.eval()
-    model = value_trace(model, model_kwargs)  # TODO model args should contribute to value tracing
-    model = TorchFunctionalToModule().apply(model)
-    model = DuplicateSharedStatelessModule().apply(model)
-    model = ModuleToModuleByClass(nn.ReLU6, nn.ReLU).apply(model)
-    model = MeanMethodToAdaptiveAvgPool2d().apply(model)
-    model = AdaptiveAvgPoolToAvgPool().apply(model, *model_args, **model_kwargs)
-    model = CollapseConsecutiveConcats().apply(model)
-    model = MoveSplitBatchNormBeforeCat().apply(model)
-    model = MergeBatchNorm().apply(model)
-    model = EqualizeGraph(equalization_iters).apply(model)
-    model.train(training_state)
-    return model
-
-
-def quantize_flexml(graph_model, *model_args, avgpool_to_depthwise_conv=False, **model_kwargs):
-    ignore_missing_keys_state = config.IGNORE_MISSING_KEYS
-    config.IGNORE_MISSING_KEYS = True
-    training_state = graph_model.training
-    graph_model.eval()
-    graph_model = flexml_inp_placeholder_handler(graph_model)
-    graph_model = flexml_act_handler(graph_model)
-    graph_model = flexml_add_output_quant_handler(graph_model)
-    graph_model = flexml_residual_handler(graph_model)
-    graph_model = flexml_wbiol_handler(graph_model)
-    graph_model = flexml_avgpool_handler(
-        graph_model, *model_args, avgpool_to_depthwise_conv=avgpool_to_depthwise_conv, **model_kwargs)
-    graph_model = DisableLastReturnQuantTensor().apply(graph_model)
-    graph_model.train(training_state)
-    config.IGNORE_MISSING_KEYS = ignore_missing_keys_state
-    return graph_model
```

### Comparing `brevitas-0.8.0/src/brevitas/graph/utils.py` & `brevitas-0.9.0/src/brevitas/graph/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,36 +1,34 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from inspect import signature
-from typing import Tuple, Any, Iterable, Dict
+from typing import Any, Dict, Iterable, Tuple
 
 import torch
 from torch import nn
 
-from brevitas.fx import Node, map_arg
 from brevitas import nn as qnn
+from brevitas.fx import map_arg
+from brevitas.fx import Node
 
 __all__ = [
     'module_class_name',
     'replace_all_uses_except',
     'signature_keys',
     'is_subseq',
     'get_module_name_and_parent',
     'set_module',
     'get_module',
     'del_module',
     'replace_module',
     'name_from_module',
     'matches_module_pattern',
     'get_output_channels',
-    'get_output_channel_dim'
-]
-
+    'get_output_channel_dim']
 
 CONV_TRANSPOSED = [
     nn.ConvTranspose1d,
     nn.ConvTranspose2d,
     nn.ConvTranspose3d,
     qnn.QuantConvTranspose1d,
     qnn.QuantConvTranspose2d]
@@ -56,14 +54,15 @@
         exceptions (List[Node]): The user nodes that should be affected.
 
     Returns:
         The list of Nodes on which this change was made.
     """
     to_process = list(to_replace.users)
     for use_node in to_process:
+
         def maybe_replace_node(n: Node) -> Node:
             if n == to_replace and use_node not in exceptions:
                 return replace_with
             else:
                 return n
 
         new_args = map_arg(use_node.args, maybe_replace_node)
@@ -153,8 +152,8 @@
     if is_conv_transposed(module):
         return 1
     else:
         return 0
 
 
 def get_output_channels(module):
-    return module.weight.shape[get_output_channel_dim(module)]
+    return module.weight.shape[get_output_channel_dim(module)]
```

### Comparing `brevitas-0.8.0/src/brevitas/inject/__init__.py` & `brevitas-0.9.0/src/brevitas/inject/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,26 +1,31 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import inspect
 
-from _dependencies.injector import Injector
-from _dependencies.injector import _InjectorType, __init__, let, injector_doc
+from _dependencies.attributes import _Replace
 from _dependencies.checks.circles import _check_circles
 from _dependencies.checks.injector import _check_attrs_redefinition
 from _dependencies.checks.injector import _check_dunder_name
 from _dependencies.checks.injector import _check_inheritance
 from _dependencies.checks.loops import _check_loops
-from _dependencies.spec import _make_init_spec, _make_this_spec, _make_dependency_spec
-from _dependencies.this import This
 from _dependencies.exceptions import DependencyError
-from _dependencies.attributes import _Replace
+from _dependencies.injector import __init__
+from _dependencies.injector import _InjectorType
+from _dependencies.injector import Injector
+from _dependencies.injector import injector_doc
+from _dependencies.injector import let
 from _dependencies.replace import _deep_replace_dependency
-from dependencies import value, this  # noqa
+from _dependencies.spec import _make_dependency_spec
+from _dependencies.spec import _make_init_spec
+from _dependencies.spec import _make_this_spec
+from _dependencies.this import This
+from dependencies import this  # noqa
+from dependencies import value
 
 
 def _replace_dependency(injector, current_attr, spec):
     replaced_dependency = injector.__dependencies__[current_attr]
     injector.__dependencies__[current_attr] = spec
     _check_loops(injector.__name__, injector.__dependencies__)
     _check_circles(injector.__dependencies__)
@@ -57,14 +62,15 @@
     SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
     LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
     DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
     THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
     (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
     OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
     """
+
     def __new__(cls, class_name, bases, namespace):
 
         if not bases:
             namespace["__dependencies__"] = {}
             namespace["__wrapped__"] = None  # Doctest module compatibility.
             namespace["_subs_tree"] = None  # Typing module compatibility.
             namespace["__signature__"] = None  # Sphinx compatibility.
@@ -115,28 +121,26 @@
                     continue
                 if len(attrs_stack) > 1:
                     message = "{!r} can not resolve attribute {!r} while building {!r}".format(  # noqa: E501
                         cls.__name__, current_attr, attrs_stack.pop()
                     )
                 else:
                     message = "{!r} can not resolve attribute {!r}".format(
-                        cls.__name__, current_attr
-                    )
+                        cls.__name__, current_attr)
                 raise DependencyError(message)
 
             marker, attribute, args, have_defaults = spec
 
             if set(args).issubset(cached):
                 kwargs = {k: cache[k] for k in args if k in cache}
 
                 try:
                     dependency = attribute(**kwargs)
-                    if ('nested' not in marker
-                            and inspect.isclass(dependency)
-                            and not current_attr.endswith("_class")):
+                    if ('nested' not in marker and inspect.isclass(dependency) and
+                            not current_attr.endswith("_class")):
                         spec = _make_init_spec(dependency)
                         replaced_dependency = _replace_dependency(cls, current_attr, spec)
                         replaced_dependencies[current_attr] = replaced_dependency
                         continue
                     elif isinstance(dependency, This):
                         spec = _make_this_spec(dependency)
                         replaced_dependency = _replace_dependency(cls, current_attr, spec)
@@ -166,13 +170,11 @@
         # to their defining function
         for attr, dep in replaced_dependencies.items():
             cls.__dependencies__[attr] = dep
 
         return cache[attrname]
 
 
-
 ExtendedInjector = _ExtendedInjectorType(
-    "Injector",
-    (),
-    {"__init__": __init__, "__doc__": injector_doc, "let": classmethod(let)})
-BaseInjector = ExtendedInjector # retrocompatibility wrt naming
+    "Injector", (), {
+        "__init__": __init__, "__doc__": injector_doc, "let": classmethod(let)})
+BaseInjector = ExtendedInjector  # retrocompatibility wrt naming
```

### Comparing `brevitas-0.8.0/src/brevitas/inject/enum.py` & `brevitas-0.9.0/src/brevitas/inject/enum.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from enum import auto
 
 from brevitas.utils.python_utils import AutoName
 
 
 class BitWidthImplType(AutoName):
     """
 
     """
     CONST = auto()
     PARAMETER = auto()
+    STATEFUL_CONST = auto()
 
 
 class QuantType(AutoName):
     """
 
     """
     BINARY = auto()
@@ -40,14 +40,22 @@
 
     """
     ROUND = auto()
     CEIL = auto()
     FLOOR = auto()
     ROUND_TO_ZERO = auto()
     DPU = auto()
+    LEARNED_ROUND = auto()
+
+
+class LearnedRoundImplType(AutoName):
+    """
+    """
+    HARD_SIGMOID = auto()
+    SIGMOID = auto()
 
 
 class ScalingImplType(AutoName):
     """
 
     """
     HE = auto()
@@ -69,8 +77,8 @@
     MAX_AVE = auto()
     MEAN_SIGMA_STD = auto()
     MEAN_LEARN_SIGMA_STD = auto()
     PERCENTILE = auto()
     # Two sided statistics
     # Typically adopted for asymmetric quantization
     MIN_MAX = auto()
-    PERCENTILE_INTERVAL = auto()
+    PERCENTILE_INTERVAL = auto()
```

### Comparing `brevitas-0.8.0/src/brevitas/jit.py` & `brevitas-0.9.0/src/brevitas/jit.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,18 +1,15 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from packaging import version
-
 import torch
 
 from brevitas.config import JIT_ENABLED
 
-
 IS_ABOVE_110 = version.parse(torch.__version__) > version.parse('1.1.0')
 
 
 def _disabled(fn):
     return fn
 
 
@@ -30,8 +27,8 @@
 
 else:
 
     script_method = _disabled
     script = _disabled
     script_method_110_disabled = _disabled
     ScriptModule = torch.nn.Module
-    Attribute = lambda val, type: val
+    Attribute = lambda val, type: val
```

### Comparing `brevitas-0.8.0/src/brevitas/loss/base_loss.py` & `brevitas-0.9.0/src/brevitas/loss/base_loss.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from abc import ABCMeta, abstractmethod
+from abc import ABCMeta
+from abc import abstractmethod
 
 from torch import nn
 
 
 class SimpleBaseLoss(object):
     __metaclass__ = ABCMeta
 
@@ -24,8 +24,8 @@
         del self.tot_loss
         self.tot_loss = 0.0
 
     def retrieve(self):
         return self.tot_loss * self.reg_coeff
 
     def log(self):
-        return self.tot_loss.detach().clone() * self.reg_coeff
+        return self.tot_loss.detach().clone() * self.reg_coeff
```

### Comparing `brevitas-0.8.0/src/brevitas/loss/weighted_bit_width.py` & `brevitas-0.9.0/src/brevitas/loss/weighted_bit_width.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,23 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import List
-from abc import ABCMeta, abstractmethod
-
+from abc import ABCMeta
+from abc import abstractmethod
 from functools import reduce
 from operator import mul
+from typing import List
 
 import torch
 from torch import nn
 
-from brevitas.utils.quant_utils import *
-from brevitas.nn.quant_linear import QuantLinear
 from brevitas.nn.quant_conv import QuantConv2d
+from brevitas.nn.quant_linear import QuantLinear
 from brevitas.quant_tensor import QuantTensor
+from brevitas.utils.quant_utils import *
 
 MEGA = 10e6
 
 
 class BitWidthWeighted(object):
     __metaclass__ = ABCMeta
 
@@ -39,24 +38,24 @@
         self.tot_num_elements = 0
 
     def retrieve(self, as_average=True):
         if self.tot_num_elements != 0 and self.weighted_bit_width_list:
             if as_average:
                 value = sum(self.weighted_bit_width_list) / self.tot_num_elements
             else:
-                value = [bit_width / self.tot_num_elements for bit_width in self.weighted_bit_width_list]
+                value = [
+                    bit_width / self.tot_num_elements for bit_width in self.weighted_bit_width_list]
         else:
             raise Exception("Number of elements to penalize can't be zero")
         return value
 
     def log(self):
         return self.retrieve(as_average=True).detach().clone()
 
 
-
 class WeightBitWidthWeightedBySize(BitWidthWeighted):
 
     def __init__(self, model):
         super(WeightBitWidthWeightedBySize, self).__init__(model=model)
         pass
 
     def register_hooks(self):
@@ -106,10 +105,7 @@
                 self.tot_num_elements += num_mops
 
         for name, module in self.model.named_modules():
             if isinstance(module, self.layer_types) \
                     and module.return_quant_tensor \
                     and module.per_elem_ops is not None:
                 module.register_forward_hook(hook_fn)
-
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/hadamard_classifier.py` & `brevitas-0.9.0/src/brevitas/nn/hadamard_classifier.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,34 +1,32 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # Based on: https://arxiv.org/abs/1801.04540
 
-
-import torch.nn as nn
 import math
+
 import torch
+import torch.nn as nn
 
 try:
     from scipy.linalg import hadamard
 except ImportError:
     hadamard = None
 
-from brevitas.function.ops_ste import ceil_ste
 from brevitas.function.ops import max_int
-from .mixin.base import QuantLayerMixin
+from brevitas.function.ops_ste import ceil_ste
 from brevitas.quant_tensor import QuantTensor
 
+from .mixin.base import QuantLayerMixin
+
 
 class HadamardClassifier(QuantLayerMixin, nn.Module):
 
-    def __init__(self,
-                 in_channels,
-                 out_channels,
-                 fixed_scale=False,
-                 return_quant_tensor: bool = False):
+    def __init__(
+            self, in_channels, out_channels, fixed_scale=False, return_quant_tensor: bool = False):
         QuantLayerMixin.__init__(self, return_quant_tensor=return_quant_tensor)
         nn.Module.__init__(self)
         if hadamard is None:
             raise Exception("Hadamard layer requires scipy to be installed.")
 
         self.out_channels = out_channels
         self.in_channels = in_channels
@@ -46,47 +44,46 @@
         output_scale = None
         output_zp = None
         output_bit_width = None
         inp = self.unpack_input(inp)
         norm = inp.value.norm(p='fro', keepdim=True) + self.eps
         out = inp.value / norm
         out = nn.functional.linear(out, self.proj[:self.out_channels, :self.in_channels])
-        out = - self.scale * out
+        out = -self.scale * out
         if inp.scale is not None:
             output_scale = inp.scale * self.scale / norm
         if inp.bit_width is not None:
             output_bit_width = self.max_output_bit_width(inp.bit_width)
-        if (self.return_quant_tensor
-                and inp.zero_point is not None
-                and (inp.zero_point != 0.0).any()):
+        if (self.return_quant_tensor and inp.zero_point is not None and
+            (inp.zero_point != 0.0).any()):
             raise RuntimeError("Computing zero point of output accumulator not supported yet.")
         else:
             output_zp = inp.zero_point
         out = QuantTensor(
             value=out,
             scale=output_scale,
             zero_point=output_zp,
             bit_width=output_bit_width,
             signed=True,
             training=self.training)
         return out
 
-
     def max_output_bit_width(self, input_bit_width):
         max_input_val = max_int(bit_width=input_bit_width, narrow_range=False, signed=False)
         max_output_val = max_input_val * self.in_channels
         output_bit_width = ceil_ste(torch.log2(max_output_val))
         return output_bit_width
 
     def state_dict(self, destination=None, prefix='', keep_vars=False):
         state_dict = super(HadamardClassifier, self).state_dict(
             destination=destination, prefix=prefix, keep_vars=keep_vars)
         del state_dict[prefix + 'proj']
         return state_dict
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
-        super(HadamardClassifier, self)._load_from_state_dict(state_dict, prefix, local_metadata, strict,
-            missing_keys, unexpected_keys, error_msgs)
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
+        super(HadamardClassifier, self)._load_from_state_dict(
+            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
         proj_key = prefix + 'proj'
         if proj_key in missing_keys:
-            missing_keys.remove(proj_key)
+            missing_keys.remove(proj_key)
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/mixin/acc.py` & `brevitas-0.9.0/src/brevitas/nn/mixin/acc.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,25 +1,27 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from abc import ABCMeta
-from typing import Type, Union, Optional
+from typing import Optional, Type, Union
 
-from brevitas.inject import Injector, ExtendedInjector
-from brevitas.quant import NoneClampQuant, NoneTruncQuant
-from brevitas.proxy.runtime_quant import TruncQuantProxyFromInjector, ClampQuantProxyFromInjector
+from brevitas.inject import ExtendedInjector
+from brevitas.inject import Injector
 from brevitas.proxy.runtime_quant import AccQuantProxyProtocol
-from .base import QuantProxyMixin
+from brevitas.proxy.runtime_quant import ClampQuantProxyFromInjector
+from brevitas.proxy.runtime_quant import TruncQuantProxyFromInjector
+from brevitas.quant import NoneClampQuant
+from brevitas.quant import NoneTruncQuant
 
+from .base import QuantProxyMixin
 
 AccQuantType = Union[AccQuantProxyProtocol, Type[Injector], Type[ExtendedInjector]]
 
 
-class QuantTruncMixin(QuantProxyMixin):
+class TruncMixin(QuantProxyMixin):
     __metaclass__ = ABCMeta
 
     def __init__(self, trunc_quant: Optional[AccQuantType], **kwargs):
         super().__init__(
             quant=trunc_quant,
             proxy_protocol=AccQuantProxyProtocol,
             none_quant_injector=NoneTruncQuant,
@@ -34,15 +36,14 @@
 
 class QuantClampMixin(QuantProxyMixin):
     __metaclass__ = ABCMeta
 
     def __init__(self, clamp_quant: Optional[AccQuantType], **kwargs):
         super().__init__(
             quant=clamp_quant,
-            proxy_from_injector_impl=ClampQuantProxyFromInjector,
             proxy_protocol=AccQuantProxyProtocol,
             none_quant_injector=NoneClampQuant,
             kwargs_prefix='',
             proxy_prefix='clamp_',
             **kwargs)
 
     @property
@@ -53,8 +54,8 @@
     def is_quant_clamp_narrow_range(self):
         assert self.is_clamp_quant_enabled
         return self.clamp_quant.is_narrow_range
 
     @property
     def is_quant_clamp_signed(self):
         assert self.is_clamp_quant_enabled
-        return self.clamp_quant.is_signed
+        return self.clamp_quant.is_signed
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/mixin/act.py` & `brevitas-0.9.0/src/brevitas/nn/mixin/act.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
+from abc import ABCMeta
+from abc import abstractmethod
+from typing import Optional, Type, Union
 from warnings import warn
-from abc import ABCMeta, abstractmethod
-from typing import Type, Union, Optional
 
 from torch.nn import Module
-from brevitas.inject import ExtendedInjector, Injector
-from brevitas.quant import NoneActQuant
+
+from brevitas.inject import ExtendedInjector
+from brevitas.inject import Injector
 from brevitas.proxy.runtime_quant import ActQuantProxyFromInjector
 from brevitas.proxy.runtime_quant import ActQuantProxyProtocol
+from brevitas.quant import NoneActQuant
 
 from .base import QuantProxyMixin
 
-
 ActQuantType = Union[ActQuantProxyProtocol, Type[Injector], Type[ExtendedInjector]]
 
 
 class QuantInputMixin(QuantProxyMixin):
     __metaclass__ = ABCMeta
 
     def __init__(self, input_quant: Optional[ActQuantType], **kwargs):
@@ -34,15 +35,15 @@
             **kwargs)
 
     @property
     def is_input_quant_enabled(self):
         return self.input_quant.is_quant_enabled
 
     @property
-    def is_quant_input_narrow_range(self): # TODO make abstract once narrow range can be cached
+    def is_quant_input_narrow_range(self):  # TODO make abstract once narrow range can be cached
         return self.input_quant.is_narrow_range
 
     @property
     @abstractmethod
     def is_quant_input_signed(self):
         pass
 
@@ -62,15 +63,14 @@
 class QuantOutputMixin(QuantProxyMixin):
     __metaclass__ = ABCMeta
 
     def __init__(self, output_quant: Optional[ActQuantType], **kwargs):
         QuantProxyMixin.__init__(
             self,
             quant=output_quant,
-            proxy_from_injector_impl=ActQuantProxyFromInjector,
             proxy_protocol=ActQuantProxyProtocol,
             none_quant_injector=NoneActQuant,
             proxy_prefix='output_',
             kwargs_prefix='output_',
             output_act_impl=None,
             output_passthrough_act=True,
             **kwargs)
@@ -116,15 +116,14 @@
             act_kwargs_prefix + 'act_impl': act_impl,
             act_kwargs_prefix + 'passthrough_act': passthrough_act}
         QuantProxyMixin.__init__(
             self,
             quant=act_quant,
             proxy_prefix=act_proxy_prefix,
             kwargs_prefix=act_kwargs_prefix,
-            proxy_from_injector_impl=ActQuantProxyFromInjector,
             proxy_protocol=ActQuantProxyProtocol,
             none_quant_injector=NoneActQuant,
             **prefixed_kwargs,
             **kwargs)
 
     @property
     def is_act_quant_enabled(self):
@@ -146,9 +145,7 @@
     @abstractmethod
     def quant_act_zero_point(self):
         pass
 
     @abstractmethod
     def quant_act_bit_width(self):
         pass
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/mixin/base.py` & `brevitas-0.9.0/src/brevitas/nn/mixin/base.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,26 +1,28 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from warnings import warn
-from abc import ABCMeta, abstractmethod
-from typing import Optional, Union, Tuple
+from abc import ABCMeta
+from abc import abstractmethod
 from inspect import isclass
 import math
+from typing import Optional, Tuple, Union
+from warnings import warn
 
+from torch import nn
+from torch import Tensor
 import torch.jit
-from torch import Tensor, nn
 from torch.nn.utils.rnn import PackedSequence
 
 from brevitas import config
-from brevitas.inject import ExtendedInjector, Injector
-from brevitas.quant_tensor import QuantTensor
 from brevitas.common import ExportMixin
+from brevitas.inject import ExtendedInjector
+from brevitas.inject import Injector
 from brevitas.nn.utils import compute_channel_view_shape
+from brevitas.quant_tensor import QuantTensor
 
 from .utils import filter_kwargs
 
 
 class _CachedIO:
 
     def __init__(self, quant_tensor: QuantTensor, metadata_only: bool):
@@ -46,15 +48,14 @@
     def signed(self):
         return self.quant_tensor.signed
 
 
 class QuantProxyMixin(object):
     __metaclass__ = ABCMeta
 
-
     def __init__(
             self,
             quant,
             proxy_protocol,
             none_quant_injector,
             proxy_prefix: str,
             kwargs_prefix: str,
@@ -71,15 +72,15 @@
             if not isinstance(quant, proxy_protocol):
                 raise RuntimeError(
                     "The quantizer passed does not adhere to the quantization protocol.")
             quant.add_tracked_module(self)
             if filter_kwargs(kwargs_prefix, kwargs):
                 warn('Keyword arguments are being passed but they not being used.')
         setattr(self, proxy_name, quant)
-        
+
 
 class QuantLayerMixin(ExportMixin):
     __metaclass__ = ABCMeta
 
     def __init__(
             self,
             return_quant_tensor: bool,
@@ -153,18 +154,16 @@
         else:
             return None
 
     def unpack_input(self, inp: Union[Tensor, QuantTensor]):
         self._set_global_is_quant_layer(True)
         # Hack to recognize a QuantTensor that has decayed to a tuple
         # when used as input to tracing (e.g. during ONNX export)
-        if (torch._C._get_tracing_state() is not None
-                and isinstance(inp, tuple)
-                and len(inp) == len(QuantTensor._fields)
-                and all([isinstance(t, Tensor) for t in inp])):
+        if (torch._C._get_tracing_state() is not None and isinstance(inp, tuple) and
+                len(inp) == len(QuantTensor._fields) and all([isinstance(t, Tensor) for t in inp])):
             inp = QuantTensor(*inp)
         if isinstance(inp, QuantTensor):
             # don't cache values during export pass
             if not self.training and not self._export_mode and self.cache_inference_quant_inp:
                 cached_inp = _CachedIO(inp.detach(), self.cache_quant_io_metadata_only)
                 self._cached_inp = cached_inp
             return inp
@@ -185,21 +184,21 @@
             return quant_output.value
 
 
 class QuantRecurrentLayerMixin(ExportMixin):
     __metaclass__ = ABCMeta
 
     def __init__(
-            self, 
+            self,
             cell: nn.Module,
-            io_quant: nn.Module, 
+            io_quant: nn.Module,
             input_size: int,
-            hidden_size: int, 
-            reverse_input: bool, 
-            quantize_output_only: bool, 
+            hidden_size: int,
+            reverse_input: bool,
+            quantize_output_only: bool,
             shared_input_hidden_weights: bool,
             return_quant_tensor: bool):
         ExportMixin.__init__(self)
         self.cell = cell
         self.io_quant = io_quant
         self.input_size = input_size
         self.hidden_size = hidden_size
@@ -242,112 +241,115 @@
     def gate_params_fwd(gate, quant_input):
         acc_scale = None
         acc_bit_width = None
         quant_weight_ih = gate.input_weight()
         quant_weight_hh = gate.hidden_weight()
         if quant_input.bit_width is not None:
             acc_bit_width = None  # TODO
-        if quant_input.scale is not None:
+        if quant_input.scale is not None and quant_weight_ih.scale is not None:
             acc_scale_shape = compute_channel_view_shape(quant_input.value, channel_dim=1)
             acc_scale = quant_weight_ih.scale.view(acc_scale_shape)
             acc_scale = acc_scale * quant_input.scale.view(acc_scale_shape)
         quant_bias = gate.bias_quant(gate.bias, acc_scale, acc_bit_width)
         return quant_weight_ih, quant_weight_hh, quant_bias
 
     def reset_parameters(self) -> None:
         stdv = 1.0 / math.sqrt(self.hidden_size)
         for name, weight in self.named_parameters():
             if 'gate' in name:
                 nn.init.uniform_(weight, -stdv, stdv)
-    
+
     def maybe_quantize_input(self, inp):
         if isinstance(inp, PackedSequence):
             raise RuntimeError("PackedSequence input currently not supported.")
         quant_input = inp
         if not self.quantize_output_only:
             quant_input = self.io_quant(quant_input)
         elif not isinstance(inp, QuantTensor):
             quant_input = QuantTensor(quant_input)
         return quant_input
-    
+
     def maybe_quantize_state(self, inp, state, quant):
         if state is None:
             batch_size = inp.size(0) if self.cell.batch_first else inp.size(1)
-            quant_state = torch.zeros(int(batch_size), self.hidden_size, dtype=inp.dtype, device=inp.device)
+            quant_state = torch.zeros(
+                int(batch_size), self.hidden_size, dtype=inp.dtype, device=inp.device)
             quant_state = QuantTensor(quant_state)
         else:
             quant_state = quant(state)
         return quant_state
 
     def pack_quant_outputs(self, quant_outputs):
         # In export mode, quant_outputs has the shape of the output concatenated value
         if self.export_mode:
             if self.return_quant_tensor:
                 return QuantTensor(
-                    quant_outputs, 
-                    self.io_quant.scale(), 
-                    self.io_quant.zero_point(), 
-                    self.io_quant.bit_width(), 
-                    self.io_quant.is_signed, 
+                    quant_outputs,
+                    self.io_quant.scale(),
+                    self.io_quant.zero_point(),
+                    self.io_quant.bit_width(),
+                    self.io_quant.is_signed,
                     self.training)
             else:
                 return quant_outputs
         seq_dim = 1 if self.cell.batch_first else 0
         if self.return_quant_tensor:
-            outputs = [QuantTensor(
-                torch.unsqueeze(quant_output[0], dim=seq_dim), 
-                quant_output[1], 
-                quant_output[2], 
-                quant_output[3], 
-                self.io_quant.is_signed, 
-                self.training) for quant_output in quant_outputs]
+            outputs = [
+                QuantTensor(
+                    torch.unsqueeze(quant_output[0], dim=seq_dim),
+                    quant_output[1],
+                    quant_output[2],
+                    quant_output[3],
+                    self.io_quant.is_signed,
+                    self.training) for quant_output in quant_outputs]
         else:
             outputs = [torch.unsqueeze(o[0], dim=seq_dim) for o in quant_outputs]
         if self.reverse_input:
             return torch.cat(list(reversed(outputs)), dim=seq_dim)
         else:
             return torch.cat(outputs, dim=seq_dim)
-    
+
     def pack_quant_state(self, quant_state, quant):
         if self.export_mode:
             if self.return_quant_tensor:
                 quant_state = QuantTensor(
-                    torch.unsqueeze(quant_state, dim=0), 
+                    torch.unsqueeze(quant_state, dim=0),
                     quant.scale(),
                     quant.zero_point(),
-                    quant.bit_width(), 
-                    quant.is_signed, 
+                    quant.bit_width(),
+                    quant.is_signed,
                     self.training)
             else:
                 quant_state = torch.unsqueeze(quant_state, dim=0)
         else:
             if self.return_quant_tensor:
                 quant_state = QuantTensor(
-                    torch.unsqueeze(quant_state[0], dim=0), 
-                    quant_state[1], 
-                    quant_state[2], 
-                    quant_state[3], 
-                    quant.is_signed, 
+                    torch.unsqueeze(quant_state[0], dim=0),
+                    quant_state[1],
+                    quant_state[2],
+                    quant_state[3],
+                    quant.is_signed,
                     self.training)
             else:
                 quant_state = torch.unsqueeze(quant_state[0], dim=0)
         return quant_state
 
     def _wrap_act_proxy(self, quant_name):
-        
+
         class _Wrapper(nn.Module):
 
             def __init__(self, module_to_wrap=None):
                 super(_Wrapper, self).__init__()
                 if module_to_wrap is None:
                     module_to_wrap = nn.Identity()
                 self.module_to_wrap = module_to_wrap
 
-            def forward(self, x: torch.Tensor) -> Tuple[
-                    Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor]]:
+            def forward(
+                self, x: torch.Tensor
+            ) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor]]:
                 x = self.module_to_wrap(x)
                 return (x, None, None, None)
 
         proxy = getattr(self.cell, quant_name)
         if proxy.fused_activation_quant_proxy is None:
             proxy = _Wrapper(proxy.fused_activation_quant_proxy)
         else:
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/mixin/parameter.py` & `brevitas-0.9.0/src/brevitas/nn/mixin/parameter.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,34 +1,35 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from warnings import warn
-from abc import ABCMeta, abstractmethod
+from abc import ABCMeta
+from abc import abstractmethod
 from typing import Optional, Type, Union
+from warnings import warn
 
-from brevitas.inject import ExtendedInjector, Injector
-from brevitas.quant import NoneWeightQuant, NoneBiasQuant
-from brevitas.proxy.parameter_quant import WeightQuantProxyFromInjector, BiasQuantProxyFromInjector
-from brevitas.proxy.parameter_quant import WeightQuantProxyProtocol, BiasQuantProxyProtocol
+from brevitas.inject import ExtendedInjector
+from brevitas.inject import Injector
+from brevitas.proxy.parameter_quant import BiasQuantProxyFromInjector
+from brevitas.proxy.parameter_quant import BiasQuantProxyProtocol
+from brevitas.proxy.parameter_quant import WeightQuantProxyFromInjector
+from brevitas.proxy.parameter_quant import WeightQuantProxyProtocol
+from brevitas.quant import NoneBiasQuant
+from brevitas.quant import NoneWeightQuant
+from brevitas.quant_tensor import QuantTensor
 
 from .base import QuantProxyMixin
 
-
 WeightQuantType = Union[WeightQuantProxyProtocol, Type[Injector], Type[ExtendedInjector]]
 BiasQuantType = Union[BiasQuantProxyProtocol, Type[Injector], Type[ExtendedInjector]]
 
 
 class QuantWeightMixin(QuantProxyMixin):
     __metaclass__ = ABCMeta
 
-    def __init__(
-            self,
-            weight_quant: Optional[WeightQuantType],
-            **kwargs):
+    def __init__(self, weight_quant: Optional[WeightQuantType], **kwargs):
         QuantProxyMixin.__init__(
             self,
             quant=weight_quant,
             proxy_protocol=WeightQuantProxyProtocol,
             none_quant_injector=NoneWeightQuant,
             kwargs_prefix='weight_',
             proxy_prefix='weight_',
@@ -47,15 +48,29 @@
     def is_quant_weight_narrow_range(self):
         return self.weight_quant.is_narrow_range
 
     @property
     def is_quant_weight_signed(self):
         return self.weight_quant.is_signed
 
-    def quant_weight(self):
+    @property
+    def weight_quant_requires_quant_input(self):
+        return self.weight_quant.requires_quant_input
+
+    def quant_weight(self, quant_input: Optional[QuantTensor] = None):
+        if self.weight_quant_requires_quant_input:
+            if quant_input is None:
+                input_bit_width = self.quant_input_bit_width()
+                input_is_signed = self.is_quant_input_signed
+            else:
+                input_bit_width = quant_input.bit_width
+                input_is_signed = quant_input.signed
+            assert input_bit_width is not None, "Input bit-width needs to be specified."
+            assert input_is_signed is not None, "Input sign needs to be specified."
+            return self.weight_quant(self.weight, input_bit_width, input_is_signed)
         return self.weight_quant(self.weight)
 
     def int_weight(self, float_datatype=False):
         return self.quant_weight().int(float_datatype)
 
     def quant_weight_scale(self):
         scale = self.quant_weight().scale
@@ -83,15 +98,14 @@
             self,
             bias_quant: Optional[BiasQuantType],
             cache_inference_bias: bool = False,
             **kwargs):
         QuantProxyMixin.__init__(
             self,
             quant=bias_quant,
-            proxy_from_injector_impl=BiasQuantProxyFromInjector,
             proxy_protocol=BiasQuantProxyProtocol,
             none_quant_injector=NoneBiasQuant,
             kwargs_prefix='bias_',
             proxy_prefix='bias_',
             **kwargs)
         self.cache_inference_quant_bias = cache_inference_bias
         self._cached_bias = None
@@ -119,15 +133,15 @@
         return quant_bias.int(float_datatype=float_datatype)
 
     def quant_bias(self):
         if self.bias is None:
             return None
         scale = self.quant_bias_scale()
         bit_width = self.quant_bias_bit_width()
-        quant_bias = self.bias_quant(self.bias, scale, bit_width)   
+        quant_bias = self.bias_quant(self.bias, scale, bit_width)
         return quant_bias
 
     def quant_bias_scale(self):
         if self.bias is None or not self.is_bias_quant_enabled:
             return None
         if not self.bias_quant.requires_input_scale and not self.bias_quant.requires_input_bit_width:
             return self.bias_quant(self.bias).scale
@@ -165,12 +179,11 @@
                     "No quant bias cache found, set cache_inference_quant_bias=True and run an "
                     "inference pass first")
             if self.training:
                 warn("Cached quant bias bit-width is being used in training mode.")
             return self._cached_bias.bit_width
 
     def register_parameter(self, name, value):
-       super(QuantBiasMixin, self).register_parameter(name, value)
-       if hasattr(self, 'bias_quant') and name == 'bias':
+        super(QuantBiasMixin, self).register_parameter(name, value)
+        if hasattr(self, 'bias_quant') and name == 'bias':
             self.bias_quant.init_tensor_quant()
             self.bias_quant.to(self.bias.device)
-
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/quant_accumulator.py` & `brevitas-0.9.0/src/brevitas/nn/quant_accumulator.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,34 +1,33 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Optional, Union, Type, Optional
+from typing import Optional, Type, Union
 
 from torch.nn import Module
 
 from brevitas.inject import BaseInjector as Injector
 from brevitas.proxy.runtime_quant import AccQuantProxyProtocol
 from brevitas.quant_tensor import QuantTensor
+
+from .mixin.acc import AccQuantType
+from .mixin.acc import QuantClampMixin
+from .mixin.acc import TruncMixin
 from .mixin.base import QuantLayerMixin
-from .mixin.acc import QuantTruncMixin, QuantClampMixin, AccQuantType
 
 
-class TruncQuantAccumulator(QuantTruncMixin, QuantLayerMixin, Module):
+class TruncQuantAccumulator(TruncMixin, QuantLayerMixin, Module):
 
     def __init__(
             self,
             trunc_quant: Optional[AccQuantType] = None,
             return_quant_tensor: bool = True,
             **kwargs):
         QuantLayerMixin.__init__(self, return_quant_tensor)
-        QuantTruncMixin.__init__(
-            self,
-            trunc_quant=trunc_quant,
-            **kwargs)
+        TruncMixin.__init__(self, trunc_quant=trunc_quant, **kwargs)
 
     @property
     def channelwise_separable(self) -> bool:
         return True
 
     @property
     def requires_export_handler(self):
@@ -44,24 +43,21 @@
 
     def __init__(
             self,
             clamp_quant: Optional[AccQuantType] = None,
             return_quant_tensor: bool = True,
             **kwargs):
         QuantLayerMixin.__init__(self, return_quant_tensor)
-        QuantClampMixin.__init__(
-            self,
-            clamp_quant=clamp_quant,
-            **kwargs)
+        QuantClampMixin.__init__(self, clamp_quant=clamp_quant, **kwargs)
 
     @property
     def channelwise_separable(self) -> bool:
         return True
 
     @property
     def requires_export_handler(self):
         return True
 
     def forward(self, input: QuantTensor):
         x = self.unpack_input(input)
         x = self.clamp_quant(x)
-        return self.pack_output(x)
+        return self.pack_output(x)
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/quant_activation.py` & `brevitas-0.9.0/src/brevitas/nn/quant_activation.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from typing import Optional
 
 from torch import nn
 
+from brevitas.inject.defaults import Int8ActPerTensorFloat
+from brevitas.inject.defaults import Int8ActPerTensorFloatMinMaxInit
 from brevitas.inject.defaults import Uint8ActPerTensorFloat
-from brevitas.inject.defaults import Int8ActPerTensorFloat, Int8ActPerTensorFloatMinMaxInit
-from .quant_layer import QuantNonLinearActLayer as QuantNLAL, ActQuantType
+
+from .quant_layer import ActQuantType
+from .quant_layer import QuantNonLinearActLayer as QuantNLAL
 
 
 class QuantReLU(QuantNLAL):
 
     def __init__(
             self,
             act_quant: Optional[ActQuantType] = Uint8ActPerTensorFloat,
@@ -94,8 +96,7 @@
             self,
             input_quant=None,
             act_impl=None,
             passthrough_act=True,
             act_quant=act_quant,
             return_quant_tensor=return_quant_tensor,
             **kwargs)
-
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/quant_bn.py` & `brevitas-0.9.0/src/brevitas/nn/quant_bn.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,25 +1,28 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from abc import ABC
 from typing import Optional
 
 import brevitas.config as config
 from brevitas.inject.defaults import Int8WeightPerTensorFloat
+
+from .quant_layer import ActQuantType
+from .quant_layer import BiasQuantType
+from .quant_layer import WeightQuantType
 from .quant_scale_bias import QuantScaleBias
 from .utils import mul_add_from_bn
-from .quant_layer import WeightQuantType, BiasQuantType, ActQuantType
 
 
 class _BatchNormToQuantScaleBias(QuantScaleBias, ABC):
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         weight_key = prefix + 'weight'
         bias_key = prefix + 'bias'
         running_mean_key = prefix + 'running_mean'
         running_var_key = prefix + 'running_var'
         num_batches_tracked_key = prefix + 'num_batches_tracked'
         if running_mean_key in state_dict and running_var_key in state_dict:
             weight_init, bias_init = mul_add_from_bn(
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/quant_conv.py` & `brevitas-0.9.0/src/brevitas/nn/quant_conv.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,27 +1,29 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Union, Tuple, Type, Optional
 import math
+from typing import Optional, Tuple, Type, Union
 
 import torch
 from torch import Tensor
-from torch.nn import Conv1d, Conv2d
+from torch.nn import Conv1d
+from torch.nn import Conv2d
 from torch.nn import functional as F
 from torch.nn.functional import conv2d
 
 from brevitas.function.ops import max_int
 from brevitas.function.ops_ste import ceil_ste
-from brevitas.quant_tensor import QuantTensor
 from brevitas.inject.defaults import Int8WeightPerTensorFloat
-from .quant_layer import QuantWeightBiasInputOutputLayer as QuantWBIOL
-from .quant_layer import WeightQuantType, BiasQuantType, ActQuantType
+from brevitas.quant_tensor import QuantTensor
 
+from .quant_layer import ActQuantType
+from .quant_layer import BiasQuantType
+from .quant_layer import QuantWeightBiasInputOutputLayer as QuantWBIOL
+from .quant_layer import WeightQuantType
 
 __all__ = ['QuantConv1d', 'QuantConv2d']
 
 
 class QuantConv1d(QuantWBIOL, Conv1d):
 
     def __init__(
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/quant_convtranspose.py` & `brevitas-0.9.0/src/brevitas/nn/quant_convtranspose.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,27 +1,30 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
+from typing import Optional, Tuple, Type, Union
 
-from typing import Union, Tuple, Type, Optional
 from packaging import version
-
 import torch
 from torch import Tensor
-from torch.nn import ConvTranspose1d, ConvTranspose2d
-from torch.nn.functional import conv_transpose1d, conv_transpose2d
+from torch.nn import ConvTranspose1d
+from torch.nn import ConvTranspose2d
+from torch.nn.functional import conv_transpose1d
+from torch.nn.functional import conv_transpose2d
 
 from brevitas import torch_version
 from brevitas.function.ops import max_int
 from brevitas.function.ops_ste import ceil_ste
-from brevitas.quant_tensor import QuantTensor
 from brevitas.inject.defaults import Int8WeightPerTensorFloat
-from .quant_layer import QuantWeightBiasInputOutputLayer as QuantWBIOL
-from .quant_layer import WeightQuantType, BiasQuantType, ActQuantType
+from brevitas.quant_tensor import QuantTensor
 
+from .quant_layer import ActQuantType
+from .quant_layer import BiasQuantType
+from .quant_layer import QuantWeightBiasInputOutputLayer as QuantWBIOL
+from .quant_layer import WeightQuantType
 
 __all__ = ['QuantConvTranspose1d', 'QuantConvTranspose2d']
 
 
 class QuantConvTranspose1d(QuantWBIOL, ConvTranspose1d):
 
     def __init__(
@@ -70,25 +73,27 @@
     def output_channel_dim(self) -> int:
         return 1
 
     @property
     def channelwise_separable(self) -> bool:
         raise self.groups == self.out_channels
 
-    def forward(self, input: Union[Tensor, QuantTensor], output_size=None) -> Union[Tensor, QuantTensor]:
+    def forward(self,
+                input: Union[Tensor, QuantTensor],
+                output_size=None) -> Union[Tensor, QuantTensor]:
         self._output_size = output_size  # cache the value temporarily
         return self.forward_impl(input)
 
     def compute_output_padding(self, inp, output_size):
         if torch_version >= version.parse('1.12'):
             return self._output_padding(
                 inp, output_size, self.stride, self.padding, self.kernel_size, num_spatial_dims=1)
         else:
-             return self._output_padding(
-                inp, output_size, self.stride, self.padding, self.kernel_size)      
+            return self._output_padding(
+                inp, output_size, self.stride, self.padding, self.kernel_size)
 
     def conv_transpose1d_zeros_pad(
             self, x: Tensor, weight: Tensor, bias: Optional[Tensor], output_padding):
         out = conv_transpose1d(
             x, weight, bias, self.stride, self.padding, output_padding, self.groups, self.dilation)
         return out
 
@@ -159,25 +164,27 @@
     def output_channel_dim(self) -> int:
         return 1
 
     @property
     def channelwise_separable(self) -> bool:
         raise self.groups == self.out_channels
 
-    def forward(self, input: Union[Tensor, QuantTensor], output_size=None) -> Union[Tensor, QuantTensor]:
+    def forward(self,
+                input: Union[Tensor, QuantTensor],
+                output_size=None) -> Union[Tensor, QuantTensor]:
         self._output_size = output_size  # cache the value temporarily
         return self.forward_impl(input)
 
     def compute_output_padding(self, inp, output_size):
         if torch_version >= version.parse('1.12'):
             return self._output_padding(
                 inp, output_size, self.stride, self.padding, self.kernel_size, num_spatial_dims=2)
         else:
-             return self._output_padding(
-                inp, output_size, self.stride, self.padding, self.kernel_size)           
+            return self._output_padding(
+                inp, output_size, self.stride, self.padding, self.kernel_size)
 
     def conv_transpose2d_zeros_pad(
             self, x: Tensor, weight: Tensor, bias: Optional[Tensor], output_padding):
         out = conv_transpose2d(
             x, weight, bias, self.stride, self.padding, output_padding, self.groups, self.dilation)
         return out
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/quant_dropout.py` & `brevitas-0.9.0/src/brevitas/nn/quant_dropout.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,27 +1,25 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from typing import Union
 
 from torch import Tensor
 from torch.nn import Dropout
 
 from brevitas.quant_tensor import QuantTensor
+
 from .mixin.base import QuantLayerMixin
 
 
 class QuantDropout(QuantLayerMixin, Dropout):
 
     def __init__(self, p: float = 0.5, return_quant_tensor: bool = True):
         Dropout.__init__(self, p=p, inplace=False)
-        QuantLayerMixin.__init__(
-            self,
-            return_quant_tensor=return_quant_tensor)
+        QuantLayerMixin.__init__(self, return_quant_tensor=return_quant_tensor)
 
     @property
     def channelwise_separable(self) -> bool:
         return True
 
     @property
     def requires_export_handler(self):
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/quant_eltwise.py` & `brevitas-0.9.0/src/brevitas/nn/quant_eltwise.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,47 +1,41 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Union, Type, List, Optional
+from typing import List, Optional, Type, Union
 
 from torch import Tensor
 from torch.nn import Module
 
-from brevitas.quant_tensor import QuantTensor
 from brevitas.inject.defaults import Int8ActPerTensorFloat
-from .quant_layer import QuantInputOutputLayer, ActQuantType
+from brevitas.quant_tensor import QuantTensor
+
+from .quant_layer import ActQuantType
+from .quant_layer import QuantInputOutputLayer
 
 
 class QuantEltwiseAdd(QuantInputOutputLayer, Module):
 
     def __init__(
             self,
             input_quant: Optional[ActQuantType] = Int8ActPerTensorFloat,
             output_quant: Optional[ActQuantType] = Int8ActPerTensorFloat,
-            tie_input_output_quant = False,
+            tie_input_output_quant=False,
             return_quant_tensor: bool = False,
             **kwargs) -> None:
         Module.__init__(self)
         QuantInputOutputLayer.__init__(
-            self,
-            input_quant,
-            output_quant,
-            tie_input_output_quant,
-            return_quant_tensor,
-            **kwargs)
+            self, input_quant, output_quant, tie_input_output_quant, return_quant_tensor, **kwargs)
 
     @property
     def channelwise_separable(self) -> bool:
         return True
 
-    def forward(
-            self,
-            input: Union[Tensor, QuantTensor],
-            other: Union[Tensor, QuantTensor]) -> Union[Tensor, QuantTensor]:
+    def forward(self, input: Union[Tensor, QuantTensor],
+                other: Union[Tensor, QuantTensor]) -> Union[Tensor, QuantTensor]:
         input = self.unpack_input(input)
         other = self.unpack_input(other)
         if self.export_mode:
             assert self.cache_quant_io_metadata_only, "Can't cache multiple inputs"
             out = self.export_handler(inp=input.value, other=other.value)
             self._set_global_is_quant_layer(False)
             return out
@@ -59,36 +53,27 @@
             input_quant: Optional[ActQuantType] = Int8ActPerTensorFloat,
             output_quant: Optional[ActQuantType] = Int8ActPerTensorFloat,
             tie_input_output_quant: bool = False,
             return_quant_tensor: bool = False,
             **kwargs):
         Module.__init__(self)
         QuantInputOutputLayer.__init__(
-            self,
-            input_quant,
-            output_quant,
-            tie_input_output_quant,
-            return_quant_tensor,
-            **kwargs)
+            self, input_quant, output_quant, tie_input_output_quant, return_quant_tensor, **kwargs)
 
     @property
     def channelwise_separable(self) -> bool:
         return True
 
-    def forward(
-            self,
-            tensor_list: Union[List[Tensor], List[QuantTensor]],
-            dim: int = 1) -> Union[Tensor, QuantTensor]:
+    def forward(self,
+                tensor_list: Union[List[Tensor], List[QuantTensor]],
+                dim: int = 1) -> Union[Tensor, QuantTensor]:
         quant_tensor_list = [self.unpack_input(t) for t in tensor_list]
         # shortcut execution through the export impl during export
         if self.export_mode:
             out = self.export_handler([qt.value for qt in quant_tensor_list])
             self._set_global_is_quant_layer(False)
             return out
         quant_tensor_list = [self.input_quant(qt) for qt in quant_tensor_list]
         # trigger an assert if scale factors and bit widths are None or different
         output = QuantTensor.cat(quant_tensor_list, dim=dim)
         quant_output = self.output_quant(output)
         return self.pack_output(quant_output)
-
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/quant_embedding.py` & `brevitas-0.9.0/src/brevitas/nn/quant_embedding.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,75 +1,71 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Union, Type, Optional
+from typing import Optional, Type, Union
 
 import torch
 from torch import Tensor
-from torch.nn import Embedding, EmbeddingBag
+from torch.nn import Embedding
+from torch.nn import EmbeddingBag
 from torch.nn.functional import embedding
 
-from brevitas.function.ops_ste import ceil_ste
 from brevitas.function.ops import max_int
-from brevitas.quant_tensor import QuantTensor
+from brevitas.function.ops_ste import ceil_ste
 from brevitas.inject.defaults import Int8WeightPerTensorFloat
+from brevitas.quant_tensor import QuantTensor
+
 from .mixin.parameter import QuantWeightMixin
 from .quant_layer import WeightQuantType
 
 __all__ = ['QuantEmbedding']
 
 
 class QuantEmbedding(QuantWeightMixin, Embedding):
 
     def __init__(
-            self, 
-            num_embeddings: int, 
-            embedding_dim: int, 
+            self,
+            num_embeddings: int,
+            embedding_dim: int,
             padding_idx: Optional[int] = None,
-            max_norm: Optional[float] = None, 
-            norm_type: float = 2., 
+            max_norm: Optional[float] = None,
+            norm_type: float = 2.,
             scale_grad_by_freq: bool = False,
-            sparse: bool = False, 
+            sparse: bool = False,
             _weight: Optional[Tensor] = None,
             weight_quant: WeightQuantType = Int8WeightPerTensorFloat,
-            return_quant_tensor = False,
+            return_quant_tensor=False,
             **kwargs) -> None:
         Embedding.__init__(
             self,
             num_embeddings=num_embeddings,
             embedding_dim=embedding_dim,
             padding_idx=padding_idx,
             max_norm=max_norm,
             norm_type=norm_type,
             scale_grad_by_freq=scale_grad_by_freq,
             sparse=sparse,
             _weight=_weight)
-        QuantWeightMixin.__init__(
-            self,
-            weight_quant=weight_quant,
-            **kwargs)
+        QuantWeightMixin.__init__(self, weight_quant=weight_quant, **kwargs)
         self.accept_quant_tensor = False
         self.return_quant_tensor = return_quant_tensor
 
     def forward(self, inp):
         quant_weight = self.quant_weight()
         out = embedding(
-            inp, 
-            quant_weight.value, 
-            self.padding_idx, 
-            self.max_norm, 
-            self.norm_type, 
-            self.scale_grad_by_freq, 
+            inp,
+            quant_weight.value,
+            self.padding_idx,
+            self.max_norm,
+            self.norm_type,
+            self.scale_grad_by_freq,
             self.sparse)
         if self.return_quant_tensor:
             scale = quant_weight.scale
             zero_point = quant_weight.zero_point
             bit_width = quant_weight.bit_width
             if any(t.numel() > 1 for t in [scale, zero_point, bit_width]):
                 raise RuntimeError("Only per-tensor quantization is supported.")
             signed = quant_weight.signed
             training = quant_weight.training
             out = QuantTensor(out, scale, zero_point, bit_width, signed, training)
         return out
-                
-
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/quant_layer.py` & `brevitas-0.9.0/src/brevitas/nn/quant_layer.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,50 +1,43 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from abc import ABCMeta, abstractmethod
-from typing import Optional, Type, Union, Callable
+from abc import ABCMeta
+from abc import abstractmethod
+from typing import Callable, Optional, Type, Union
 
 import torch
 from torch import Tensor
-from torch.nn import Module, Parameter
+from torch.nn import Module
+from torch.nn import Parameter
 
 from brevitas.quant_tensor import QuantTensor
-from .mixin.base import _CachedIO
-from .mixin import *
 
-from .utils import rename_state_dict_by_prefix, compute_channel_view_shape
+from .mixin import *
+from .mixin.base import _CachedIO
+from .utils import compute_channel_view_shape
 from .utils import merge_bn
+from .utils import rename_state_dict_by_prefix
 
 
-class QuantNonLinearActLayer(
-    QuantNonLinearActMixin,
-    QuantInputMixin,
-    QuantLayerMixin,
-    Module):
+class QuantNonLinearActLayer(QuantNonLinearActMixin, QuantInputMixin, QuantLayerMixin, Module):
     __metaclass__ = ABCMeta
 
     def __init__(
             self,
             act_impl: Optional[Type[Module]],
             passthrough_act: bool,
             input_quant: Optional[ActQuantType],
             act_quant: Optional[ActQuantType],
             return_quant_tensor: bool,
             **kwargs):
         Module.__init__(self)
         QuantLayerMixin.__init__(self, return_quant_tensor)
         QuantInputMixin.__init__(self, input_quant, **kwargs)
-        QuantNonLinearActMixin.__init__(
-            self,
-            act_impl,
-            passthrough_act,
-            act_quant,
-            **kwargs)
+        QuantNonLinearActMixin.__init__(self, act_impl, passthrough_act, act_quant, **kwargs)
 
     @property
     def channelwise_separable(self) -> bool:
         return True
 
     @property
     def requires_export_handler(self):
@@ -130,41 +123,39 @@
         if self.is_act_quant_enabled:
             return self.act_quant.bit_width()
         elif self._cached_out is not None:
             return self._cached_out.bit_width
         else:
             return None
 
-    def quant_output_bit_width(self):   # overrides from QuantLayerMixin
+    def quant_output_bit_width(self):  # overrides from QuantLayerMixin
         return self.quant_act_bit_width()
 
     def forward(self, input: Union[Tensor, QuantTensor]):
         input = self.unpack_input(input)
         quant_input = self.input_quant(input)
         # shortcut execution through the export impl during export
         if self.export_mode:
             out = self.export_handler(quant_input.value)
             self._set_global_is_quant_layer(False)
             return out
         out = self.act_quant(quant_input)
         out = self.pack_output(out)
         return out
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         # for retrocompatibility
         rename_state_dict_by_prefix(prefix + 'act_quant_proxy', prefix + 'act_quant', state_dict)
         super(QuantNonLinearActLayer, self)._load_from_state_dict(
             state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
 
 
-class QuantInputOutputLayer(
-        QuantOutputMixin,
-        QuantInputMixin,
-        QuantLayerMixin):
+class QuantInputOutputLayer(QuantOutputMixin, QuantInputMixin, QuantLayerMixin):
     __metaclass__ = ABCMeta
 
     def __init__(
             self,
             input_quant: Optional[ActQuantType],
             output_quant: Optional[ActQuantType],
             tie_input_output_quant: bool,
@@ -192,15 +183,15 @@
             return self.input_quant.is_signed
         elif self._cached_inp is not None:
             return self._cached_inp.signed
         else:
             return None
 
     @property
-    def is_quant_output_signed(self)  -> Optional[bool]:  # tri-valued logic output:
+    def is_quant_output_signed(self) -> Optional[bool]:  # tri-valued logic output:
         if self.is_output_quant_enabled:
             return self.output_quant.is_signed
         elif self._cached_out is not None:
             return self._cached_out.signed
         else:
             return None
 
@@ -249,18 +240,15 @@
             return self.output_quant.bit_width()
         elif self._cached_out is not None:
             return self._cached_out.bit_width
         else:
             return None
 
 
-class QuantWeightBiasInputOutputLayer(
-        QuantBiasMixin,
-        QuantWeightMixin,
-        QuantInputOutputLayer):
+class QuantWeightBiasInputOutputLayer(QuantBiasMixin, QuantWeightMixin, QuantInputOutputLayer):
     __metaclass__ = ABCMeta
 
     def __init__(
             self,
             weight_quant: Optional[WeightQuantType],
             bias_quant: Optional[BiasQuantType],
             input_quant: Optional[ActQuantType],
@@ -283,18 +271,17 @@
 
     @abstractmethod
     def max_acc_bit_width(self, input_bit_width: Tensor, quant_weight_bit_width: Tensor):
         pass
 
     @property
     def requires_export_handler(self):
-        return (self.is_input_quant_enabled
-                or self.is_weight_quant_enabled
-                or self.is_bias_quant_enabled
-                or self.is_output_quant_enabled)
+        return (
+            self.is_input_quant_enabled or self.is_weight_quant_enabled or
+            self.is_bias_quant_enabled or self.is_output_quant_enabled)
 
     @property
     def per_elem_ops(self):  # optional, so concrete impl + error if not overridden
         raise NotImplementedError
 
     def merge_bn_in(self, bn):
         merge_bn(self, bn, output_channel_dim=self.output_channel_dim)
@@ -310,59 +297,55 @@
         # shortcut execution through the export impl during export
         if self.export_mode:
             out = self.export_handler(inp.value)
             self._set_global_is_quant_layer(False)
             return out
 
         quant_input = self.input_quant(inp)
-        quant_weight = self.quant_weight()
+        quant_weight = self.quant_weight(quant_input)
 
-        if quant_input.bit_width is not None:
+        if quant_input.bit_width is not None and quant_weight.bit_width is not None:
             output_bit_width = self.max_acc_bit_width(quant_input.bit_width, quant_weight.bit_width)
-        if quant_input.scale is not None:
+        if quant_input.scale is not None and quant_weight.scale is not None:
             output_scale_shape = compute_channel_view_shape(inp, channel_dim=1)
             output_scale = quant_weight.scale.view(output_scale_shape)
             output_scale = output_scale * quant_input.scale.view(output_scale_shape)
         if quant_input.signed is not None:
             output_signed = inp.signed or quant_weight.signed
 
         if self.bias is not None:
             quant_bias = self.bias_quant(self.bias, output_scale, output_bit_width)
             if not self.training and self.cache_inference_quant_bias:
                 self._cached_bias = _CachedIO(quant_bias.detach(), metadata_only=False)
 
             output_tensor = self.inner_forward_impl(
                 quant_input.value, quant_weight.value, quant_bias.value)
 
-            if (output_scale is not None
-                    and (quant_bias.scale is None
-                         or (quant_bias.scale is not None
-                             and quant_bias.scale.data_ptr() != output_scale.data_ptr()))):
-                output_zero_point = - quant_bias.value.view(output_scale_shape) / output_scale
+            if (output_scale is not None and
+                (quant_bias.scale is None or
+                 (quant_bias.scale is not None and
+                  quant_bias.scale.data_ptr() != output_scale.data_ptr()))):
+                output_zero_point = -quant_bias.value.view(output_scale_shape) / output_scale
 
             if quant_bias.bit_width is not None and output_bit_width is not None:
                 output_bit_width = torch.where(
                     quant_bias.bit_width > output_bit_width, quant_bias.bit_width, output_bit_width)
                 output_bit_width = output_bit_width + 1
         else:
             output_tensor = self.inner_forward_impl(quant_input.value, quant_weight.value, None)
 
         if self.return_quant_tensor and not self.is_output_quant_enabled:
-            if (quant_input.zero_point is not None
-                    and ((quant_input.zero_point != 0.0).any()
-                         or (quant_weight.zero_point != 0.0).any())):
+            if (quant_input.zero_point is not None and quant_weight.zero_point is not None and
+                ((quant_input.zero_point != 0.0).any() or (quant_weight.zero_point != 0.0).any())):
                 raise RuntimeError("Computing zero point of output accumulator not supported yet.")
             elif quant_input.zero_point is not None and output_zero_point is None:
                 output_zero_point = quant_input.zero_point
 
         quant_output = QuantTensor(
             value=output_tensor,
             scale=output_scale,
             zero_point=output_zero_point,
             bit_width=output_bit_width,
             signed=output_signed,
             training=self.training)
         quant_output = self.output_quant(quant_output)
         return self.pack_output(quant_output)
-
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/quant_linear.py` & `brevitas-0.9.0/src/brevitas/nn/quant_linear.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Union, Type, Optional
+from typing import Optional, Type, Union
 
 import torch
 from torch import Tensor
 from torch.nn import Linear
 from torch.nn.functional import linear
 
-from brevitas.function.ops_ste import ceil_ste
 from brevitas.function.ops import max_int
-from brevitas.quant_tensor import QuantTensor
+from brevitas.function.ops_ste import ceil_ste
 from brevitas.inject.defaults import Int8WeightPerTensorFloat
+from brevitas.quant_tensor import QuantTensor
+
+from .quant_layer import ActQuantType
+from .quant_layer import BiasQuantType
 from .quant_layer import QuantWeightBiasInputOutputLayer as QuantWBIOL
-from .quant_layer import WeightQuantType, BiasQuantType, ActQuantType
+from .quant_layer import WeightQuantType
 
 __all__ = ['QuantLinear']
 
 
 class QuantLinear(QuantWBIOL, Linear):
 
     def __init__(
@@ -67,10 +69,7 @@
 
     def max_acc_bit_width(self, input_bit_width, weight_bit_width):
         max_input_val = max_int(bit_width=input_bit_width, signed=False, narrow_range=False)
         max_fc_val = self.weight_quant.max_uint_value(weight_bit_width)
         max_output_val = max_input_val * max_fc_val * self.in_features
         output_bit_width = ceil_ste(torch.log2(max_output_val))
         return output_bit_width
-
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/quant_max_pool.py` & `brevitas-0.9.0/src/brevitas/nn/quant_max_pool.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from typing import Union
 
 from torch import Tensor
-from torch.nn import MaxPool1d, MaxPool2d
+from torch.nn import MaxPool1d
+from torch.nn import MaxPool2d
 
 from brevitas.quant_tensor import QuantTensor
+
 from .mixin.base import QuantLayerMixin
 
 
 class QuantMaxPool1d(QuantLayerMixin, MaxPool1d):
 
     def __init__(
             self,
@@ -26,17 +27,15 @@
             self,
             kernel_size=kernel_size,
             stride=stride,
             padding=padding,
             dilation=dilation,
             return_indices=return_indices,
             ceil_mode=ceil_mode)
-        QuantLayerMixin.__init__(
-            self,
-            return_quant_tensor=return_quant_tensor)
+        QuantLayerMixin.__init__(self, return_quant_tensor=return_quant_tensor)
 
     @property
     def channelwise_separable(self) -> bool:
         return True
 
     @property
     def requires_export_handler(self):
@@ -65,17 +64,15 @@
             self,
             kernel_size=kernel_size,
             stride=stride,
             padding=padding,
             dilation=dilation,
             return_indices=return_indices,
             ceil_mode=ceil_mode)
-        QuantLayerMixin.__init__(
-            self,
-            return_quant_tensor=return_quant_tensor)
+        QuantLayerMixin.__init__(self, return_quant_tensor=return_quant_tensor)
 
     @property
     def channelwise_separable(self) -> bool:
         return True
 
     @property
     def requires_export_handler(self):
@@ -84,8 +81,8 @@
     def forward(self, input: Union[Tensor, QuantTensor]):
         x = self.unpack_input(input)
         if self.export_mode:
             out = self.export_handler(x.value)
             self._set_global_is_quant_layer(False)
             return out
         x = x.set(value=super().forward(x.value))
-        return self.pack_output(x)
+        return self.pack_output(x)
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/quant_rnn.py` & `brevitas-0.9.0/src/brevitas/nn/quant_rnn.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,54 +1,60 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from abc import ABCMeta, abstractmethod
-from typing import Optional, Tuple, List
-import math
+from abc import ABCMeta
+from abc import abstractmethod
 from functools import partial
+import math
+from typing import List, Optional, Tuple
 
 import torch
-from torch import nn, Tensor
+from torch import nn
+from torch import Tensor
 import torch.nn.functional as F
 
 import brevitas
-from brevitas.nn.mixin import QuantWeightMixin, QuantBiasMixin
-from brevitas.nn import QuantIdentity, QuantTanh, QuantSigmoid
-from brevitas.quant_tensor import QuantTensor
-from brevitas.quant import Int8WeightPerTensorFloat, Int32Bias
-from brevitas.quant import Int8ActPerTensorFloat, Uint8ActPerTensorFloat
+from brevitas.nn import QuantIdentity
+from brevitas.nn import QuantSigmoid
+from brevitas.nn import QuantTanh
+from brevitas.nn.mixin import QuantBiasMixin
+from brevitas.nn.mixin import QuantWeightMixin
 from brevitas.nn.mixin.base import QuantRecurrentLayerMixin
+from brevitas.quant import Int8ActPerTensorFloat
+from brevitas.quant import Int8WeightPerTensorFloat
+from brevitas.quant import Int32Bias
+from brevitas.quant import Uint8ActPerTensorFloat
+from brevitas.quant_tensor import QuantTensor
 
-
-QuantTupleShortEnabled = List[
-    Tuple[Tensor, Tensor, Tensor, Tensor]]
-QuantTupleShortDisabled = List[
-    Tuple[Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor]]]
-QuantTupleLongEnabled = List[
-    Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]]
-QuantTupleLongDisabled = List[
-    Tuple[Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor]]]
+QuantTupleShortEnabled = List[Tuple[Tensor, Tensor, Tensor, Tensor]]
+QuantTupleShortDisabled = List[Tuple[Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor]]]
+QuantTupleLongEnabled = List[Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]]
+QuantTupleLongDisabled = List[Tuple[Tensor,
+                                    Optional[Tensor],
+                                    Optional[Tensor],
+                                    Optional[Tensor],
+                                    Optional[Tensor],
+                                    Optional[Tensor]]]
 
 
 class GateWeight(QuantWeightMixin, nn.Module):
-    
+
     def __init__(self, input_features, output_features, weight_quant, **kwargs):
         nn.Module.__init__(self)
         self.weight = nn.Parameter(torch.randn(output_features, input_features))
         QuantWeightMixin.__init__(self, weight_quant=weight_quant, **kwargs)
 
     @property
     def output_channel_dim(self):
         return 0
 
     @property
     def out_channels(self):
         return self.weight.size(self.output_channel_dim)
-    
+
     def forward(self):
         return self.weight_quant(self.weight)
 
 
 class GateParams(QuantBiasMixin, nn.Module):
 
     def __init__(
@@ -56,16 +62,15 @@
         nn.Module.__init__(self)
         if bias:
             self.bias = nn.Parameter(torch.randn(hidden_size))
         else:
             self.bias = None
         QuantBiasMixin.__init__(self, bias_quant, **kwargs)
         if input_weight is None:
-            input_weight = GateWeight(
-                input_size, hidden_size, weight_quant=weight_quant, **kwargs)
+            input_weight = GateWeight(input_size, hidden_size, weight_quant=weight_quant, **kwargs)
         self.input_weight = input_weight
         # The quantizer is shared among input-to-hidden and hidden-to-hidden weights
         self.hidden_weight = GateWeight(
             hidden_size, hidden_size, weight_quant=input_weight.weight_quant)
 
 
 # Simply putting the forward in a standalone script function is not enough
@@ -76,15 +81,15 @@
 
     def __init__(self, fast_impl: bool, quant_enabled: bool):
         super(_QuantStatesInit, self).__init__()
         self.fast_impl = fast_impl
         self.quant_enabled = quant_enabled
 
     def forward(self):
-        if self.fast_impl: 
+        if self.fast_impl:
             if self.quant_enabled:
                 quant_states = torch.jit.annotate(QuantTupleShortEnabled, [])
             else:
                 quant_states = torch.jit.annotate(QuantTupleShortDisabled, [])
         else:
             if self.quant_enabled:
                 quant_states = torch.jit.annotate(QuantTupleLongEnabled, [])
@@ -93,217 +98,212 @@
         return quant_states
 
 
 class _QuantRNNCell(nn.Module):
     __constants__ = ['reverse_input', 'batch_first']
 
     def __init__(
-            self, 
-            act_fn: nn.Module, 
+            self,
+            act_fn: nn.Module,
             gate_acc_quant: nn.Module,
-            output_quant: nn.Module, 
-            reverse_input: bool, 
+            output_quant: nn.Module,
+            reverse_input: bool,
             batch_first: bool,
             output_quant_enabled: bool,
             fast_impl: bool):
         super(_QuantRNNCell, self).__init__()
         self.act_fn = act_fn
         self.gate_acc_quant = gate_acc_quant
         self.output_quant = output_quant
         self.reverse_input = reverse_input
         self.batch_first = batch_first
         self.hidden_states_init = _QuantStatesInit(fast_impl, output_quant_enabled)
 
-    def forward_iter(
-            self, quant_input, quant_state, quant_weight_ih, quant_weight_hh, quant_bias):
+    def forward_iter(self, quant_input, quant_state, quant_weight_ih, quant_weight_hh, quant_bias):
         quant_gate_ih = F.linear(quant_input, quant_weight_ih)
         quant_gate_hh = F.linear(quant_state, quant_weight_hh)
         quant_gate = self.gate_acc_quant(quant_gate_ih + quant_gate_hh + quant_bias)[0]
         quant_gate = self.act_fn(quant_gate)
         quant_state_tuple = self.output_quant(quant_gate)
         return quant_state_tuple
 
     def forward(
-            self, 
-            quant_input: Tensor, 
-            quant_state: Tensor, 
-            quant_weight_ih: Tensor, 
-            quant_weight_hh: Tensor, 
+            self,
+            quant_input: Tensor,
+            quant_state: Tensor,
+            quant_weight_ih: Tensor,
+            quant_weight_hh: Tensor,
             quant_bias: Tensor):
         if self.batch_first:
             quant_inputs = quant_input.unbind(1)
         else:
             quant_inputs = quant_input.unbind(0)
         end = len(quant_inputs)
         step = 1
         index = 0
         quant_outputs = self.hidden_states_init()
         if self.reverse_input:
             index = end - 1
-            step = - 1
+            step = -1
         for _ in range(end):
             quant_input = quant_inputs[index]
             quant_state_tuple = self.forward_iter(
                 quant_input, quant_state, quant_weight_ih, quant_weight_hh, quant_bias)
             index = index + step
             quant_outputs += [quant_state_tuple]
             quant_state = quant_state_tuple[0]
         return quant_outputs
 
 
 class _QuantLSTMCell(nn.Module):
     __constants__ = ['reverse_input', 'batch_first', 'cifg']
 
     def __init__(
-            self, 
-            output_quant, 
-            cell_state_quant, 
+            self,
+            output_quant,
+            cell_state_quant,
             input_acc_quant,
             forget_acc_quant,
             cell_acc_quant,
             output_acc_quant,
-            input_sigmoid_quant, 
-            forget_sigmoid_quant, 
-            cell_tanh_quant, 
-            output_sigmoid_quant, 
+            input_sigmoid_quant,
+            forget_sigmoid_quant,
+            cell_tanh_quant,
+            output_sigmoid_quant,
             hidden_state_tanh_quant,
             reverse_input: bool,
             batch_first: bool,
             cifg: bool,
             output_quant_enabled: bool,
             cell_state_quant_enabled: bool,
             fast_impl: bool):
         super(_QuantLSTMCell, self).__init__()
         self.output_quant = output_quant
         self.cell_state_quant = cell_state_quant
         self.input_acc_quant = input_acc_quant
         self.forget_acc_quant = forget_acc_quant
         self.cell_acc_quant = cell_acc_quant
         self.output_acc_quant = output_acc_quant
-        self.input_sigmoid_quant = input_sigmoid_quant 
-        self.forget_sigmoid_quant = forget_sigmoid_quant 
-        self.cell_tanh_quant = cell_tanh_quant 
+        self.input_sigmoid_quant = input_sigmoid_quant
+        self.forget_sigmoid_quant = forget_sigmoid_quant
+        self.cell_tanh_quant = cell_tanh_quant
         self.output_sigmoid_quant = output_sigmoid_quant
         self.hidden_state_tanh_quant = hidden_state_tanh_quant
         self.reverse_input = reverse_input
         self.batch_first = batch_first
         self.cifg = cifg
         self.hidden_states_init = _QuantStatesInit(fast_impl, output_quant_enabled)
         self.cell_states_init = _QuantStatesInit(fast_impl, cell_state_quant_enabled)
 
     def forward_iter(
-            self, 
-            quant_input: Tensor, 
-            quant_hidden_state: Tensor, 
-            quant_cell_state: Tensor, 
-            quant_weight_ii: Tensor, 
+            self,
+            quant_input: Tensor,
+            quant_hidden_state: Tensor,
+            quant_cell_state: Tensor,
+            quant_weight_ii: Tensor,
             quant_weight_if: Tensor,
             quant_weight_ic: Tensor,
             quant_weight_io: Tensor,
-            quant_weight_hi: Tensor, 
+            quant_weight_hi: Tensor,
             quant_weight_hf: Tensor,
             quant_weight_hc: Tensor,
-            quant_weight_ho: Tensor,            
+            quant_weight_ho: Tensor,
             quant_bias_input: Tensor,
             quant_bias_forget: Tensor,
             quant_bias_cell: Tensor,
             quant_bias_output: Tensor):
         # Input gate
         quant_ii_gate = F.linear(quant_input, quant_weight_ii)
         quant_hi_gate = F.linear(quant_hidden_state, quant_weight_hi)
-        quant_input_gate = self.input_acc_quant(
-            quant_ii_gate + quant_hi_gate + quant_bias_input)[0]
+        quant_input_gate = self.input_acc_quant(quant_ii_gate + quant_hi_gate + quant_bias_input)[0]
         quant_input_gate = self.input_sigmoid_quant(quant_input_gate)[0]
         # Forget gate
         if self.cifg:
-            quant_ones = self.input_sigmoid_quant.tensor_quant(
-                torch.ones_like(quant_input_gate))[0]
+            quant_ones = self.input_sigmoid_quant.tensor_quant(torch.ones_like(quant_input_gate))[0]
             # CIFG is defined as 1 - input_gate, in line with ONNXRuntime
             quant_forget_gate = quant_ones - quant_input_gate
         else:
             quant_if_gate = F.linear(quant_input, quant_weight_if)
             quant_hf_gate = F.linear(quant_hidden_state, quant_weight_hf)
             quant_forget_gate = self.forget_acc_quant(
                 quant_if_gate + quant_hf_gate + quant_bias_forget)[0]
-            quant_forget_gate = self.forget_sigmoid_quant(quant_forget_gate)[0]        
+            quant_forget_gate = self.forget_sigmoid_quant(quant_forget_gate)[0]
         # Cell gate
         quant_ic_gate = F.linear(quant_input, quant_weight_ic)
         quant_hc_gate = F.linear(quant_hidden_state, quant_weight_hc)
-        quant_cell_gate = self.cell_acc_quant(
-            quant_ic_gate + quant_hc_gate + quant_bias_cell)[0]
+        quant_cell_gate = self.cell_acc_quant(quant_ic_gate + quant_hc_gate + quant_bias_cell)[0]
         quant_cell_gate = self.cell_tanh_quant(quant_cell_gate)[0]
         # Output gate
         quant_io_gate = F.linear(quant_input, quant_weight_io)
         quant_ho_gate = F.linear(quant_hidden_state, quant_weight_ho)
-        quant_out_gate = self.output_acc_quant(
-            quant_io_gate + quant_ho_gate + quant_bias_output)[0]
+        quant_out_gate = self.output_acc_quant(quant_io_gate + quant_ho_gate + quant_bias_output)[0]
         quant_out_gate = self.output_sigmoid_quant(quant_out_gate)[0]
         quant_forget_cell = self.cell_state_quant(quant_forget_gate * quant_cell_state)[0]
         quant_inp_cell = self.cell_state_quant(quant_input_gate * quant_cell_gate)[0]
         quant_cell_state_tuple = self.cell_state_quant(quant_forget_cell + quant_inp_cell)
         quant_hidden_state_tanh = self.hidden_state_tanh_quant(quant_cell_state_tuple[0])[0]
         quant_hidden_state = quant_out_gate * quant_hidden_state_tanh
         quant_hidden_state_tuple = self.output_quant(quant_hidden_state)
         return quant_hidden_state_tuple, quant_cell_state_tuple
 
     def forward(
-            self, 
-            quant_input: Tensor, 
-            quant_hidden_state: Tensor, 
-            quant_cell_state: Tensor, 
-            quant_weight_ii: Tensor, 
+            self,
+            quant_input: Tensor,
+            quant_hidden_state: Tensor,
+            quant_cell_state: Tensor,
+            quant_weight_ii: Tensor,
             quant_weight_if: Tensor,
             quant_weight_ic: Tensor,
             quant_weight_io: Tensor,
-            quant_weight_hi: Tensor, 
+            quant_weight_hi: Tensor,
             quant_weight_hf: Tensor,
             quant_weight_hc: Tensor,
             quant_weight_ho: Tensor,
             quant_bias_input: Tensor,
             quant_bias_forget: Tensor,
             quant_bias_cell: Tensor,
             quant_bias_output: Tensor):
         if self.batch_first:
-            seq_dim = 1  
+            seq_dim = 1
         else:
             seq_dim = 0
         quant_inputs = quant_input.unbind(seq_dim)
         end = len(quant_inputs)
         step = 1
         index = 0
         if self.reverse_input:
             index = end - 1
-            step = - 1
+            step = -1
         quant_hidden_states = self.hidden_states_init()
         quant_cell_states = self.cell_states_init()
         for _ in range(end):
             quant_input = quant_inputs[index]
             quant_hidden_state_tuple, quant_cell_state_tuple = self.forward_iter(
-                quant_input, 
-                quant_hidden_state, 
-                quant_cell_state, 
-                quant_weight_ii, 
+                quant_input,
+                quant_hidden_state,
+                quant_cell_state,
+                quant_weight_ii,
                 quant_weight_if,
                 quant_weight_ic,
                 quant_weight_io,
-                quant_weight_hi, 
+                quant_weight_hi,
                 quant_weight_hf,
                 quant_weight_hc,
                 quant_weight_ho,
                 quant_bias_input,
                 quant_bias_forget,
                 quant_bias_cell,
                 quant_bias_output)
             index = index + step
             quant_hidden_states += [quant_hidden_state_tuple]
             quant_hidden_state = quant_hidden_state_tuple[0]
             quant_cell_states += [quant_cell_state_tuple]
-            quant_cell_state = quant_cell_state_tuple[0]        
+            quant_cell_state = quant_cell_state_tuple[0]
         return quant_hidden_states, quant_hidden_states[-1], quant_cell_states[-1]
-    
+
 
 class _QuantRNNLayer(QuantRecurrentLayerMixin, nn.Module):
 
     def __init__(
             self,
             input_size: int,
             hidden_size: int,
@@ -318,15 +318,16 @@
             shared_input_hidden_weights: bool,
             return_quant_tensor: bool,
             nonlinearity: str,
             input_weight: GateWeight = None,
             **kwargs):
         nn.Module.__init__(self)
         io_quant = QuantIdentity(io_quant, act_kwargs_prefix='io_', **kwargs)
-        gate_acc_quant = QuantIdentity(gate_acc_quant, act_kwargs_prefix='gate_acc_quant_', **kwargs)
+        gate_acc_quant = QuantIdentity(
+            gate_acc_quant, act_kwargs_prefix='gate_acc_quant_', **kwargs)
         if nonlinearity == 'tanh':
             act_fn = nn.Tanh()
         elif nonlinearity == 'relu':
             act_fn = nn.ReLU()
         else:
             raise RuntimeError(f"{nonlinearity} not supported.")
         cell = _QuantRNNCell(
@@ -334,17 +335,17 @@
             gate_acc_quant.act_quant,
             io_quant.act_quant,
             reverse_input,
             batch_first,
             io_quant.act_quant.is_quant_enabled,
             fast_impl=False)
         QuantRecurrentLayerMixin.__init__(
-            self, 
+            self,
             cell=cell,
-            io_quant=io_quant.act_quant, 
+            io_quant=io_quant.act_quant,
             input_size=input_size,
             hidden_size=hidden_size,
             reverse_input=reverse_input,
             quantize_output_only=quantize_output_only,
             shared_input_hidden_weights=shared_input_hidden_weights,
             return_quant_tensor=return_quant_tensor)
         self.gate_params = GateParams(
@@ -368,34 +369,33 @@
         if self._fast_cell is not None:
             return self._fast_cell
         else:
             # lazy late init to make sure the correct fused act quant proxy is captured
             # since on every sharing among layers at init time it is re-initialized
             self._fast_cell = _QuantRNNCell(
                 self.cell.act_fn,
-                self._wrap_act_proxy('gate_acc_quant'), 
-                self._wrap_act_proxy('output_quant'), 
+                self._wrap_act_proxy('gate_acc_quant'),
+                self._wrap_act_proxy('output_quant'),
                 self.cell.reverse_input,
                 self.cell.batch_first,
                 self.cell.output_quant.is_quant_enabled,
                 fast_impl=True)
             if brevitas.config.JIT_ENABLED:
                 self._fast_cell = torch.jit.script(self._fast_cell)
             return self._fast_cell
-    
+
     def forward(self, inp, state):
         quant_input = self.maybe_quantize_input(inp)
         quant_weight_ih, quant_weight_hh, quant_bias = self.gate_params_fwd(
             self.gate_params, quant_input)
         if quant_bias.value is None:
             quant_bias = torch.tensor(0., device=quant_input.value.device)
         else:
             quant_bias = quant_bias.value
-        quant_state = self.maybe_quantize_state(
-            quant_input.value, state, self.cell.output_quant)
+        quant_state = self.maybe_quantize_state(quant_input.value, state, self.cell.output_quant)
         if self.export_mode:
             cell = self.export_handler
         elif self.fast_mode:
             cell = self.fast_cell
         else:
             cell = self.cell
         quant_outputs = cell(
@@ -467,64 +467,68 @@
             input_input_weight: GateWeight = None,
             input_forget_weight: GateWeight = None,
             input_cell_weight: GateWeight = None,
             input_output_weight: GateWeight = None,
             **kwargs):
         nn.Module.__init__(self)
         io_quant = QuantIdentity(io_quant, act_kwargs_prefix='io_', **kwargs)
-        cell_state_quant = QuantIdentity(cell_state_quant, act_kwargs_prefix='cell_state_', **kwargs)
-        
+        cell_state_quant = QuantIdentity(
+            cell_state_quant, act_kwargs_prefix='cell_state_', **kwargs)
+
         # Quantizers for the output accumulator of each gate
         input_acc_quant = QuantIdentity(gate_acc_quant, act_kwargs_prefix='gate_acc_', **kwargs)
         if shared_intra_layer_gate_acc_quant:
             forget_acc_quant = input_acc_quant
             cell_acc_quant = input_acc_quant
             output_acc_quant = input_acc_quant
         else:
             cell_acc_quant = QuantIdentity(gate_acc_quant, act_kwargs_prefix='gate_acc_', **kwargs)
-            output_acc_quant = QuantIdentity(gate_acc_quant, act_kwargs_prefix='gate_acc_', **kwargs)
+            output_acc_quant = QuantIdentity(
+                gate_acc_quant, act_kwargs_prefix='gate_acc_', **kwargs)
             if cifg:
                 # Avoid dealing with None and set it to the input one
                 forget_acc_quant = input_acc_quant
             else:
-                forget_acc_quant = QuantIdentity(gate_acc_quant, act_kwargs_prefix='gate_acc_', **kwargs)
+                forget_acc_quant = QuantIdentity(
+                    gate_acc_quant, act_kwargs_prefix='gate_acc_', **kwargs)
 
         # Internal quantized activations
         input_sigmoid_quant = QuantSigmoid(sigmoid_quant, act_kwargs_prefix='sigmoid_', **kwargs)
         cell_tanh_quant = QuantTanh(tanh_quant, act_kwargs_prefix='tanh_', **kwargs)
         output_sigmoid_quant = QuantSigmoid(sigmoid_quant, act_kwargs_prefix='sigmoid_', **kwargs)
         hidden_state_tanh_quant = QuantTanh(tanh_quant, act_kwargs_prefix='tanh_', **kwargs)
         if cifg:
-            # Avoid dealing with None and set it to the input one 
+            # Avoid dealing with None and set it to the input one
             forget_sigmoid_quant = input_sigmoid_quant
         else:
-            forget_sigmoid_quant = QuantSigmoid(sigmoid_quant, act_kwargs_prefix='sigmoid_', **kwargs)
-        
+            forget_sigmoid_quant = QuantSigmoid(
+                sigmoid_quant, act_kwargs_prefix='sigmoid_', **kwargs)
+
         cell = _QuantLSTMCell(
-            output_quant=io_quant.act_quant, 
-            cell_state_quant=cell_state_quant.act_quant, 
+            output_quant=io_quant.act_quant,
+            cell_state_quant=cell_state_quant.act_quant,
             input_acc_quant=input_acc_quant.act_quant,
             forget_acc_quant=forget_acc_quant.act_quant,
             cell_acc_quant=cell_acc_quant.act_quant,
             output_acc_quant=output_acc_quant.act_quant,
             input_sigmoid_quant=input_sigmoid_quant.act_quant,
-            forget_sigmoid_quant=forget_sigmoid_quant.act_quant, 
-            cell_tanh_quant=cell_tanh_quant.act_quant, 
-            output_sigmoid_quant=output_sigmoid_quant.act_quant, 
+            forget_sigmoid_quant=forget_sigmoid_quant.act_quant,
+            cell_tanh_quant=cell_tanh_quant.act_quant,
+            output_sigmoid_quant=output_sigmoid_quant.act_quant,
             hidden_state_tanh_quant=hidden_state_tanh_quant.act_quant,
             reverse_input=reverse_input,
             batch_first=batch_first,
             cifg=cifg,
             output_quant_enabled=io_quant.act_quant.is_quant_enabled,
             cell_state_quant_enabled=cell_state_quant.act_quant.is_quant_enabled,
             fast_impl=False)
         QuantRecurrentLayerMixin.__init__(
-            self, 
+            self,
             cell=cell,
-            io_quant=io_quant.act_quant, 
+            io_quant=io_quant.act_quant,
             input_size=input_size,
             hidden_size=hidden_size,
             reverse_input=reverse_input,
             quantize_output_only=quantize_output_only,
             shared_input_hidden_weights=shared_input_hidden_weights,
             return_quant_tensor=return_quant_tensor)
 
@@ -533,27 +537,33 @@
         if shared_intra_layer_weight_quant:
             # Share the input-to-hidden input weight quantizer, which is also shared with hidden-to-hidden
             weight_quant = self.input_gate_params.input_weight.weight_quant
         if cifg:
             self.forget_gate_params = None
         else:
             self.forget_gate_params = GateParams(
-                input_size, hidden_size, bias, weight_quant, bias_quant, input_input_weight, **kwargs)
+                input_size,
+                hidden_size,
+                bias,
+                weight_quant,
+                bias_quant,
+                input_input_weight,
+                **kwargs)
         self.cell_gate_params = GateParams(
             input_size, hidden_size, bias, weight_quant, bias_quant, input_cell_weight, **kwargs)
         self.output_gate_params = GateParams(
             input_size, hidden_size, bias, weight_quant, bias_quant, input_output_weight, **kwargs)
         self.shared_cell_state_quant = shared_cell_state_quant
         self.cifg = cifg
         self.reset_parameters()
 
     @property
     def weights_to_share(self):
         if self.shared_input_hidden_weights:
-            out= {
+            out = {
                 'input_input_weight': self.input_gate_params.input_weight,
                 'input_cell_weight': self.cell_gate_params.input_weight,
                 'input_output_weight': self.output_gate_params.input_weight}
             if not self.cifg:
                 out['input_forget_weight'] = self.forget_gate_params.input_weight
             return out
         else:
@@ -572,41 +582,41 @@
             return self._fast_cell
         else:
             # lazy late init to make sure the correct fused act quant proxy is captured
             # since on every sharing among layers at init time it is re-initialized
             self._fast_cell = _QuantLSTMCell(
                 # Wrap a None or act_fn only proxy to still return a tuple
                 # whenever quantization is disabled
-                output_quant=self._wrap_act_proxy('output_quant'), 
-                cell_state_quant=self._wrap_act_proxy('cell_state_quant'), 
+                output_quant=self._wrap_act_proxy('output_quant'),
+                cell_state_quant=self._wrap_act_proxy('cell_state_quant'),
                 input_acc_quant=self._wrap_act_proxy('input_acc_quant'),
                 forget_acc_quant=self._wrap_act_proxy('forget_acc_quant'),
                 cell_acc_quant=self._wrap_act_proxy('cell_acc_quant'),
                 output_acc_quant=self._wrap_act_proxy('output_acc_quant'),
                 input_sigmoid_quant=self._wrap_act_proxy('input_sigmoid_quant'),
-                forget_sigmoid_quant=self._wrap_act_proxy('forget_sigmoid_quant'), 
-                cell_tanh_quant=self._wrap_act_proxy('cell_tanh_quant'), 
-                output_sigmoid_quant=self._wrap_act_proxy('output_sigmoid_quant'), 
-                hidden_state_tanh_quant=self._wrap_act_proxy('hidden_state_tanh_quant'),                
+                forget_sigmoid_quant=self._wrap_act_proxy('forget_sigmoid_quant'),
+                cell_tanh_quant=self._wrap_act_proxy('cell_tanh_quant'),
+                output_sigmoid_quant=self._wrap_act_proxy('output_sigmoid_quant'),
+                hidden_state_tanh_quant=self._wrap_act_proxy('hidden_state_tanh_quant'),
                 reverse_input=self.cell.reverse_input,
                 batch_first=self.cell.batch_first,
                 cifg=self.cell.cifg,
                 output_quant_enabled=self.cell.output_quant.is_quant_enabled,
                 cell_state_quant_enabled=self.cell.cell_state_quant.is_quant_enabled,
                 fast_impl=True)
             if brevitas.config.JIT_ENABLED:
                 self._fast_cell = torch.jit.script(self._fast_cell)
             return self._fast_cell
 
     def forward(self, inp, hidden_state, cell_state):
         quant_input = self.maybe_quantize_input(inp)
         quant_weight_ii, quant_weight_hi, quant_bias_input = self.gate_params_fwd(
-            self.input_gate_params, quant_input) 
+            self.input_gate_params, quant_input)
         quant_weight_ic, quant_weight_hc, quant_bias_cell = self.gate_params_fwd(
-            self.cell_gate_params, quant_input) 
+            self.cell_gate_params, quant_input)
         quant_weight_io, quant_weight_ho, quant_bias_output = self.gate_params_fwd(
             self.output_gate_params, quant_input)
         if self.cifg:
             # Avoid dealing with None and set it the same as the forget one
             quant_weight_if = quant_weight_ii
             quant_weight_hf = quant_weight_hi
             quant_bias_forget = quant_bias_input
@@ -638,44 +648,45 @@
         if self.export_mode:
             cell = self.export_handler
         elif self.fast_mode:
             cell = self.fast_cell
         else:
             cell = self.cell
         quant_outputs, quant_hidden_state, quant_cell_state = cell(
-            quant_input.value, 
-            quant_hidden_state.value, 
-            quant_cell_state.value, 
-            quant_weight_ii=quant_weight_ii.value, 
-            quant_weight_if=quant_weight_if.value, 
-            quant_weight_ic=quant_weight_ic.value, 
-            quant_weight_io=quant_weight_io.value, 
-            quant_weight_hi=quant_weight_hi.value, 
-            quant_weight_hf=quant_weight_hf.value, 
-            quant_weight_hc=quant_weight_hc.value, 
-            quant_weight_ho=quant_weight_ho.value, 
+            quant_input.value,
+            quant_hidden_state.value,
+            quant_cell_state.value,
+            quant_weight_ii=quant_weight_ii.value,
+            quant_weight_if=quant_weight_if.value,
+            quant_weight_ic=quant_weight_ic.value,
+            quant_weight_io=quant_weight_io.value,
+            quant_weight_hi=quant_weight_hi.value,
+            quant_weight_hf=quant_weight_hf.value,
+            quant_weight_hc=quant_weight_hc.value,
+            quant_weight_ho=quant_weight_ho.value,
             quant_bias_input=quant_bias_input,
             quant_bias_forget=quant_bias_forget,
             quant_bias_cell=quant_bias_cell,
             quant_bias_output=quant_bias_output)
         quant_outputs = self.pack_quant_outputs(quant_outputs)
         quant_hidden_state = self.pack_quant_state(quant_hidden_state, self.cell.output_quant)
         quant_cell_state = self.pack_quant_state(quant_cell_state, self.cell.cell_state_quant)
         return quant_outputs, quant_hidden_state, quant_cell_state
 
     def _load_from_state_dict(
-            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         hs = self.hidden_size
 
         def bias(gate_name):
             bias_name = f'{prefix}{gate_name}_gate_params.bias'
             if not bias_name in state_dict.keys():
                 state_dict[bias_name] = torch.zeros(hs)
-            return state_dict[bias_name] 
-        
+            return state_dict[bias_name]
+
         def _set_weight(gate_name, value, input_name):
             key = f'{prefix}{gate_name}_gate_params.{input_name}_weight.weight'
             state_dict[key] = value
 
         set_input_weight = partial(_set_weight, input_name='input')
         set_hidden_weight = partial(_set_weight, input_name='hidden')
 
@@ -717,58 +728,58 @@
             return_quant_tensor: bool,
             **kwargs):
         super(QuantRecurrentStackBase, self).__init__()
         if shared_input_hidden_weights and not bidirectional:
             raise RuntimeError("Shared input-hidden weights requires bidirectional=True.")
         if return_quant_tensor and io_quant is None:
             raise RuntimeError("return_quant_tensor=True requires io_quant != None.")
-        
+
         self.num_directions = 2 if bidirectional else 1
         layers = []
         # Add io_quant to kwargs. This allows easy overwriting during sharing
         kwargs['io_quant'] = io_quant
         for layer in range(num_layers):
             layer_input_size = input_size if layer == 0 else hidden_size * self.num_directions
-            quantize_output_only = bool(layer) 
+            quantize_output_only = bool(layer)
             # return_quant_tensor is required for bias quantization of internal layers
             layer_return_quant_tensor = return_quant_tensor or layer < num_layers - 1
             directions = []
             left_to_right = layer_impl(
-                input_size=layer_input_size, 
-                hidden_size=hidden_size, 
+                input_size=layer_input_size,
+                hidden_size=hidden_size,
                 reverse_input=False,
-                quantize_output_only=quantize_output_only, 
+                quantize_output_only=quantize_output_only,
                 shared_input_hidden_weights=shared_input_hidden_weights,
                 return_quant_tensor=layer_return_quant_tensor,
                 **kwargs)
             directions.append(left_to_right)
-            # Update kwargs with shared quantizers. Overwrite io_quant and any 
-            # other quantizer that should be shared. The quantizers of the 
+            # Update kwargs with shared quantizers. Overwrite io_quant and any
+            # other quantizer that should be shared. The quantizers of the
             # first left-to-right layer are shared to all directions and layers
             kwargs.update(**left_to_right.quantizers_to_share)
             if bidirectional:
                 shared_weights = left_to_right.weights_to_share
                 right_to_left = layer_impl(
-                    input_size=layer_input_size, 
-                    hidden_size=hidden_size, 
+                    input_size=layer_input_size,
+                    hidden_size=hidden_size,
                     reverse_input=True,
-                    quantize_output_only=quantize_output_only, 
+                    quantize_output_only=quantize_output_only,
                     shared_input_hidden_weights=shared_input_hidden_weights,
                     return_quant_tensor=layer_return_quant_tensor,
                     **shared_weights,
                     **kwargs)
                 directions.append(right_to_left)
             layers.append(nn.ModuleList(directions))
         self.layers = nn.ModuleList(layers)
 
-    def forward(self, inp, hx = None):
+    def forward(self, inp, hx=None):
         output_states = []
         for l, layer in enumerate(self.layers):
             dir_outputs, dir_states = [], []
-            for d, direction in enumerate(layer):    
+            for d, direction in enumerate(layer):
                 layer_state = hx[2 * l + d] if hx is not None else hx
                 out, out_state = direction(inp, layer_state)
                 dir_outputs += [out]
                 dir_states += [out_state]
             if len(dir_outputs) > 1:
                 out = torch.cat(dir_outputs, dim=-1)
                 output_states += [torch.cat(dir_states, dim=0)]
@@ -779,47 +790,48 @@
         if len(output_states) > 1:
             output_states = torch.cat(output_states, dim=0)
         else:
             output_states = output_states[0]
         return out, output_states
 
     def _load_from_state_dict(
-            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         for name, value in list(state_dict.items()):
             for index in range(len(self.layers)):
                 layer_name = f'_l{index}'
                 reverse_layer_name = layer_name + '_reverse'
                 if reverse_layer_name in name:
                     param_name = name[len(prefix):-len(reverse_layer_name)]
                     state_dict[f'{prefix}layers.{index}.1.{param_name}'] = value
                     del state_dict[name]
                 elif layer_name in name:
                     param_name = name[len(prefix):-len(layer_name)]
                     state_dict[f'{prefix}layers.{index}.0.{param_name}'] = value
                     del state_dict[name]
         super(QuantRecurrentStackBase, self)._load_from_state_dict(
-            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)  
+            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
 
 
 class QuantRNN(QuantRecurrentStackBase):
 
     def __init__(
             self,
             input_size: int,
             hidden_size: int,
             num_layers: int = 1,
             nonlinearity: str = 'tanh',
             bias: bool = True,
             batch_first: bool = False,
             bidirectional: bool = False,
-            weight_quant = Int8WeightPerTensorFloat,
-            bias_quant = Int32Bias,
-            io_quant = Int8ActPerTensorFloat,
-            gate_acc_quant = Int8ActPerTensorFloat,
-            shared_input_hidden_weights = False,
+            weight_quant=Int8WeightPerTensorFloat,
+            bias_quant=Int32Bias,
+            io_quant=Int8ActPerTensorFloat,
+            gate_acc_quant=Int8ActPerTensorFloat,
+            shared_input_hidden_weights=False,
             return_quant_tensor: bool = False,
             **kwargs):
         super(QuantRNN, self).__init__(
             layer_impl=_QuantRNNLayer,
             input_size=input_size,
             hidden_size=hidden_size,
             num_layers=num_layers,
@@ -842,27 +854,27 @@
             self,
             input_size: int,
             hidden_size: int,
             num_layers: int = 1,
             bias: bool = True,
             batch_first: bool = False,
             bidirectional: bool = False,
-            weight_quant = Int8WeightPerTensorFloat,
-            bias_quant = Int32Bias,
-            io_quant = Int8ActPerTensorFloat,
-            gate_acc_quant = Int8ActPerTensorFloat,
-            sigmoid_quant = Uint8ActPerTensorFloat,
-            tanh_quant = Int8ActPerTensorFloat,
-            cell_state_quant = Int8ActPerTensorFloat,
+            weight_quant=Int8WeightPerTensorFloat,
+            bias_quant=Int32Bias,
+            io_quant=Int8ActPerTensorFloat,
+            gate_acc_quant=Int8ActPerTensorFloat,
+            sigmoid_quant=Uint8ActPerTensorFloat,
+            tanh_quant=Int8ActPerTensorFloat,
+            cell_state_quant=Int8ActPerTensorFloat,
             coupled_input_forget_gates: bool = False,
-            cat_output_cell_states = True,
-            shared_input_hidden_weights = False,
-            shared_intra_layer_weight_quant = False,
-            shared_intra_layer_gate_acc_quant = False,
-            shared_cell_state_quant = True,
+            cat_output_cell_states=True,
+            shared_input_hidden_weights=False,
+            shared_intra_layer_weight_quant=False,
+            shared_intra_layer_gate_acc_quant=False,
+            shared_cell_state_quant=True,
             return_quant_tensor: bool = False,
             **kwargs):
         super(QuantLSTM, self).__init__(
             layer_impl=_QuantLSTMLayer,
             input_size=input_size,
             hidden_size=hidden_size,
             num_layers=num_layers,
@@ -883,19 +895,19 @@
             shared_cell_state_quant=shared_cell_state_quant,
             return_quant_tensor=return_quant_tensor,
             **kwargs)
         if cat_output_cell_states and cell_state_quant is not None and not shared_cell_state_quant:
             raise RuntimeError("Concatenating cell states requires shared cell quantizers.")
         self.cat_output_cell_states = cat_output_cell_states
 
-    def forward(self, inp, hx = None, cx = None):
+    def forward(self, inp, hx=None, cx=None):
         output_hidden_states, output_cell_states = [], []
         for l, layer in enumerate(self.layers):
             dir_outputs, dir_hidden_states, dir_cell_states = [], [], []
-            for d, direction in enumerate(layer):    
+            for d, direction in enumerate(layer):
                 layer_hidden_state = hx[2 * l + d] if hx is not None else hx
                 layer_cell_state = cx[2 * l + d] if cx is not None else cx
                 out, out_hidden_state, out_cell_state = direction(inp, layer_hidden_state, layer_cell_state)
                 dir_outputs += [out]
                 dir_hidden_states += [out_hidden_state]
                 dir_cell_states += [out_cell_state]
             if len(dir_outputs) > 1:
@@ -917,9 +929,8 @@
             output_hidden_states = torch.cat(output_hidden_states, dim=0)
             if self.cat_output_cell_states:
                 output_cell_states = torch.cat(output_cell_states, dim=0)
         else:
             output_hidden_states = output_hidden_states[0]
             if self.cat_output_cell_states:
                 output_cell_states = output_cell_states[0]
-        return out, (output_hidden_states, output_cell_states)    
-
+        return out, (output_hidden_states, output_cell_states)
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/quant_scale_bias.py` & `brevitas-0.9.0/src/brevitas/nn/quant_scale_bias.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Union, Type, Optional
+from typing import Optional, Type, Union
 
 import torch
 from torch import Tensor
-from torch.nn import Module, Parameter
+from torch.nn import Module
+from torch.nn import Parameter
 
-from brevitas.function.ops_ste import ceil_ste
 from brevitas.function.ops import max_int
-from brevitas.quant_tensor import QuantTensor
+from brevitas.function.ops_ste import ceil_ste
 from brevitas.inject.defaults import Int8WeightPerTensorFloat
-from .quant_layer import QuantWeightBiasInputOutputLayer as QuantWBIOL
-from .quant_layer import WeightQuantType, BiasQuantType, ActQuantType
+from brevitas.quant_tensor import QuantTensor
 
+from .quant_layer import ActQuantType
+from .quant_layer import BiasQuantType
+from .quant_layer import QuantWeightBiasInputOutputLayer as QuantWBIOL
+from .quant_layer import WeightQuantType
 
 __all__ = ['ScaleBias', 'QuantScaleBias']
 
 
 class ScaleBias(Module):
 
     def __init__(self, num_features: int, bias: bool, runtime_shape=(1, -1, 1, 1)):
@@ -66,17 +68,14 @@
     def out_channels(self):
         return self.num_features
 
     @property
     def channelwise_separable(self) -> bool:
         return True
 
-    def quant_weight(self):
-        return self.weight_quant(self.weight.view(-1, 1))  # TODO check if the view is needed
-
     def forward(self, inp: Union[Tensor, QuantTensor]) -> Union[Tensor, QuantTensor]:
         return self.forward_impl(inp)
 
     def inner_forward_impl(self, input: Tensor, quant_weight: Tensor, quant_bias: Optional[Tensor]):
         quant_weight = quant_weight.view(self.runtime_shape)
         quant_bias = quant_bias.view(self.runtime_shape)
         output_tensor = input * quant_weight + quant_bias
@@ -84,14 +83,7 @@
 
     def max_acc_bit_width(self, input_bit_width, weight_bit_width):
         max_input_val = max_int(bit_width=input_bit_width, signed=False, narrow_range=False)
         max_weight_val = self.weight_quant.max_uint_value(weight_bit_width)
         max_output_val = max_input_val * max_weight_val
         output_bit_width = ceil_ste(torch.log2(max_output_val))
         return output_bit_width
-
-
-
-
-
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas/nn/quant_upsample.py` & `brevitas-0.9.0/src/brevitas/nn/quant_upsample.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,37 +1,35 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from typing import Union
 
 from torch import Tensor
-from torch.nn import Upsample, UpsamplingBilinear2d, UpsamplingNearest2d
+from torch.nn import Upsample
+from torch.nn import UpsamplingBilinear2d
+from torch.nn import UpsamplingNearest2d
 from torch.nn.functional import interpolate
 
 from brevitas.function import round_ste
 from brevitas.quant_tensor import QuantTensor
+
 from .mixin.base import QuantLayerMixin
 
 
 class QuantUpsample(QuantLayerMixin, Upsample):
 
     def __init__(
             self,
             size=None,
             scale_factor=None,
             mode='nearest',
-            align_corners = None,
+            align_corners=None,
             return_quant_tensor: bool = True):
         Upsample.__init__(
-            self,
-            size=size,
-            scale_factor=scale_factor,
-            mode=mode,
-            align_corners=align_corners)
+            self, size=size, scale_factor=scale_factor, mode=mode, align_corners=align_corners)
         QuantLayerMixin.__init__(self, return_quant_tensor)
 
     @property
     def channelwise_separable(self) -> bool:
         return True
 
     @property
@@ -52,18 +50,15 @@
         y = x.set(value=y_value)
         return self.pack_output(y)
 
 
 class QuantUpsamplingBilinear2d(QuantLayerMixin, UpsamplingBilinear2d):
 
     def __init__(self, size=None, scale_factor=None, return_quant_tensor: bool = True):
-        UpsamplingBilinear2d.__init__(
-            self,
-            size=size,
-            scale_factor=scale_factor)
+        UpsamplingBilinear2d.__init__(self, size=size, scale_factor=scale_factor)
         QuantLayerMixin.__init__(self, return_quant_tensor)
 
     @property
     def channelwise_separable(self) -> bool:
         return True
 
     @property
@@ -83,18 +78,15 @@
         y = x.set(value=y_value)
         return self.pack_output(y)
 
 
 class QuantUpsamplingNearest2d(QuantLayerMixin, UpsamplingNearest2d):
 
     def __init__(self, size=None, scale_factor=None, return_quant_tensor: bool = True):
-        UpsamplingNearest2d.__init__(
-            self,
-            size=size,
-            scale_factor=scale_factor)
+        UpsamplingNearest2d.__init__(self, size=size, scale_factor=scale_factor)
         QuantLayerMixin.__init__(self, return_quant_tensor)
 
     @property
     def channelwise_separable(self) -> bool:
         return True
 
     @property
@@ -106,9 +98,7 @@
         if self.export_mode:
             out = self.export_handler(x.value)
             self._set_global_is_quant_layer(False)
             return out
         y_value = interpolate(x.value, self.size, self.scale_factor, self.mode, self.align_corners)
         y = x.set(value=y_value)
         return self.pack_output(y)
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas/ops/autograd_ste_ops.py` & `brevitas-0.9.0/src/brevitas/ops/autograd_ste_ops.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,25 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 """
 Implementation of various torch.autograd.Function with straight-through estimators.
 """
 
 from typing import Tuple
 
 import torch
-from torch.autograd import Function
 from torch import Tensor
+from torch.autograd import Function
 
-from brevitas.function.ops import tensor_clamp, binary_sign, round_to_zero, tensor_clamp_, dpu_round
+from brevitas.function.ops import binary_sign
+from brevitas.function.ops import dpu_round
+from brevitas.function.ops import round_to_zero
+from brevitas.function.ops import tensor_clamp
+from brevitas.function.ops import tensor_clamp_
 
 __all__ = [
     'ScalarClampSteFn',
     'ScalarClampMinSteFn',
     'TensorClampSteFn',
     'InplaceTensorClampSteFn',
     'RoundToZeroSteFn',
@@ -33,16 +36,15 @@
     'floor_ste_impl',
     'ceil_ste_impl',
     'round_to_zero_ste_impl',
     'scalar_clamp_min_ste_impl',
     'scalar_clamp_ste_impl',
     'tensor_clamp_ste_impl',
     'abs_binary_sign_grad_impl',
-    'dpu_round_ste_impl'
-]
+    'dpu_round_ste_impl']
 
 
 class ScalarClampSteFn(Function):
     """
     Autograd function that implements ``torch.clamp`` with a straight-through gradient estimator
     for the gradient of y w.r.t. to x, while the gradient of y w.r.t. to ``min_val`` and ``min_val``
     are always ``None``.
@@ -94,15 +96,15 @@
     @staticmethod
     def symbolic(g, x: Tensor, min_val: float):
         y = g.op('Clip', x, torch.tensor(min_val))
         return y
 
 
 class TensorClampSteFn(Function):
-    """ 
+    """
     Autograd function that implements :func:`~brevitas.function.ops.tensor_clamp` with a
     straight-through gradient estimator for the gradient of y w.r.t. to x, while the gradient of y
     w.r.t. to min_val and max_val is always None.
 
     ``TensorClampSteFn.apply(*args)`` is first aliased to :func:`tensor_clamp_ste_impl(*args)
     <brevitas.ops.autograd_ste_ops.tensor_clamp_ste_impl>` and then wrapped by
     :func:`~brevitas.function.ops_ste.tensor_clamp` when env ``BREVITAS_JIT=0``.
@@ -213,15 +215,15 @@
 
     @staticmethod
     def symbolic(g, x: Tensor):
         raise NotImplementedError
 
 
 class CeilSteFn(Function):
-    """ 
+    """
     Autograd function that implements :func:`torch.ceil` with a straight-through gradient estimator.
 
     ``CeilSteFn.apply(*args)`` is first aliased to :func:`ceil_ste_impl(*args)
     <brevitas.ops.autograd_ste_ops.ceil_ste_impl>` and then wrapped by
     :func:`~brevitas.function.ops_ste.ceil_ste` when env ``BREVITAS_JIT=0``.
     See :func:`~brevitas.function.ops_ste.ceil_ste` for details on the interface and
     examples.
@@ -239,15 +241,15 @@
     @staticmethod
     def symbolic(g, x: Tensor):
         y = g.op('Ceil', x)
         return y
 
 
 class FloorSteFn(Function):
-    """ 
+    """
     Autograd function that implements :func:`torch.floor` with a straight-through gradient estimator.
 
     ``FloorSteFn.apply(*args)`` is first aliased to :func:`floor_ste_impl(*args)
     <brevitas.ops.autograd_ste_ops.floor_ste_impl>` and then wrapped by
     :func:`~brevitas.function.ops_ste.floor_ste` when env ``BREVITAS_JIT=0``.
     See :func:`~brevitas.function.ops_ste.floor_ste` for details on the interface and
     examples.
@@ -265,20 +267,20 @@
     @staticmethod
     def symbolic(g, x: Tensor):
         y = g.op('Floor', x)
         return y
 
 
 class BinarySignSteFn(Function):
-    """ 
+    """
     Autograd function that implements :func:`~brevitas.function.ops.binary_sign` with a
     straight-through gradient estimator.
 
-    ``BinarySignSteFn.apply(*args)`` is first aliased to 
-    :func:`binary_sign_ste_impl(*args)<brevitas.ops.autograd_ste_ops.binary_sign_ste_impl>` 
+    ``BinarySignSteFn.apply(*args)`` is first aliased to
+    :func:`binary_sign_ste_impl(*args)<brevitas.ops.autograd_ste_ops.binary_sign_ste_impl>`
     and then wrapped by :func:`~brevitas.function.ops_ste.binary_sign_ste` when env ``BREVITAS_JIT=0``.
     See :func:`~brevitas.function.ops_ste.binary_sign_ste` for details on the interface and
     examples.
     """
 
     @staticmethod
     def forward(ctx, x: Tensor) -> Tensor:
@@ -297,15 +299,15 @@
         positive_mask = g.op('Cast', positive_mask, to_i=torch.onnx.TensorProtoDataType.FLOAT)
         negative_mask = g.op('Cast', negative_mask, to_i=torch.onnx.TensorProtoDataType.FLOAT)
         y = g.op('Sub', positive_mask, negative_mask)
         return y
 
 
 class TernarySignSteFn(Function):
-    """ 
+    """
     Autograd function that implements :func:`torch.sign` with a straight-through gradient estimator.
 
     ``TernarySignSteFn.apply(*args)`` is first aliased to :func:`ternary_sign_ste_impl(*args)
     <brevitas.ops.autograd_ste_ops.ternary_sign_ste_impl>` and then wrapped by
     :func:`~brevitas.function.ops_ste.ternary_sign_ste` when env ``BREVITAS_JIT=0``.
     See :func:`~brevitas.function.ops_ste.ternary_sign_ste` for details on the interface and
     examples.
```

### Comparing `brevitas-0.8.0/src/brevitas/proxy/parameter_quant.py` & `brevitas-0.9.0/src/brevitas/proxy/parameter_quant.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,30 +1,30 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from abc import ABCMeta, abstractmethod
-from typing import Tuple, Optional, List
-from typing_extensions import Protocol, runtime_checkable
+from abc import ABCMeta
+from abc import abstractmethod
+from typing import List, Optional, Tuple
 
 import torch
 from torch import Tensor
+from typing_extensions import Protocol
+from typing_extensions import runtime_checkable
 
 from brevitas.function import max_int
 from brevitas.quant_tensor import QuantTensor
 
-from .quant_proxy import QuantProxyFromInjector, QuantProxyProtocol
-
+from .quant_proxy import QuantProxyFromInjector
+from .quant_proxy import QuantProxyProtocol
 
 __all__ = [
     'WeightQuantProxyFromInjector',
     'BiasQuantProxyFromInjector',
     'WeightQuantProxyProtocol',
-    'BiasQuantProxyProtocol'
-]
+    'BiasQuantProxyProtocol']
 
 
 @runtime_checkable
 class WeightQuantProxyProtocol(QuantProxyProtocol, Protocol):
 
     def forward(self, x: torch.Tensor) -> QuantTensor:
         ...
@@ -32,17 +32,15 @@
 
 @runtime_checkable
 class BiasQuantProxyProtocol(QuantProxyProtocol, Protocol):
     requires_input_bit_width: bool
     requires_input_scale: bool
 
     def forward(
-            self,
-            x: Tensor,
-            input_scale: Optional[Tensor],
+            self, x: Tensor, input_scale: Optional[Tensor],
             input_bit_width: Optional[Tensor]) -> QuantTensor:
         ...
 
 
 class ParameterQuantProxyFromInjector(QuantProxyFromInjector):
     __metaclass__ = ABCMeta
 
@@ -64,14 +62,18 @@
 
 class WeightQuantProxyFromInjector(ParameterQuantProxyFromInjector, WeightQuantProxyProtocol):
 
     @property
     def tracked_parameter_list(self):
         return [m.weight for m in self.tracked_module_list if m.weight is not None]
 
+    @property
+    def requires_quant_input(self):
+        return False
+
     def scale(self):
         scale = self.__call__(self.tracked_parameter_list[0]).scale
         return scale
 
     def zero_point(self):
         zero_point = self.__call__(self.tracked_parameter_list[0]).zero_point
         return zero_point
@@ -106,14 +108,46 @@
             impl = self.export_handler if self.export_mode else self.tensor_quant
             out, scale, zero_point, bit_width, pre_scale, pre_zero_point = impl(x)
             return QuantTensor(out, scale, zero_point, bit_width, self.is_signed, self.training)
         else:  # quantization disabled
             return QuantTensor(x, training=self.training)
 
 
+class DecoupledWeightQuantWithInputProxyFromInjector(DecoupledWeightQuantProxyFromInjector):
+
+    @property
+    def requires_quant_input(self):
+        return True
+
+    def scale(self):
+        raise NotImplementedError
+
+    def zero_point(self):
+        raise NotImplementedError
+
+    def bit_width(self):
+        raise NotImplementedError
+
+    def pre_scale(self):
+        raise NotImplementedError
+
+    def pre_zero_point(self):
+        raise NotImplementedError
+
+    def forward(
+            self, x: torch.Tensor, input_bit_width: torch.Tensor,
+            input_is_signed: bool) -> QuantTensor:
+        if self.is_quant_enabled:
+            impl = self.export_handler if self.export_mode else self.tensor_quant
+            out, scale, zero_point, bit_width, pre_scale, pre_zero_point = impl(x, input_bit_width, input_is_signed)
+            return QuantTensor(out, scale, zero_point, bit_width, self.is_signed, self.training)
+        else:  # quantization disabled
+            return QuantTensor(x, training=self.training)
+
+
 class BiasQuantProxyFromInjector(ParameterQuantProxyFromInjector, BiasQuantProxyProtocol):
 
     @property
     def tracked_parameter_list(self):
         return [m.bias for m in self.tracked_module_list if m.bias is not None]
 
     @property
@@ -169,8 +203,7 @@
             elif not self.requires_input_scale and not self.requires_input_bit_width:
                 out, out_scale, out_zp, out_bit_width = impl(x)
             else:
                 raise RuntimeError("Internally defined bit-width required")
             return QuantTensor(out, out_scale, out_zp, out_bit_width, self.is_signed, self.training)
         else:
             return QuantTensor(x, training=self.training)
-
```

### Comparing `brevitas-0.8.0/src/brevitas/proxy/quant_proxy.py` & `brevitas-0.9.0/src/brevitas/proxy/quant_proxy.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,27 +1,28 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from abc import ABCMeta, abstractmethod
+from abc import ABCMeta
+from abc import abstractmethod
 from typing import Optional
-from typing_extensions import Protocol, runtime_checkable
 
-from torch import tensor, nn
+from torch import nn
+from torch import tensor
+from typing_extensions import Protocol
+from typing_extensions import runtime_checkable
 
-from brevitas.inject import BaseInjector as Injector
+from brevitas import config
+from brevitas.common import ExportMixin
 from brevitas.core.utils import StatelessBuffer
+from brevitas.inject import BaseInjector as Injector
 from brevitas.utils.quant_utils import float_to_int_impl_to_enum
-from brevitas.common import ExportMixin
-from brevitas import config
 
 __all__ = [
     'QuantProxyProtocol',
-    'QuantProxyFromInjector',
-]
+    'QuantProxyFromInjector',]
 
 
 def _is_signed(quant_injector):
     if 'signed' in quant_injector:
         return quant_injector.signed
     return None
 
@@ -33,15 +34,16 @@
 
 
 def _rounding_mode(quant_injector):
     if 'float_to_int_impl_type' in quant_injector:
         return str(quant_injector.float_to_int_impl_type)
     elif 'float_to_int_impl' in quant_injector:
         try:
-            return str(float_to_int_impl_to_enum(quant_injector.float_to_int_impl))
+            impl_type = float_to_int_impl_to_enum(quant_injector.float_to_int_impl)
+            return str(impl_type).upper()
         except:
             return None
     else:
         return None
 
 
 def _update_state_dict_impl(quant_injector):
@@ -62,18 +64,15 @@
     def add_tracked_module(self, module: nn.Module) -> None:
         ...
 
 
 class QuantProxyFromInjector(ExportMixin, nn.Module, QuantProxyProtocol):
     __metaclass__ = ABCMeta
 
-    def __init__(
-            self,
-            quant_layer: nn.Module,
-            quant_injector: Injector) -> None:
+    def __init__(self, quant_layer: nn.Module, quant_injector: Injector) -> None:
         ExportMixin.__init__(self)
         nn.Module.__init__(self)
         QuantProxyProtocol.__init__(self)
         self.update_state_dict_impl = _update_state_dict_impl(quant_injector)
         self.quant_injector = quant_injector
         self._zero_hw_sentinel = StatelessBuffer(tensor(0.0))
         self.tensor_quant = None
@@ -117,16 +116,17 @@
         if module is not None:
             self.tracked_module_list.append(module)
             self.update_tracked_modules()
             self.init_tensor_quant()
         else:
             raise RuntimeError("Trying to add None as a parent module.")
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         if self.update_state_dict_impl is not None:
             self.update_state_dict_impl(prefix, state_dict)
         super(QuantProxyFromInjector, self)._load_from_state_dict(
             state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
         # reload tensor_quant on changes of the state_dict
         # this is called after the parent module state_dict is restored (e.g. weights)
         # so init_tensor_quant takes into account new data from the parent module,
@@ -134,8 +134,7 @@
         # for the parameter already, it's not overwritten
         if config.REINIT_ON_STATE_DICT_LOAD:
             self.init_tensor_quant()
         # for retrocompatibility with when it wasn't removed
         zero_hw_sentinel_key = prefix + 'zero_hw_sentinel'
         if zero_hw_sentinel_key in unexpected_keys:
             unexpected_keys.remove(zero_hw_sentinel_key)
-
```

### Comparing `brevitas-0.8.0/src/brevitas/proxy/runtime_quant.py` & `brevitas-0.9.0/src/brevitas/proxy/runtime_quant.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,31 +1,30 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
+from typing import Optional, Tuple, Union
 
-from typing import Optional, Union, Tuple
-from typing_extensions import Protocol, runtime_checkable
-
-from torch import Tensor, nn
+from torch import nn
+from torch import Tensor
 from torch.nn import Identity
+from typing_extensions import Protocol
+from typing_extensions import runtime_checkable
 
 import brevitas
 from brevitas.quant_tensor import QuantTensor
 
-from .quant_proxy import QuantProxyFromInjector, QuantProxyProtocol
-
-
+from .quant_proxy import QuantProxyFromInjector
+from .quant_proxy import QuantProxyProtocol
 
 __all__ = [
     'ActQuantProxyProtocol',
     'AccQuantProxyProtocol',
     'ActQuantProxyFromInjector',
     'TruncQuantProxyFromInjector',
-    'ClampQuantProxyFromInjector'
-]
+    'ClampQuantProxyFromInjector']
 
 
 def _is_passthrough_act(quant_injector):
     if 'act_impl' not in quant_injector:
         return True
     elif quant_injector.act_impl is None:
         return True
@@ -61,16 +60,16 @@
 
 class _TensorQuantDisabledIdentity(brevitas.jit.ScriptModule):
 
     def __init__(self, module_to_wrap=None):
         super(_TensorQuantDisabledIdentity, self).__init__()
 
     @brevitas.jit.script_method
-    def forward(self, x: Tensor) -> Tuple[
-            Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor]]:
+    def forward(self,
+                x: Tensor) -> Tuple[Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor]]:
         return (x, None, None, None)
 
 
 class FusedActivationQuantProxy(brevitas.jit.ScriptModule):
 
     def __init__(self, activation_impl, tensor_quant):
         super(FusedActivationQuantProxy, self).__init__()
@@ -98,29 +97,27 @@
     @is_quant_enabled.setter
     def is_quant_enabled(self, is_quant_enabled):
         self._is_quant_enabled = is_quant_enabled
 
     def init_tensor_quant(self):
         tensor_quant = self.quant_injector.tensor_quant
         if 'act_impl' in self.quant_injector:
-            act_impl = self.quant_injector.act_impl 
-        else: 
+            act_impl = self.quant_injector.act_impl
+        else:
             act_impl = None
         is_act_enabled = _is_act_enabled(act_impl, tensor_quant)
         is_quant_enabled = tensor_quant is not None
         self.is_quant_enabled = is_quant_enabled
         if is_act_enabled and is_quant_enabled:
-            self.fused_activation_quant_proxy = FusedActivationQuantProxy(
-                act_impl, tensor_quant)
+            self.fused_activation_quant_proxy = FusedActivationQuantProxy(act_impl, tensor_quant)
         elif is_act_enabled and not is_quant_enabled:
             self.fused_activation_quant_proxy = FusedActivationQuantProxy(
                 act_impl, _TensorQuantDisabledIdentity())
         elif not is_act_enabled and is_quant_enabled:
-            self.fused_activation_quant_proxy = FusedActivationQuantProxy(
-                Identity(), tensor_quant)
+            self.fused_activation_quant_proxy = FusedActivationQuantProxy(Identity(), tensor_quant)
         else:
             self.fused_activation_quant_proxy = None
 
     def scale(self, force_eval=True):
         current_status = self.training
         if force_eval:
             self.eval()
@@ -146,19 +143,22 @@
             if isinstance(y, QuantTensor):
                 y = y.value
             if self.export_mode:
                 y = self.fused_activation_quant_proxy.activation_impl(y)
                 y = self.export_handler(y)
             else:
                 y = self.fused_activation_quant_proxy(y)
-            if isinstance(y, tuple):
+            # If y is an empty QuantTensor, we need to check if this is a passthrough proxy,
+            # otherwise return an empty QuantTensor
+            if isinstance(y, tuple) and not any(map(lambda f: f is None, y)):
                 return QuantTensor(*y, signed=self.is_signed, training=self.training)
             elif self.is_passthrough_act:  # preserve scale/zp/bit/sign even without output quant
-                return QuantTensor(
-                    y, x.scale, x.zero_point, x.bit_width, x.signed, self.training)
+                if isinstance(y, tuple):
+                    y = y[0]
+                return QuantTensor(y, x.scale, x.zero_point, x.bit_width, x.signed, self.training)
             else:
                 return QuantTensor(y, training=self.training)
         else:
             if isinstance(x, QuantTensor):  # passthrough
                 return x
             else:
                 return QuantTensor(x, training=self.training)
@@ -185,27 +185,26 @@
 
     def forward(self, x: QuantTensor):
         if self.is_quant_enabled:
             if self.export_mode:
                 out_tuple = self.export_handler(
                     x.value, x.scale, x.zero_point, x.bit_width, x.signed)
             else:
-                out_tuple = self.tensor_quant(
-                    x.value, x.scale, x.zero_point, x.bit_width)
+                out_tuple = self.tensor_quant(x.value, x.scale, x.zero_point, x.bit_width)
             out_value, out_scale, out_zp, out_bit_width = out_tuple
-            return QuantTensor(
-                out_value, out_scale, out_zp, out_bit_width, x.signed, self.training)
+            return QuantTensor(out_value, out_scale, out_zp, out_bit_width, x.signed, self.training)
         else:
             return x
 
-    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
-                              missing_keys, unexpected_keys, error_msgs):
+    def _load_from_state_dict(
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
+            error_msgs):
         super(TruncQuantProxyFromInjector, self)._load_from_state_dict(
             state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
         # for retrocompatibility with when it wasn't removed and it was called differently
         zhs = 'zero_hw_sentinel'
         zhs_key = prefix + zhs
         zhs_old_prefix_key = '.'.join(prefix.split('.')[:-2]) + '.accumulator_quant.' + zhs
         if zhs in unexpected_keys:
             unexpected_keys.remove(zhs_key)
         if zhs_old_prefix_key in unexpected_keys:
-            unexpected_keys.remove(zhs_old_prefix_key)
+            unexpected_keys.remove(zhs_old_prefix_key)
```

### Comparing `brevitas-0.8.0/src/brevitas/proxy/utils.py` & `brevitas-0.9.0/src/brevitas/proxy/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from torch.nn import Module
 
 
 class ConvertRuntimeStatsToParameter:
 
     def __init__(self, restrict_scaling_impl: Module):
         self.restrict_scaling_impl = restrict_scaling_impl
```

### Comparing `brevitas-0.8.0/src/brevitas/quant/binary.py` & `brevitas-0.9.0/src/brevitas/quant/binary.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,20 +1,17 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
+from brevitas.core.function_wrapper import InplaceTensorClampSte
+from brevitas.core.function_wrapper import TensorClamp
 from brevitas.quant.base import SignedBinaryClampedConst
-from brevitas.core.function_wrapper import InplaceTensorClampSte, TensorClamp
-from brevitas.quant.solver import WeightQuantSolver, ActQuantSolver
-
+from brevitas.quant.solver import ActQuantSolver
+from brevitas.quant.solver import WeightQuantSolver
 
-__all__ = [
-    'SignedBinaryWeightPerTensorConst',
-    'SignedBinaryActPerTensorConst'
-]
+__all__ = ['SignedBinaryWeightPerTensorConst', 'SignedBinaryActPerTensorConst']
 
 
 class SignedBinaryWeightPerTensorConst(SignedBinaryClampedConst, WeightQuantSolver):
     """
     Signed binary weight quantizer with constant scale factor and inplace clipping to the scale.
 
     Examples:
@@ -31,9 +28,7 @@
     Examples:
         >>> from brevitas.nn import QuantIdentity
         >>> act = QuantIdentity(act_quant=SignedBinaryActPerTensorConst)
     """
     tensor_clamp_impl = TensorClamp
     min_val = -1.0
     max_val = 1.0
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas/quant/shifted_scaled_int.py` & `brevitas-0.9.0/src/brevitas/quant/shifted_scaled_int.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,63 +1,75 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from brevitas.quant.base import *
-from brevitas.quant.solver.weight import WeightQuantSolver
-from brevitas.quant.solver.bias import BiasQuantSolver
 from brevitas.quant.solver.act import ActQuantSolver
+from brevitas.quant.solver.bias import BiasQuantSolver
 from brevitas.quant.solver.trunc import TruncQuantSolver
+from brevitas.quant.solver.weight import WeightQuantSolver
 
 __all__ = [
+    'ShiftedUint8ActPerTensorFixedPoint',
     'ShiftedUint8ActPerTensorFloat',
     'ShiftedUint8WeightPerTensorFloat',
-    'ShiftedUint8WeightPerChannelFloat'
-]
+    'ShiftedUint8WeightPerChannelFloat']
+
+
+class ShiftedUint8ActPerTensorFixedPoint(ShiftedParamFromPercentileUintQuant,
+                                         ParamFromRuntimePercentileIntervalScaling,
+                                         PerTensorPoTScaling8bit,
+                                         ActQuantSolver):
+    """
+    8-bit per-tensor unsigned int fixed-point activations quantizer with
+    integer zero point. Both zero-point and scale factors are learned parameters initialized from
+    runtime statistics.
+
+        Examples:
+        >>> from brevitas.nn import QuantReLU
+        >>> act = QuantReLU(act_quant=ShiftedUint8ActPerTensorFixedPoint)
+    """
+    pass
 
 
-class ShiftedUint8ActPerTensorFloat(
-    ShiftedParamFromPercentileUintQuant,
-    ParamFromRuntimePercentileIntervalScaling,
-    PerTensorFloatScaling8bit,
-    ActQuantSolver):
+class ShiftedUint8ActPerTensorFloat(ShiftedParamFromPercentileUintQuant,
+                                    ParamFromRuntimePercentileIntervalScaling,
+                                    PerTensorFloatScaling8bit,
+                                    ActQuantSolver):
     """
     8-bit per-tensor unsigned int activations quantizer with floating-point scale factor and
     integer zero point. Both zero-point and scale factors are learned parameters initialized from
     runtime statistics.
 
         Examples:
         >>> from brevitas.nn import QuantReLU
         >>> act = QuantReLU(act_quant=ShiftedUint8ActPerTensorFloat)
     """
     pass
 
 
-class ShiftedUint8WeightPerTensorFloat(
-    ShiftedMinUintQuant,
-    MinMaxStatsScaling,
-    PerTensorFloatScaling8bit,
-    WeightQuantSolver):
+class ShiftedUint8WeightPerTensorFloat(ShiftedMinUintQuant,
+                                       MinMaxStatsScaling,
+                                       PerTensorFloatScaling8bit,
+                                       WeightQuantSolver):
     """
     8-bit per-tensor unsigned int weight quantizer with floating-point per-tensor scale factor and integer
     zero point. Both zero-point and scale factors are based on backpropagated statistics of the
     weight tensor.
 
     Examples:
         >>> from brevitas.nn import QuantLinear
         >>> fc = QuantLinear(10, 5, bias=False, weight_quant=ShiftedUint8WeightPerTensorFloat)
     """
     pass
 
 
-class ShiftedUint8WeightPerChannelFloat(
-    ShiftedMinUintQuant,
-    MinMaxStatsScaling,
-    PerChannelFloatScaling8bit,
-    WeightQuantSolver):
+class ShiftedUint8WeightPerChannelFloat(ShiftedMinUintQuant,
+                                        MinMaxStatsScaling,
+                                        PerChannelFloatScaling8bit,
+                                        WeightQuantSolver):
     """
     8-bit per-tensor unsigned int weight quantizer with floating-point per-channel scale factor and integer
     zero point. Both zero-point and scale factors are based on backpropagated statistics of the
     weight tensor.
 
     Examples:
         >>> from brevitas.nn import QuantLinear
```

### Comparing `brevitas-0.8.0/src/brevitas/quant/solver/bias.py` & `brevitas-0.9.0/src/brevitas/quant/solver/bias.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,28 +1,27 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from brevitas.inject import ExtendedInjector, value
+from brevitas.core.function_wrapper import Identity
+from brevitas.core.quant import PrescaledRestrictIntQuant
+from brevitas.core.quant import PrescaledRestrictIntQuantWithInputBitWidth
+from brevitas.core.quant import RescalingIntQuant
+from brevitas.inject import ExtendedInjector
+from brevitas.inject import value
+from brevitas.inject.enum import QuantType
 from brevitas.proxy import BiasQuantProxyFromInjector
 from brevitas.quant.solver.common import *
 from brevitas.quant.solver.parameter import *
-from brevitas.inject.enum import QuantType
-from brevitas.core.function_wrapper import Identity
-from brevitas.core.quant import RescalingIntQuant, PrescaledRestrictIntQuant
-from brevitas.core.quant import PrescaledRestrictIntQuantWithInputBitWidth
-
 
 __all__ = [
     'BiasQuantSolver',
     'SolveBiasTensorQuantFromEnum',
     'SolveBiasScalingStatsInputConcatDimFromModule',
     'SolveBiasBitWidthImplFromEnum',
-    'SolveBiasScalingPerOutputChannelShapeFromModule'
-]
+    'SolveBiasScalingPerOutputChannelShapeFromModule']
 
 
 class SolveBiasScalingStatsInputConcatDimFromModule(ExtendedInjector):
     scaling_stats_input_concat_dim = 0  # bias has only 1 dimension by definition
 
 
 class SolveBiasScalingPerOutputChannelShapeFromModule(ExtendedInjector):
@@ -62,28 +61,27 @@
             raise RuntimeError(f'{quant_type} not supported.')
         elif quant_type == QuantType.BINARY:
             raise RuntimeError(f'{quant_type} not supported.')
         else:
             raise RuntimeError(f'{quant_type} not recognized.')
 
 
-class BiasQuantSolver(
-        SolveScalingStatsInputViewShapeImplFromEnum,
-        SolveParameterScalingShape,
-        SolveStatsReduceDimFromEnum,
-        SolveScalingStatsOpFromEnum,
-        SolveTensorQuantFloatToIntImplFromEnum,
-        SolveRestrictScalingImplFromEnum,
-        SolveIntScalingImplFromEnum,
-        SolveParameterScalingImplFromEnum,
-        SolveParameterTensorClampImplFromEnum,
-        SolveParameterScalingInitFromEnum,
-        SolveBiasBitWidthImplFromEnum,
-        SolveBiasScalingPerOutputChannelShapeFromModule,
-        SolveBiasScalingStatsInputConcatDimFromModule,
-        SolveBiasTensorQuantFromEnum):
+class BiasQuantSolver(SolveScalingStatsInputViewShapeImplFromEnum,
+                      SolveParameterScalingShape,
+                      SolveStatsReduceDimFromEnum,
+                      SolveScalingStatsOpFromEnum,
+                      SolveTensorQuantFloatToIntImplFromEnum,
+                      SolveRestrictScalingImplFromEnum,
+                      SolveIntScalingImplFromEnum,
+                      SolveParameterScalingImplFromEnum,
+                      SolveParameterTensorClampImplFromEnum,
+                      SolveParameterScalingInitFromEnum,
+                      SolveBiasBitWidthImplFromEnum,
+                      SolveBiasScalingPerOutputChannelShapeFromModule,
+                      SolveBiasScalingStatsInputConcatDimFromModule,
+                      SolveBiasTensorQuantFromEnum):
     """
     Translate enum directives to bias-specific quantization core modules.
     It should be placed last in the list of classes a quantizer inherits from,
     to make sure overrides are correctly captured.
     """
     proxy_class = BiasQuantProxyFromInjector
```

### Comparing `brevitas-0.8.0/src/brevitas/quant/solver/clamp.py` & `brevitas-0.9.0/src/brevitas/quant/solver/clamp.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,17 +1,19 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from brevitas.proxy import ClampQuantProxyFromInjector
+from brevitas.core.bit_width import BitWidthConst
+from brevitas.core.bit_width import BitWidthImplType
+from brevitas.core.bit_width import MsbClampBitWidth
+from brevitas.core.bit_width import RemoveBitwidthParameter
 from brevitas.core.quant import PrescaledRestrictIntQuantWithInputBitWidth
-from brevitas.core.bit_width import MsbClampBitWidth, RemoveBitwidthParameter
-from brevitas.core.bit_width import BitWidthImplType, BitWidthConst
-from brevitas.inject import ExtendedInjector, value
+from brevitas.inject import ExtendedInjector
+from brevitas.inject import value
 from brevitas.inject.enum import QuantType
+from brevitas.proxy import ClampQuantProxyFromInjector
 
 
 class SolveClampTensorQuantFromEnum(ExtendedInjector):
 
     @value
     def tensor_quant(quant_type):
         if quant_type == QuantType.FP:
@@ -44,16 +46,14 @@
                 return RemoveBitwidthParameter
             else:
                 raise RuntimeError(f'{quant_type} not recognized.')
         else:
             return None
 
 
-class ClampQuantSolver(
-        SolveClampBitWidthImplFromEnum,
-        SolveClampTensorQuantFromEnum):
+class ClampQuantSolver(SolveClampBitWidthImplFromEnum, SolveClampTensorQuantFromEnum):
     """
     Translate enum directives to clamping-specific quantization core modules.
     It should be placed last in the list of classes a quantizer inherits from,
     to make sure overrides are correctly captured.
     """
-    proxy_class = ClampQuantProxyFromInjector
+    proxy_class = ClampQuantProxyFromInjector
```

### Comparing `brevitas-0.8.0/src/brevitas/quant/solver/common.py` & `brevitas-0.9.0/src/brevitas/quant/solver/common.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,58 +1,64 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from brevitas.core.quant import *
-from brevitas.core.function_wrapper import *
-from brevitas.core.scaling import *
-from brevitas.core.restrict_val import *
 from brevitas.core.bit_width import *
+from brevitas.core.function_wrapper import *
+from brevitas.core.function_wrapper.learned_round import LearnedRoundHardSigmoid
+from brevitas.core.function_wrapper.learned_round import LearnedRoundSigmoid
+from brevitas.core.function_wrapper.learned_round import LearnedRoundSte
+from brevitas.core.quant import *
 from brevitas.core.quant import QuantType
-from brevitas.core.stats import *
+from brevitas.core.restrict_val import *
+from brevitas.core.scaling import *
 from brevitas.core.scaling import ScalingImplType
-from brevitas.inject import ExtendedInjector, value
-
+from brevitas.core.stats import *
+from brevitas.inject import ExtendedInjector
+from brevitas.inject import value
+from brevitas.inject.enum import LearnedRoundImplType
 
 __all__ = [
     'solve_bit_width_impl_from_enum',
     'solve_restrict_value_impl_from_enum',
     'solve_float_to_int_impl_from_enum',
     'SolveAffineRescalingFromEnum',
     'SolveIntQuantFromEnum',
     'SolveTensorQuantFloatToIntImplFromEnum',
     'SolveIntScalingImplFromEnum',
     'SolveRestrictScalingImplFromEnum',
     'SolveScalingStatsOpFromEnum',
     'SolveBitWidthImplFromEnum',
     'SolveStatsReduceDimFromEnum',
-    'SolveScalingStatsInputViewShapeImplFromEnum'
-]
+    'SolveScalingStatsInputViewShapeImplFromEnum']
 
 
 def solve_float_to_int_impl_from_enum(impl_type):
     if impl_type == FloatToIntImplType.ROUND:
         return RoundSte
     elif impl_type == FloatToIntImplType.FLOOR:
         return FloorSte
     elif impl_type == FloatToIntImplType.CEIL:
         return CeilSte
     elif impl_type == FloatToIntImplType.ROUND_TO_ZERO:
         return RoundToZeroSte
     elif impl_type == FloatToIntImplType.DPU:
         return DPURoundSte
+    elif impl_type == FloatToIntImplType.LEARNED_ROUND:
+        return LearnedRoundSte
     else:
         raise Exception(f"{impl_type} not recognized.")
 
 
 def solve_bit_width_impl_from_enum(impl_type):
     if impl_type == BitWidthImplType.CONST:
         return BitWidthConst
     elif impl_type == BitWidthImplType.PARAMETER:
         return BitWidthParameter
+    elif impl_type == BitWidthImplType.STATEFUL_CONST:
+        return BitWidthStatefulConst
     else:
         raise Exception(f"{impl_type} not recognized.")
 
 
 def solve_restrict_value_impl_from_enum(impl_type):
     if impl_type == RestrictValueType.FP:
         return FloatRestrictValue
@@ -126,29 +132,49 @@
 
 class SolveTensorQuantFloatToIntImplFromEnum(ExtendedInjector):
 
     @value
     def float_to_int_impl(float_to_int_impl_type):
         return solve_float_to_int_impl_from_enum(float_to_int_impl_type)
 
+    @value
+    def learned_round_impl(learned_round_impl_type):
+        if learned_round_impl_type == LearnedRoundImplType.SIGMOID:
+            return LearnedRoundSigmoid
+        if learned_round_impl_type == LearnedRoundImplType.HARD_SIGMOID:
+            return LearnedRoundHardSigmoid
+
+    @value
+    def learned_round_init(tracked_parameter_list):
+        if len(tracked_parameter_list) > 1:
+            raise RuntimeError('LearnedRound does not support shared quantizers')
+        return torch.full(tracked_parameter_list[0].shape, 0.)
+
+
+class SolveTensorQuantFloatToIntImplFromEnum(ExtendedInjector):
+
+    @value
+    def float_to_int_impl(float_to_int_impl_type):
+        return solve_float_to_int_impl_from_enum(float_to_int_impl_type)
+
 
 class SolveIntScalingImplFromEnum(ExtendedInjector):
 
     @value
     def int_scaling_impl(restrict_scaling_type):
         if restrict_scaling_type == RestrictValueType.FP:
             return IntScaling
         elif restrict_scaling_type == RestrictValueType.LOG_FP:
             return IntScaling
         elif restrict_scaling_type == RestrictValueType.POWER_OF_TWO:
             return PowerOfTwoIntScaling
         else:
             raise RuntimeError(f"{restrict_scaling_type} not recognized.")
-    
-    
+
+
 class SolveStatsReduceDimFromEnum(ExtendedInjector):
 
     @value
     def stats_reduce_dim(scaling_stats_op, scaling_per_output_channel):
         if scaling_stats_op == StatsOp.MAX_AVE or scaling_per_output_channel:
             return SCALING_STATS_REDUCE_DIM
         else:
```

### Comparing `brevitas-0.8.0/src/brevitas/quant/solver/parameter.py` & `brevitas-0.9.0/src/brevitas/quant/solver/parameter.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,34 +1,34 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
 import math
 from typing import List
 
-from dependencies import value, this
+from dependencies import this
+from dependencies import value
 import torch
 from torch import Tensor
 
-from brevitas.core.function_wrapper import TensorClamp, TensorClampSte
-from brevitas.core.scaling import *
 from brevitas.core.bit_width import *
+from brevitas.core.function_wrapper import TensorClamp
+from brevitas.core.function_wrapper import TensorClampSte
+from brevitas.core.scaling import *
 from brevitas.core.scaling import ScalingImplType
 from brevitas.inject import ExtendedInjector
 from brevitas.quant.solver.common import *
 
-
 __all__ = [
     'ScalingConstInit',
     'ParameterFromStatsScalingInit',
     'HeScalingInit',
     'SolveParameterTensorClampImplFromEnum',
     'SolveParameterScalingInitFromEnum',
     'SolveParameterScalingImplFromEnum',
-    'SolveParameterScalingShape'
-]
+    'SolveParameterScalingShape']
 
 
 class ScalingConstInit:
 
     def __init__(self, scaling_const):
         self.scaling_const = scaling_const
 
@@ -56,23 +56,22 @@
         for param in self.tracked_parameter_list:
             two_dim_param = param.view(param.shape[0], -1)
             scaling_init += math.sqrt(2.0 / two_dim_param.shape[1])
         scaling_init /= len(self.tracked_parameter_list)
         return torch.tensor(scaling_init)
 
 
-
 class SolveParameterTensorClampImplFromEnum(ExtendedInjector):
 
     @value
     def tensor_clamp_impl(bit_width_impl_type, scaling_impl_type):
-        if (bit_width_impl_type == BitWidthImplType.PARAMETER
-                or scaling_impl_type == ScalingImplType.AFFINE_STATS
-                or scaling_impl_type == ScalingImplType.PARAMETER_FROM_STATS
-                or scaling_impl_type == ScalingImplType.PARAMETER):
+        if (bit_width_impl_type == BitWidthImplType.PARAMETER or
+                scaling_impl_type == ScalingImplType.AFFINE_STATS or
+                scaling_impl_type == ScalingImplType.PARAMETER_FROM_STATS or
+                scaling_impl_type == ScalingImplType.PARAMETER):
             return TensorClamp
         else:
             return TensorClampSte
 
 
 class SolveParameterScalingInitFromEnum(ExtendedInjector):
 
@@ -130,8 +129,8 @@
     @value
     def scaling_shape(scaling_per_output_channel):
         # this pattern of returning this.something allows to resolve scaling_output_channel_shape
         # only when scaling_per_output_channel is True
         if scaling_per_output_channel:
             return this.scaling_per_output_channel_shape
         else:
-            return SCALAR_SHAPE
+            return SCALAR_SHAPE
```

### Comparing `brevitas-0.8.0/src/brevitas/quant/solver/trunc.py` & `brevitas-0.9.0/src/brevitas/quant/solver/trunc.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from brevitas.proxy import TruncQuantProxyFromInjector
 from brevitas.core.quant import TruncIntQuant
-from brevitas.inject import ExtendedInjector, value
+from brevitas.inject import ExtendedInjector
+from brevitas.inject import value
 from brevitas.inject.enum import QuantType
+from brevitas.proxy import TruncQuantProxyFromInjector
 from brevitas.quant.solver.common import SolveBitWidthImplFromEnum
 from brevitas.quant.solver.common import SolveTensorQuantFloatToIntImplFromEnum
 
 
 class SolveTruncTensorQuantFromEnum(ExtendedInjector):
 
     @value
@@ -22,17 +22,16 @@
             raise RuntimeError(f'{quant_type} not supported for truncation.')
         elif quant_type == QuantType.BINARY:
             raise RuntimeError(f'{quant_type} not supported for truncation.')
         else:
             raise RuntimeError(f'{quant_type} not recognized.')
 
 
-class TruncQuantSolver(
-        SolveBitWidthImplFromEnum,
-        SolveTensorQuantFloatToIntImplFromEnum,
-        SolveTruncTensorQuantFromEnum):
+class TruncQuantSolver(SolveBitWidthImplFromEnum,
+                       SolveTensorQuantFloatToIntImplFromEnum,
+                       SolveTruncTensorQuantFromEnum):
     """
     Translate enum directives to truncation-specific quantization core modules.
     It should be placed last in the list of classes a quantizer inherits from,
     to make sure overrides are correctly captured.
     """
     proxy_class = TruncQuantProxyFromInjector
```

### Comparing `brevitas-0.8.0/src/brevitas/quant/solver/weight.py` & `brevitas-0.9.0/src/brevitas/quant/solver/weight.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from brevitas.core.quant import *
 from brevitas.core.quant import QuantType
+from brevitas.inject import ExtendedInjector
+from brevitas.inject import this
+from brevitas.inject import value
 from brevitas.proxy import WeightQuantProxyFromInjector
 from brevitas.quant.solver.common import *
 from brevitas.quant.solver.parameter import *
-from brevitas.inject import ExtendedInjector, value, this
-
 
 __all__ = [
     'SolveWeightTensorQuantFromEnum',
     'SolveWeightScalingStatsInputDimsFromModule',
     'SolveWeightScalingPerOutputChannelShapeFromModule',
-    'WeightQuantSolver'
-]
+    'WeightQuantSolver']
+
 
 class SolveWeightTensorQuantFromEnum(SolveIntQuantFromEnum):
 
     @value
     def tensor_quant(quant_type):
         if quant_type == QuantType.FP:
             return None
@@ -84,31 +84,27 @@
     def output_channel_dim(module):
         if isinstance(module, tuple):
             assert all(m.output_channel_dim == module[0].output_channel_dim for m in module)
             module = module[0]
         return module.output_channel_dim
 
 
-class WeightQuantSolver(
-        SolveWeightScalingStatsInputDimsFromModule,
-        SolveScalingStatsInputViewShapeImplFromEnum,
-        SolveStatsReduceDimFromEnum,
-        SolveScalingStatsOpFromEnum,
-        SolveBitWidthImplFromEnum,
-        SolveTensorQuantFloatToIntImplFromEnum,
-        SolveRestrictScalingImplFromEnum,
-        SolveIntScalingImplFromEnum,
-        SolveParameterScalingImplFromEnum,
-        SolveParameterTensorClampImplFromEnum,
-        SolveParameterScalingInitFromEnum,
-        SolveParameterScalingShape,
-        SolveWeightScalingPerOutputChannelShapeFromModule,
-        SolveWeightTensorQuantFromEnum):
+class WeightQuantSolver(SolveWeightScalingStatsInputDimsFromModule,
+                        SolveScalingStatsInputViewShapeImplFromEnum,
+                        SolveStatsReduceDimFromEnum,
+                        SolveScalingStatsOpFromEnum,
+                        SolveBitWidthImplFromEnum,
+                        SolveTensorQuantFloatToIntImplFromEnum,
+                        SolveRestrictScalingImplFromEnum,
+                        SolveIntScalingImplFromEnum,
+                        SolveParameterScalingImplFromEnum,
+                        SolveParameterTensorClampImplFromEnum,
+                        SolveParameterScalingInitFromEnum,
+                        SolveParameterScalingShape,
+                        SolveWeightScalingPerOutputChannelShapeFromModule,
+                        SolveWeightTensorQuantFromEnum):
     """
     Translate enum and shape directives to weight-specific quantization core modules.
     It should be placed last in the list of classes a quantizer inherits from,
     to make sure overrides are correctly captured.
     """
     proxy_class = WeightQuantProxyFromInjector
-
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas/quant/ternary.py` & `brevitas-0.9.0/src/brevitas/quant/ternary.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,39 +1,39 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from brevitas.quant.base import NarrowIntQuant, PerTensorConstScaling2bit
-from brevitas.core.function_wrapper import TensorClamp, InplaceTensorClampSte
-from brevitas.quant.solver import WeightQuantSolver, ActQuantSolver
-
+from brevitas.core.function_wrapper import InplaceTensorClampSte
+from brevitas.core.function_wrapper import TensorClamp
+from brevitas.quant.base import NarrowIntQuant
+from brevitas.quant.base import PerTensorConstScaling2bit
+from brevitas.quant.solver import ActQuantSolver
+from brevitas.quant.solver import WeightQuantSolver
 
 __all__ = [
     'SignedTernaryWeightPerTensorConst',
-    'SignedTernaryActPerTensorConst',
-]
+    'SignedTernaryActPerTensorConst',]
 
 
-class SignedTernaryWeightPerTensorConst(
-    NarrowIntQuant, PerTensorConstScaling2bit, WeightQuantSolver):
+class SignedTernaryWeightPerTensorConst(NarrowIntQuant,
+                                        PerTensorConstScaling2bit,
+                                        WeightQuantSolver):
     """
     Signed ternary weight quantizer with constant scale factor and inplace clipping to the scale.
 
     Examples:
         >>> from brevitas.nn import QuantLinear
         >>> fc = QuantLinear(10, 5, bias=False, weight_quant=SignedTernaryWeightPerTensorConst)
         >>> fc.quant_weight()
     """
     tensor_clamp_impl = InplaceTensorClampSte
     scaling_const = 0.1
 
 
-class SignedTernaryActPerTensorConst(
-    NarrowIntQuant, PerTensorConstScaling2bit, ActQuantSolver):
+class SignedTernaryActPerTensorConst(NarrowIntQuant, PerTensorConstScaling2bit, ActQuantSolver):
     """
     Examples:
         >>> from brevitas.nn import QuantIdentity
         >>> act = QuantIdentity(act_quant=SignedTernaryActPerTensorConst)
     """
     tensor_clamp_impl = TensorClamp
     min_val = -1.0
-    max_val = 1.0
+    max_val = 1.0
```

### Comparing `brevitas-0.8.0/src/brevitas/quant_tensor/__init__.py` & `brevitas-0.9.0/src/brevitas/quant_tensor/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,19 +1,21 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from abc import ABC
-from typing import Optional, NamedTuple
+from typing import NamedTuple, Optional
 
 import torch
 from torch import Tensor
 
-from brevitas.function.ops_ste import ceil_ste, round_ste
-from brevitas.function.ops import max_int, min_int
+from brevitas.function.ops import max_int
+from brevitas.function.ops import min_int
+from brevitas.function.ops_ste import ceil_ste
+from brevitas.function.ops_ste import round_ste
+
 from .torch_handler import QUANT_TENSOR_FN_HANDLER
 
 IS_VALID_ATOL = 1e-5
 
 
 class QuantTensorBase(NamedTuple):
     value: Tensor
@@ -78,51 +80,50 @@
             return self.training_t.item()
         else:
             return None
 
     def __torch_function__(self, func, types, args=(), kwargs=None):
         if kwargs is None:
             kwargs = {}
-        if (func not in QUANT_TENSOR_FN_HANDLER
-                or not all(issubclass(t, QuantTensor) for t in types)
-                or not (_is_all_nested_not_none(args) and _is_all_nested_not_none(kwargs))):
+        if (func not in QUANT_TENSOR_FN_HANDLER or
+                not all(issubclass(t, QuantTensor) for t in types) or
+                not (_is_all_nested_not_none(args) and _is_all_nested_not_none(kwargs))):
             args = _unpack_quant_tensor(args)
             kwargs = _unpack_quant_tensor(kwargs)
             return func(*args, **kwargs)
         return QUANT_TENSOR_FN_HANDLER[func](*args, **kwargs)
 
     @property
     def tensor(self):
         return self.value
 
     @property
     def is_not_none(self):
-        return (self.value is not None
-                and self.scale is not None
-                and self.zero_point is not None
-                and self.bit_width is not None
-                and self.signed is not None)
+        return (
+            self.value is not None and self.scale is not None and self.zero_point is not None and
+            self.bit_width is not None and self.signed is not None)
 
     @property
     def _pre_round_int_value(self):
         int_value = self.value / self.scale
         int_value = int_value + self.zero_point
         return int_value
 
     @property
     def is_valid(self):
         if self.is_not_none:
             with torch.no_grad():
                 pre_round_int_value = self._pre_round_int_value
                 rounded_int_value = torch.round(pre_round_int_value)
-                is_int = torch.isclose(pre_round_int_value, rounded_int_value, atol=IS_VALID_ATOL).all()
+                is_int = torch.isclose(
+                    pre_round_int_value, rounded_int_value, atol=IS_VALID_ATOL).all()
                 if self.bit_width >= 2:
                     if self.signed:
                         is_upper_b = (2.0 ** (self.bit_width - 1) - 1 >= rounded_int_value).all()
-                        is_lower_b = (- 2.0 ** (self.bit_width - 1) <= rounded_int_value).all()
+                        is_lower_b = (-2.0 ** (self.bit_width - 1) <= rounded_int_value).all()
                     else:
                         is_upper_b = (2.0 ** self.bit_width - 1 >= rounded_int_value).all()
                         is_lower_b = (0. <= rounded_int_value).all()
                     return (is_int & is_upper_b & is_lower_b).item()
                 else:  # binary case
                     unique_vals = rounded_int_value.unique(
                         sorted=False, return_counts=False, return_inverse=False)
@@ -270,19 +271,24 @@
             if all([isinstance(qt, QuantTensor) and qt.is_not_none for qt in tensors]):
                 for qt in tensors[1:]:
                     first_qt.check_scaling_factors_same(qt)
                     first_qt.check_zero_points_same(qt)
                     first_qt.check_bit_width_same(qt)
                     first_qt.check_sign_same(qt)
                 output_value = torch.cat([qt.value for qt in tensors], dim=dim)
-                output_scale = sum([qt.scale for qt in tensors]) / len(tensors)
-                output_zero_point = sum([qt.zero_point for qt in tensors]) / len(tensors)
-                output_bit_width = sum([qt.bit_width for qt in tensors]) / len(tensors)
-                output_signed = first_qt.signed  # they are the same
                 output_training = any([qt.training for qt in tensors])
+                if output_training:
+                    output_scale = sum([qt.scale for qt in tensors]) / len(tensors)
+                    output_zero_point = sum([qt.zero_point for qt in tensors]) / len(tensors)
+                    output_bit_width = sum([qt.bit_width for qt in tensors]) / len(tensors)
+                else:  # at eval time, they are the same
+                    output_scale = first_qt.scale
+                    output_zero_point = first_qt.zero_point
+                    output_bit_width = first_qt.bit_width
+                output_signed = first_qt.signed  # they are the same
                 return QuantTensor(
                     value=output_value,
                     scale=output_scale,
                     zero_point=output_zero_point,
                     bit_width=output_bit_width,
                     signed=output_signed,
                     training=output_training)
@@ -290,15 +296,15 @@
                 tensors = [qt.value if isinstance(qt, QuantTensor) else qt for qt in tensors]
                 output_value = torch.cat(tensors, dim=dim)
                 return QuantTensor(output_value)
 
     # Reference: https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types
 
     def __neg__(self):
-        neg_value = (- self.int(float_datatype=True) - self.zero_point) * self.scale
+        neg_value = (-self.int(float_datatype=True) - self.zero_point) * self.scale
         if self.signed:
             return QuantTensor(
                 value=neg_value,
                 scale=self.scale,
                 zero_point=self.zero_point,
                 bit_width=self.bit_width,
                 signed=self.signed,
@@ -308,14 +314,41 @@
                 value=neg_value,
                 scale=self.scale,
                 zero_point=self.zero_point,
                 bit_width=self.bit_width + 1,
                 signed=True,
                 training=self.training)
 
+    def to(self, *args, **kwargs):
+        return QuantTensor(
+            self.value.to(*args, **kwargs),
+            self.scale.to(*args, **kwargs) if self.scale is not None else None,
+            self.zero_point.to(*args, **kwargs) if self.zero_point is not None else None,
+            self.bit_width.to(*args, **kwargs) if self.bit_width is not None else None,
+            self.signed,
+            self.training)
+
+    def cuda(self, *args, **kwargs):
+        return QuantTensor(
+            self.value.cuda(*args, **kwargs),
+            self.scale.cuda(*args, **kwargs) if self.scale is not None else None,
+            self.zero_point.cuda(*args, **kwargs) if self.zero_point is not None else None,
+            self.bit_width.cuda(*args, **kwargs) if self.bit_width is not None else None,
+            self.signed,
+            self.training)
+
+    def cpu(self, *args, **kwargs):
+        return QuantTensor(
+            self.value.cpu(*args, **kwargs),
+            self.scale.cpu(*args, **kwargs) if self.scale is not None else None,
+            self.zero_point.cpu(*args, **kwargs) if self.zero_point is not None else None,
+            self.bit_width.cpu(*args, **kwargs) if self.bit_width is not None else None,
+            self.signed,
+            self.training)
+
     def __add__(self, other):
         if isinstance(other, QuantTensor) and self.is_not_none and other.is_not_none:
             self.check_scaling_factors_same(other)
             output_value = self.value + other.value
             output_scale = (self.scale + other.scale) / 2
             output_zero_point = self.zero_point + other.zero_point
             max_val = max_int(signed=self.signed, narrow_range=False, bit_width=self.bit_width)
@@ -365,15 +398,15 @@
         elif isinstance(other, QuantTensor):
             output = QuantTensor(self.value * other.value)
         else:
             output = QuantTensor(self.value * other)
         return output
 
     def __sub__(self, other):
-        return self.__add__(- other)
+        return self.__add__(-other)
 
     def __truediv__(self, other):
         if isinstance(other, QuantTensor) and self.is_not_none and other.is_not_none:
             output_tensor = self.value / other.tensor
             output_scale = self.scale / other.scale
             output_bit_width = self.bit_width - other.bit_width
             output_signed = self.signed or other.signed
@@ -405,8 +438,8 @@
                 bit_width=self.bit_width - 1,
                 signed=False,
                 training=self.training)
         else:
             return self
 
     def __pos__(self):
-        return self
+        return self
```

### Comparing `brevitas-0.8.0/src/brevitas/quant_tensor/torch_handler.py` & `brevitas-0.9.0/src/brevitas/quant_tensor/torch_handler.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import functools
 
-import brevitas
 import torch
 import torch.nn.functional as F
 
+import brevitas
 
 QUANT_TENSOR_FN_HANDLER = {}
 
 
 def implements(torch_function):
+
     @functools.wraps(torch_function)
     def decorator(func):
         QUANT_TENSOR_FN_HANDLER[torch_function] = func
         return func
+
     return decorator
 
 
 def quant_invariant_handler(fn, inp, *args, **kwargs):
     out_value = fn(inp.value, *args, **kwargs)
     if inp.is_not_none:
         return inp.set(value=out_value)
@@ -115,7 +116,41 @@
     return quant_invariant_handler(F.adaptive_max_pool2d, *args, **kwargs)
 
 
 @implements(F.adaptive_max_pool3d)
 def adaptive_max_pool3d_handler(*args, **kwargs):
     return quant_invariant_handler(F.adaptive_max_pool3d, *args, **kwargs)
 
+
+@implements(F.interpolate)
+def interpolate_handler(
+        inp,
+        size=None,
+        scale_factor=None,
+        mode='nearest',
+        align_corners=None,
+        recompute_scale_factor=None,
+        **kwargs):  # support newer kwargs added in recent pytorch versions
+    if mode == 'nearest' or mode == 'nearest_exact':
+        return quant_invariant_handler(
+            F.interpolate,
+            inp,
+            size=size,
+            scale_factor=scale_factor,
+            mode=mode,
+            align_corners=align_corners,
+            recompute_scale_factor=recompute_scale_factor,
+            **kwargs)
+    else:
+        return F.interpolate(
+            inp.value,
+            size=size,
+            scale_factor=scale_factor,
+            mode=mode,
+            align_corners=align_corners,
+            recompute_scale_factor=recompute_scale_factor,
+            **kwargs)
+
+
+@implements(F.pixel_shuffle)
+def pixel_shuffle_handler(*args, **kwargs):
+    return quant_invariant_handler(F.pixel_shuffle_handler, *args, **kwargs)
```

### Comparing `brevitas-0.8.0/src/brevitas/utils/jit_utils.py` & `brevitas-0.9.0/src/brevitas/utils/jit_utils.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,37 +1,38 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import inspect
 
 import torch
+
 try:
     from torch._jit_internal import get_torchscript_modifier
 except:
     get_torchscript_modifier = None
 
-
 from dependencies import Injector
+from packaging import version
+
+from brevitas import torch_version
 from brevitas.inject import ExtendedInjector
 from brevitas.jit import IS_ABOVE_110
-from .python_utils import patch
 
-from brevitas import torch_version
-from packaging import version
+from .python_utils import patch
 
 
 def _get_modifier_wrapper(fn):
     if inspect.isclass(fn) and issubclass(fn, (Injector, ExtendedInjector)):
         return None
     else:
         return get_torchscript_modifier(fn)
 
 
 if IS_ABOVE_110:
+
     def jit_patches_generator():
         return [patch(torch._jit_internal, 'get_torchscript_modifier', _get_modifier_wrapper)]
 else:
     jit_patches_generator = None
 
 
 def clear_class_registry():
```

### Comparing `brevitas-0.8.0/src/brevitas/utils/logging.py` & `brevitas-0.9.0/src/brevitas/utils/logging.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from typing import Dict
-from abc import ABCMeta, abstractmethod
+from abc import ABCMeta
+from abc import abstractmethod
 from functools import partial
+from typing import Dict
 
 import torch
 from torch import nn
 
 from brevitas.utils.quant_utils import *
 
 
@@ -29,27 +29,31 @@
 
     def __init__(self, model):
         super(LogWeightBitWidth, self).__init__(model=model)
         pass
 
     def register_hooks(self):
         for name, module in self.model.named_modules():
+
             def hook_fn(module, input, output, name):
                 (quant_weight, scale, bit_width) = output
                 self.bit_width_dict[name] = bit_width.detach().clone()
+
             if has_learned_weight_bit_width(module):
                 module.register_forward_hook(partial(hook_fn, name=name))
 
 
 class LogActivationBitWidth(LogBitWidth):
 
     def __init__(self, model):
         super(LogActivationBitWidth, self).__init__(model=model)
         pass
 
     def register_hooks(self):
         for name, module in self.model.named_modules():
+
             def hook_fn(module, input, output, name):
                 (quant_act, scale, bit_width) = output
                 self.bit_width_dict[name] = bit_width.detach().clone()
+
             if has_learned_activation_bit_width(module):
-                module.register_forward_hook(partial(hook_fn, name=name))
+                module.register_forward_hook(partial(hook_fn, name=name))
```

### Comparing `brevitas-0.8.0/src/brevitas/utils/quant_utils.py` & `brevitas-0.9.0/src/brevitas/utils/quant_utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-from brevitas.core.quant import RescalingIntQuant
 from brevitas.core.bit_width import BitWidthParameter
 from brevitas.core.function_wrapper import *
+from brevitas.core.quant import RescalingIntQuant
 from brevitas.inject.enum import FloatToIntImplType
 
 
 def has_learned_weight_bit_width(module):
     from brevitas.proxy.parameter_quant import WeightQuantProxyFromInjector
 
     if isinstance(module, WeightQuantProxyFromInjector) \
@@ -17,15 +16,16 @@
                            BitWidthParameter):
         return True
     else:
         return False
 
 
 def has_learned_activation_bit_width(module):
-    from brevitas.proxy.runtime_quant import ActQuantProxyFromInjector, FusedActivationQuantProxy
+    from brevitas.proxy.runtime_quant import ActQuantProxyFromInjector
+    from brevitas.proxy.runtime_quant import FusedActivationQuantProxy
 
     if isinstance(module, ActQuantProxyFromInjector) \
             and isinstance(module.fused_activation_quant_proxy, FusedActivationQuantProxy) \
             and isinstance(module.fused_activation_quant_proxy.tensor_quant, RescalingIntQuant) \
             and isinstance(module.fused_activation_quant_proxy.tensor_quant.msb_clamp_bit_width_impl,
                            BitWidthParameter):
         return True
@@ -41,8 +41,8 @@
     elif isinstance(module, FloorSte):
         return FloatToIntImplType.FLOOR
     elif isinstance(module, CeilSte):
         return FloatToIntImplType.CEIL
     elif isinstance(module, DPURoundSte):
         return DPURoundSte
     else:
-        return None
+        return None
```

### Comparing `brevitas-0.8.0/src/brevitas/utils/torch_utils.py` & `brevitas-0.9.0/src/brevitas/utils/torch_utils.py`

 * *Ordering differences only*

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
+import copy
 
 from torch.nn import Sequential
-import copy
 
 
 class TupleSequential(Sequential):
 
     def output(self, mod, input):
         if isinstance(input, tuple):
             return mod(*input)
```

### Comparing `brevitas-0.8.0/src/brevitas.egg-info/PKG-INFO` & `brevitas-0.9.0/src/brevitas.egg-info/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: brevitas
-Version: 0.8.0
+Version: 0.9.0
 Summary: Quantization-aware training in PyTorch
 Home-page: https://github.com/Xilinx/brevitas
 Author: Alessandro Pappalardo
 Author-email: alessand@xilinx.com
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown
 Provides-Extra: notebook
@@ -23,19 +23,19 @@
 
 [![Downloads](https://pepy.tech/badge/brevitas)](https://pepy.tech/project/brevitas)
 [![Gitter](https://badges.gitter.im/xilinx-brevitas/community.svg)](https://gitter.im/xilinx-brevitas/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)
 ![Pytest](https://github.com/Xilinx/brevitas/workflows/Pytest/badge.svg?branch=master)
 ![Examples Pytest](https://github.com/Xilinx/brevitas/workflows/Examples%20Pytest/badge.svg?branch=master)
 [![DOI](https://zenodo.org/badge/140494324.svg)](https://zenodo.org/badge/latestdoi/140494324)
 
-Brevitas is a PyTorch library for neural network quantization, with a focus on *quantization-aware training (QAT)*.
+Brevitas is a PyTorch library for neural network quantization, with support for both *post-training quantization (PTQ)* and *quantization-aware training (QAT)*.
 
 **Please note that Brevitas is a research project and not an official Xilinx product.**
 
-If you like this project please consider  this repo, as it is the simplest and best way to support it. 
+If you like this project please consider  this repo, as it is the simplest and best way to support it.
 
 If you have issues, comments, or are just looking for advices on training quantized neural networks, open an issue or a discussion.
 
 ## Cite as
 
 If you adopt Brevitas in your work, please cite it as:
 ```
@@ -48,21 +48,22 @@
   url          = {https://doi.org/10.5281/zenodo.3333552}
 }
 ```
 
 
 ## History
 
+- *2023/04/21* - Release version 0.9.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.9.0).
 - *2023/01/10* - Release version 0.8.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.8.0).
 - *2021/12/13* - Release version 0.7.1, fix a bunch of issues. Added TVMCon 2021 tutorial notebook.
 - *2021/11/03* - Re-release version 0.7.0 (build 1) on PyPI to fix a packaging issue.
 - *2021/10/29* - Release version 0.7.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.7.0).
 - *2021/06/04* - Release version 0.6.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.6.0).
 - *2021/05/24* - Release version 0.5.1, fix a bunch of minor issues. See [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.5.1).
-- *2021/05/06* - Release version 0.5.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.5.0). 
+- *2021/05/06* - Release version 0.5.0, see the [release notes](https://github.com/Xilinx/brevitas/releases/tag/v0.5.0).
 - *2021/03/15* - Release version 0.4.0, add support for \_\_torch_function\_\_ to QuantTensor.
 - *2021/03/04* - Release version 0.3.1, fix bug w/ act initialization from statistics w/ IGNORE_MISSING_KEYS=1.
 - *2021/03/01* - Release version 0.3.0, implements enum and shape solvers within extended dependency injectors. This allows declarative quantizers to be self-contained.
 - *2021/02/04* - Release version 0.2.1, includes various bugfixes of QuantTensor w/ zero-point.
 - *2021/01/30* - First release version 0.2.0 on PyPI.
 
 ## Requirements
@@ -78,13 +79,7 @@
 ```bash
 pip install brevitas
 ```
 
 ## Getting started
 
 Check out available info at https://xilinx.github.io/brevitas/getting_started .
-
-
-
-
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas.egg-info/SOURCES.txt` & `brevitas-0.9.0/src/brevitas.egg-info/SOURCES.txt`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,12 @@
+.pre-commit-config.yaml
 LICENSE
 MANIFEST.in
 README.md
+pyproject.toml
 setup.py
 notebooks/01_quant_tensor_quant_conv2d_overview.ipynb
 notebooks/02_quant_activation_overview.ipynb
 notebooks/03_anatomy_of_a_quantizer.ipynb
 notebooks/Brevitas_TVMCon2021.ipynb
 notebooks/ONNX_export_tutorial.ipynb
 notebooks/quantized_recurrent.ipynb
@@ -37,25 +39,27 @@
 src/brevitas/core/utils.py
 src/brevitas/core/zero_point.py
 src/brevitas/core/bit_width/__init__.py
 src/brevitas/core/bit_width/const.py
 src/brevitas/core/bit_width/parameter.py
 src/brevitas/core/function_wrapper/__init__.py
 src/brevitas/core/function_wrapper/clamp.py
+src/brevitas/core/function_wrapper/learned_round.py
 src/brevitas/core/function_wrapper/misc.py
 src/brevitas/core/function_wrapper/ops_ste.py
 src/brevitas/core/function_wrapper/shape.py
 src/brevitas/core/quant/__init__.py
 src/brevitas/core/quant/binary.py
 src/brevitas/core/quant/delay.py
 src/brevitas/core/quant/int.py
 src/brevitas/core/quant/int_base.py
 src/brevitas/core/quant/ternary.py
 src/brevitas/core/scaling/__init__.py
 src/brevitas/core/scaling/int_scaling.py
+src/brevitas/core/scaling/pre_scaling.py
 src/brevitas/core/scaling/runtime.py
 src/brevitas/core/scaling/standalone.py
 src/brevitas/core/stats/__init__.py
 src/brevitas/core/stats/stats_op.py
 src/brevitas/core/stats/stats_wrapper.py
 src/brevitas/core/stats/view_wrapper.py
 src/brevitas/csrc/autograd_ste_ops.cpp
@@ -133,14 +137,16 @@
 src/brevitas/fx/backport/torch_function/signatures.py
 src/brevitas/graph/__init__.py
 src/brevitas/graph/base.py
 src/brevitas/graph/calibrate.py
 src/brevitas/graph/equalize.py
 src/brevitas/graph/fixed_point.py
 src/brevitas/graph/per_input.py
+src/brevitas/graph/quantize.py
+src/brevitas/graph/quantize_impl.py
 src/brevitas/graph/standardize.py
 src/brevitas/graph/utils.py
 src/brevitas/graph/target/__init__.py
 src/brevitas/graph/target/flexml.py
 src/brevitas/inject/__init__.py
 src/brevitas/inject/defaults.py
 src/brevitas/inject/enum.py
@@ -157,14 +163,15 @@
 src/brevitas/nn/quant_convtranspose.py
 src/brevitas/nn/quant_dropout.py
 src/brevitas/nn/quant_eltwise.py
 src/brevitas/nn/quant_embedding.py
 src/brevitas/nn/quant_layer.py
 src/brevitas/nn/quant_linear.py
 src/brevitas/nn/quant_max_pool.py
+src/brevitas/nn/quant_mha.py
 src/brevitas/nn/quant_rnn.py
 src/brevitas/nn/quant_scale_bias.py
 src/brevitas/nn/quant_upsample.py
 src/brevitas/nn/utils.py
 src/brevitas/nn/mixin/__init__.py
 src/brevitas/nn/mixin/acc.py
 src/brevitas/nn/mixin/act.py
@@ -201,51 +208,70 @@
 src/brevitas/quant_tensor/torch_handler.py
 src/brevitas/utils/__init__.py
 src/brevitas/utils/jit_utils.py
 src/brevitas/utils/logging.py
 src/brevitas/utils/python_utils.py
 src/brevitas/utils/quant_utils.py
 src/brevitas/utils/torch_utils.py
+src/brevitas_examples/README.md
 src/brevitas_examples/__init__.py
 src/brevitas_examples/bnn_pynq/README.md
 src/brevitas_examples/bnn_pynq/__init__.py
 src/brevitas_examples/bnn_pynq/bnn_pynq_train.py
 src/brevitas_examples/bnn_pynq/logger.py
 src/brevitas_examples/bnn_pynq/trainer.py
 src/brevitas_examples/bnn_pynq/cfg/__init__.py
 src/brevitas_examples/bnn_pynq/cfg/cnv_1w1a.ini
 src/brevitas_examples/bnn_pynq/cfg/cnv_1w2a.ini
 src/brevitas_examples/bnn_pynq/cfg/cnv_2w2a.ini
 src/brevitas_examples/bnn_pynq/cfg/lfc_1w1a.ini
 src/brevitas_examples/bnn_pynq/cfg/lfc_1w2a.ini
+src/brevitas_examples/bnn_pynq/cfg/resnet18_3w3a.ini
+src/brevitas_examples/bnn_pynq/cfg/resnet18_4w4a.ini
 src/brevitas_examples/bnn_pynq/cfg/sfc_1w1a.ini
 src/brevitas_examples/bnn_pynq/cfg/sfc_1w2a.ini
 src/brevitas_examples/bnn_pynq/cfg/sfc_2w2a.ini
 src/brevitas_examples/bnn_pynq/cfg/tfc_1w1a.ini
 src/brevitas_examples/bnn_pynq/cfg/tfc_1w2a.ini
 src/brevitas_examples/bnn_pynq/cfg/tfc_2w2a.ini
 src/brevitas_examples/bnn_pynq/models/CNV.py
 src/brevitas_examples/bnn_pynq/models/FC.py
 src/brevitas_examples/bnn_pynq/models/__init__.py
 src/brevitas_examples/bnn_pynq/models/common.py
 src/brevitas_examples/bnn_pynq/models/losses.py
+src/brevitas_examples/bnn_pynq/models/resnet.py
 src/brevitas_examples/bnn_pynq/models/tensor_norm.py
 src/brevitas_examples/imagenet_classification/README.md
 src/brevitas_examples/imagenet_classification/__init__.py
-src/brevitas_examples/imagenet_classification/imagenet_val.py
-src/brevitas_examples/imagenet_classification/cfg/__init__.py
-src/brevitas_examples/imagenet_classification/cfg/quant_mobilenet_v1_4b.ini
-src/brevitas_examples/imagenet_classification/cfg/quant_proxylessnas_mobile14_4b.ini
-src/brevitas_examples/imagenet_classification/cfg/quant_proxylessnas_mobile14_4b5b.ini
-src/brevitas_examples/imagenet_classification/cfg/quant_proxylessnas_mobile14_hadamard_4b.ini
+src/brevitas_examples/imagenet_classification/utils.py
+src/brevitas_examples/imagenet_classification/models/README.md
 src/brevitas_examples/imagenet_classification/models/__init__.py
 src/brevitas_examples/imagenet_classification/models/common.py
 src/brevitas_examples/imagenet_classification/models/mobilenetv1.py
 src/brevitas_examples/imagenet_classification/models/proxylessnas.py
 src/brevitas_examples/imagenet_classification/models/vgg.py
+src/brevitas_examples/imagenet_classification/models/cfg/__init__.py
+src/brevitas_examples/imagenet_classification/models/cfg/quant_mobilenet_v1.ini
+src/brevitas_examples/imagenet_classification/models/cfg/quant_mobilenet_v1_4b.ini
+src/brevitas_examples/imagenet_classification/models/cfg/quant_mobilenet_v1_4b_round_avgpool.ini
+src/brevitas_examples/imagenet_classification/models/cfg/quant_proxylessnas_mobile14_4b.ini
+src/brevitas_examples/imagenet_classification/models/cfg/quant_proxylessnas_mobile14_4b5b.ini
+src/brevitas_examples/imagenet_classification/models/cfg/quant_proxylessnas_mobile14_4b_round_avgpool.ini
+src/brevitas_examples/imagenet_classification/models/cfg/quant_proxylessnas_mobile14_hadamard_4b.ini
+src/brevitas_examples/imagenet_classification/ptq/README.md
+src/brevitas_examples/imagenet_classification/ptq/RESULTS_IMGCLSMOB.csv
+src/brevitas_examples/imagenet_classification/ptq/RESULTS_TORCHVISION.csv
+src/brevitas_examples/imagenet_classification/ptq/RESULTS_TORCHVISION_BEST_CONFIGS.csv
+src/brevitas_examples/imagenet_classification/ptq/ptq_benchmark.py
+src/brevitas_examples/imagenet_classification/ptq/ptq_common.py
+src/brevitas_examples/imagenet_classification/ptq/ptq_evaluate.py
+src/brevitas_examples/imagenet_classification/ptq/utils.py
+src/brevitas_examples/imagenet_classification/qat/README.md
+src/brevitas_examples/imagenet_classification/qat/__init__.py
+src/brevitas_examples/imagenet_classification/qat/imagenet_val.py
 src/brevitas_examples/speech_to_text/README.md
 src/brevitas_examples/speech_to_text/__init__.py
 src/brevitas_examples/speech_to_text/get_librispeech_data.py
 src/brevitas_examples/speech_to_text/quartznet_val.py
 src/brevitas_examples/speech_to_text/cfg/__init__.py
 src/brevitas_examples/speech_to_text/cfg/quant_quartznet_perchannelscaling_4b.ini
 src/brevitas_examples/speech_to_text/cfg/quant_quartznet_perchannelscaling_8b.ini
```

### Comparing `brevitas-0.8.0/src/brevitas.egg-info/entry_points.txt` & `brevitas-0.9.0/src/brevitas.egg-info/entry_points.txt`

 * *Files 16% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 [console_scripts]
 brevitas_bnn_pynq_train = brevitas_examples.bnn_pynq.bnn_pynq_train:main
-brevitas_flexml_imagenet_calibration = brevitas_examples.imagenet_classification.flexml_imagenet_calibration:main
-brevitas_imagenet_val = brevitas_examples.imagenet_classification.imagenet_val:main
 brevitas_melgan_preprocess = brevitas_examples.text_to_speech.preprocess_dataset:main
 brevitas_melgan_val = brevitas_examples.text_to_speech.melgan_val:main
+brevitas_ptq_imagenet_benchmark = brevitas_examples.imagenet_classification.ptq.ptq_benchmark:main
+brevitas_ptq_imagenet_val = brevitas_examples.imagenet_classification.ptq.ptq_evaluate:main
+brevitas_qat_imagenet_val = brevitas_examples.imagenet_classification.qat.imagenet_val:main
 brevitas_quartznet_preprocess = brevitas_examples.speech_to_text.get_librispeech_data:main
 brevitas_quartznet_val = brevitas_examples.speech_to_text.quartznet_val:main
```

### Comparing `brevitas-0.8.0/src/brevitas.egg-info/requires.txt` & `brevitas-0.9.0/src/brevitas.egg-info/requires.txt`

 * *Files 21% similar despite different names*

```diff
@@ -1,74 +1,79 @@
-torch>=1.5.1
+dependencies==2.0.1
 packaging
+pre-commit
 setuptools
-dependencies==2.0.1
+torch>=1.5.1
 typing-extensions>=3.7.4
 
+[:python_version < "3.7"]
+dataclasses
+
 [docs]
-pydata-sphinx-theme
 m2r2
-sphinx==5.3.0
-sphinxemoji
-sphinxcontrib-napoleon
-sphinx-autodoc-typehints
 nbsphinx
 nbsphinx-link
+pydata-sphinx-theme
+sphinx==5.3.0
+sphinx-autodoc-typehints
 sphinx-gallery==0.10.1
+sphinxcontrib-napoleon
+sphinxemoji
 
 [export]
 onnx
 onnxoptimizer
 
 [finn_integration]
 bitstring
-toposort
-qonnx
-onnxruntime
 onnx
 onnxoptimizer
+onnxruntime
+qonnx
+toposort
 
 [hadamard]
 scipy
 
 [notebook]
 jupyter
 nbmake
 
 [ort_integration]
-onnxruntime
 onnx
 onnxoptimizer
+onnxruntime
 
 [stt]
+inflect
+librosa
+numba
 pillow>=4.3.0
-ruamel.yaml
 requests
-librosa
-inflect
-unidecode
-torch-stft
+ruamel.yaml
+soundfile
 sox
+torch-stft
 tqdm
-soundfile
-numba
+unidecode
 
 [test]
+hypothesis
+mock
 pytest
-pytest_cases
 pytest-mock
-mock
-hypothesis
+pytest-xdist
+pytest_cases
 scipy
 torchvision
 
 [tts]
-soundfile
 librosa
 numpy
-scipy
-tqdm
 pillow
 pyyaml
+scipy
+soundfile
+tqdm
 
 [vision]
 torchvision
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/bnn_pynq/README.md` & `brevitas-0.9.0/src/brevitas_examples/bnn_pynq/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 # BNN-PYNQ Brevitas experiments
 
 This repo contains training scripts and pretrained models to recreate the LFC and CNV models
 used in the [BNN-PYNQ](https://github.com/Xilinx/BNN-PYNQ) repo using [Brevitas](https://github.com/Xilinx/brevitas).
-These pretrained models and training scripts are courtesy of 
+These pretrained models and training scripts are courtesy of
 [Alessandro Pappalardo](https://github.com/volcacius) and [Ussama Zahid](https://github.com/ussamazahid96).
 
 ## Experiments
 
 | Name     | Input quantization           | Weight quantization | Activation quantization | Dataset       | Top1 accuracy |
 |----------|------------------------------|---------------------|-------------------------|---------------|---------------|
 | TFC_1W1A | 1 bit                        | 1 bit               | 1 bit                   |  MNIST        |    93.17%     |
@@ -16,20 +16,21 @@
 | SFC_1W2A | 2 bit                        | 1 bit               | 2 bit                   |  MNIST        |    98.31%     |
 | SFC_2W2A | 2 bit                        | 2 bit               | 2 bit                   |  MNIST        |    98.66%     |
 | LFC_1W1A | 1 bit                        | 1 bit               | 1 bit                   |  MNIST        |    98.88%     |
 | LFC_1W2A | 2 bit                        | 1 bit               | 2 bit                   |  MNIST        |    98.99%     |
 | CNV_1W1A | 8 bit                        | 1 bit               | 1 bit                   |  CIFAR10      |    84.22%     |
 | CNV_1W2A | 8 bit                        | 1 bit               | 2 bit                   |  CIFAR10      |    87.80%     |
 | CNV_2W2A | 8 bit                        | 2 bit               | 2 bit                   |  CIFAR10      |    89.03%     |
+| RESNET18_4W4A | 8 bit (assumed)         | 4 bit               | 4 bit                   |  CIFAR10      |    92.60%     |
 
 ## Train
 
 A few notes on training:
 - An experiments folder at */path/to/experiments* must exist before launching the training.
-- Set training to 1000 epochs for 1W1A networks, 500 otherwise. 
+- Set training to 1000 epochs for 1W1A networks, 500 otherwise with the `--epochs` flag.
 - Enabling the JIT with the env flag BREVITAS_JIT=1 significantly speeds up training.
 
 To start training a model from scratch, e.g. LFC_1W1A, run:
  ```bash
 BREVITAS_JIT=1 brevitas_bnn_pynq_train --network LFC_1W1A --experiments /path/to/experiments
  ```
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/bnn_pynq/bnn_pynq_train.py` & `brevitas-0.9.0/src/brevitas_examples/bnn_pynq/bnn_pynq_train.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import argparse
 import os
 import sys
 
 import torch
+
 from brevitas_examples.bnn_pynq.trainer import Trainer
 
 # Pytorch precision
 torch.set_printoptions(precision=10)
 
 
 # Util method to add mutually exclusive boolean
@@ -38,40 +38,46 @@
     parser = argparse.ArgumentParser(description="PyTorch MNIST/CIFAR10 Training")
     # I/O
     parser.add_argument("--datadir", default="./data/", help="Dataset location")
     parser.add_argument("--experiments", default="./experiments", help="Path to experiments folder")
     parser.add_argument("--dry_run", action="store_true", help="Disable output files generation")
     parser.add_argument("--log_freq", type=int, default=10)
     # Execution modes
-    parser.add_argument("--evaluate", dest="evaluate", action="store_true", help="evaluate model on validation set")
-    parser.add_argument("--resume", dest="resume", type=none_or_str,
-                        help="Resume from checkpoint. Overrides --pretrained flag.")
+    parser.add_argument(
+        "--evaluate", dest="evaluate", action="store_true", help="evaluate model on validation set")
+    parser.add_argument(
+        "--resume",
+        dest="resume",
+        type=none_or_str,
+        help="Resume from checkpoint. Overrides --pretrained flag.")
     add_bool_arg(parser, "detect_nan", default=False)
     # Compute resources
     parser.add_argument("--num_workers", default=4, type=int, help="Number of workers")
     parser.add_argument("--gpus", type=none_or_str, default="0", help="Comma separated GPUs")
     # Optimizer hyperparams
     parser.add_argument("--batch_size", default=100, type=int, help="batch size")
     parser.add_argument("--lr", default=0.02, type=float, help="Learning rate")
     parser.add_argument("--optim", type=none_or_str, default="ADAM", help="Optimizer to use")
     parser.add_argument("--loss", type=none_or_str, default="SqrHinge", help="Loss function to use")
     parser.add_argument("--scheduler", default="FIXED", type=none_or_str, help="LR Scheduler")
-    parser.add_argument("--milestones", type=none_or_str, default='100,150,200,250', help="Scheduler milestones")
+    parser.add_argument(
+        "--milestones", type=none_or_str, default='100,150,200,250', help="Scheduler milestones")
     parser.add_argument("--momentum", default=0.9, type=float, help="Momentum")
     parser.add_argument("--weight_decay", default=0, type=float, help="Weight decay")
     parser.add_argument("--epochs", default=1000, type=int, help="Number of epochs")
     parser.add_argument("--random_seed", default=1, type=int, help="Random seed")
     # Neural network Architecture
     parser.add_argument("--network", default="LFC_1W1A", type=str, help="neural network")
     parser.add_argument("--pretrained", action='store_true', help="Load pretrained model")
     parser.add_argument("--strict", action='store_true', help="Strict state dictionary loading")
     return parser.parse_args(args)
 
 
 class objdict(dict):
+
     def __getattr__(self, name):
         if name in self:
             return self[name]
         else:
             raise AttributeError("No such attribute: " + name)
 
     def __setattr__(self, name, value):
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/bnn_pynq/logger.py` & `brevitas-0.9.0/src/brevitas_examples/bnn_pynq/logger.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import logging
-import sys
 import os
+import sys
 
 
 class AverageMeter(object):
     """Computes and stores the average and current value"""
 
     def __init__(self):
         self.reset()
@@ -23,23 +22,25 @@
         self.val = val
         self.sum += val * n
         self.count += n
         self.avg = self.sum / self.count
 
 
 class TrainingEpochMeters(object):
+
     def __init__(self):
         self.batch_time = AverageMeter()
         self.data_time = AverageMeter()
         self.losses = AverageMeter()
         self.top1 = AverageMeter()
         self.top5 = AverageMeter()
 
 
 class EvalEpochMeters(object):
+
     def __init__(self):
         self.model_time = AverageMeter()
         self.loss_time = AverageMeter()
         self.losses = AverageMeter()
         self.top1 = AverageMeter()
         self.top5 = AverageMeter()
 
@@ -65,33 +66,38 @@
             self.log.addHandler(file_hdlr)
             self.log.propagate = False
 
     def info(self, arg):
         self.log.info(arg)
 
     def eval_batch_cli_log(self, epoch_meters, batch, tot_batches):
-        self.info('Test: [{0}/{1}]\t'
-                  'Model Time {model_time.val:.3f} ({model_time.avg:.3f})\t'
-                  'Loss Time {loss_time.val:.3f} ({loss_time.avg:.3f})\t'
-                  'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
-                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\t'
-                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\t'
-                  .format(batch, tot_batches,
-                          model_time=epoch_meters.model_time,
-                          loss_time=epoch_meters.loss_time,
-                          loss=epoch_meters.losses,
-                          top1=epoch_meters.top1,
-                          top5=epoch_meters.top5))
+        self.info(
+            'Test: [{0}/{1}]\t'
+            'Model Time {model_time.val:.3f} ({model_time.avg:.3f})\t'
+            'Loss Time {loss_time.val:.3f} ({loss_time.avg:.3f})\t'
+            'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
+            'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\t'
+            'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\t'.format(
+                batch,
+                tot_batches,
+                model_time=epoch_meters.model_time,
+                loss_time=epoch_meters.loss_time,
+                loss=epoch_meters.losses,
+                top1=epoch_meters.top1,
+                top5=epoch_meters.top5))
 
     def training_batch_cli_log(self, epoch_meters, epoch, batch, tot_batches):
-        self.info('Epoch: [{0}][{1}/{2}]\t'
-                         'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
-                         'Data {data_time.val:.3f} ({data_time.avg:.3f})\t'
-                         'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
-                         'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\t'
-                         'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\t'
-                         .format(epoch, batch, tot_batches,
-                                 batch_time=epoch_meters.batch_time,
-                                 data_time=epoch_meters.data_time,
-                                 loss=epoch_meters.losses,
-                                 top1=epoch_meters.top1,
-                                 top5=epoch_meters.top5))
+        self.info(
+            'Epoch: [{0}][{1}/{2}]\t'
+            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
+            'Data {data_time.val:.3f} ({data_time.avg:.3f})\t'
+            'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
+            'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\t'
+            'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\t'.format(
+                epoch,
+                batch,
+                tot_batches,
+                batch_time=epoch_meters.batch_time,
+                data_time=epoch_meters.data_time,
+                loss=epoch_meters.losses,
+                top1=epoch_meters.top1,
+                top5=epoch_meters.top5))
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/bnn_pynq/models/CNV.py` & `brevitas-0.9.0/src/brevitas_examples/bnn_pynq/models/CNV.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,25 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import torch
-from torch.nn import Module, ModuleList, BatchNorm2d, MaxPool2d, BatchNorm1d
+from torch.nn import BatchNorm1d
+from torch.nn import BatchNorm2d
+from torch.nn import MaxPool2d
+from torch.nn import Module
+from torch.nn import ModuleList
 
-from brevitas.nn import QuantConv2d, QuantIdentity, QuantLinear
 from brevitas.core.restrict_val import RestrictValueType
-from .tensor_norm import TensorNorm
-from .common import CommonWeightQuant, CommonActQuant
+from brevitas.nn import QuantConv2d
+from brevitas.nn import QuantIdentity
+from brevitas.nn import QuantLinear
 
+from .common import CommonActQuant
+from .common import CommonWeightQuant
+from .tensor_norm import TensorNorm
 
 CNV_OUT_CH_POOL = [(64, False), (64, True), (128, False), (128, True), (256, False), (256, False)]
 INTERMEDIATE_FC_FEATURES = [(256, 512), (512, 512)]
 LAST_FC_IN_FEATURES = 512
 LAST_FC_PER_OUT_CH_SCALING = False
 POOL_SIZE = 2
 KERNEL_SIZE = 3
@@ -32,53 +38,53 @@
             bit_width=in_bit_width,
             min_val=- 1.0,
             max_val=1.0 - 2.0 ** (-7),
             narrow_range=False,
             restrict_scaling_type=RestrictValueType.POWER_OF_TWO))
 
         for out_ch, is_pool_enabled in CNV_OUT_CH_POOL:
-            self.conv_features.append(QuantConv2d(
-                kernel_size=KERNEL_SIZE,
-                in_channels=in_ch,
-                out_channels=out_ch,
-                bias=False,
-                weight_quant=CommonWeightQuant,
-                weight_bit_width=weight_bit_width))
+            self.conv_features.append(
+                QuantConv2d(
+                    kernel_size=KERNEL_SIZE,
+                    in_channels=in_ch,
+                    out_channels=out_ch,
+                    bias=False,
+                    weight_quant=CommonWeightQuant,
+                    weight_bit_width=weight_bit_width))
             in_ch = out_ch
             self.conv_features.append(BatchNorm2d(in_ch, eps=1e-4))
-            self.conv_features.append(QuantIdentity(
-                act_quant=CommonActQuant,
-                bit_width=act_bit_width))
+            self.conv_features.append(
+                QuantIdentity(act_quant=CommonActQuant, bit_width=act_bit_width))
             if is_pool_enabled:
                 self.conv_features.append(MaxPool2d(kernel_size=2))
 
         for in_features, out_features in INTERMEDIATE_FC_FEATURES:
-            self.linear_features.append(QuantLinear(
-                in_features=in_features,
-                out_features=out_features,
+            self.linear_features.append(
+                QuantLinear(
+                    in_features=in_features,
+                    out_features=out_features,
+                    bias=False,
+                    weight_quant=CommonWeightQuant,
+                    weight_bit_width=weight_bit_width))
+            self.linear_features.append(BatchNorm1d(out_features, eps=1e-4))
+            self.linear_features.append(
+                QuantIdentity(act_quant=CommonActQuant, bit_width=act_bit_width))
+
+        self.linear_features.append(
+            QuantLinear(
+                in_features=LAST_FC_IN_FEATURES,
+                out_features=num_classes,
                 bias=False,
                 weight_quant=CommonWeightQuant,
                 weight_bit_width=weight_bit_width))
-            self.linear_features.append(BatchNorm1d(out_features, eps=1e-4))
-            self.linear_features.append(QuantIdentity(
-                act_quant=CommonActQuant,
-                bit_width=act_bit_width))
-
-        self.linear_features.append(QuantLinear(
-            in_features=LAST_FC_IN_FEATURES,
-            out_features=num_classes,
-            bias=False,
-            weight_quant=CommonWeightQuant,
-            weight_bit_width=weight_bit_width))
         self.linear_features.append(TensorNorm())
-        
-        for m in self.modules():
-          if isinstance(m, QuantConv2d) or isinstance(m, QuantLinear):
-            torch.nn.init.uniform_(m.weight.data, -1, 1)
 
+        for m in self.modules():
+            if isinstance(m, QuantConv2d) or isinstance(m, QuantLinear):
+                torch.nn.init.uniform_(m.weight.data, -1, 1)
 
     def clip_weights(self, min_val, max_val):
         for mod in self.conv_features:
             if isinstance(mod, QuantConv2d):
                 mod.weight.data.clamp_(min_val, max_val)
         for mod in self.linear_features:
             if isinstance(mod, QuantLinear):
@@ -96,14 +102,14 @@
 
 def cnv(cfg):
     weight_bit_width = cfg.getint('QUANT', 'WEIGHT_BIT_WIDTH')
     act_bit_width = cfg.getint('QUANT', 'ACT_BIT_WIDTH')
     in_bit_width = cfg.getint('QUANT', 'IN_BIT_WIDTH')
     num_classes = cfg.getint('MODEL', 'NUM_CLASSES')
     in_channels = cfg.getint('MODEL', 'IN_CHANNELS')
-    net = CNV(weight_bit_width=weight_bit_width,
-              act_bit_width=act_bit_width,
-              in_bit_width=in_bit_width,
-              num_classes=num_classes,
-              in_ch=in_channels)
+    net = CNV(
+        weight_bit_width=weight_bit_width,
+        act_bit_width=act_bit_width,
+        in_bit_width=in_bit_width,
+        num_classes=num_classes,
+        in_ch=in_channels)
     return net
-
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/bnn_pynq/models/FC.py` & `brevitas-0.9.0/src/brevitas_examples/bnn_pynq/models/FC.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,70 +1,77 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import ast
 from functools import reduce
 from operator import mul
 
-from torch.nn import Module, ModuleList, BatchNorm1d, Dropout
 import torch
+from torch.nn import BatchNorm1d
+from torch.nn import Dropout
+from torch.nn import Module
+from torch.nn import ModuleList
+
+from brevitas.nn import QuantIdentity
+from brevitas.nn import QuantLinear
 
-from brevitas.nn import QuantIdentity, QuantLinear
-from .common import CommonWeightQuant, CommonActQuant
+from .common import CommonActQuant
+from .common import CommonWeightQuant
 from .tensor_norm import TensorNorm
 
 DROPOUT = 0.2
 
 
 class FC(Module):
 
     def __init__(
-            self,
-            num_classes,
-            weight_bit_width,
-            act_bit_width,
-            in_bit_width,
-            in_channels,
-            out_features,
-            in_features=(28, 28)):
+        self,
+        num_classes,
+        weight_bit_width,
+        act_bit_width,
+        in_bit_width,
+        in_channels,
+        out_features,
+        in_features=(28, 28)):
         super(FC, self).__init__()
 
         self.features = ModuleList()
         self.features.append(QuantIdentity(act_quant=CommonActQuant, bit_width=in_bit_width))
         self.features.append(Dropout(p=DROPOUT))
         in_features = reduce(mul, in_features)
         for out_features in out_features:
-            self.features.append(QuantLinear(
-                in_features=in_features,
-                out_features=out_features,
-                bias=False,
-                weight_bit_width=weight_bit_width,
-                weight_quant=CommonWeightQuant))
+            self.features.append(
+                QuantLinear(
+                    in_features=in_features,
+                    out_features=out_features,
+                    bias=False,
+                    weight_bit_width=weight_bit_width,
+                    weight_quant=CommonWeightQuant))
             in_features = out_features
             self.features.append(BatchNorm1d(num_features=in_features))
             self.features.append(QuantIdentity(act_quant=CommonActQuant, bit_width=act_bit_width))
             self.features.append(Dropout(p=DROPOUT))
-        self.features.append(QuantLinear(
+        self.features.append(
+            QuantLinear(
                 in_features=in_features,
                 out_features=num_classes,
                 bias=False,
                 weight_bit_width=weight_bit_width,
                 weight_quant=CommonWeightQuant))
         self.features.append(TensorNorm())
 
         for m in self.modules():
-          if isinstance(m, QuantLinear):
-            torch.nn.init.uniform_(m.weight.data, -1, 1)
+            if isinstance(m, QuantLinear):
+                torch.nn.init.uniform_(m.weight.data, -1, 1)
 
     def clip_weights(self, min_val, max_val):
         for mod in self.features:
             if isinstance(mod, QuantLinear):
                 mod.weight.data.clamp_(min_val, max_val)
-    
+
     def forward(self, x):
         x = x.view(x.shape[0], -1)
         x = 2.0 * x - torch.tensor([1.0], device=x.device)
         for mod in self.features:
             x = mod(x)
         return x
 
@@ -79,8 +86,8 @@
     net = FC(
         weight_bit_width=weight_bit_width,
         act_bit_width=act_bit_width,
         in_bit_width=in_bit_width,
         in_channels=in_channels,
         out_features=out_features,
         num_classes=num_classes)
-    return net
+    return net
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/bnn_pynq/models/__init__.py` & `brevitas-0.9.0/src/brevitas_examples/bnn_pynq/models/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,39 +1,47 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-import os
 from configparser import ConfigParser
+import os
 
 import torch
 from torch import hub
 
-__all__ = ['cnv_1w1a', 'cnv_1w2a', 'cnv_2w2a',
-           'sfc_1w1a', 'sfc_1w2a', 'sfc_2w2a',
-           'tfc_1w1a', 'tfc_1w2a', 'tfc_2w2a',
-           'lfc_1w1a', 'lfc_1w2a',
-           'model_with_cfg']
+__all__ = [
+    'cnv_1w1a',
+    'cnv_1w2a',
+    'cnv_2w2a',
+    'sfc_1w1a',
+    'sfc_1w2a',
+    'sfc_2w2a',
+    'tfc_1w1a',
+    'tfc_1w2a',
+    'tfc_2w2a',
+    'lfc_1w1a',
+    'lfc_1w2a',
+    'resnet18_4w4a',
+    'model_with_cfg']
 
 from .CNV import cnv
 from .FC import fc
+from .resnet import quant_resnet18
+
+model_impl = {'CNV': cnv, 'FC': fc, 'RESNET18': quant_resnet18}
 
-model_impl = {
-    'CNV': cnv,
-    'FC': fc,
-}
 
 def get_model_cfg(name):
     cfg = ConfigParser()
     current_dir = os.path.dirname(os.path.abspath(__file__))
     config_path = os.path.join(current_dir, '..', 'cfg', name.lower() + '.ini')
-    assert os.path.exists(config_path)
+    assert os.path.exists(config_path), f"{config_path} not found."
     cfg.read(config_path)
     return cfg
 
+
 def model_with_cfg(name, pretrained):
     cfg = get_model_cfg(name)
     arch = cfg.get('MODEL', 'ARCH')
     model = model_impl[arch](cfg)
     if pretrained:
         checkpoint = cfg.get('MODEL', 'PRETRAINED_URL')
         state_dict = hub.load_state_dict_from_url(checkpoint, progress=True, map_location='cpu')
@@ -92,7 +100,15 @@
 
 
 def lfc_1w2a(pretrained=True):
     model, _ = model_with_cfg('lfc_1w2a', pretrained)
     return model
 
 
+def resnet18_4w4a(pretrained=False):
+    model, _ = model_with_cfg('resnet18_4w4a', pretrained)
+    return model
+
+
+def resnet18_3w3a(pretrained=False):
+    model, _ = model_with_cfg('resnet18_3w3a', pretrained)
+    return model
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/bnn_pynq/models/common.py` & `brevitas-0.9.0/src/brevitas_examples/bnn_pynq/models/common.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 from dependencies import value
 
-from brevitas.inject import ExtendedInjector
-from brevitas.quant.solver import WeightQuantSolver, ActQuantSolver
 from brevitas.core.bit_width import BitWidthImplType
 from brevitas.core.quant import QuantType
-from brevitas.core.restrict_val import RestrictValueType, FloatToIntImplType
+from brevitas.core.restrict_val import FloatToIntImplType
+from brevitas.core.restrict_val import RestrictValueType
 from brevitas.core.scaling import ScalingImplType
 from brevitas.core.zero_point import ZeroZeroPoint
+from brevitas.inject import ExtendedInjector
+from brevitas.quant.solver import ActQuantSolver
+from brevitas.quant.solver import WeightQuantSolver
 
 
 class CommonQuant(ExtendedInjector):
     bit_width_impl_type = BitWidthImplType.CONST
     scaling_impl_type = ScalingImplType.CONST
     restrict_scaling_type = RestrictValueType.FP
     zero_point_impl = ZeroZeroPoint
@@ -35,8 +36,8 @@
 
 class CommonWeightQuant(CommonQuant, WeightQuantSolver):
     scaling_const = 1.0
 
 
 class CommonActQuant(CommonQuant, ActQuantSolver):
     min_val = -1.0
-    max_val = 1.0
+    max_val = 1.0
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/bnn_pynq/models/losses.py` & `brevitas-0.9.0/src/brevitas_examples/bnn_pynq/models/losses.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,34 +1,36 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import torch
-import torch.nn as nn
 from torch.autograd import Function
+import torch.nn as nn
+
 
 class squared_hinge_loss(Function):
+
     @staticmethod
     def forward(ctx, predictions, targets):
-        ctx.save_for_backward(predictions, targets) 
-        output = 1.-predictions.mul(targets)
+        ctx.save_for_backward(predictions, targets)
+        output = 1. - predictions.mul(targets)
         output[output.le(0.)] = 0.
         loss = torch.mean(output.mul(output))
-        return loss 
+        return loss
 
     @staticmethod
     def backward(ctx, grad_output):
-       predictions, targets = ctx.saved_tensors
-       output=1.-predictions.mul(targets)
-       output[output.le(0.)]=0.
-       grad_output.resize_as_(predictions).copy_(targets).mul_(-2.).mul_(output)
-       grad_output.mul_(output.ne(0).float())
-       grad_output.div_(predictions.numel())
-       return grad_output, None    
+        predictions, targets = ctx.saved_tensors
+        output = 1. - predictions.mul(targets)
+        output[output.le(0.)] = 0.
+        grad_output.resize_as_(predictions).copy_(targets).mul_(-2.).mul_(output)
+        grad_output.mul_(output.ne(0).float())
+        grad_output.div_(predictions.numel())
+        return grad_output, None
+
 
 class SqrHingeLoss(nn.Module):
     # Squared Hinge Loss
     def __init__(self):
         super(SqrHingeLoss, self).__init__()
-    
+
     def forward(self, input, target):
-        return squared_hinge_loss.apply(input, target)
+        return squared_hinge_loss.apply(input, target)
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/bnn_pynq/models/tensor_norm.py` & `brevitas-0.9.0/src/brevitas_examples/bnn_pynq/models/tensor_norm.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import torch
 import torch.nn as nn
 import torch.nn.init as init
 
 
 class TensorNorm(nn.Module):
+
     def __init__(self, eps=1e-4, momentum=0.1):
         super().__init__()
 
         self.eps = eps
         self.momentum = momentum
         self.weight = nn.Parameter(torch.rand(1))
         self.bias = nn.Parameter(torch.rand(1))
@@ -26,13 +26,16 @@
         init.zeros_(self.bias)
 
     def forward(self, x):
         if self.training:
             mean = x.mean()
             unbias_var = x.var(unbiased=True)
             biased_var = x.var(unbiased=False)
-            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.detach()
-            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.detach()
+            self.running_mean = (1 -
+                                 self.momentum) * self.running_mean + self.momentum * mean.detach()
+            self.running_var = (
+                1 - self.momentum) * self.running_var + self.momentum * unbias_var.detach()
             inv_std = 1 / (biased_var + self.eps).pow(0.5)
             return (x - mean) * inv_std * self.weight + self.bias
         else:
-            return ((x - self.running_mean) / (self.running_var + self.eps).pow(0.5)) * self.weight + self.bias
+            return ((x - self.running_mean) /
+                    (self.running_var + self.eps).pow(0.5)) * self.weight + self.bias
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/bnn_pynq/trainer.py` & `brevitas-0.9.0/src/brevitas_examples/bnn_pynq/trainer.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,45 +1,49 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-import random
+from datetime import datetime
 import os
+import random
 import time
-from datetime import datetime
-from packaging.version import parse
 
+from packaging.version import parse
 import torch
-import torchvision
-import torch.optim as optim
 from torch import nn
+import torch.optim as optim
 from torch.optim.lr_scheduler import MultiStepLR
 from torch.utils.data import DataLoader
+import torchvision
 from torchvision import transforms
-from torchvision.datasets import MNIST, CIFAR10
+from torchvision.datasets import CIFAR10
+from torchvision.datasets import MNIST
 
-from .logger import Logger, TrainingEpochMeters, EvalEpochMeters
+from .logger import EvalEpochMeters
+from .logger import Logger
+from .logger import TrainingEpochMeters
 from .models import model_with_cfg
 from .models.losses import SqrHingeLoss
 
 
 class MirrorMNIST(MNIST):
 
     if parse(torchvision.__version__) < parse('0.9.1'):
 
-        resources = [
-            ("https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz",
+        resources = [(
+            "https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz",
             "f68b3c2dcbeaaa9fbdd348bbdeb94873"),
-            ("https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz",
-            "d53e105ee54ea40749a09fcbcd1e9432"),
-            ("https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz",
-            "9fb629c4189551a2d022fa330f9573f3"),
-            ("https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz",
-            "ec29112dd5afa0611ce80d1b7f02629c")
-        ]
+                     (
+                         "https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz",
+                         "d53e105ee54ea40749a09fcbcd1e9432"),
+                     (
+                         "https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz",
+                         "9fb629c4189551a2d022fa330f9573f3"),
+                     (
+                         "https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz",
+                         "ec29112dd5afa0611ce80d1b7f02629c")]
 
 
 def accuracy(output, target, topk=(1,)):
     """Computes the precision@k for the specified values of k"""
     maxk = max(topk)
     batch_size = target.size(0)
 
@@ -51,24 +55,25 @@
     for k in topk:
         correct_k = correct[:k].flatten().float().sum(0)
         res.append(correct_k.mul_(100.0 / batch_size))
     return res
 
 
 class Trainer(object):
+
     def __init__(self, args):
 
         model, cfg = model_with_cfg(args.network, args.pretrained)
 
         # Init arguments
         self.args = args
-        prec_name = "_{}W{}A".format(cfg.getint('QUANT', 'WEIGHT_BIT_WIDTH'),
-                                     cfg.getint('QUANT', 'ACT_BIT_WIDTH'))
-        experiment_name = '{}{}_{}'.format(args.network, prec_name,
-                                           datetime.now().strftime('%Y%m%d_%H%M%S'))
+        prec_name = "_{}W{}A".format(
+            cfg.getint('QUANT', 'WEIGHT_BIT_WIDTH'), cfg.getint('QUANT', 'ACT_BIT_WIDTH'))
+        experiment_name = '{}{}_{}'.format(
+            args.network, prec_name, datetime.now().strftime('%Y%m%d_%H%M%S'))
         self.output_dir_path = os.path.join(args.experiments, experiment_name)
 
         if self.args.resume:
             self.output_dir_path, _ = os.path.split(args.resume)
             self.output_dir_path, _ = os.path.split(self.output_dir_path)
 
         if not args.dry_run:
@@ -85,42 +90,34 @@
 
         # Datasets
         transform_to_tensor = transforms.Compose([transforms.ToTensor()])
 
         dataset = cfg.get('MODEL', 'DATASET')
         self.num_classes = cfg.getint('MODEL', 'NUM_CLASSES')
         if dataset == 'CIFAR10':
-            train_transforms_list = [transforms.RandomCrop(32, padding=4),
-                                     transforms.RandomHorizontalFlip(),
-                                     transforms.ToTensor()]
+            train_transforms_list = [
+                transforms.RandomCrop(32, padding=4),
+                transforms.RandomHorizontalFlip(),
+                transforms.ToTensor()]
             transform_train = transforms.Compose(train_transforms_list)
             builder = CIFAR10
 
         elif dataset == 'MNIST':
             transform_train = transform_to_tensor
             builder = MirrorMNIST
         else:
             raise Exception("Dataset not supported: {}".format(args.dataset))
 
-        train_set = builder(root=args.datadir,
-                            train=True,
-                            download=True,
-                            transform=transform_train)
-        test_set = builder(root=args.datadir,
-                           train=False,
-                           download=True,
-                           transform=transform_to_tensor)
-        self.train_loader = DataLoader(train_set,
-                                       batch_size=args.batch_size,
-                                       shuffle=True,
-                                       num_workers=args.num_workers)
-        self.test_loader = DataLoader(test_set,
-                                      batch_size=args.batch_size,
-                                      shuffle=False,
-                                      num_workers=args.num_workers)
+        train_set = builder(root=args.datadir, train=True, download=True, transform=transform_train)
+        test_set = builder(
+            root=args.datadir, train=False, download=True, transform=transform_to_tensor)
+        self.train_loader = DataLoader(
+            train_set, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)
+        self.test_loader = DataLoader(
+            test_set, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)
 
         # Init starting values
         self.starting_epoch = 1
         self.best_val_acc = 0
 
         # Setup device
         if args.gpus is not None:
@@ -143,45 +140,45 @@
         if args.gpus is not None and len(args.gpus) > 1:
             model = nn.DataParallel(model, args.gpus)
         self.model = model
 
         # Loss function
         if args.loss == 'SqrHinge':
             self.criterion = SqrHingeLoss()
-        else:
+        elif args.loss == 'CrossEntropy':
             self.criterion = nn.CrossEntropyLoss()
+        else:
+            raise ValueError(f"{args.loss} not supported.")
         self.criterion = self.criterion.to(device=self.device)
 
         # Init optimizer
         if args.optim == 'ADAM':
-            self.optimizer = optim.Adam(self.model.parameters(),
-                                        lr=args.lr,
-                                        weight_decay=args.weight_decay)
+            self.optimizer = optim.Adam(
+                self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
         elif args.optim == 'SGD':
-            self.optimizer = optim.SGD(self.model.parameters(),
-                                       lr=self.args.lr,
-                                       momentum=self.args.momentum,
-                                       weight_decay=self.args.weight_decay)
+            self.optimizer = optim.SGD(
+                self.model.parameters(),
+                lr=self.args.lr,
+                momentum=self.args.momentum,
+                weight_decay=self.args.weight_decay)
 
         # Resume optimizer, if any
         if args.resume and not args.evaluate:
             self.logger.log.info("Loading optimizer checkpoint")
             if 'optim_dict' in package.keys():
                 self.optimizer.load_state_dict(package['optim_dict'])
             if 'epoch' in package.keys():
                 self.starting_epoch = package['epoch']
             if 'best_val_acc' in package.keys():
                 self.best_val_acc = package['best_val_acc']
 
         # LR scheduler
         if args.scheduler == 'STEP':
             milestones = [int(i) for i in args.milestones.split(',')]
-            self.scheduler = MultiStepLR(optimizer=self.optimizer,
-                                         milestones=milestones,
-                                         gamma=0.1)
+            self.scheduler = MultiStepLR(optimizer=self.optimizer, milestones=milestones, gamma=0.1)
         elif args.scheduler == 'FIXED':
             self.scheduler = None
         else:
             raise Exception("Unrecognized scheduler {}".format(self.args.scheduler))
 
         # Resume scheduler, if any
         if args.resume and not args.evaluate and self.scheduler is not None:
@@ -190,16 +187,16 @@
     def checkpoint_best(self, epoch, name):
         best_path = os.path.join(self.checkpoints_dir_path, name)
         self.logger.info("Saving checkpoint model to {}".format(best_path))
         torch.save({
             'state_dict': self.model.state_dict(),
             'optim_dict': self.optimizer.state_dict(),
             'epoch': epoch + 1,
-            'best_val_acc': self.best_val_acc,
-        }, best_path)
+            'best_val_acc': self.best_val_acc,},
+                   best_path)
 
     def train_model(self):
 
         # training starts
         if self.args.detect_nan:
             torch.autograd.set_detect_anomaly(True)
 
@@ -217,16 +214,16 @@
                 (input, target) = data
                 input = input.to(self.device, non_blocking=True)
                 target = target.to(self.device, non_blocking=True)
 
                 # for hingeloss only
                 if isinstance(self.criterion, SqrHingeLoss):
                     target = target.unsqueeze(1)
-                    target_onehot = torch.Tensor(target.size(0), self.num_classes).to(self.device,
-                                                                                      non_blocking=True)
+                    target_onehot = torch.Tensor(target.size(0), self.num_classes).to(
+                        self.device, non_blocking=True)
                     target_onehot.fill_(-1)
                     target_onehot.scatter_(1, target, 1)
                     target = target.squeeze()
                     target_var = target_onehot
                 else:
                     target_var = target
 
@@ -239,26 +236,27 @@
                 loss = self.criterion(output, target_var)
 
                 # compute gradient and do SGD step
                 self.optimizer.zero_grad()
                 loss.backward()
                 self.optimizer.step()
 
-                self.model.clip_weights(-1, 1)
+                if hasattr(self.model, 'clip_weights'):
+                    self.model.clip_weights(-1, 1)
 
                 # measure elapsed time
                 epoch_meters.batch_time.update(time.time() - start_batch)
 
                 if i % int(self.args.log_freq) == 0 or i == len(self.train_loader) - 1:
                     prec1, prec5 = accuracy(output.detach(), target, topk=(1, 5))
                     epoch_meters.losses.update(loss.item(), input.size(0))
                     epoch_meters.top1.update(prec1.item(), input.size(0))
                     epoch_meters.top5.update(prec5.item(), input.size(0))
-                    self.logger.training_batch_cli_log(epoch_meters, epoch, i,
-                                                       len(self.train_loader))
+                    self.logger.training_batch_cli_log(
+                        epoch_meters, epoch, i, len(self.train_loader))
 
                 # training batch ends
                 start_data_loading = time.time()
 
             # Set the learning rate
             if self.scheduler is not None:
                 self.scheduler.step(epoch)
@@ -296,16 +294,16 @@
 
             input = input.to(self.device, non_blocking=True)
             target = target.to(self.device, non_blocking=True)
 
             # for hingeloss only
             if isinstance(self.criterion, SqrHingeLoss):
                 target = target.unsqueeze(1)
-                target_onehot = torch.Tensor(target.size(0), self.num_classes).to(self.device,
-                                                                                  non_blocking=True)
+                target_onehot = torch.Tensor(target.size(0), self.num_classes).to(
+                    self.device, non_blocking=True)
                 target_onehot.fill_(-1)
                 target_onehot.scatter_(1, target, 1)
                 target = target.squeeze()
                 target_var = target_onehot
             else:
                 target_var = target
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/imagenet_classification/cfg/quant_proxylessnas_mobile14_hadamard_4b.ini` & `brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/cfg/quant_proxylessnas_mobile14_hadamard_4b.ini`

 * *Files 22% similar despite different names*

```diff
@@ -3,18 +3,19 @@
 PRETRAINED_URL: https://github.com/Xilinx/brevitas/releases/download/quant_proxylessnas_mobile14_hadamard_4b-r0/quant_proxylessnas_mobile14_hadamard_4b-4acbfa9f.pth
 HADAMARD_CLASSIFIER = TRUE
 
 [QUANT]
 FIRST_LAYER_WEIGHT_BIT_WIDTH: 8
 BIT_WIDTH: 4
 DEPTHWISE_BIT_WIDTH: 4
+ROUND_AVG_POOL = FALSE
 
 [PREPROCESS]
 # 128 / 255 = 0.5019607843137255
 
 MEAN_0: 0.5019607843137255
 MEAN_1: 0.5019607843137255
 MEAN_2: 0.5019607843137255
 
 STD_0: 0.5019607843137255
 STD_1: 0.5019607843137255
-STD_2: 0.5019607843137255
+STD_2: 0.5019607843137255
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/imagenet_classification/models/mobilenetv1.py` & `brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/mobilenetv1.py`

 * *Files 19% similar despite different names*

```diff
@@ -21,30 +21,35 @@
 FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 SOFTWARE.
 """
 
-
 __all__ = ['quant_mobilenet_v1']
 
 from torch import nn
 from torch.nn import Sequential
 
-from brevitas.nn import QuantConv2d, QuantLinear, QuantReLU, QuantAvgPool2d
+from brevitas.nn import QuantConv2d
+from brevitas.nn import QuantLinear
+from brevitas.nn import QuantReLU
+from brevitas.nn import TruncAvgPool2d
 from brevitas.quant import IntBias
 
-from .common import CommonIntActQuant, CommonUintActQuant
-from .common import CommonIntWeightPerChannelQuant, CommonIntWeightPerTensorQuant
+from .common import CommonIntActQuant
+from .common import CommonIntWeightPerChannelQuant
+from .common import CommonIntWeightPerTensorQuant
+from .common import CommonUintActQuant
 
 FIRST_LAYER_BIT_WIDTH = 8
 
 
 class DwsConvBlock(nn.Module):
+
     def __init__(
             self,
             in_channels,
             out_channels,
             stride,
             bit_width,
             pw_activation_scaling_per_channel=False):
@@ -99,14 +104,15 @@
             weight_quant=CommonIntWeightPerChannelQuant,
             weight_bit_width=weight_bit_width)
         self.bn = nn.BatchNorm2d(num_features=out_channels, eps=bn_eps)
         self.activation = QuantReLU(
             act_quant=CommonUintActQuant,
             bit_width=act_bit_width,
             per_channel_broadcastable_shape=(1, out_channels, 1, 1),
+            scaling_stats_permute_dims=(1, 0, 2, 3),
             scaling_per_output_channel=activation_scaling_per_channel,
             return_quant_tensor=True)
 
     def forward(self, x):
         x = self.conv(x)
         x = self.bn(x)
         x = self.activation(x)
@@ -116,14 +122,15 @@
 class MobileNet(nn.Module):
 
     def __init__(
             self,
             channels,
             first_stage_stride,
             bit_width,
+            round_average_pool=True,
             in_channels=3,
             num_classes=1000):
         super(MobileNet, self).__init__()
         init_block_channels = channels[0][0]
 
         self.features = Sequential()
         init_block = ConvBlock(
@@ -146,17 +153,24 @@
                     out_channels=out_channels,
                     stride=stride,
                     bit_width=bit_width,
                     pw_activation_scaling_per_channel=pw_activation_scaling_per_channel)
                 stage.add_module('unit{}'.format(j + 1), mod)
                 in_channels = out_channels
             self.features.add_module('stage{}'.format(i + 1), stage)
-        self.final_pool = QuantAvgPool2d(kernel_size=7, stride=1, bit_width=bit_width)
+        # Exporting to torch or ONNX qcdq requires round
+        avgpool_float_to_int_impl_type = 'round' if round_average_pool else 'floor'
+        self.final_pool = TruncAvgPool2d(
+            kernel_size=7,
+            stride=1,
+            bit_width=bit_width,
+            float_to_int_impl_type=avgpool_float_to_int_impl_type)
         self.output = QuantLinear(
-            in_channels, num_classes,
+            in_channels,
+            num_classes,
             bias=True,
             bias_quant=IntBias,
             weight_quant=CommonIntWeightPerTensorQuant,
             weight_bit_width=bit_width)
 
     def forward(self, x):
         x = self.features(x)
@@ -168,20 +182,19 @@
 
 def quant_mobilenet_v1(cfg):
 
     channels = [[32], [64], [128, 128], [256, 256], [512, 512, 512, 512, 512, 512], [1024, 1024]]
     first_stage_stride = False
     width_scale = float(cfg.get('MODEL', 'WIDTH_SCALE'))
     bit_width = cfg.getint('QUANT', 'BIT_WIDTH')
+    round_avgpool = cfg.getboolean('QUANT', 'ROUND_AVG_POOL')
 
     if width_scale != 1.0:
         channels = [[int(cij * width_scale) for cij in ci] for ci in channels]
 
     net = MobileNet(
         channels=channels,
         first_stage_stride=first_stage_stride,
+        round_average_pool=round_avgpool,
         bit_width=bit_width)
 
     return net
-
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/imagenet_classification/models/proxylessnas.py` & `brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/proxylessnas.py`

 * *Files 4% similar despite different names*

```diff
@@ -20,22 +20,27 @@
 SOFTWARE.
 """
 
 __all__ = ['quant_proxylessnas_mobile14']
 
 import torch.nn as nn
 
-from brevitas.nn import QuantConv2d, QuantLinear, HadamardClassifier
-from brevitas.nn import QuantAvgPool2d, QuantReLU, QuantIdentity
+from brevitas.nn import HadamardClassifier
+from brevitas.nn import QuantConv2d
+from brevitas.nn import QuantIdentity
+from brevitas.nn import QuantLinear
+from brevitas.nn import QuantReLU
+from brevitas.nn import TruncAvgPool2d
 from brevitas.quant import IntBias
 
 from .common import *
 
 
 class ConvBlock(nn.Module):
+
     def __init__(
             self,
             in_channels,
             out_channels,
             kernel_size,
             stride,
             padding,
@@ -62,26 +67,28 @@
         self.bn = nn.BatchNorm2d(num_features=out_channels, eps=bn_eps)
         if shared_act is None:
             self.activ = QuantReLU(
                 act_quant=CommonUintActQuant,
                 bit_width=act_bit_width,
                 scaling_per_output_channel=act_scaling_per_channel,
                 per_channel_broadcastable_shape=(1, out_channels, 1, 1),
+                scaling_stats_permute_dims=(1, 0, 2, 3),
                 return_quant_tensor=return_quant_tensor)
         else:
             self.activ = shared_act
 
     def forward(self, x):
         x = self.conv(x)
         x = self.bn(x)
         x = self.activ(x)
         return x
 
 
 class ProxylessBlock(nn.Module):
+
     def __init__(
             self,
             in_channels,
             out_channels,
             kernel_size,
             stride,
             bn_eps,
@@ -139,14 +146,15 @@
             x = self.bc_conv(x)
         x = self.dw_conv(x)
         x = self.pw_conv(x)
         return x
 
 
 class ProxylessUnit(nn.Module):
+
     def __init__(
             self,
             in_channels,
             out_channels,
             kernel_size,
             stride,
             bn_eps,
@@ -185,27 +193,29 @@
         x = self.body(x)
         x = identity + x
         x = self.shared_act(x)
         return x
 
 
 class ProxylessNAS(nn.Module):
+
     def __init__(
             self,
             channels,
             init_block_channels,
             final_block_channels,
             residuals,
             shortcuts,
             kernel_sizes,
             expansions,
             bit_width,
             depthwise_bit_width,
             first_layer_weight_bit_width,
             hadamard_classifier,
+            round_average_pool=True,
             bn_eps=1e-3,
             in_channels=3,
             num_classes=1000):
         super(ProxylessNAS, self).__init__()
         self.features = nn.Sequential()
 
         init_block = ConvBlock(
@@ -271,20 +281,24 @@
             act_scaling_per_channel=False,
             act_bit_width=bit_width,
             weight_bit_width=bit_width,
             bias=False,
             return_quant_tensor=True)
         self.features.add_module("final_block", final_block)
         in_channels = final_block_channels
-        self.final_pool = QuantAvgPool2d(kernel_size=7, stride=1, bit_width=bit_width)
+        # Exporting to torch or ONNX qcdq requires round
+        avgpool_float_to_int_impl_type = 'round' if round_average_pool else 'floor'
+        self.final_pool = TruncAvgPool2d(
+            kernel_size=7,
+            stride=1,
+            bit_width=bit_width,
+            float_to_int_impl_type=avgpool_float_to_int_impl_type)
         if hadamard_classifier:
             self.output = HadamardClassifier(
-                in_channels=in_channels,
-                out_channels=num_classes,
-                fixed_scale=False)
+                in_channels=in_channels, out_channels=num_classes, fixed_scale=False)
         else:
             self.output = QuantLinear(
                 in_features=in_channels,
                 out_features=num_classes,
                 bias=True,
                 bias_quant=IntBias,
                 weight_bit_width=bit_width,
@@ -309,21 +323,23 @@
     final_block_channels = 1792
     shortcuts = [[0], [0, 1, 1, 1], [0, 1, 1, 1], [0, 1, 1, 1, 0, 1, 1, 1], [0, 1, 1, 1, 0]]
 
     bit_width = int(cfg.get('QUANT', 'BIT_WIDTH'))
     first_layer_weight_bit_width = int(cfg.get('QUANT', 'FIRST_LAYER_WEIGHT_BIT_WIDTH'))
     depthwise_bit_width = int(cfg.get('QUANT', 'DEPTHWISE_BIT_WIDTH'))
     hadamard_classifier = cfg.getboolean('MODEL', 'HADAMARD_CLASSIFIER')
+    round_avgpool = cfg.getboolean('QUANT', 'ROUND_AVG_POOL')
 
     net = ProxylessNAS(
         channels=channels,
         init_block_channels=init_block_channels,
         final_block_channels=final_block_channels,
         residuals=residuals,
         shortcuts=shortcuts,
         kernel_sizes=kernel_sizes,
         expansions=expansions,
         bit_width=bit_width,
         first_layer_weight_bit_width=first_layer_weight_bit_width,
         depthwise_bit_width=depthwise_bit_width,
-        hadamard_classifier=hadamard_classifier)
+        hadamard_classifier=hadamard_classifier,
+        round_average_pool=round_avgpool)
     return net
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/imagenet_classification/models/vgg.py` & `brevitas-0.9.0/src/brevitas_examples/imagenet_classification/models/vgg.py`

 * *Files 8% similar despite different names*

```diff
@@ -27,64 +27,93 @@
 # CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 # OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 # Based on the torchvision implementation
 # https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py
 
-
 import torch
 import torch.nn as nn
 
-from brevitas.nn import QuantConv2d, QuantLinear
-from brevitas.nn import QuantAvgPool2d, QuantReLU, QuantIdentity
+from brevitas.nn import QuantConv2d
+from brevitas.nn import QuantIdentity
+from brevitas.nn import QuantLinear
+from brevitas.nn import QuantReLU
+from brevitas.nn import TruncAvgPool2d
 
 from .common import *
 
 __all__ = [
     'QuantVGG',
     'quant_vgg11',
     'quant_vgg11_bn',
     'quant_vgg13',
     'quant_vgg13_bn',
     'quant_vgg16',
     'quant_vgg16_bn',
     'quant_vgg19_bn',
-    'quant_vgg19',
-]
-
+    'quant_vgg19',]
 
 cfgs = {
     'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
     'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
     'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
-    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],
-}
+    'E': [
+        64,
+        64,
+        'M',
+        128,
+        128,
+        'M',
+        256,
+        256,
+        256,
+        256,
+        'M',
+        512,
+        512,
+        512,
+        512,
+        'M',
+        512,
+        512,
+        512,
+        512,
+        'M'],}
 
 
 class QuantVGG(nn.Module):
 
     def __init__(self, cfg, batch_norm, bit_width=8, num_classes=1000):
         super(QuantVGG, self).__init__()
         self.features = make_layers(cfg, batch_norm, bit_width)
-        self.avgpool = QuantAvgPool2d(kernel_size=(7, 7), stride=1, bit_width=bit_width)
+        self.avgpool = TruncAvgPool2d(kernel_size=(7, 7), stride=1, bit_width=bit_width)
         self.classifier = nn.Sequential(
             QuantLinear(
-                512 * 7 * 7, 4096, bias=True,
-                weight_quant=CommonIntWeightPerChannelQuant, weight_bit_width=bit_width),
+                512 * 7 * 7,
+                4096,
+                bias=True,
+                weight_quant=CommonIntWeightPerChannelQuant,
+                weight_bit_width=bit_width),
             QuantReLU(act_quant=CommonUintActQuant, bit_width=bit_width),
             nn.Dropout(),
             QuantLinear(
-                4096, 4096, bias=True,
-                weight_quant=CommonIntWeightPerChannelQuant, weight_bit_width=bit_width),
+                4096,
+                4096,
+                bias=True,
+                weight_quant=CommonIntWeightPerChannelQuant,
+                weight_bit_width=bit_width),
             QuantReLU(act_quant=CommonUintActQuant, bit_width=bit_width),
             nn.Dropout(),
             QuantLinear(
-                4096, num_classes, bias=False,
-                weight_quant=CommonIntWeightPerTensorQuant, weight_bit_width=bit_width),
+                4096,
+                num_classes,
+                bias=False,
+                weight_quant=CommonIntWeightPerTensorQuant,
+                weight_bit_width=bit_width),
         )
         self._initialize_weights()
 
     def forward(self, x):
         x = self.features(x)
         x = self.avgpool(x)
         x = torch.flatten(x, 1)
@@ -110,27 +139,34 @@
     layers = []
     in_channels = 3
     for v in cfg:
         if v == 'M':
             layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
         else:
             conv2d = QuantConv2d(
-                in_channels, v, kernel_size=3, stride=1, padding=1, groups=1, bias=not batch_norm,
-                weight_bit_width=bit_width, weight_quant=CommonIntWeightPerChannelQuant)
+                in_channels,
+                v,
+                kernel_size=3,
+                stride=1,
+                padding=1,
+                groups=1,
+                bias=not batch_norm,
+                weight_bit_width=bit_width,
+                weight_quant=CommonIntWeightPerChannelQuant)
             act = QuantReLU(
                 act_quant=CommonUintActQuant, bit_width=bit_width, return_quant_tensor=True)
             if batch_norm:
                 layers += [conv2d, nn.BatchNorm2d(v), act]
             else:
                 layers += [conv2d, act]
             in_channels = v
     return nn.Sequential(*layers)
 
 
-def _quant_vgg(cfg, batch_norm,  **kwargs):
+def _quant_vgg(cfg, batch_norm, **kwargs):
     model = QuantVGG(cfgs[cfg], batch_norm=batch_norm, **kwargs)
     return model
 
 
 def quant_vgg11(**kwargs):
     return _quant_vgg('A', False, **kwargs)
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/README.md` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/README.md`

 * *Files 0% similar despite different names*

```diff
@@ -21,14 +21,14 @@
 To evaluate a pretrained quantized model on LibriSpeech:
 
  - Install pytorch from the [Pytorch Website](https://pytorch.org/), and Cython with the following command:
  `python install --upgrade cython`
  - Install SoX (this [guide](https://at.projects.genivi.org/wiki/display/PROJ/Installation+of+SoX+on+different+Platforms)
  may be helpful)
  - After cloning the repository, install Brevitas and QuartzNet requirements with `pip install .[stt]`
- - Pass the name of the model as an input to the evaluation script, as well as the path the json generated by `brevitas_quartznet_preprocess`. The required checkpoint will be downloaded automatically. 
- 
+ - Pass the name of the model as an input to the evaluation script, as well as the path the json generated by `brevitas_quartznet_preprocess`. The required checkpoint will be downloaded automatically.
+
  For example, for the evaluation on GPU 0:
 
 ```
 brevitas_quartznet_val --data-json /path/to/dataset/json --model quant_quartznet_pertensorscaling_8b --gpu 0 --pretrained
 ```
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/cfg/quant_quartznet_perchannelscaling_4b.ini` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/cfg/quant_quartznet_perchannelscaling_4b.ini`

 * *Files 1% similar despite different names*

```diff
@@ -13,9 +13,7 @@
 ENCODER_SCALING_PER_OUTPUT_CHANNEL: True
 DECODER_SCALING_PER_OUTPUT_CHANNEL: False
 
 [ACTIVATIONS]
 INNER_SCALING_PER_CHANNEL: True
 OTHER_SCALING_PER_CHANNEL: False
 ABS_ACT_VAL: 1
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/cfg/quant_quartznet_perchannelscaling_8b.ini` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/cfg/quant_quartznet_perchannelscaling_8b.ini`

 * *Files 1% similar despite different names*

```diff
@@ -13,10 +13,7 @@
 ENCODER_SCALING_PER_OUTPUT_CHANNEL: False
 DECODER_SCALING_PER_OUTPUT_CHANNEL: False
 
 [ACTIVATIONS]
 INNER_SCALING_PER_CHANNEL: False
 OTHER_SCALING_PER_CHANNEL: False
 ABS_ACT_VAL: 1
-
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/cfg/quant_quartznet_pertensorscaling_8b.ini` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/cfg/quant_quartznet_pertensorscaling_8b.ini`

 * *Files 1% similar despite different names*

```diff
@@ -13,9 +13,7 @@
 ENCODER_SCALING_PER_OUTPUT_CHANNEL: False
 DECODER_SCALING_PER_OUTPUT_CHANNEL: False
 
 [ACTIVATIONS]
 INNER_SCALING_PER_CHANNEL: False
 OTHER_SCALING_PER_CHANNEL: False
 ABS_ACT_VAL: 1
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/cfg/topology/quartznet15x5.yaml` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/cfg/topology/quartznet15x5.yaml`

 * *Files identical despite different names*

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/get_librispeech_data.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/get_librispeech_data.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,16 +35,15 @@
 URLS = {
     'TRAIN_CLEAN_100': ("http://www.openslr.org/resources/12/train-clean-100.tar.gz"),
     'TRAIN_CLEAN_360': ("http://www.openslr.org/resources/12/train-clean-360.tar.gz"),
     'TRAIN_OTHER_500': ("http://www.openslr.org/resources/12/train-other-500.tar.gz"),
     'DEV_CLEAN': "http://www.openslr.org/resources/12/dev-clean.tar.gz",
     'DEV_OTHER': "http://www.openslr.org/resources/12/dev-other.tar.gz",
     'TEST_CLEAN': "http://www.openslr.org/resources/12/test-clean.tar.gz",
-    'TEST_OTHER': "http://www.openslr.org/resources/12/test-other.tar.gz",
-}
+    'TEST_OTHER': "http://www.openslr.org/resources/12/test-other.tar.gz",}
 
 
 def __maybe_download_file(destination: str, source: str):
     """
     Downloads source to destination if it doesn't exist.
     If exists, skips download
     Args:
@@ -95,15 +94,15 @@
     for root, dirnames, filenames in os.walk(data_folder):
         for filename in fnmatch.filter(filenames, '*.trans.txt'):
             files.append((os.path.join(root, filename), root))
 
     for transcripts_file, root in tqdm(files):
         with open(transcripts_file, encoding="utf-8") as fin:
             for line in fin:
-                id, text = line[: line.index(" ")], line[line.index(" ") + 1 :]
+                id, text = line[:line.index(" ")], line[line.index(" ") + 1:]
                 transcript_text = text.lower().strip()
 
                 # Convert FLAC file to WAV
                 flac_file = os.path.join(root, id + ".flac")
                 wav_file = os.path.join(dst_folder, id + ".wav")
                 if not os.path.exists(wav_file):
                     Transformer().build(flac_file, wav_file)
@@ -133,16 +132,22 @@
         filepath = os.path.join(data_root, data_set + ".tar.gz")
         print("Getting {0}".format(data_set))
         __maybe_download_file(filepath, data_set.upper())
         print("Extracting {0}".format(data_set))
         __extract_file(filepath, data_root)
         print("Processing {0}".format(data_set))
         __process_data(
-            os.path.join(os.path.join(data_root, "LibriSpeech"), data_set.replace("_", "-"),),
-            os.path.join(os.path.join(data_root, "LibriSpeech"), data_set.replace("_", "-"),) + "-processed",
+            os.path.join(
+                os.path.join(data_root, "LibriSpeech"),
+                data_set.replace("_", "-"),
+            ),
+            os.path.join(
+                os.path.join(data_root, "LibriSpeech"),
+                data_set.replace("_", "-"),
+            ) + "-processed",
             os.path.join(data_root, data_set + ".json"),
         )
     print('Done!')
 
 
 if __name__ == "__main__":
-    main()
+    main()
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/__init__.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,37 +12,36 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
-from .data_layer import (
-        AudioToTextDataLayer)
-from .greedy_ctc_decoder import GreedyCTCDecoder
-from .quartznet import quartznet
-from .losses import CTCLossNM
-from .helpers import *
-
-import os
 from configparser import ConfigParser
+import os
+
 from ruamel.yaml import YAML
 from torch import hub
 
-__all__ = ['AudioToTextDataLayer',
-           'quartznet',
-           'quant_quartznet_perchannelscaling_4b',
-           'quant_quartznet_perchannelscaling_8b',
-           'quant_quartznet_pertensorscaling_8b']
+from .data_layer import AudioToTextDataLayer
+from .greedy_ctc_decoder import GreedyCTCDecoder
+from .helpers import *
+from .losses import CTCLossNM
+from .quartznet import quartznet
+
+__all__ = [
+    'AudioToTextDataLayer',
+    'quartznet',
+    'quant_quartznet_perchannelscaling_4b',
+    'quant_quartznet_perchannelscaling_8b',
+    'quant_quartznet_pertensorscaling_8b']
 
 name = "quarznet_release"
 model_impl = {
-    'quartznet': quartznet,
-}
+    'quartznet': quartznet,}
 
 
 def model_with_cfg(name, pretrained, export_mode):
     cfg = ConfigParser()
     current_dir = os.path.dirname(os.path.abspath(__file__))
     config_path = os.path.join(current_dir, '..', 'cfg', name + '.ini')
     assert os.path.exists(config_path)
@@ -55,16 +54,18 @@
         quartnzet_params = yaml.load(f)
     model = model_impl[arch](cfg, quartnzet_params, export_mode)
     if pretrained:
         pretrained_encoder_url = cfg.get('MODEL', 'PRETRAINED_ENCODER_URL')
         pretrained_decoder_url = cfg.get('MODEL', 'PRETRAINED_DECODER_URL')
         print("=> Loading encoder checkpoint from:'{}'".format(pretrained_encoder_url))
         print("=> Loading decoder checkpoint from:'{}'".format(pretrained_decoder_url))
-        checkpoint_enc = torch.hub.load_state_dict_from_url(pretrained_encoder_url, progress=True, map_location='cpu')
-        checkpoint_dec = torch.hub.load_state_dict_from_url(pretrained_decoder_url, progress=True, map_location='cpu')
+        checkpoint_enc = torch.hub.load_state_dict_from_url(
+            pretrained_encoder_url, progress=True, map_location='cpu')
+        checkpoint_dec = torch.hub.load_state_dict_from_url(
+            pretrained_decoder_url, progress=True, map_location='cpu')
         model.restore_checkpoints(checkpoint_enc, checkpoint_dec)
     return model, cfg
 
 
 def quant_quartznet_perchannelscaling_4b(pretrained=True, export_mode=False):
     model, _ = model_with_cfg('quant_quartznet_perchannelscaling_4b', pretrained, export_mode)
     return model
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/audio_preprocessing.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/audio_preprocessing.py`

 * *Files 1% similar despite different names*

```diff
@@ -15,58 +15,62 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """
 This file contains neural modules responsible for preprocessing audio data.
 """
-__all__ = ['AudioPreprocessing',
-           'AudioPreprocessor',
-           'AudioToMFCCPreprocessor',
-           'AudioToMelSpectrogramPreprocessor',
-           'AudioToSpectrogramPreprocessor',
-           'MultiplyBatch',
-           'SpectrogramAugmentation']
+__all__ = [
+    'AudioPreprocessing',
+    'AudioPreprocessor',
+    'AudioToMFCCPreprocessor',
+    'AudioToMelSpectrogramPreprocessor',
+    'AudioToSpectrogramPreprocessor',
+    'MultiplyBatch',
+    'SpectrogramAugmentation']
 
 from abc import abstractmethod
 import math
+
 import torch
 import torch.nn as nn
+
 try:
     import torchaudio
     have_torchaudio = True
 except ModuleNotFoundError:
     have_torchaudio = False
     print('Could not import torchaudio. Some features might not work.')
 
 from .parts.features import FilterbankFeatures
-from .parts.spectr_augment import SpecAugment, SpecCutout
+from .parts.spectr_augment import SpecAugment
+from .parts.spectr_augment import SpecCutout
 
 
 class AudioPreprocessor(nn.Module):
     """
     A base class for Neural Modules that performs audio preprocessing,
     transforming the wav files to features.
     """
+
     def __init__(self, win_length, hop_length, **kwargs):
         super().__init__()
 
         self.win_length = win_length
         self.hop_length = hop_length
 
         # self.disable_casts = (self._opt_level == Optimization.mxprO1)
 
         self.torch_windows = {
             'hann': torch.hann_window,
             'hamming': torch.hamming_window,
             'blackman': torch.blackman_window,
             'bartlett': torch.bartlett_window,
             'ones': torch.ones,
-            None: torch.ones
-        }
+            None: torch.ones}
 
     @torch.no_grad()
     def forward(self, input_signal, length):
         # if self.disable_casts:
         #     with amp.disable_casts():
         #         processed_signal = self.get_features(
         #                 input_signal.to(torch.float), length)
@@ -107,36 +111,38 @@
         window (str): Windowing function for fft. can be one of ['hann',
             'hamming', 'blackman', 'bartlett', 'none', 'null']
             Defaults to "hann"
         normalized (bool): Whether to normalize by magnitude after stft
     """
 
     def __init__(
-            self, *,
+            self,
+            *,
             sample_rate=16000,
             window_size=0.02,
             window_stride=0.01,
             n_window_size=None,
             n_window_stride=None,
             n_fft=None,
             window="hann",
             normalized=True,
-            **kwargs
-    ):
+            **kwargs):
         if not have_torchaudio:
             raise ModuleNotFoundError(
                 "torchaudio is not installed but is necessary for "
                 "AudioToSpectrogramPreprocessor. We recommend you try "
                 "building it from source for the PyTorch version you have.")
         if window_size and n_window_size:
-            raise ValueError(f"{self} received both window_size and "
-                             f"n_window_size. Only one should be specified.")
+            raise ValueError(
+                f"{self} received both window_size and "
+                f"n_window_size. Only one should be specified.")
         if window_stride and n_window_stride:
-            raise ValueError(f"{self} received both window_stride and "
-                             f"n_window_stride. Only one should be specified.")
+            raise ValueError(
+                f"{self} received both window_stride and "
+                f"n_window_stride. Only one should be specified.")
         if window_size:
             n_window_size = int(window_size * sample_rate)
         if window_stride:
             n_window_stride = int(window_stride * sample_rate)
 
         super().__init__(n_window_size, n_window_stride, **kwargs)
 
@@ -155,16 +161,15 @@
         # Create featurizer.
         # Calls torch.stft under the hood, and is hard-coded to use center=True
         self.featurizer = torchaudio.transforms.Spectrogram(
             n_fft=self.n_fft,
             win_length=self.win_length,
             hop_length=self.hop_length,
             window_fn=window_fn,
-            normalized=normalized
-        )
+            normalized=normalized)
         self.featurizer.to(self._device)
 
     def get_features(self, input_signal, length):
         return self.featurizer(input_signal)
 
 
 class AudioToMelSpectrogramPreprocessor(AudioPreprocessor):
@@ -227,44 +232,46 @@
             Defaults to 0
         mag_power (float): The power that the linear spectrogram is raised to
             prior to multiplication with mel basis.
             Defaults to 2 for a power spec
     """
 
     def __init__(
-            self, *,
+            self,
+            *,
             sample_rate=16000,
             window_size=0.02,
             window_stride=0.01,
             n_window_size=None,
             n_window_stride=None,
             window="hann",
             normalize="per_feature",
             n_fft=None,
             preemph=0.97,
             features=64,
             lowfreq=0,
             highfreq=None,
             log=True,
             log_zero_guard_type="add",
-            log_zero_guard_value=2**-24,
+            log_zero_guard_value=2 ** -24,
             dither=1e-5,
             pad_to=16,
             frame_splicing=1,
             stft_conv=False,
             pad_value=0,
             mag_power=2.,
-            **kwargs
-    ):
+            **kwargs):
         if window_size and n_window_size:
-            raise ValueError(f"{self} received both window_size and "
-                             f"n_window_size. Only one should be specified.")
+            raise ValueError(
+                f"{self} received both window_size and "
+                f"n_window_size. Only one should be specified.")
         if window_stride and n_window_stride:
-            raise ValueError(f"{self} received both window_stride and "
-                             f"n_window_stride. Only one should be specified.")
+            raise ValueError(
+                f"{self} received both window_stride and "
+                f"n_window_stride. Only one should be specified.")
         if window_size:
             n_window_size = int(window_size * sample_rate)
         if window_stride:
             n_window_stride = int(window_stride * sample_rate)
 
         super().__init__(n_window_size, n_window_stride, **kwargs)
 
@@ -284,16 +291,15 @@
             log_zero_guard_value=log_zero_guard_value,
             dither=dither,
             pad_to=pad_to,
             frame_splicing=frame_splicing,
             stft_conv=stft_conv,
             pad_value=pad_value,
             mag_power=mag_power,
-            logger=None
-        )
+            logger=None)
         # self.featurizer.to(self._device)
 
     def get_features(self, input_signal, length):
         return self.featurizer(input_signal, length)
 
     def get_seq_len(self, seq_len):
         return self.featurizer.get_seq_len(seq_len)
@@ -337,15 +343,16 @@
         dct_type: Type of discrete cosine transform to use
         norm: Type of norm to use
         log: Whether to use log-mel spectrograms instead of db-scaled.
             Defaults to True.
     """
 
     def __init__(
-            self, *,
+            self,
+            *,
             sample_rate=16000,
             window_size=0.02,
             window_stride=0.01,
             n_window_size=None,
             n_window_stride=None,
             window='hann',
             n_fft=None,
@@ -359,19 +366,21 @@
             **kwargs):
         if not have_torchaudio:
             raise ModuleNotFoundError(
                 "torchaudio is not installed but is necessary for "
                 "AudioToMFCCPreprocessor. We recommend you try "
                 "building it from source for the PyTorch version you have.")
         if window_size and n_window_size:
-            raise ValueError(f"{self} received both window_size and "
-                             f"n_window_size. Only one should be specified.")
+            raise ValueError(
+                f"{self} received both window_size and "
+                f"n_window_size. Only one should be specified.")
         if window_stride and n_window_stride:
-            raise ValueError(f"{self} received both window_stride and "
-                             f"n_window_stride. Only one should be specified.")
+            raise ValueError(
+                f"{self} received both window_stride and "
+                f"n_window_stride. Only one should be specified.")
         # Get win_length (n_window_size) and hop_length (n_window_stride)
         if window_size:
             n_window_size = int(window_size * sample_rate)
         if window_stride:
             n_window_stride = int(window_stride * sample_rate)
 
         super().__init__(n_window_size, n_window_stride, **kwargs)
@@ -398,16 +407,15 @@
         # Use torchaudio's implementation of MFCCs as featurizer
         self.featurizer = torchaudio.transforms.MFCC(
             sample_rate=sample_rate,
             n_mfcc=n_mfcc,
             dct_type=dct_type,
             norm=norm,
             log_mels=log,
-            melkwargs=mel_kwargs
-        )
+            melkwargs=mel_kwargs)
         self.featurizer.to(self._device)
 
     def get_features(self, input_signal, length):
         return self.featurizer(input_signal)
 
 
 class SpectrogramAugmentation(nn.Module):
@@ -440,46 +448,41 @@
             Defaults to 5.
         rect_time (int): maximum size of cut rectangles along the time
             dimension
             Defaults to 25.
     """
 
     def __init__(
-            self, *,
+            self,
+            *,
             freq_masks=0,
             time_masks=0,
             freq_width=10,
             time_width=10,
             rect_masks=0,
             rect_time=5,
             rect_freq=20,
             rng=None,
-            **kwargs
-    ):
+            **kwargs):
         nn.Module.__init__(self)
 
         if rect_masks > 0:
             self.spec_cutout = SpecCutout(
-                rect_masks=rect_masks,
-                rect_time=rect_time,
-                rect_freq=rect_freq,
-                rng=rng
-            )
+                rect_masks=rect_masks, rect_time=rect_time, rect_freq=rect_freq, rng=rng)
             # self.spec_cutout.to(self._device)
         else:
             self.spec_cutout = lambda x: x
 
         if freq_masks + time_masks > 0:
             self.spec_augment = SpecAugment(
                 freq_masks=freq_masks,
                 time_masks=time_masks,
                 freq_width=freq_width,
                 time_width=time_width,
-                rng=rng
-            )
+                rng=rng)
             # self.spec_augment.to(self._device)
         else:
             self.spec_augment = lambda x: x
 
     def forward(self, input_spec):
         augmented_spec = self.spec_cutout(input_spec)
         augmented_spec = self.spec_augment(augmented_spec)
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/data_layer.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/data_layer.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,31 +18,35 @@
 
 """
 This package contains Neural Modules responsible for ASR-related
 data layers.
 """
 __all__ = ['AudioToTextDataLayer']
 from functools import partial
+
 import torch
 import torch.nn as nn
 
-from .parts.dataset import (AudioDataset, seq_collate_fn)
+from .parts.dataset import AudioDataset
+from .parts.dataset import seq_collate_fn
 from .parts.features import WaveformFeaturizer
 
+
 def pad_to(x, k=8):
     """Pad int value up to divisor of k.
 
     Examples:
         >>> pad_to(31, 8)
         32
 
     """
 
     return x + (x % k > 0) * (k - x % k)
 
+
 class AudioToTextDataLayer(nn.Module):
     """Data Layer for general ASR tasks.
 
     Module which reads ASR labeled data. It accepts comma-separated
     JSON manifest files describing the correspondence between wav audio files
     and their transcripts. JSON files should be of the following format::
 
@@ -94,15 +98,16 @@
             Defaults to True.
         num_workers (int): See PyTorch DataLoader.
             Defaults to 0.
         perturb_config (dict): Currently disabled.
     """
 
     def __init__(
-            self, *,
+            self,
+            *,
             manifest_filepath,
             labels,
             batch_size,
             sample_rate=16000,
             int_values=False,
             bos_id=None,
             eos_id=None,
@@ -113,59 +118,56 @@
             trim_silence=False,
             load_audio=True,
             drop_last=False,
             shuffle=True,
             num_workers=4,
             placement='cpu',
             # perturb_config=None,
-            **kwargs
-    ):
+            **kwargs):
         super().__init__()
 
         self._featurizer = WaveformFeaturizer(
             sample_rate=sample_rate, int_values=int_values, augmentor=None)
 
         # Set up dataset
-        dataset_params = {'manifest_filepath': manifest_filepath,
-                          'labels': labels,
-                          'featurizer': self._featurizer,
-                          'max_duration': max_duration,
-                          'min_duration': min_duration,
-                          'normalize': normalize_transcripts,
-                          'trim': trim_silence,
-                          'bos_id': bos_id,
-                          'eos_id': eos_id,
-                          'logger': None,
-                          'load_audio': load_audio}
+        dataset_params = {
+            'manifest_filepath': manifest_filepath,
+            'labels': labels,
+            'featurizer': self._featurizer,
+            'max_duration': max_duration,
+            'min_duration': min_duration,
+            'normalize': normalize_transcripts,
+            'trim': trim_silence,
+            'bos_id': bos_id,
+            'eos_id': eos_id,
+            'logger': None,
+            'load_audio': load_audio}
 
         self._dataset = AudioDataset(**dataset_params)
 
         # Set up data loader
         if placement == 'cuda':
             print('Parallelizing DATALAYER')
-            sampler = torch.utils.data.distributed.DistributedSampler(
-                self._dataset)
+            sampler = torch.utils.data.distributed.DistributedSampler(self._dataset)
         else:
             sampler = None
 
         pad_id = 0 if pad_id is None else pad_id
         self._dataloader = torch.utils.data.DataLoader(
             dataset=self._dataset,
             batch_size=batch_size,
             collate_fn=partial(seq_collate_fn, token_pad_value=pad_id),
             drop_last=drop_last,
             shuffle=shuffle if sampler is None else False,
             sampler=sampler,
-            num_workers=num_workers
-        )
+            num_workers=num_workers)
 
     def __len__(self):
         return len(self._dataset)
 
     @property
     def dataset(self):
         return None
 
     @property
     def data_iterator(self):
         return self._dataloader
-
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/greedy_ctc_decoder.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/greedy_ctc_decoder.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,18 +14,20 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import torch
 import torch.nn as nn
 
+
 class GreedyCTCDecoder(nn.Module):
     """
     Greedy decoder that computes the argmax over a softmax distribution
     """
+
     def __init__(self):
         nn.Module.__init__(self)
 
     def forward(self, log_probs):
         with torch.no_grad():
             argmx = log_probs.argmax(dim=-1, keepdim=False)
             return argmx
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/helpers.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/helpers.py`

 * *Files 4% similar despite different names*

```diff
@@ -41,19 +41,16 @@
                 decoded_prediction.append(p)
             previous = p
         hypothesis = ''.join([labels_map[c] for c in decoded_prediction])
         hypotheses.append(hypothesis)
     return hypotheses
 
 
-def monitor_asr_train_progress(tensors: list,
-                               labels: list,
-                               eval_metric='WER',
-                               tb_logger=None,
-                               logger=None):
+def monitor_asr_train_progress(
+        tensors: list, labels: list, eval_metric='WER', tb_logger=None, logger=None):
     """
     Takes output of greedy ctc decoder and performs ctc decoding algorithm to
     remove duplicates and special symbol. Prints sample to screen, computes
     and logs AVG WER to console and (optionally) Tensorboard
     Args:
       tensors: A list of 3 tensors (predictions, targets, target_lengths)
       labels: A list of labels
@@ -73,16 +70,15 @@
 
         # iterate over batch
         for ind in range(targets_cpu_tensor.shape[0]):
             tgt_len = tgt_lenths_cpu_tensor[ind].item()
             target = targets_cpu_tensor[ind][:tgt_len].numpy().tolist()
             reference = ''.join([labels_map[c] for c in target])
             references.append(reference)
-        hypotheses = __ctc_decoder_predictions_tensor(
-            tensors[1], labels=labels)
+        hypotheses = __ctc_decoder_predictions_tensor(tensors[1], labels=labels)
 
     eval_metric = eval_metric.upper()
     if eval_metric not in {'WER', 'CER'}:
         raise ValueError('eval_metric must be \'WER\' or \'CER\'')
     use_cer = True if eval_metric == 'CER' else False
 
     tag = f'training_batch_{eval_metric}'
@@ -108,16 +104,15 @@
 def __gather_predictions(predictions_list: list, labels: list) -> list:
     results = []
     for prediction in predictions_list:
         results += __ctc_decoder_predictions_tensor(prediction, labels=labels)
     return results
 
 
-def __gather_transcripts(transcript_list: list, transcript_len_list: list,
-                         labels: list) -> list:
+def __gather_transcripts(transcript_list: list, transcript_len_list: list, labels: list) -> list:
     results = []
     labels_map = dict([(i, labels[i]) for i in range(len(labels))])
     # iterate over workers
     for t, ln in zip(transcript_list, transcript_len_list):
         # iterate over batch
         t_lc = t.long().cpu()
         ln_lc = ln.long().cpu()
@@ -143,47 +138,40 @@
         global_vars['logits'] = []
     # if not 'transcript_lengths' in global_vars.keys():
     #  global_vars['transcript_lengths'] = []
     for kv, v in tensors.items():
         if kv.startswith('loss'):
             global_vars['EvalLoss'] += __gather_losses(v)
         elif kv.startswith('predictions'):
-            global_vars['predictions'] += __gather_predictions(
-                v, labels=labels)
+            global_vars['predictions'] += __gather_predictions(v, labels=labels)
         elif kv.startswith('transcript_length'):
             transcript_len_list = v
         elif kv.startswith('transcript'):
             transcript_list = v
         elif kv.startswith('output'):
             global_vars['logits'] += v
 
-    global_vars['transcripts'] += __gather_transcripts(transcript_list,
-                                                       transcript_len_list,
-                                                       labels=labels)
+    global_vars['transcripts'] += __gather_transcripts(
+        transcript_list, transcript_len_list, labels=labels)
 
 
-def process_evaluation_epoch(global_vars: dict,
-                             eval_metric='WER',
-                             tag=None,
-                             logger=None):
+def process_evaluation_epoch(global_vars: dict, eval_metric='WER', tag=None, logger=None):
     """
     Calculates the aggregated loss and WER across the entire evaluation dataset
     """
     eloss = torch.mean(torch.stack(global_vars['EvalLoss'])).item()
     hypotheses = global_vars['predictions']
     references = global_vars['transcripts']
 
     eval_metric = eval_metric.upper()
     if eval_metric not in {'WER', 'CER'}:
         raise ValueError('eval_metric must be \'WER\' or \'CER\'')
     use_cer = True if eval_metric == 'CER' else False
 
-    wer = word_error_rate(hypotheses=hypotheses,
-                          references=references,
-                          use_cer=use_cer)
+    wer = word_error_rate(hypotheses=hypotheses, references=references, use_cer=use_cer)
 
     if tag is None:
         if logger:
             logger.info(f"==========>>>>>>Evaluation Loss: {eloss}")
             logger.info(f"==========>>>>>>Evaluation {eval_metric}: "
                         f"{wer*100 : 5.2f}%")
         else:
@@ -196,20 +184,16 @@
             logger.info(f"==========>>>>>>Evaluation Loss {tag}: {eloss}")
             logger.info(f"==========>>>>>>Evaluation {eval_metric} {tag}: "
                         f"{wer*100 : 5.2f}%")
         else:
             print(f"==========>>>>>>Evaluation Loss {tag}: {eloss}")
             print(f"==========>>>>>>Evaluation {eval_metric} {tag}:"
                   f" {wer*100 : 5.2f}%")
-        return {f"Evaluation_Loss_{tag}": eloss,
-                f"Evaluation_{eval_metric}_{tag}": wer}
+        return {f"Evaluation_Loss_{tag}": eloss, f"Evaluation_{eval_metric}_{tag}": wer}
 
 
 def post_process_predictions(predictions, labels):
     return __gather_predictions(predictions, labels=labels)
 
 
-def post_process_transcripts(
-        transcript_list, transcript_len_list, labels):
-    return __gather_transcripts(transcript_list,
-                                transcript_len_list,
-                                labels=labels)
+def post_process_transcripts(transcript_list, transcript_len_list, labels):
+    return __gather_transcripts(transcript_list, transcript_len_list, labels=labels)
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/losses.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/losses.py`

 * *Files 4% similar despite different names*

```diff
@@ -29,24 +29,21 @@
     """
 
     def __init__(self, *, num_classes, **kwargs):
         nn.Module.__init__(self)
 
         # self._blank = self.local_parameters.get('blank', 0)
         self._blank = num_classes
-        self._criterion = nn.CTCLoss(blank=self._blank,
-                                     reduction='none')
+        self._criterion = nn.CTCLoss(blank=self._blank, reduction='none')
 
     def _loss(self, log_probs, targets, input_length, target_length):
         input_length = input_length.long()
         target_length = target_length.long()
         targets = targets.long()
-        loss = self._criterion(log_probs.transpose(1, 0), targets,
-                               input_length,
-                               target_length)
+        loss = self._criterion(log_probs.transpose(1, 0), targets, input_length, target_length)
         # note that this is different from reduction = 'mean'
         # because we are not dividing by target lengths
         loss = torch.mean(loss)
         return loss
 
     def _loss_function(self, **kwargs):
         return self._loss(*(kwargs.values()))
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/metrics.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/metrics.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,15 +12,14 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
 from typing import List
 
 
 def __levenshtein(a: List, b: List) -> int:
     """Calculates the Levenshtein distance between a and b.
     The code was copied from: http://hetland.org/coding/python/levenshtein.py
     """
@@ -39,17 +38,15 @@
             if a[j - 1] != b[i - 1]:
                 change = change + 1
             current[j] = min(add, delete, change)
 
     return current[n]
 
 
-def word_error_rate(hypotheses: List[str],
-                    references: List[str],
-                    use_cer=False) -> float:
+def word_error_rate(hypotheses: List[str], references: List[str], use_cer=False) -> float:
     """
     Computes Average Word Error rate between two texts represented as
     corresponding lists of string. Hypotheses and references must have same
     length.
 
     Args:
       hypotheses: list of hypotheses
@@ -60,16 +57,15 @@
     """
     scores = 0
     words = 0
     if len(hypotheses) != len(references):
         raise ValueError(
             "In word error rate calculation, hypotheses and reference"
             " lists must have the same number of elements. But I got:"
-            "{0} and {1} correspondingly".format(len(hypotheses),
-                                                 len(references)))
+            "{0} and {1} correspondingly".format(len(hypotheses), len(references)))
     for h, r in zip(hypotheses, references):
         if use_cer:
             h_list = list(h)
             r_list = list(r)
         else:
             h_list = h.split()
             r_list = r.split()
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/__init__.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/__init__.py`

 * *Files 11% similar despite different names*

```diff
@@ -12,12 +12,13 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from .manifest import ManifestEN, ManifestBase
 from .dataset import AudioDataset
 from .features import WaveformFeaturizer
+from .manifest import ManifestBase
+from .manifest import ManifestEN
 
 __all__ = ['ManifestEN', 'ManifestBase', 'AudioDataset', 'WaveformFeaturizer']
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/cleaners.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/cleaners.py`

 * *Files 7% similar despite different names*

```diff
@@ -12,82 +12,78 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import inflect
 import re
 
+import inflect
 from unidecode import unidecode
 
 NUM_CHECK = re.compile(r'([$]?)(^|\s)(\S*[0-9]\S*)(?=(\s|$)((\S*)(\s|$))?)')
 
 TIME_CHECK = re.compile(r'([0-9]{1,2}):([0-9]{2})(am|pm)?')
 CURRENCY_CHECK = re.compile(r'\$')
 ORD_CHECK = re.compile(r'([0-9]+)(st|nd|rd|th)')
 THREE_CHECK = re.compile(r'([0-9]{3})([.,][0-9]{1,2})?([!.?])?$')
 DECIMAL_CHECK = re.compile(r'([.,][0-9]{1,2})$')
 
-ABBREVIATIONS_COMMON = [(re.compile('\\b%s\\.' % x[0]), x[1]) for x in
-                        [
-                            ("ms", "miss"),
-                            ("mrs", "misess"),
-                            ("mr", "mister"),
-                            ("messrs", "messeurs"),
-                            ("dr", "doctor"),
-                            ("drs", "doctors"),
-                            ("st", "saint"),
-                            ("co", "company"),
-                            ("jr", "junior"),
-                            ("sr", "senior"),
-                            ("rev", "reverend"),
-                            ("hon", "honorable"),
-                            ("sgt", "sergeant"),
-                            ("capt", "captain"),
-                            ("maj", "major"),
-                            ("col", "colonel"),
-                            ("lt", "lieutenant"),
-                            ("gen", "general"),
-                            ("prof", "professor"),
-                            ("lb", "pounds"),
-                            ("rep", "representative"),
-                            ("st", "street"),
-                            ("ave", "avenue"),
-                            ("etc", "et cetera"),
-                            ("jan", "january"),
-                            ("feb", "february"),
-                            ("mar", "march"),
-                            ("apr", "april"),
-                            ("jun", "june"),
-                            ("jul", "july"),
-                            ("aug", "august"),
-                            ("sep", "september"),
-                            ("oct", "october"),
-                            ("nov", "november"),
-                            ("dec", "december"),
-                        ]]
-
-ABBREVIATIONS_EXPANDED = [(re.compile('\\b%s\\.' % x[0]), x[1]) for x in
-                          [
-                            ("ltd", "limited"),
-                            ("fig", "figure"),
-                            ("figs", "figures"),
-                            ("gent", "gentlemen"),
-                            ("ft", "fort"),
-                            ("esq", "esquire"),
-                            ("prep", "preperation"),
-                            ("bros", "brothers"),
-                            ("ind", "independent"),
-                            ("mme", "madame"),
-                            ("pro", "professional"),
-                            ("vs", "versus"),
-                            ("inc", "include"),
-                          ]]
+ABBREVIATIONS_COMMON = [(re.compile('\\b%s\\.' % x[0]), x[1]) for x in [
+    ("ms", "miss"),
+    ("mrs", "misess"),
+    ("mr", "mister"),
+    ("messrs", "messeurs"),
+    ("dr", "doctor"),
+    ("drs", "doctors"),
+    ("st", "saint"),
+    ("co", "company"),
+    ("jr", "junior"),
+    ("sr", "senior"),
+    ("rev", "reverend"),
+    ("hon", "honorable"),
+    ("sgt", "sergeant"),
+    ("capt", "captain"),
+    ("maj", "major"),
+    ("col", "colonel"),
+    ("lt", "lieutenant"),
+    ("gen", "general"),
+    ("prof", "professor"),
+    ("lb", "pounds"),
+    ("rep", "representative"),
+    ("st", "street"),
+    ("ave", "avenue"),
+    ("etc", "et cetera"),
+    ("jan", "january"),
+    ("feb", "february"),
+    ("mar", "march"),
+    ("apr", "april"),
+    ("jun", "june"),
+    ("jul", "july"),
+    ("aug", "august"),
+    ("sep", "september"),
+    ("oct", "october"),
+    ("nov", "november"),
+    ("dec", "december"),]]
+
+ABBREVIATIONS_EXPANDED = [(re.compile('\\b%s\\.' % x[0]), x[1]) for x in [
+    ("ltd", "limited"),
+    ("fig", "figure"),
+    ("figs", "figures"),
+    ("gent", "gentlemen"),
+    ("ft", "fort"),
+    ("esq", "esquire"),
+    ("prep", "preperation"),
+    ("bros", "brothers"),
+    ("ind", "independent"),
+    ("mme", "madame"),
+    ("pro", "professional"),
+    ("vs", "versus"),
+    ("inc", "include"),]]
 
 inflect = inflect.engine()
 
 
 def clean_text(string, table, punctuation_to_replace):
     warn_common_chars(string)
     string = unidecode(string)
@@ -98,16 +94,17 @@
     string = clean_punctuations(string, table, punctuation_to_replace)
     string = re.sub(r'\s+', " ", string).strip()
     return string
 
 
 def warn_common_chars(string):
     if re.search(r'[]', string):
-        print("WARNING: Your transcript contains one of '' or '' which we do"
-              "not currently handle")
+        print(
+            "WARNING: Your transcript contains one of '' or '' which we do"
+            "not currently handle")
 
 
 def clean_numbers(string):
     cleaner = NumberCleaner()
     string = NUM_CHECK.sub(cleaner.clean, string)
     return string
 
@@ -119,22 +116,21 @@
         for regex, replacement in ABBREVIATIONS_EXPANDED:
             string = re.sub(regex, replacement, string)
     return string
 
 
 def clean_punctuations(string, table, punctuation_to_replace):
     for punc, replacement in punctuation_to_replace.items():
-        string = re.sub('\\{}'.format(punc),
-                        " {} ".format(replacement),
-                        string)
+        string = re.sub('\\{}'.format(punc), " {} ".format(replacement), string)
     string = string.translate(table)
     return string
 
 
 class NumberCleaner():
+
     def __init__(self):
         super().__init__()
         self.reset()
 
     def reset(self):
         self.curr_num = []
         self.currency = None
@@ -153,14 +149,15 @@
         if decimal:
             whole_num += "." + decimal
             return inflect.number_to_words(whole_num)
         else:
             # Check if there are non-numbers
             def convert_to_word(match):
                 return " " + inflect.number_to_words(match.group(0)) + " "
+
             return re.sub(r'[0-9,]+', convert_to_word, whole_num)
 
     def clean(self, match):
         ws = match.group(2)
         number = match.group(3)
         _proceeding_symbol = match.group(7)
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/common.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/common.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,21 +1,20 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-import brevitas.nn as quant_nn
 from brevitas.core.quant import QuantType
-from brevitas.core.scaling import ScalingImplType
 from brevitas.core.restrict_val import RestrictValueType
+from brevitas.core.scaling import ScalingImplType
 from brevitas.core.stats import StatsOp
+import brevitas.nn as quant_nn
+
 global ACT_MIN_VAL, ACT_MAX_VAL
 brevitas_activations = {
     "hardtanh": quant_nn.QuantHardTanh,
-    "relu": quant_nn.QuantReLU,
-}
+    "relu": quant_nn.QuantReLU,}
 
 QUANT_TYPE = QuantType.INT
 QUANT_TYPE_BIAS = QuantType.FP
 
 SCALING_MIN_VAL = 2e-16
 ACT_SCALING_IMPL_TYPE = ScalingImplType.PARAMETER
 ACT_SCALING_PER_CHANNEL = False
@@ -24,46 +23,82 @@
 WEIGHT_SCALING_IMPL_TYPE = ScalingImplType.STATS
 WEIGHT_SCALING_STATS_OP = StatsOp.MAX
 WEIGHT_NARROW_RANGE = True
 BIAS_CONFIGS = False
 
 
 def make_quantization_input(bit_width, absolute_act_val, scaling_per_channel):
-    return quant_nn.QuantHardTanh(bit_width=bit_width, scaling_per_output_channel=scaling_per_channel, quant_type=QUANT_TYPE,
-                                  scaling_impl_type=ACT_SCALING_IMPL_TYPE, scaling_min_val=SCALING_MIN_VAL, restrict_scaling_type=ACT_RESTRICT_SCALING_TYPE,
-                                  max_val=absolute_act_val, min_val=-absolute_act_val, return_quant_tensor=False)
+    return quant_nn.QuantHardTanh(
+        bit_width=bit_width,
+        scaling_per_output_channel=scaling_per_channel,
+        quant_type=QUANT_TYPE,
+        scaling_impl_type=ACT_SCALING_IMPL_TYPE,
+        scaling_min_val=SCALING_MIN_VAL,
+        restrict_scaling_type=ACT_RESTRICT_SCALING_TYPE,
+        max_val=absolute_act_val,
+        min_val=-absolute_act_val,
+        return_quant_tensor=False)
 
 
 def make_norm_scale(bit_width, absolute_act_val, scaling_per_channel):
-    return quant_nn.QuantHardTanh(bit_width=bit_width, scaling_per_output_channel=scaling_per_channel, quant_type=QUANT_TYPE,
-                                  scaling_impl_type=ACT_SCALING_IMPL_TYPE, scaling_min_val=SCALING_MIN_VAL, restrict_scaling_type=ACT_RESTRICT_SCALING_TYPE,
-                                  max_val=absolute_act_val, min_val=-absolute_act_val, scaling_stats_permute_dims=(1, 0, 2),
-                                  return_quant_tensor=True)
+    return quant_nn.QuantHardTanh(
+        bit_width=bit_width,
+        scaling_per_output_channel=scaling_per_channel,
+        quant_type=QUANT_TYPE,
+        scaling_impl_type=ACT_SCALING_IMPL_TYPE,
+        scaling_min_val=SCALING_MIN_VAL,
+        restrict_scaling_type=ACT_RESTRICT_SCALING_TYPE,
+        max_val=absolute_act_val,
+        min_val=-absolute_act_val,
+        scaling_stats_permute_dims=(1, 0, 2),
+        return_quant_tensor=True)
 
 
 def make_jasper_activation(activation, channels, bit_width, absolute_act_val, scaling_per_channel):
     brevitas_activation = brevitas_activations[activation]
-    return brevitas_activation(bit_width=bit_width, scaling_per_output_channel=scaling_per_channel, quant_type=QUANT_TYPE,
-                               scaling_impl_type=ACT_SCALING_IMPL_TYPE, scaling_min_val=SCALING_MIN_VAL, restrict_scaling_type=ACT_RESTRICT_SCALING_TYPE,
-                               max_val=absolute_act_val, per_channel_broadcastable_shape=(1, channels, 1),
-                               scaling_stats_permute_dims=(1, 0, 2), return_quant_tensor=False)
-
-
-def make_quantconv1d(feat_in, classes, kernel_size, bit_width, scaling_per_channel, bias,
-                     stride=1, padding=0, dilation=1, groups=1):
-
-    return quant_nn.QuantConv1d(in_channels=feat_in, out_channels=classes, kernel_size=kernel_size,
-                                stride=stride, padding=padding, dilation=dilation, groups=groups,
-                                bias=bias,
-                                weight_bit_width=bit_width,
-                                weight_scaling_stats_op=WEIGHT_SCALING_STATS_OP,
-                                weight_scaling_per_output_channel=scaling_per_channel,
-                                weight_quant_type=QUANT_TYPE,
-                                weight_narrow_range=WEIGHT_NARROW_RANGE,
-                                weight_scaling_impl_type=WEIGHT_SCALING_IMPL_TYPE,
-                                weight_scaling_min_val=SCALING_MIN_VAL,
-                                bias_bit_width=bit_width,
-                                bias_quant_type=QUANT_TYPE_BIAS,
-                                bias_narrow_range=BIAS_CONFIGS,
-                                compute_output_scale=BIAS_CONFIGS,
-                                compute_output_bit_width=BIAS_CONFIGS,
-                                return_quant_tensor=False)
+    return brevitas_activation(
+        bit_width=bit_width,
+        scaling_per_output_channel=scaling_per_channel,
+        quant_type=QUANT_TYPE,
+        scaling_impl_type=ACT_SCALING_IMPL_TYPE,
+        scaling_min_val=SCALING_MIN_VAL,
+        restrict_scaling_type=ACT_RESTRICT_SCALING_TYPE,
+        max_val=absolute_act_val,
+        per_channel_broadcastable_shape=(1, channels, 1),
+        scaling_stats_permute_dims=(1, 0, 2),
+        return_quant_tensor=False)
+
+
+def make_quantconv1d(
+        feat_in,
+        classes,
+        kernel_size,
+        bit_width,
+        scaling_per_channel,
+        bias,
+        stride=1,
+        padding=0,
+        dilation=1,
+        groups=1):
+
+    return quant_nn.QuantConv1d(
+        in_channels=feat_in,
+        out_channels=classes,
+        kernel_size=kernel_size,
+        stride=stride,
+        padding=padding,
+        dilation=dilation,
+        groups=groups,
+        bias=bias,
+        weight_bit_width=bit_width,
+        weight_scaling_stats_op=WEIGHT_SCALING_STATS_OP,
+        weight_scaling_per_output_channel=scaling_per_channel,
+        weight_quant_type=QUANT_TYPE,
+        weight_narrow_range=WEIGHT_NARROW_RANGE,
+        weight_scaling_impl_type=WEIGHT_SCALING_IMPL_TYPE,
+        weight_scaling_min_val=SCALING_MIN_VAL,
+        bias_bit_width=bit_width,
+        bias_quant_type=QUANT_TYPE_BIAS,
+        bias_narrow_range=BIAS_CONFIGS,
+        compute_output_scale=BIAS_CONFIGS,
+        compute_output_bit_width=BIAS_CONFIGS,
+        return_quant_tensor=False)
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/dataset.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/dataset.py`

 * *Files 5% similar despite different names*

```diff
@@ -22,15 +22,16 @@
 # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 # SOFTWARE.
 
 import torch
 from torch.utils.data import Dataset
 
-from .manifest import ManifestBase, ManifestEN
+from .manifest import ManifestBase
+from .manifest import ManifestEN
 
 
 def seq_collate_fn(batch, token_pad_value=0):
     """collate batch of audio sig, audio len, tokens, tokens len
 
     Args:
         batch (Optional[FloatTensor], Optional[LongTensor], LongTensor,
@@ -53,16 +54,15 @@
             if sig_len < max_audio_len:
                 pad = (0, max_audio_len - sig_len)
                 sig = torch.nn.functional.pad(sig, pad)
             audio_signal.append(sig)
         tokens_i_len = tokens_i_len.item()
         if tokens_i_len < max_tokens_len:
             pad = (0, max_tokens_len - tokens_i_len)
-            tokens_i = torch.nn.functional.pad(
-                tokens_i, pad, value=token_pad_value)
+            tokens_i = torch.nn.functional.pad(tokens_i, pad, value=token_pad_value)
         tokens.append(tokens_i)
 
     if has_audio:
         audio_signal = torch.stack(audio_signal)
         audio_lengths = torch.stack(audio_lengths)
     else:
         audio_signal, audio_lengths = None, None
@@ -131,14 +131,15 @@
         blank_index: blank character index, default = -1
         unk_index: unk_character index, default = -1
         normalize: whether to normalize transcript text (default): True
         bos_id: Id of beginning of sequence symbol to append if not None
         eos_id: Id of end of sequence symbol to append if not None
         load_audio: Boolean flag indicate whether do or not load audio
     """
+
     def __init__(
             self,
             manifest_filepath,
             labels,
             featurizer,
             max_duration=None,
             min_duration=None,
@@ -149,43 +150,42 @@
             trim=False,
             bos_id=None,
             eos_id=None,
             logger=False,
             load_audio=True,
             manifest_class=ManifestEN):
         m_paths = manifest_filepath.split(',')
-        self.manifest = manifest_class(m_paths, labels,
-                                       max_duration=max_duration,
-                                       min_duration=min_duration,
-                                       max_utts=max_utts,
-                                       blank_index=blank_index,
-                                       unk_index=unk_index,
-                                       normalize=normalize,
-                                       logger=logger)
+        self.manifest = manifest_class(
+            m_paths,
+            labels,
+            max_duration=max_duration,
+            min_duration=min_duration,
+            max_utts=max_utts,
+            blank_index=blank_index,
+            unk_index=unk_index,
+            normalize=normalize,
+            logger=logger)
         self.featurizer = featurizer
         self.trim = trim
         self.eos_id = eos_id
         self.bos_id = bos_id
         self.load_audio = load_audio
         if logger:
             logger.info(
                 "Dataset loaded with {0:.2f} hours. Filtered {1:.2f} "
                 "hours.".format(
-                    self.manifest.duration / 3600,
-                    self.manifest.filtered_duration / 3600))
+                    self.manifest.duration / 3600, self.manifest.filtered_duration / 3600))
 
     def __getitem__(self, index):
         sample = self.manifest[index]
         if self.load_audio:
             duration = sample['duration'] if 'duration' in sample else 0
             offset = sample['offset'] if 'offset' in sample else 0
-            features = self.featurizer.process(sample['audio_filepath'],
-                                               offset=offset,
-                                               duration=duration,
-                                               trim=self.trim)
+            features = self.featurizer.process(
+                sample['audio_filepath'], offset=offset, duration=duration, trim=self.trim)
             f, fl = features, torch.tensor(features.shape[0]).long()
             # f = f / (torch.max(torch.abs(f)) + 1e-5)
         else:
             f, fl = None, None
 
         t, tl = sample["tokens"], len(sample["tokens"])
         if self.bos_id is not None:
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/features.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/features.py`

 * *Files 3% similar despite different names*

```diff
@@ -20,30 +20,30 @@
 # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 # SOFTWARE.
 
 import math
+
 import librosa
 import torch
 import torch.nn as nn
+from torch_stft import STFT
+
 from .perturb import AudioAugmentor
 from .segment import AudioSegment
-from torch_stft import STFT
 
 CONSTANT = 1e-5
 
 
 def normalize_batch(x, seq_len, normalize_type):
     if normalize_type == "per_feature":
-        x_mean = torch.zeros((seq_len.shape[0], x.shape[1]), dtype=x.dtype,
-                             device=x.device)
-        x_std = torch.zeros((seq_len.shape[0], x.shape[1]), dtype=x.dtype,
-                            device=x.device)
+        x_mean = torch.zeros((seq_len.shape[0], x.shape[1]), dtype=x.dtype, device=x.device)
+        x_std = torch.zeros((seq_len.shape[0], x.shape[1]), dtype=x.dtype, device=x.device)
         for i in range(x.shape[0]):
             x_mean[i, :] = x[i, :, :seq_len[i]].mean(dim=1)
             x_std[i, :] = x[i, :, :seq_len[i]].std(dim=1)
         # make sure x_std is not zero
         x_std += CONSTANT
         return (x - x_mean.unsqueeze(2)) / x_std.unsqueeze(2)
     elif normalize_type == "all_features":
@@ -69,29 +69,32 @@
     seq = [x]
     for n in range(1, frame_splicing):
         seq.append(torch.cat([x[:, :, :n], x[:, :, n:]], dim=2))
     return torch.cat(seq, dim=1)
 
 
 class WaveformFeaturizer(object):
+
     def __init__(self, sample_rate=16000, int_values=False, augmentor=None):
         self.augmentor = augmentor if augmentor is not None else \
             AudioAugmentor()
         self.sample_rate = sample_rate
         self.int_values = int_values
 
     def max_augmentation_length(self, length):
         return self.augmentor.max_augmentation_length(length)
 
     def process(self, file_path, offset=0, duration=0, trim=False):
         audio = AudioSegment.from_file(
             file_path,
             target_sr=self.sample_rate,
             int_values=self.int_values,
-            offset=offset, duration=duration, trim=trim)
+            offset=offset,
+            duration=duration,
+            trim=trim)
         return self.process_segment(audio)
 
     def process_segment(self, audio_segment):
         self.augmentor.perturb(audio_segment)
         return torch.tensor(audio_segment.samples, dtype=torch.float)
 
     @classmethod
@@ -100,62 +103,60 @@
             aa = AudioAugmentor.from_config(perturbation_configs)
         else:
             aa = None
 
         sample_rate = input_config.get("sample_rate", 16000)
         int_values = input_config.get("int_values", False)
 
-        return cls(sample_rate=sample_rate, int_values=int_values,
-                   augmentor=aa)
+        return cls(sample_rate=sample_rate, int_values=int_values, augmentor=aa)
 
 
 class FeaturizerFactory(object):
+
     def __init__(self):
         pass
 
     @classmethod
     def from_config(cls, input_cfg, perturbation_configs=None):
-        return WaveformFeaturizer.from_config(
-            input_cfg,
-            perturbation_configs=perturbation_configs)
+        return WaveformFeaturizer.from_config(input_cfg, perturbation_configs=perturbation_configs)
 
 
 class FilterbankFeatures(nn.Module):
     """Featurizer that converts wavs to Mel Spectrograms.
     See AudioToMelSpectrogramPreprocessor for args.
     """
+
     def __init__(
-            self, *,
+            self,
+            *,
             sample_rate=16000,
             n_window_size=320,
             n_window_stride=160,
             window="hann",
             normalize="per_feature",
             n_fft=None,
             preemph=0.97,
             nfilt=64,
             lowfreq=0,
             highfreq=None,
             log=True,
             log_zero_guard_type="add",
-            log_zero_guard_value=2**-24,
+            log_zero_guard_value=2 ** -24,
             dither=CONSTANT,
             pad_to=16,
             max_duration=16.7,
             frame_splicing=1,
             stft_conv=False,
             pad_value=0,
             mag_power=2.,
-            logger=None
-    ):
+            logger=None):
         super(FilterbankFeatures, self).__init__()
-        if (n_window_size is None or n_window_stride is None
-                or not isinstance(n_window_size, int)
-                or not isinstance(n_window_stride, int)
-                or n_window_size <= 0 or n_window_stride <= 0):
+        if (n_window_size is None or n_window_stride is None or
+                not isinstance(n_window_size, int) or not isinstance(n_window_stride, int) or
+                n_window_size <= 0 or n_window_stride <= 0):
             raise ValueError(
                 f"{self} got an invalid value for either n_window_size or "
                 f"n_window_stride. Both must be positive ints.")
         if logger:
             logger.info(f"PADDING: {pad_to}")
         else:
             print(f"PADDING: {pad_to}")
@@ -169,63 +170,60 @@
             if logger:
                 logger.info("STFT using conv")
             else:
                 print("STFT using conv")
 
             # Create helper class to patch forward func for use with AMP
             class STFTPatch(STFT):
+
                 def __init__(self, *params, **kw_params):
                     super(STFTPatch, self).__init__(*params, **kw_params)
 
                 def forward(self, input_data):
                     return super(STFTPatch, self).transform(input_data)[0]
 
-            self.stft = STFTPatch(self.n_fft, self.hop_length,
-                                  self.win_length, window)
+            self.stft = STFTPatch(self.n_fft, self.hop_length, self.win_length, window)
 
         else:
             print("STFT using torch")
             torch_windows = {
                 'hann': torch.hann_window,
                 'hamming': torch.hamming_window,
                 'blackman': torch.blackman_window,
                 'bartlett': torch.bartlett_window,
-                'none': None,
-            }
+                'none': None,}
             window_fn = torch_windows.get(window, None)
-            window_tensor = window_fn(self.win_length,
-                                      periodic=False) if window_fn else None
+            window_tensor = window_fn(self.win_length, periodic=False) if window_fn else None
             self.register_buffer("window", window_tensor)
             self.stft = lambda x: torch.stft(
-                            x, n_fft=self.n_fft,
-                            hop_length=self.hop_length,
-                            win_length=self.win_length,
-                            center=True,
-                            window=self.window.to(dtype=torch.float))
+                x,
+                n_fft=self.n_fft,
+                hop_length=self.hop_length,
+                win_length=self.win_length,
+                center=True,
+                window=self.window.to(dtype=torch.float))
 
         self.normalize = normalize
         self.log = log
         self.dither = dither
         self.frame_splicing = frame_splicing
         self.nfilt = nfilt
         self.preemph = preemph
         self.pad_to = pad_to
         highfreq = highfreq or sample_rate / 2
 
         filterbanks = torch.tensor(
-            librosa.filters.mel(sample_rate, self.n_fft, n_mels=nfilt,
-                                fmin=lowfreq, fmax=highfreq),
+            librosa.filters.mel(sample_rate, self.n_fft, n_mels=nfilt, fmin=lowfreq, fmax=highfreq),
             dtype=torch.float).unsqueeze(0)
         # self.fb = filterbanks
         # self.window = window_tensor
         self.register_buffer("fb", filterbanks)
 
         # Calculate maximum sequence length
-        max_length = self.get_seq_len(
-            torch.tensor(max_duration * sample_rate, dtype=torch.float))
+        max_length = self.get_seq_len(torch.tensor(max_duration * sample_rate, dtype=torch.float))
         max_pad = pad_to - (max_length % pad_to)
         self.max_length = max_length + max_pad
         self.pad_value = pad_value
         self.mag_power = mag_power
 
         # We want to avoid taking the log of zero
         # There are two options: either adding or clamping to a small value
@@ -262,17 +260,15 @@
 
         # dither
         if self.dither > 0:
             x += self.dither * torch.randn_like(x)
 
         # do preemphasis
         if self.preemph is not None:
-            x = torch.cat(
-                (x[:, 0].unsqueeze(1), x[:, 1:] - self.preemph * x[:, :-1]),
-                dim=1)
+            x = torch.cat((x[:, 0].unsqueeze(1), x[:, 1:] - self.preemph * x[:, :-1]), dim=1)
 
         x = self.stft(x)
 
         # get power spectrum
         if self.mag_power != 1.:
             x = x.pow(self.mag_power)
         if not self.stft_conv:
@@ -299,23 +295,19 @@
             x = normalize_batch(x, seq_len, normalize_type=self.normalize)
 
         # mask to zero any values beyond seq_len in batch, pad to multiple of
         # `pad_to` (for efficiency)
         max_len = x.size(-1)
         mask = torch.arange(max_len).to(x.device)
         mask = mask.expand(x.size(0), max_len) >= seq_len.unsqueeze(1)
-        x = x.masked_fill(
-            mask.unsqueeze(1).type(torch.bool).to(device=x.device),
-            self.pad_value)
+        x = x.masked_fill(mask.unsqueeze(1).type(torch.bool).to(device=x.device), self.pad_value)
         del mask
         pad_to = self.pad_to
         if not self.training:
             pad_to = 16
         if pad_to == "max":
-            x = nn.functional.pad(x, (0, self.max_length - x.size(-1)),
-                                  value=self.pad_value)
+            x = nn.functional.pad(x, (0, self.max_length - x.size(-1)), value=self.pad_value)
         elif pad_to > 0:
             pad_amt = x.size(-1) % pad_to
             if pad_amt != 0:
-                x = nn.functional.pad(x, (0, pad_to - pad_amt),
-                                      value=self.pad_value)
+                x = nn.functional.pad(x, (0, pad_to - pad_amt), value=self.pad_value)
         return x
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/manifest.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/manifest.py`

 * *Files 6% similar despite different names*

```diff
@@ -26,25 +26,27 @@
 import json
 import string
 
 from .cleaners import clean_text
 
 
 class ManifestBase:
-    def __init__(self,
-                 manifest_paths,
-                 labels,
-                 max_duration=None,
-                 min_duration=None,
-                 sort_by_duration=False,
-                 max_utts=0,
-                 blank_index=-1,
-                 unk_index=-1,
-                 normalize=True,
-                 logger=None):
+
+    def __init__(
+            self,
+            manifest_paths,
+            labels,
+            max_duration=None,
+            min_duration=None,
+            sort_by_duration=False,
+            max_utts=0,
+            blank_index=-1,
+            unk_index=-1,
+            normalize=True,
+            logger=None):
         self.min_duration = min_duration
         self.max_duration = max_duration
         self.sort_by_duration = sort_by_duration
         self.max_utts = max_utts
         self.blank_index = blank_index
         self.unk_index = unk_index
         self.normalize = normalize
@@ -75,38 +77,35 @@
                 filtered_duration += item['duration']
                 continue
             if normalize:
                 text = self.normalize_text(text, labels, logger=self.logger)
             if not isinstance(text, str):
                 self.logger.warning(
                     "WARNING: Got transcript: {}. It is not a "
-                    "string. Dropping data point".format(text)
-                )
+                    "string. Dropping data point".format(text))
                 filtered_duration += item['duration']
                 continue
             # item['text'] = text
 
             # tokenize transcript text
             item["tokens"] = self.tokenize_transcript(
-                    text, self.labels_map, self.unk_index, self.blank_index)
+                text, self.labels_map, self.unk_index, self.blank_index)
 
             # support files using audio_filename
             if 'audio_filename' in item and 'audio_filepath' not in item:
                 self.logger.warning(
                     "Malformed manifest: The key audio_filepath was not "
-                    "found in the manifest. Using audio_filename instead."
-                )
+                    "found in the manifest. Using audio_filename instead.")
                 item['audio_filepath'] = item['audio_filename']
 
             data.append(item)
             duration += item['duration']
 
             if max_utts > 0 and len(data) >= max_utts:
-                self.logger.info(
-                    'Stop parsing due to max_utts ({})'.format(max_utts))
+                self.logger.info('Stop parsing due to max_utts ({})'.format(max_utts))
                 break
 
         if sort_by_duration:
             data = sorted(data, key=lambda x: x['duration'])
         self._data = data
         self._size = len(data)
         self._duration = duration
@@ -169,27 +168,24 @@
 
     @property
     def data(self):
         return list(self._data)
 
 
 class ManifestEN(ManifestBase):
+
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
 
     @staticmethod
     def normalize_text(text, labels, logger=None):
         # Punctuation to remove
         punctuation = string.punctuation
         # Define punctuation that will be handled by text cleaner
-        punctuation_to_replace = {
-            "+": "plus",
-            "&": "and",
-            "%": "percent"
-        }
+        punctuation_to_replace = {"+": "plus", "&": "and", "%": "percent"}
         for char in punctuation_to_replace:
             punctuation = punctuation.replace(char, "")
         # We might also want to consider:
         # @ -> at
         # -> number, pound, hashtag
         # ~ -> tilde
         # _ -> underscore
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/perturb.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/perturb.py`

 * *Files 3% similar despite different names*

```diff
@@ -29,22 +29,24 @@
 from scipy import signal
 
 from .manifest import ManifestEN
 from .segment import AudioSegment
 
 
 class Perturbation(object):
+
     def max_augmentation_length(self, length):
         return length
 
     def perturb(self, data):
         raise NotImplementedError
 
 
 class SpeedPerturbation(Perturbation):
+
     def __init__(self, min_speed_rate=0.85, max_speed_rate=1.15, rng=None):
         self._min_rate = min_speed_rate
         self._max_rate = max_speed_rate
         self._rng = random.Random() if rng is None else rng
 
     def max_augmentation_length(self, length):
         return length * self._max_rate
@@ -54,40 +56,42 @@
         if speed_rate <= 0:
             raise ValueError("speed_rate should be greater than zero.")
         # print("DEBUG: speed:", speed_rate)
         data._samples = librosa.effects.time_stretch(data._samples, speed_rate)
 
 
 class GainPerturbation(Perturbation):
+
     def __init__(self, min_gain_dbfs=-10, max_gain_dbfs=10, rng=None):
         self._min_gain_dbfs = min_gain_dbfs
         self._max_gain_dbfs = max_gain_dbfs
         self._rng = random.Random() if rng is None else rng
 
     def perturb(self, data):
         gain = self._rng.uniform(self._min_gain_dbfs, self._max_gain_dbfs)
         # print("DEBUG: gain:", gain)
         data._samples = data._samples * (10. ** (gain / 20.))
 
 
 class ImpulsePerturbation(Perturbation):
+
     def __init__(self, manifest_path=None, rng=None):
         self._manifest = ManifestEN(manifest_path)
         self._rng = random.Random() if rng is None else rng
 
     def perturb(self, data):
         impulse_record = self._rng.sample(self._manifest.data, 1)[0]
-        impulse = AudioSegment.from_file(impulse_record['audio_filepath'],
-                                         target_sr=data.sample_rate)
+        impulse = AudioSegment.from_file(
+            impulse_record['audio_filepath'], target_sr=data.sample_rate)
         # print("DEBUG: impulse:", impulse_record['audio_filepath'])
-        data._samples = signal.fftconvolve(
-            data.samples, impulse.samples, "full")
+        data._samples = signal.fftconvolve(data.samples, impulse.samples, "full")
 
 
 class ShiftPerturbation(Perturbation):
+
     def __init__(self, min_shift_ms=-5.0, max_shift_ms=5.0, rng=None):
         self._min_shift_ms = min_shift_ms
         self._max_shift_ms = max_shift_ms
         self._rng = random.Random() if rng is None else rng
 
     def perturb(self, data):
         shift_ms = self._rng.uniform(self._min_shift_ms, self._max_shift_ms)
@@ -101,52 +105,50 @@
             data._samples[:-shift_samples] = 0
         elif shift_samples > 0:
             data._samples[:-shift_samples] = data._samples[shift_samples:]
             data._samples[-shift_samples:] = 0
 
 
 class NoisePerturbation(Perturbation):
-    def __init__(self, manifest_path=None, min_snr_db=40, max_snr_db=50,
-                 max_gain_db=300.0, rng=None):
+
+    def __init__(
+            self, manifest_path=None, min_snr_db=40, max_snr_db=50, max_gain_db=300.0, rng=None):
         self._manifest = ManifestEN(manifest_path)
         self._rng = random.Random() if rng is None else rng
         self._min_snr_db = min_snr_db
         self._max_snr_db = max_snr_db
         self._max_gain_db = max_gain_db
 
     def perturb(self, data):
         snr_db = self._rng.uniform(self._min_snr_db, self._max_snr_db)
         noise_record = self._rng.sample(self._manifest.data, 1)[0]
-        noise = AudioSegment.from_file(noise_record['audio_filepath'],
-                                       target_sr=data.sample_rate)
-        noise_gain_db = min(data.rms_db - noise.rms_db - snr_db,
-                            self._max_gain_db)
+        noise = AudioSegment.from_file(noise_record['audio_filepath'], target_sr=data.sample_rate)
+        noise_gain_db = min(data.rms_db - noise.rms_db - snr_db, self._max_gain_db)
         # print("DEBUG: noise:", snr_db, noise_gain_db, noise_record[
         # 'audio_filepath'])
 
         # calculate noise segment to use
         start_time = self._rng.uniform(0.0, noise.duration - data.duration)
-        noise.subsegment(start_time=start_time,
-                         end_time=start_time + data.duration)
+        noise.subsegment(start_time=start_time, end_time=start_time + data.duration)
 
         # adjust gain for snr purposes and superimpose
         noise.gain_db(noise_gain_db)
         data._samples = data._samples + noise.samples
 
 
 perturbation_types = {
     "speed": SpeedPerturbation,
     "gain": GainPerturbation,
     "impulse": ImpulsePerturbation,
     "shift": ShiftPerturbation,
-    "noise": NoisePerturbation
-}
+    "noise": NoisePerturbation}
 
 
 class AudioAugmentor(object):
+
     def __init__(self, perturbations=None, rng=None):
         self._rng = random.Random() if rng is None else rng
         self._pipeline = perturbations if perturbations is not None else []
 
     def perturb(self, segment):
         for (prob, p) in self._pipeline:
             if self._rng.random() < prob:
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/quartznet.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/quartznet.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,29 +12,31 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Tuple, Optional, List
+from typing import List, Optional, Tuple
 
 import torch
-import torch.nn as nn
 from torch import Tensor
-from .common import *
-from brevitas.quant_tensor import QuantTensor
+import torch.nn as nn
+
 from brevitas.nn import QuantConv1d
-from brevitas.nn.utils import mul_add_from_bn, rename_state_dict_by_postfix
+from brevitas.nn.utils import mul_add_from_bn
+from brevitas.nn.utils import rename_state_dict_by_postfix
+from brevitas.quant_tensor import QuantTensor
+
+from .common import *
 
 jasper_activations = {
     "hardtanh": nn.Hardtanh,
     "relu": nn.ReLU,
-    "selu": nn.SELU,
-}
+    "selu": nn.SELU,}
 
 
 def init_weights(m, mode='xavier_uniform'):
     if isinstance(m, MaskedConv1d):
         init_weights(m.conv, mode)
     if isinstance(m, nn.Conv1d):
         if mode == 'xavier_uniform':
@@ -64,46 +66,65 @@
         return (dilation * kernel_size) // 2 - 1
     return kernel_size // 2
 
 
 class MaskedConv1d(nn.Module):
     __constants__ = ["use_conv_mask", "real_out_channels", "heads"]
 
-    def __init__(self, in_channels, out_channels, kernel_size, scaling_per_channel, bit_width,
-                 stride=1, padding=0, dilation=1, groups=1, heads=-1, bias=False, use_mask=True):
+    def __init__(
+            self,
+            in_channels,
+            out_channels,
+            kernel_size,
+            scaling_per_channel,
+            bit_width,
+            stride=1,
+            padding=0,
+            dilation=1,
+            groups=1,
+            heads=-1,
+            bias=False,
+            use_mask=True):
         super(MaskedConv1d, self).__init__()
 
         if not (heads == -1 or groups == in_channels):
             raise ValueError("Only use heads for depthwise convolutions")
 
         self.real_out_channels = out_channels
         if heads != -1:
             in_channels = heads
             out_channels = heads
             groups = heads
-        self.conv = make_quantconv1d(in_channels, out_channels, kernel_size, bias=bias,
-                                     stride=stride, padding=padding, dilation=dilation,
-                                     groups=groups, scaling_per_channel=scaling_per_channel, bit_width=bit_width)
+        self.conv = make_quantconv1d(
+            in_channels,
+            out_channels,
+            kernel_size,
+            bias=bias,
+            stride=stride,
+            padding=padding,
+            dilation=dilation,
+            groups=groups,
+            scaling_per_channel=scaling_per_channel,
+            bit_width=bit_width)
         self.channelwise_separable = (in_channels == out_channels) and (in_channels == groups)
         self.use_mask = use_mask
         self.heads = heads
 
     def get_seq_len(self, lens):
-        return ((lens + 2 * self.conv.padding[0] - self.conv.dilation[0] * (
-                self.conv.kernel_size[0] - 1) - 1) / self.conv.stride[0] + 1)
+        return ((
+            lens + 2 * self.conv.padding[0] - self.conv.dilation[0] *
+            (self.conv.kernel_size[0] - 1) - 1) / self.conv.stride[0] + 1)
 
     def forward(self, x, lens):
         if self.use_mask:
             lens = lens.to(dtype=torch.long)
             max_len = x.size(2)
             mask = torch.arange(max_len).to(lens.device) \
                        .expand(len(lens), max_len) >= lens.unsqueeze(1)
-            x = x.masked_fill(
-                mask.unsqueeze(1).to(device=x.device), 0
-            )
+            x = x.masked_fill(mask.unsqueeze(1).to(device=x.device), 0)
             # del mask
             lens = self.get_seq_len(lens)
 
         sh = x.shape
         if self.heads != -1:
             x = x.view(-1, self.heads, sh[-1])
 
@@ -134,24 +155,40 @@
 
         return x
 
 
 class JasperBlock(nn.Module):
     __constants__ = ["conv_mask", "separable", "residual_mode", "res", "mconv"]
 
-    def __init__(self, inplanes, planes, bit_width,
-                 absolute_act_val,
-                 activation_inner_scaling_per_output_channel, activation_other_scaling_per_output_channel,
-                 weight_scaling_per_output_channel,
-                 repeat=3, kernel_size=11, stride=1,
-                 dilation=1, padding='same', dropout=0.2, activation=None,
-                 residual=True, groups=1, separable=False,
-                 heads=-1, normalization="batch",
-                 norm_groups=1, residual_mode='add',
-                 residual_panes=[], conv_mask=False, fused_bn=False):
+    def __init__(
+            self,
+            inplanes,
+            planes,
+            bit_width,
+            absolute_act_val,
+            activation_inner_scaling_per_output_channel,
+            activation_other_scaling_per_output_channel,
+            weight_scaling_per_output_channel,
+            repeat=3,
+            kernel_size=11,
+            stride=1,
+            dilation=1,
+            padding='same',
+            dropout=0.2,
+            activation=None,
+            residual=True,
+            groups=1,
+            separable=False,
+            heads=-1,
+            normalization="batch",
+            norm_groups=1,
+            residual_mode='add',
+            residual_panes=[],
+            conv_mask=False,
+            fused_bn=False):
         super(JasperBlock, self).__init__()
 
         if padding != "same":
             raise ValueError("currently only 'same' padding is supported")
         self.fused_bn = fused_bn
         padding_val = get_same_padding(kernel_size[0], stride[0], dilation[0])
         self.conv_mask = conv_mask
@@ -159,156 +196,213 @@
         self.residual_mode = residual_mode
         self.conv_module_to_merge = []
         inplanes_loop = inplanes
         conv = nn.ModuleList()
         self.norm_depthwise = nn.ModuleList()
         for _ in range(repeat - 1):
             if separable:
-                self.norm_depthwise.extend(
-                    [make_norm_scale(bit_width=bit_width,
-                                     absolute_act_val=absolute_act_val,
-                                     scaling_per_channel=activation_other_scaling_per_output_channel)]
-                )
-            conv.extend(self._get_conv_bn_layer(
+                self.norm_depthwise.extend([
+                    make_norm_scale(
+                        bit_width=bit_width,
+                        absolute_act_val=absolute_act_val,
+                        scaling_per_channel=activation_other_scaling_per_output_channel)])
+            conv.extend(
+                self._get_conv_bn_layer(
+                    inplanes_loop,
+                    planes,
+                    kernel_size=kernel_size,
+                    stride=stride,
+                    dilation=dilation,
+                    padding=padding_val,
+                    groups=groups,
+                    heads=heads,
+                    separable=separable,
+                    normalization=normalization,
+                    norm_groups=norm_groups,
+                    bit_width=bit_width,
+                    scaling_per_channel=weight_scaling_per_output_channel))
+
+            conv.extend(
+                self._get_act_dropout_layer(
+                    drop_prob=dropout,
+                    activation=activation,
+                    channels=planes,
+                    bit_width=bit_width,
+                    absolute_act_val=absolute_act_val,
+                    scaling_per_channel=activation_inner_scaling_per_output_channel))
+
+            inplanes_loop = planes
+
+        if separable:
+            self.norm_depthwise.extend([
+                make_norm_scale(
+                    bit_width=bit_width,
+                    absolute_act_val=absolute_act_val,
+                    scaling_per_channel=activation_other_scaling_per_output_channel)])
+        conv.extend(
+            self._get_conv_bn_layer(
                 inplanes_loop,
                 planes,
                 kernel_size=kernel_size,
                 stride=stride,
                 dilation=dilation,
                 padding=padding_val,
                 groups=groups,
                 heads=heads,
                 separable=separable,
                 normalization=normalization,
                 norm_groups=norm_groups,
                 bit_width=bit_width,
                 scaling_per_channel=weight_scaling_per_output_channel))
 
-            conv.extend(self._get_act_dropout_layer(
-                drop_prob=dropout,
-                activation=activation,
-                channels=planes,
-                bit_width=bit_width,
-                absolute_act_val=absolute_act_val,
-                scaling_per_channel=activation_inner_scaling_per_output_channel))
-
-            inplanes_loop = planes
-
-        if separable:
-            self.norm_depthwise.extend(
-                [make_norm_scale(bit_width=bit_width,
-                                 absolute_act_val=absolute_act_val,
-                                 scaling_per_channel=activation_other_scaling_per_output_channel)]
-            )
-        conv.extend(self._get_conv_bn_layer(
-            inplanes_loop,
-            planes,
-            kernel_size=kernel_size,
-            stride=stride,
-            dilation=dilation,
-            padding=padding_val,
-            groups=groups,
-            heads=heads,
-            separable=separable,
-            normalization=normalization,
-            norm_groups=norm_groups,
-            bit_width=bit_width,
-            scaling_per_channel=weight_scaling_per_output_channel))
-
         self.mconv = conv
 
         res_panes = residual_panes.copy()
         self.dense_residual = residual
 
         if residual:
             res_list = nn.ModuleList()
             if len(residual_panes) == 0:
                 res_panes = [inplanes]
                 self.dense_residual = False
             for ip in res_panes:
-                res_list.append(nn.ModuleList(self._get_conv_bn_layer(
-                    ip,
-                    planes,
-                    kernel_size=1,
-                    normalization=normalization,
-                    norm_groups=norm_groups,
-                    bit_width=bit_width,
-                    scaling_per_channel=weight_scaling_per_output_channel)))
+                res_list.append(
+                    nn.ModuleList(
+                        self._get_conv_bn_layer(
+                            ip,
+                            planes,
+                            kernel_size=1,
+                            normalization=normalization,
+                            norm_groups=norm_groups,
+                            bit_width=bit_width,
+                            scaling_per_channel=weight_scaling_per_output_channel)))
             self.res = res_list
             self.quant_normalization = make_norm_scale(
-                bit_width=bit_width, absolute_act_val=absolute_act_val,
+                bit_width=bit_width,
+                absolute_act_val=absolute_act_val,
                 scaling_per_channel=activation_other_scaling_per_output_channel)
         else:
             self.res = None
             self.quant_normalization = None
 
         self.mout = nn.Sequential(
             *self._get_act_dropout_layer(
                 drop_prob=dropout,
                 activation=activation,
                 channels=inplanes_loop,
                 absolute_act_val=absolute_act_val,
                 scaling_per_channel=activation_other_scaling_per_output_channel,
-                bit_width=bit_width)
-        )
+                bit_width=bit_width))
 
-    def _get_conv(self, in_channels, out_channels, bit_width, scaling_per_channel, kernel_size=11,
-                  stride=1, dilation=1, padding=0, bias=False,
-                  groups=1, heads=-1, separable=False):
+    def _get_conv(
+            self,
+            in_channels,
+            out_channels,
+            bit_width,
+            scaling_per_channel,
+            kernel_size=11,
+            stride=1,
+            dilation=1,
+            padding=0,
+            bias=False,
+            groups=1,
+            heads=-1,
+            separable=False):
         use_mask = self.conv_mask
         if use_mask:
-            return MaskedConv1d(in_channels, out_channels, kernel_size,
-                                stride=stride,
-                                dilation=dilation, padding=padding, bias=bias,
-                                groups=groups, heads=heads,
-                                use_mask=use_mask, scaling_per_channel=scaling_per_channel, bit_width=bit_width)
+            return MaskedConv1d(
+                in_channels,
+                out_channels,
+                kernel_size,
+                stride=stride,
+                dilation=dilation,
+                padding=padding,
+                bias=bias,
+                groups=groups,
+                heads=heads,
+                use_mask=use_mask,
+                scaling_per_channel=scaling_per_channel,
+                bit_width=bit_width)
         else:
-            return make_quantconv1d(in_channels, out_channels, kernel_size, stride=stride,
-                                    dilation=dilation, padding=padding, groups=groups, bias=bias,
-                                    scaling_per_channel=scaling_per_channel, bit_width=bit_width)
-
-    def _get_conv_bn_layer(self, in_channels, out_channels, bit_width, scaling_per_channel, kernel_size=11,
-                           stride=1, dilation=1, padding=0, bias=False,
-                           groups=1, heads=-1, separable=False,
-                           normalization="batch", norm_groups=1):
+            return make_quantconv1d(
+                in_channels,
+                out_channels,
+                kernel_size,
+                stride=stride,
+                dilation=dilation,
+                padding=padding,
+                groups=groups,
+                bias=bias,
+                scaling_per_channel=scaling_per_channel,
+                bit_width=bit_width)
+
+    def _get_conv_bn_layer(
+            self,
+            in_channels,
+            out_channels,
+            bit_width,
+            scaling_per_channel,
+            kernel_size=11,
+            stride=1,
+            dilation=1,
+            padding=0,
+            bias=False,
+            groups=1,
+            heads=-1,
+            separable=False,
+            normalization="batch",
+            norm_groups=1):
         if norm_groups == -1:
             norm_groups = out_channels
 
         if separable:
             layers = [
-                self._get_conv(in_channels,
-                               in_channels,
-                               kernel_size=kernel_size,
-                               stride=stride,
-                               dilation=dilation, padding=padding,
-                               groups=in_channels, heads=heads, bias=bias,
-                               scaling_per_channel=scaling_per_channel, bit_width=bit_width),
-                self._get_conv(in_channels, out_channels, kernel_size=1,
-                               stride=1,
-                               dilation=1, padding=0, groups=groups, bias=bias,
-                               scaling_per_channel=scaling_per_channel, bit_width=bit_width)
-            ]
+                self._get_conv(
+                    in_channels,
+                    in_channels,
+                    kernel_size=kernel_size,
+                    stride=stride,
+                    dilation=dilation,
+                    padding=padding,
+                    groups=in_channels,
+                    heads=heads,
+                    bias=bias,
+                    scaling_per_channel=scaling_per_channel,
+                    bit_width=bit_width),
+                self._get_conv(
+                    in_channels,
+                    out_channels,
+                    kernel_size=1,
+                    stride=1,
+                    dilation=1,
+                    padding=0,
+                    groups=groups,
+                    bias=bias,
+                    scaling_per_channel=scaling_per_channel,
+                    bit_width=bit_width)]
         else:
             layers = [
-                self._get_conv(in_channels, out_channels, kernel_size=kernel_size,
-                               scaling_per_channel=scaling_per_channel, bit_width=bit_width,
-                               stride=stride, bias=bias,
-                               dilation=dilation, padding=padding,
-                               groups=groups)
-            ]
+                self._get_conv(
+                    in_channels,
+                    out_channels,
+                    kernel_size=kernel_size,
+                    scaling_per_channel=scaling_per_channel,
+                    bit_width=bit_width,
+                    stride=stride,
+                    bias=bias,
+                    dilation=dilation,
+                    padding=padding,
+                    groups=groups)]
 
         if normalization == "group":
-            layers.append(nn.GroupNorm(
-                num_groups=norm_groups, num_channels=out_channels))
+            layers.append(nn.GroupNorm(num_groups=norm_groups, num_channels=out_channels))
         elif normalization == "instance":
-            layers.append(nn.GroupNorm(
-                num_groups=out_channels, num_channels=out_channels))
+            layers.append(nn.GroupNorm(num_groups=out_channels, num_channels=out_channels))
         elif normalization == "layer":
-            layers.append(nn.GroupNorm(
-                num_groups=1, num_channels=out_channels))
+            layers.append(nn.GroupNorm(num_groups=1, num_channels=out_channels))
         elif normalization == "batch":
             if self.fused_bn:
                 self.conv_module_to_merge.append(layers[-1])
                 layers.append(nn.Identity())
             else:
                 layers.append(nn.BatchNorm1d(out_channels, eps=1e-3, momentum=0.1))
         else:
@@ -316,22 +410,32 @@
                 f"Normalization method ({normalization}) does not match"
                 f" one of [batch, layer, group, instance].")
 
         if groups > 1:
             layers.append(GroupShuffle(groups, out_channels))
         return layers
 
-    def _get_act_dropout_layer(self, channels, bit_width, absolute_act_val, scaling_per_channel, drop_prob=0.2, activation=None):
+    def _get_act_dropout_layer(
+            self,
+            channels,
+            bit_width,
+            absolute_act_val,
+            scaling_per_channel,
+            drop_prob=0.2,
+            activation=None):
         if activation is None:
             raise Exception("Activation required")
         layers = [
-            make_jasper_activation(activation, channels, bit_width=bit_width, absolute_act_val=absolute_act_val,
-                                   scaling_per_channel=scaling_per_channel),
-            nn.Dropout(p=drop_prob)
-        ]
+            make_jasper_activation(
+                activation,
+                channels,
+                bit_width=bit_width,
+                absolute_act_val=absolute_act_val,
+                scaling_per_channel=scaling_per_channel),
+            nn.Dropout(p=drop_prob)]
         return layers
 
     def forward(self, input_: Tuple[List[Tensor], Optional[Tensor]]):
         # type: (Tuple[List[Tensor], Optional[Tensor]]) -> Tuple[List[Tensor], Optional[Tensor]] # nopep8
         lens_orig = None
         xs = input_[0]
         if len(input_) == 2:
@@ -384,62 +488,48 @@
         out = self.mout(out)
         if self.res is not None and self.dense_residual:
             return xs + [out], lens
 
         return [out], lens
 
     def _load_from_state_dict(
-            self,
-            state_dict,
-            prefix,
-            local_metadata,
-            strict,
-            missing_keys,
-            unexpected_keys,
+            self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys,
             error_msgs):
         if not self.conv_mask:
             rename_state_dict_by_postfix('conv.weight', 'weight', state_dict)
         if self.fused_bn:
             self.fuse_bn(state_dict, prefix)
         super(JasperBlock, self)._load_from_state_dict(
-            state_dict,
-            prefix,
-            local_metadata,
-            strict,
-            missing_keys,
-            unexpected_keys,
-            error_msgs)
+            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
         # fix pretrained models with declared but unused extra quantization layers
         extra_k = 'quant_normalization'
         is_prefix_to_fix = any([prefix == 'encoder.' + p for p in ["0.", "16.", "17."]])
         if is_prefix_to_fix:
             for i, k in enumerate(unexpected_keys):
                 if extra_k in k:
                     del unexpected_keys[i]
 
-    def fuse_bn(self, state_dict,
-            prefix):
+    def fuse_bn(self, state_dict, prefix):
         index = 0
         flag = False
         keys_to_check = []
         keys_to_delete = []
         for k in state_dict.keys():
             if k.startswith(prefix):
                 keys_to_check.append(k)
                 if k.split('.')[-1] == 'running_mean':
                     flag = True
 
-
         if flag:
             for name in keys_to_check:
                 prefix_long = name.split('.')[:-1]
                 if name.split('.')[-1] == "running_mean":
                     bn_prefix = '.'.join(prefix_long)
                     module_number = int(prefix_long[-1])
-                    conv_name = prefix_long[:-1] + [str(module_number-1)]
+                    conv_name = prefix_long[:-1] + [str(module_number - 1)]
                     if self.conv_mask:
                         conv_name = conv_name + ['conv']
                     conv_name = '.'.join(conv_name)
                     conv_mod = self.conv_module_to_merge[index]
                     index = index + 1
                     bn_weight_key = '.'.join([bn_prefix, 'weight'])
                     bn_bias_key = '.'.join([bn_prefix, 'bias'])
@@ -479,9 +569,7 @@
                     # Get rid of statistics after using them
                 else:
                     state_dict[name] = state_dict[name]
         for k in list(state_dict.keys()):
             if k in keys_to_delete:
                 del state_dict[k]
         assert len(self.conv_module_to_merge) == index
-
-
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/segment.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/segment.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,16 +35,15 @@
     :param samples: Audio samples [num_samples x num_channels].
     :type samples: ndarray.float32
     :param sample_rate: Audio sample rate.
     :type sample_rate: int
     :raises TypeError: If the sample data type is not float or int.
     """
 
-    def __init__(self, samples, sample_rate, target_sr=None, trim=False,
-                 trim_db=60):
+    def __init__(self, samples, sample_rate, target_sr=None, trim=False, trim_db=60):
         """Create audio segment from samples.
         Samples are convert float32 internally, with int scaled to [-1, 1].
         """
         samples = self._convert_samples_to_float32(samples)
         if target_sr is not None and target_sr != sample_rate:
             samples = librosa.core.resample(samples, sample_rate, target_sr)
             sample_rate = target_sr
@@ -69,17 +68,18 @@
 
     def __ne__(self, other):
         """Return whether two objects are unequal."""
         return not self.__eq__(other)
 
     def __str__(self):
         """Return human-readable representation of segment."""
-        return ("%s: num_samples=%d, sample_rate=%d, duration=%.2fsec, "
-                "rms=%.2fdB" % (type(self), self.num_samples, self.sample_rate,
-                                self.duration, self.rms_db))
+        return (
+            "%s: num_samples=%d, sample_rate=%d, duration=%.2fsec, "
+            "rms=%.2fdB" %
+            (type(self), self.num_samples, self.sample_rate, self.duration, self.rms_db))
 
     @staticmethod
     def _convert_samples_to_float32(samples):
         """Convert sample type to float32.
         Audio sample type is usually integer or float-point.
         Integers will be scaled to [-1, 1] in float32.
         """
@@ -90,16 +90,16 @@
         elif samples.dtype in np.sctypes['float']:
             pass
         else:
             raise TypeError("Unsupported sample type: %s." % samples.dtype)
         return float32_samples
 
     @classmethod
-    def from_file(cls, filename, target_sr=None, int_values=False, offset=0,
-                  duration=0, trim=False):
+    def from_file(
+            cls, filename, target_sr=None, int_values=False, offset=0, duration=0, trim=False):
         """
         Load a file supported by librosa and return as an AudioSegment.
         :param filename: path of file to load
         :param target_sr: the desired sample rate
         :param int_values: if true, load samples as 32-bit integers
         :param offset: offset in seconds when loading audio
         :param duration: duration in seconds when loading audio
@@ -115,19 +115,15 @@
             else:
                 samples = f.read(dtype=dtype)
 
         samples = samples.transpose()
         return cls(samples, sample_rate, target_sr=target_sr, trim=trim)
 
     @classmethod
-    def segment_from_file(cls,
-                          filename,
-                          target_sr=None,
-                          n_segments=0,
-                          trim=False):
+    def segment_from_file(cls, filename, target_sr=None, n_segments=0, trim=False):
         """Grabs n_segments number of samples from filename randomly from the
         file as opposed to at a specified offset.
         """
         with sf.SoundFile(filename, 'r') as f:
             sample_rate = f.samplerate
             if n_segments > 0 and len(f) > n_segments:
                 max_audio_start = len(f) - n_segments
@@ -167,17 +163,16 @@
     def pad(self, pad_size, symmetric=False):
         """Add zero padding to the sample. The pad size is given in number
         of samples.
         If symmetric=True, `pad_size` will be added to both sides. If false,
         `pad_size`
         zeros will be added only to the end.
         """
-        self._samples = np.pad(self._samples,
-                               (pad_size if symmetric else 0, pad_size),
-                               mode='constant')
+        self._samples = np.pad(
+            self._samples, (pad_size if symmetric else 0, pad_size), mode='constant')
 
     def subsegment(self, start_time=None, end_time=None):
         """Cut the AudioSegment between given boundaries.
         Note that this is an in-place transformation.
         :param start_time: Beginning of subsegment in seconds.
         :type start_time: float
         :param end_time: End of subsegment in seconds.
@@ -192,20 +187,19 @@
             start_time = self.duration + start_time
         if end_time < 0.0:
             end_time = self.duration + end_time
         if start_time < 0.0:
             raise ValueError("The slice start position (%f s) is out of "
                              "bounds." % start_time)
         if end_time < 0.0:
-            raise ValueError(
-                "The slice end position (%f s) is out of bounds." %
-                end_time)
+            raise ValueError("The slice end position (%f s) is out of bounds." % end_time)
         if start_time > end_time:
-            raise ValueError("The slice start position (%f s) is later than "
-                             "the end position (%f s)." % (
-                                 start_time, end_time))
+            raise ValueError(
+                "The slice start position (%f s) is later than "
+                "the end position (%f s)." % (start_time, end_time))
         if end_time > self.duration:
-            raise ValueError("The slice end position (%f s) is out of bounds "
-                             "(> %f s)" % (end_time, self.duration))
+            raise ValueError(
+                "The slice end position (%f s) is out of bounds "
+                "(> %f s)" % (end_time, self.duration))
         start_sample = int(round(start_time * self._sample_rate))
         end_sample = int(round(end_time * self._sample_rate))
         self._samples = self._samples[start_sample:end_sample]
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/parts/spectr_augment.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/parts/spectr_augment.py`

 * *Files 18% similar despite different names*

```diff
@@ -30,22 +30,16 @@
 
     params:
     freq_masks - how many frequency segments should be cut
     time_masks - how many time segments should be cut
     freq_width - maximum number of frequencies to be cut in one segment
     time_width - maximum number of time steps to be cut in one segment
     """
-    def __init__(
-        self,
-        freq_masks=0,
-        time_masks=0,
-        freq_width=10,
-        time_width=10,
-        rng=None
-    ):
+
+    def __init__(self, freq_masks=0, time_masks=0, freq_width=10, time_width=10, rng=None):
         super(SpecAugment, self).__init__()
 
         self._rng = random.Random() if rng is None else rng
 
         self.freq_masks = freq_masks
         self.time_masks = time_masks
 
@@ -56,29 +50,26 @@
     def forward(self, x):
         sh = x.shape
 
         mask = torch.zeros(x.shape).byte()
 
         for idx in range(sh[0]):
             for i in range(self.freq_masks):
-                x_left = int(self._rng.uniform(
-                    0, sh[1] - self.freq_width))
+                x_left = int(self._rng.uniform(0, sh[1] - self.freq_width))
 
                 w = int(self._rng.uniform(0, self.freq_width))
 
                 mask[idx, x_left:x_left + w, :] = 1
 
             for i in range(self.time_masks):
-                y_left = int(self._rng.uniform(
-                    0, sh[2] - self.time_width))
+                y_left = int(self._rng.uniform(0, sh[2] - self.time_width))
 
                 w = int(self._rng.uniform(0, self.time_width))
 
-                mask[idx, :,
-                     y_left:y_left + w] = 1
+                mask[idx, :, y_left:y_left + w] = 1
 
         x = x.masked_fill(mask.type(torch.bool).to(device=x.device), 0)
 
         return x
 
 
 class SpecCutout(nn.Module):
@@ -87,21 +78,16 @@
     as described in (https://arxiv.org/abs/1708.04552).
 
     params:
     rect_masks - how many rectangular masks should be cut
     rect_freq - maximum size of cut rectangles along the frequency dimension
     rect_time - maximum size of cut rectangles along the time dimension
     """
-    def __init__(
-        self,
-        rect_masks=0,
-        rect_time=5,
-        rect_freq=20,
-        rng=None
-    ):
+
+    def __init__(self, rect_masks=0, rect_time=5, rect_freq=20, rng=None):
         super(SpecCutout, self).__init__()
 
         self._rng = random.Random() if rng is None else rng
 
         self.rect_masks = rect_masks
         self.rect_time = rect_time
         self.rect_freq = rect_freq
@@ -110,21 +96,18 @@
     def forward(self, x):
         sh = x.shape
 
         mask = torch.zeros(x.shape).byte()
 
         for idx in range(sh[0]):
             for i in range(self.rect_masks):
-                rect_x = int(self._rng.uniform(
-                    0, sh[1] - self.rect_freq))
-                rect_y = int(self._rng.uniform(
-                    0, sh[2] - self.rect_time))
+                rect_x = int(self._rng.uniform(0, sh[1] - self.rect_freq))
+                rect_y = int(self._rng.uniform(0, sh[2] - self.rect_time))
 
                 w_x = int(self._rng.uniform(0, self.rect_time))
                 w_y = int(self._rng.uniform(0, self.rect_freq))
 
-                mask[idx, rect_x:rect_x + w_x,
-                     rect_y:rect_y + w_y] = 1
+                mask[idx, rect_x:rect_x + w_x, rect_y:rect_y + w_y] = 1
 
         x = x.masked_fill(mask.type(torch.bool).to(device=x.device), 0)
 
         return x
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet/quartznet.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet/quartznet.py`

 * *Files 3% similar despite different names*

```diff
@@ -12,24 +12,25 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from typing import Optional
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
-from typing import Optional
 
 from .audio_preprocessing import AudioToMelSpectrogramPreprocessor
-from .parts.quartznet import JasperBlock, init_weights
-from .parts.common import *
 from .greedy_ctc_decoder import GreedyCTCDecoder
+from .parts.common import *
+from .parts.quartznet import init_weights
+from .parts.quartznet import JasperBlock
 
 
 class JasperEncoder(nn.Module):
     """
     Jasper Encoder creates the pre-processing (prologue), Jasper convolution
     block, and the first 3 post-processing (epilogue) layers as described in
     Jasper (https://arxiv.org/abs/1904.03288)
@@ -83,15 +84,16 @@
         init_mode (str): Describes how neural network parameters are
             initialized. Options are ['xavier_uniform', 'xavier_normal',
             'kaiming_uniform','kaiming_normal'].
             Defaults to "xavier_uniform".
     """
 
     def __init__(
-            self, *,
+            self,
+            *,
             jasper,
             outer_bit_width,
             inner_bit_width,
             weight_scaling_per_output_channel,
             absolute_act_val,
             activation_inner_scaling_per_output_channel,
             activation_other_scaling_per_output_channel,
@@ -100,16 +102,15 @@
             fused_bn=False,
             normalization_mode="batch",
             residual_mode="add",
             norm_groups=-1,
             conv_mask=True,
             frame_splicing=1,
             init_mode='xavier_uniform',
-            **kwargs
-    ):
+            **kwargs):
         nn.Module.__init__(self)
 
         feat_in = feat_in * frame_splicing
 
         residual_panes = []
         encoder_layers = []
         self.dense_residual = False
@@ -125,38 +126,40 @@
                 residual_panes.append(feat_in)
                 dense_res = residual_panes
                 self.dense_residual = True
             groups = lcfg.get('groups', 1)
             separable = lcfg.get('separable', False)
             heads = lcfg.get('heads', -1)
             encoder_layers.append(
-                JasperBlock(feat_in,
-                            lcfg['filters'],
-                            repeat=lcfg['repeat'],
-                            kernel_size=lcfg['kernel'],
-                            stride=lcfg['stride'],
-                            dilation=lcfg['dilation'],
-                            dropout=lcfg['dropout'],
-                            residual=lcfg['residual'],
-                            groups=groups,
-                            fused_bn=fused_bn,
-                            separable=separable,
-                            heads=heads,
-                            residual_mode=residual_mode,
-                            normalization=normalization_mode,
-                            norm_groups=norm_groups,
-                            activation=activation,
-                            residual_panes=dense_res,
-                            conv_mask=conv_mask,
-                            bit_width=bit_width,
-                            absolute_act_val=absolute_act_val,
-                            activation_inner_scaling_per_output_channel=activation_inner_scaling_per_output_channel,
-                            activation_other_scaling_per_output_channel=activation_other_scaling_per_output_channel,
-                            weight_scaling_per_output_channel=weight_scaling_per_output_channel),
-                            )
+                JasperBlock(
+                    feat_in,
+                    lcfg['filters'],
+                    repeat=lcfg['repeat'],
+                    kernel_size=lcfg['kernel'],
+                    stride=lcfg['stride'],
+                    dilation=lcfg['dilation'],
+                    dropout=lcfg['dropout'],
+                    residual=lcfg['residual'],
+                    groups=groups,
+                    fused_bn=fused_bn,
+                    separable=separable,
+                    heads=heads,
+                    residual_mode=residual_mode,
+                    normalization=normalization_mode,
+                    norm_groups=norm_groups,
+                    activation=activation,
+                    residual_panes=dense_res,
+                    conv_mask=conv_mask,
+                    bit_width=bit_width,
+                    absolute_act_val=absolute_act_val,
+                    activation_inner_scaling_per_output_channel=
+                    activation_inner_scaling_per_output_channel,
+                    activation_other_scaling_per_output_channel=
+                    activation_other_scaling_per_output_channel,
+                    weight_scaling_per_output_channel=weight_scaling_per_output_channel),)
             feat_in = lcfg['filters']
 
         self.encoder = nn.Sequential(*encoder_layers)
         self.apply(lambda x: init_weights(x, mode=init_mode))
         # self.to(self._device)
 
     def forward(self, audio_signal, length=None):
@@ -180,38 +183,44 @@
         init_mode (str): Describes how neural network parameters are
             initialized. Options are ['xavier_uniform', 'xavier_normal',
             'kaiming_uniform','kaiming_normal'].
             Defaults to "xavier_uniform".
     """
 
     def __init__(
-            self, *,
+            self,
+            *,
             feat_in,
             num_classes,
             bit_width,
             weight_scaling_per_channel,
             init_mode="xavier_uniform",
-            **kwargs
-    ):
+            **kwargs):
         nn.Module.__init__(self)
 
         self._feat_in = feat_in
         # Add 1 for blank char
         self._num_classes = num_classes + 1
 
         self.decoder_layers = nn.Sequential(
-            make_quantconv1d(self._feat_in, self._num_classes, kernel_size=1,bias=True, bit_width=bit_width,
-                             scaling_per_channel=weight_scaling_per_channel))
+            make_quantconv1d(
+                self._feat_in,
+                self._num_classes,
+                kernel_size=1,
+                bias=True,
+                bit_width=bit_width,
+                scaling_per_channel=weight_scaling_per_channel))
         self.apply(lambda x: init_weights(x, mode=init_mode))
 
     def forward(self, encoder_output):
-        return F.log_softmax(self.decoder_layers(encoder_output).
-                             transpose(1, 2), dim=-1)
+        return F.log_softmax(self.decoder_layers(encoder_output).transpose(1, 2), dim=-1)
+
 
 class Quartznet(nn.Module):
+
     def __init__(self, preprocessing, encoder, decoder, greedyctcdecoder):
         super(Quartznet, self).__init__()
         self.preprocessing = preprocessing
         self.encoder = encoder
         self.decoder = decoder
         self.greedy_ctc_decoder = greedyctcdecoder
 
@@ -236,33 +245,36 @@
         print("Checkpoint restored")
 
 
 def quartznet(cfg, quartzet_params, export_mode):
 
     outer_bit_width = cfg.getint('QUANT', 'OUTER_LAYERS_BIT_WIDTH')
     inner_bit_width = cfg.getint('QUANT', 'INNER_LAYERS_BIT_WIDTH')
-    activation_inner_scaling_per_output_channel = cfg.getboolean('ACTIVATIONS', 'INNER_SCALING_PER_CHANNEL')
-    activation_other_scaling_per_output_channel = cfg.getboolean('ACTIVATIONS', 'OTHER_SCALING_PER_CHANNEL')
+    activation_inner_scaling_per_output_channel = cfg.getboolean(
+        'ACTIVATIONS', 'INNER_SCALING_PER_CHANNEL')
+    activation_other_scaling_per_output_channel = cfg.getboolean(
+        'ACTIVATIONS', 'OTHER_SCALING_PER_CHANNEL')
     absolute_act_val = cfg.getint('ACTIVATIONS', 'ABS_ACT_VAL')
-    encoder_weight_scaling_per_output_channel = cfg.getboolean('WEIGHT', 'ENCODER_SCALING_PER_OUTPUT_CHANNEL')
-    decoder_weight_scaling_per_output_channel = cfg.getboolean('WEIGHT', 'DECODER_SCALING_PER_OUTPUT_CHANNEL')
+    encoder_weight_scaling_per_output_channel = cfg.getboolean(
+        'WEIGHT', 'ENCODER_SCALING_PER_OUTPUT_CHANNEL')
+    decoder_weight_scaling_per_output_channel = cfg.getboolean(
+        'WEIGHT', 'DECODER_SCALING_PER_OUTPUT_CHANNEL')
     fused_bn = cfg.getboolean('QUANT', 'FUSED_BN')
 
     vocab = quartzet_params['labels']
     sample_rate = quartzet_params['sample_rate']
     feat_in_encoder = quartzet_params["AudioToMelSpectrogramPreprocessor"]["features"]
     feat_in_decoder = quartzet_params["JasperEncoder"]["jasper"][-1]["filters"]
 
     if export_mode:
         quartzet_params['JasperEncoder']['conv_mask'] = False  # no conv masking in export mode
         data_preprocessor = None  # no built in preprocessing in export mode
     else:
         data_preprocessor = AudioToMelSpectrogramPreprocessor(
-        sample_rate=sample_rate,
-        **quartzet_params["AudioToMelSpectrogramPreprocessor"])
+            sample_rate=sample_rate, **quartzet_params["AudioToMelSpectrogramPreprocessor"])
 
     encoder = JasperEncoder(
         feat_in=feat_in_encoder,
         weight_scaling_per_output_channel=encoder_weight_scaling_per_output_channel,
         inner_bit_width=inner_bit_width,
         outer_bit_width=outer_bit_width,
         absolute_act_val=absolute_act_val,
@@ -277,8 +289,7 @@
         weight_scaling_per_channel=decoder_weight_scaling_per_output_channel,
         num_classes=len(vocab))
 
     greedy_decoder = GreedyCTCDecoder()
 
     model = Quartznet(data_preprocessor, encoder, decoder, greedy_decoder)
     return model
-
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/speech_to_text/quartznet_val.py` & `brevitas-0.9.0/src/brevitas_examples/speech_to_text/quartznet_val.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,35 +1,33 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
 import argparse
 import copy
 import os
+import random
 
 from ruamel.yaml import YAML
-import random
 import torch
+import torch.backends.cudnn as cudnn
 
 from brevitas_examples.speech_to_text.quartznet import AudioToTextDataLayer
+from brevitas_examples.speech_to_text.quartznet import model_with_cfg
 from brevitas_examples.speech_to_text.quartznet.helpers import post_process_predictions
 from brevitas_examples.speech_to_text.quartznet.helpers import post_process_transcripts
 from brevitas_examples.speech_to_text.quartznet.helpers import word_error_rate
-import torch.backends.cudnn as cudnn
-from brevitas_examples.speech_to_text.quartznet import model_with_cfg
 
 SEED = 123456
 
-
-
 parser = argparse.ArgumentParser(description='Quartznet')
 parser.add_argument("--data-json", type=str, required=True)
 parser.add_argument("--gpu", type=int, required=False, help='GPU number')
 parser.add_argument('--pretrained', action='store_true', help='Load pretrained checkpoint')
-parser.add_argument('--model', type=str, default='quant_quartznet_perchannelscaling_4b', help='Name of the model')
+parser.add_argument(
+    '--model', type=str, default='quant_quartznet_perchannelscaling_4b', help='Name of the model')
 parser.add_argument('--batch-size', default=64, type=int, help='Batch size')
 
 
 def main():
     random.seed(SEED)
     torch.manual_seed(SEED)
 
@@ -63,33 +61,29 @@
     # Set Eval mode
     model.eval()
 
     encoder_weights = sum(p.numel() for p in model.encoder.parameters() if p.requires_grad)
     decoder_weights = sum(p.numel() for p in model.decoder.parameters() if p.requires_grad)
 
     print('================================')
-    print(
-        f"Number of parameters in encoder: {encoder_weights}")
-    print(
-        f"Number of parameters in decoder: {decoder_weights}")
-    print(
-        f"Total number of parameters in decoder: "
-        f"{encoder_weights + decoder_weights}")
+    print(f"Number of parameters in encoder: {encoder_weights}")
+    print(f"Number of parameters in decoder: {decoder_weights}")
+    print(f"Total number of parameters in decoder: "
+          f"{encoder_weights + decoder_weights}")
     print('================================')
 
-
     if args.gpu is not None:
         cudnn.benchmark = True
         torch.cuda.set_device(args.gpu)
         loc = 'cuda:{}'.format(args.gpu)
     else:
         loc = 'cpu'
     print(f'Running on device: {loc}')
     model.to(loc)
-    
+
     predictions = []
     transcripts = []
     transcripts_len = []
     with torch.no_grad():
         for data in data_layer.data_iterator:
             tensors = []
             for d in data:
@@ -100,12 +94,12 @@
             predictions.append(predictions_e1)
             transcripts.append(transcript_e1)
             transcripts_len.append(transcript_len_e1)
 
         greedy_hypotheses = post_process_predictions(predictions, vocab)
         references = post_process_transcripts(transcripts, transcripts_len, vocab)
         wer = word_error_rate(hypotheses=greedy_hypotheses, references=references)
-        print("Greedy WER {:.2f}%".format(wer*100))
+        print("Greedy WER {:.2f}%".format(wer * 100))
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/text_to_speech/README.md` & `brevitas-0.9.0/src/brevitas_examples/text_to_speech/README.md`

 * *Files 5% similar despite different names*

```diff
@@ -17,14 +17,14 @@
 find /path/to/LJSpeech/LJSpeech-1.1/wavs -type f | head -10 | xargs cp -t /path/to/validation/folder
 ```
 
 To evaluate a pretrained quantized model on LJSpeech1.1:
 
  - After cloning the repository, install Brevitas and MelGAN requirements with `pip install .[tts]`
  - Preprocess the dataset with `brevitas_melgan_preprocess --name quant_melgan_8b --data-path /path/to/validation/folder`
- - Pass the name of the model as an input to the evaluation script. The required checkpoint will be downloaded automatically. 
- 
+ - Pass the name of the model as an input to the evaluation script. The required checkpoint will be downloaded automatically.
+
  For example, for the evaluation on GPU 0:
 
 ```
 brevitas_melgan_val --input-folder /path/to/validation/folder --model quant_melgan_8b --gpu 0 --pretrained
 ```
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/text_to_speech/melgan/__init__.py` & `brevitas-0.9.0/src/brevitas_examples/text_to_speech/melgan/__init__.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,21 +1,19 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-import os
 from configparser import ConfigParser
+import os
 
 from torch import hub
 
 from .melgan import *
 
 model_impl = {
-    'melgan': melgan,
-}
+    'melgan': melgan,}
 
 
 def model_with_cfg(name, pretrained):
     cfg = ConfigParser()
     current_dir = os.path.dirname(os.path.abspath(__file__))
     config_path = os.path.join(current_dir, '..', 'cfg', name + '.ini')
     assert os.path.exists(config_path)
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/text_to_speech/melgan/common.py` & `brevitas-0.9.0/src/brevitas_examples/text_to_speech/melgan/common.py`

 * *Files 12% similar despite different names*

```diff
@@ -24,19 +24,20 @@
 FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."""
 
-import brevitas.nn as quant_nn
+import torch.nn as nn
+
 from brevitas.core.quant import QuantType
 from brevitas.core.scaling import ScalingImplType
 from brevitas.core.stats import StatsOp
-import torch.nn as nn
+import brevitas.nn as quant_nn
 
 QUANT_TYPE = QuantType.INT
 QUANT_TYPE_BIAS = QuantType.FP
 
 SCALING_MIN_VAL = 2e-9
 ACT_SCALING_IMPL_TYPE = ScalingImplType.CONST
 ACT_SCALING_PER_CHANNEL = False
@@ -46,87 +47,95 @@
 WEIGHT_SCALING_IMPL_TYPE = ScalingImplType.PARAMETER_FROM_STATS
 WEIGHT_SCALING_STATS_OP = StatsOp.MAX
 WEIGHT_NARROW_RANGE = True
 BIAS_CONFIGS = False
 
 
 class Identity(nn.Module):
+
     def __init__(self):
         super(Identity, self).__init__()
 
     def forward(self, x):
         return x
 
 
-def make_quantconv1d(feat_in, feat_out, kernel_size, stride, padding, bit_width, dilation=1, group=1):
-    return quant_nn.QuantConv1d(in_channels=feat_in, out_channels=feat_out, kernel_size=kernel_size,
-                                stride=stride,
-                                padding=padding,
-                                dilation=dilation,
-                                groups=group,
-                                weight_bit_width=bit_width,
-                                weight_quant_type=QUANT_TYPE,
-                                weight_narrow_range=WEIGHT_NARROW_RANGE,
-                                weight_scaling_impl_type=WEIGHT_SCALING_IMPL_TYPE,
-                                weight_scaling_stats_op=WEIGHT_SCALING_STATS_OP,
-                                weight_scaling_min_val=SCALING_MIN_VAL,
-                                bias_bit_width=bit_width,
-                                bias_quant_type=QUANT_TYPE_BIAS,
-                                bias_narrow_range=BIAS_CONFIGS,
-                                compute_output_scale=BIAS_CONFIGS,
-                                compute_output_bit_width=BIAS_CONFIGS,
-                                return_quant_tensor=False)
+def make_quantconv1d(
+        feat_in, feat_out, kernel_size, stride, padding, bit_width, dilation=1, group=1):
+    return quant_nn.QuantConv1d(
+        in_channels=feat_in,
+        out_channels=feat_out,
+        kernel_size=kernel_size,
+        stride=stride,
+        padding=padding,
+        dilation=dilation,
+        groups=group,
+        weight_bit_width=bit_width,
+        weight_quant_type=QUANT_TYPE,
+        weight_narrow_range=WEIGHT_NARROW_RANGE,
+        weight_scaling_impl_type=WEIGHT_SCALING_IMPL_TYPE,
+        weight_scaling_stats_op=WEIGHT_SCALING_STATS_OP,
+        weight_scaling_min_val=SCALING_MIN_VAL,
+        bias_bit_width=bit_width,
+        bias_quant_type=QUANT_TYPE_BIAS,
+        bias_narrow_range=BIAS_CONFIGS,
+        compute_output_scale=BIAS_CONFIGS,
+        compute_output_bit_width=BIAS_CONFIGS,
+        return_quant_tensor=False)
 
 
 def make_transpconv1d(feat_in, feat_out, kernel_size, stride, padding, bit_width, dilation=1):
-    return quant_nn.QuantConvTranspose1d(in_channels=feat_in, out_channels=feat_out, kernel_size=kernel_size,
-                                         stride=stride,
-                                         padding=padding,
-                                         dilation=dilation,
-                                         weight_bit_width=bit_width,
-                                         weight_quant_type=QUANT_TYPE,
-                                         weight_narrow_range=WEIGHT_NARROW_RANGE,
-                                         weight_scaling_impl_type=WEIGHT_SCALING_IMPL_TYPE,
-                                         weight_scaling_stats_op=WEIGHT_SCALING_STATS_OP,
-                                         weight_scaling_min_val=SCALING_MIN_VAL,
-                                         bias_bit_width=bit_width,
-                                         bias_quant_type=QUANT_TYPE_BIAS,
-                                         bias_narrow_range=BIAS_CONFIGS,
-                                         compute_output_scale=BIAS_CONFIGS,
-                                         compute_output_bit_width=BIAS_CONFIGS,
-                                         return_quant_tensor=False)
+    return quant_nn.QuantConvTranspose1d(
+        in_channels=feat_in,
+        out_channels=feat_out,
+        kernel_size=kernel_size,
+        stride=stride,
+        padding=padding,
+        dilation=dilation,
+        weight_bit_width=bit_width,
+        weight_quant_type=QUANT_TYPE,
+        weight_narrow_range=WEIGHT_NARROW_RANGE,
+        weight_scaling_impl_type=WEIGHT_SCALING_IMPL_TYPE,
+        weight_scaling_stats_op=WEIGHT_SCALING_STATS_OP,
+        weight_scaling_min_val=SCALING_MIN_VAL,
+        bias_bit_width=bit_width,
+        bias_quant_type=QUANT_TYPE_BIAS,
+        bias_narrow_range=BIAS_CONFIGS,
+        compute_output_scale=BIAS_CONFIGS,
+        compute_output_bit_width=BIAS_CONFIGS,
+        return_quant_tensor=False)
 
 
 def make_relu_activation(bit_width):
-    return quant_nn.QuantReLU(bit_width=bit_width,
-                              max_val=ACT_MAX_VAL,
-                              quant_type=QUANT_TYPE,
-                              scaling_impl_type=ACT_SCALING_IMPL_TYPE,
-                              scaling_min_val=SCALING_MIN_VAL,
-                              return_quant_tensor=False
-                              )
+    return quant_nn.QuantReLU(
+        bit_width=bit_width,
+        max_val=ACT_MAX_VAL,
+        quant_type=QUANT_TYPE,
+        scaling_impl_type=ACT_SCALING_IMPL_TYPE,
+        scaling_min_val=SCALING_MIN_VAL,
+        return_quant_tensor=False)
 
 
 def make_hardtanh_activation(bit_width, return_quant_tensor=False):
-    return quant_nn.QuantHardTanh(bit_width=bit_width,
-                                  max_val=ACT_MAX_VAL,
-                                  min_val=ACT_MIN_VAL,
-                                  quant_type=QUANT_TYPE,
-                                  scaling_impl_type=ACT_SCALING_IMPL_TYPE,
-                                  scaling_min_val=SCALING_MIN_VAL,
-                                  return_quant_tensor=return_quant_tensor
-                                  )
+    return quant_nn.QuantHardTanh(
+        bit_width=bit_width,
+        max_val=ACT_MAX_VAL,
+        min_val=ACT_MIN_VAL,
+        quant_type=QUANT_TYPE,
+        scaling_impl_type=ACT_SCALING_IMPL_TYPE,
+        scaling_min_val=SCALING_MIN_VAL,
+        return_quant_tensor=return_quant_tensor)
 
 
 def make_tanh_activation(bit_width):
-    return quant_nn.QuantTanh(bit_width=bit_width,
-                              quant_type=QUANT_TYPE,
-                              scaling_min_val=SCALING_MIN_VAL,
-                              return_quant_tensor=False
-                              )
+    return quant_nn.QuantTanh(
+        bit_width=bit_width,
+        quant_type=QUANT_TYPE,
+        scaling_min_val=SCALING_MIN_VAL,
+        return_quant_tensor=False)
 
 
 def make_leakyRelu_activation(bit_width):
     el1 = nn.LeakyReLU()
     el2 = make_hardtanh_activation(bit_width=bit_width)
     layer = nn.Sequential(el1, el2)
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/text_to_speech/melgan/generator_brevitas.py` & `brevitas-0.9.0/src/brevitas_examples/text_to_speech/melgan/generator_brevitas.py`

 * *Files 8% similar despite different names*

```diff
@@ -27,54 +27,50 @@
 DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."""
 
 import torch
-from .res_stack_brevitas import ResStack
+
 from .common import *
+from .res_stack_brevitas import ResStack
 
 MAX_WAV_VALUE = 32768.0
 
 
 class Generator(nn.Module):
+
     def __init__(self, mel_channel, bit_width, last_layer_bit_width):
         super(Generator, self).__init__()
         self.mel_channel = mel_channel
 
         self.generator = nn.Sequential(
-            nn.utils.weight_norm(make_quantconv1d(mel_channel, 512, kernel_size=7, stride=1, padding=3,
-                                                  bit_width=bit_width)),
-
+            nn.utils.weight_norm(
+                make_quantconv1d(
+                    mel_channel, 512, kernel_size=7, stride=1, padding=3, bit_width=bit_width)),
             make_leakyRelu_activation(bit_width=bit_width),
-
-            nn.utils.weight_norm(make_transpconv1d(512, 256, kernel_size=16, stride=8, padding=4,
-                                                   bit_width=bit_width)),
-
+            nn.utils.weight_norm(
+                make_transpconv1d(
+                    512, 256, kernel_size=16, stride=8, padding=4, bit_width=bit_width)),
             ResStack(256, bit_width=bit_width),
-
             make_leakyRelu_activation(bit_width),
-            nn.utils.weight_norm(make_transpconv1d(256, 128, kernel_size=16, stride=8, padding=4,
-                                                   bit_width=bit_width)),
-
+            nn.utils.weight_norm(
+                make_transpconv1d(
+                    256, 128, kernel_size=16, stride=8, padding=4, bit_width=bit_width)),
             ResStack(128, bit_width=bit_width),
-
             make_leakyRelu_activation(bit_width),
-            nn.utils.weight_norm(make_transpconv1d(128, 64, kernel_size=4, stride=2, padding=1,
-                                                   bit_width=bit_width)),
-
+            nn.utils.weight_norm(
+                make_transpconv1d(128, 64, kernel_size=4, stride=2, padding=1,
+                                  bit_width=bit_width)),
             ResStack(64, bit_width=bit_width),
-
             make_leakyRelu_activation(bit_width),
             nn.utils.weight_norm(
                 make_transpconv1d(64, 32, kernel_size=4, stride=2, padding=1, bit_width=bit_width)),
-
             ResStack(32, bit_width=bit_width),
-
             make_leakyRelu_activation(bit_width),
             nn.utils.weight_norm(
                 make_quantconv1d(32, 1, kernel_size=7, stride=1, padding=3, bit_width=bit_width)),
             make_tanh_activation(bit_width=last_layer_bit_width),
         )
 
     def forward(self, mel):
@@ -110,15 +106,15 @@
         audio = audio.clamp(min=-MAX_WAV_VALUE, max=MAX_WAV_VALUE - 1)
         audio = audio.short()
 
         return audio
 
 
 '''
-    to run this, fix 
+    to run this, fix
     from . import ResStack
     into
     from res_stack import ResStack
 '''
 if __name__ == '__main__':
     model = Generator(7)
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/text_to_speech/melgan/melgan.py` & `brevitas-0.9.0/src/brevitas_examples/text_to_speech/melgan/melgan.py`

 * *Files identical despite different names*

### Comparing `brevitas-0.8.0/src/brevitas_examples/text_to_speech/melgan/res_stack_brevitas.py` & `brevitas-0.9.0/src/brevitas_examples/text_to_speech/melgan/res_stack_brevitas.py`

 * *Files 9% similar despite different names*

```diff
@@ -26,35 +26,47 @@
 FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."""
 
-from .common import *
 from brevitas.quant_tensor import QuantTensor
 
+from .common import *
+
 
 class ResStack(nn.Module):
+
     def __init__(self, channel, bit_width):
         super(ResStack, self).__init__()
         self.scale_norm = make_hardtanh_activation(bit_width=bit_width, return_quant_tensor=True)
         self.layers = nn.ModuleList([
             nn.Sequential(
                 make_leakyRelu_activation(bit_width),
-
-                nn.utils.weight_norm(make_quantconv1d(channel, channel, kernel_size=3, stride=1, padding=3 ** i,
-                                                      dilation=3 ** i, bit_width=bit_width)),
-
+                nn.utils.weight_norm(
+                    make_quantconv1d(
+                        channel,
+                        channel,
+                        kernel_size=3,
+                        stride=1,
+                        padding=3 ** i,
+                        dilation=3 ** i,
+                        bit_width=bit_width)),
                 make_leakyRelu_activation(bit_width),
-                nn.utils.weight_norm(make_quantconv1d(channel, channel, kernel_size=3, stride=1, padding=1,
-                                                      dilation=1, bit_width=bit_width)),
-            )
-            for i in range(3)
-        ])
+                nn.utils.weight_norm(
+                    make_quantconv1d(
+                        channel,
+                        channel,
+                        kernel_size=3,
+                        stride=1,
+                        padding=1,
+                        dilation=1,
+                        bit_width=bit_width)),
+            ) for i in range(3)])
 
     def forward(self, x):
         for layer in self.layers:
             x = self.scale_norm(x)
             if isinstance(x, QuantTensor):
                 x_unp, _, _ = x
             else:
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/text_to_speech/melgan_val.py` & `brevitas-0.9.0/src/brevitas_examples/text_to_speech/melgan_val.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,34 +1,36 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-import glob
-import tqdm
-import torch
 import argparse
-from scipy.io.wavfile import write
+import glob
 
+from scipy.io.wavfile import write
+import torch
 import torch.backends.cudnn as cudnn
+import tqdm
+
 import brevitas.config
+
 from .melgan import model_with_cfg
 
 brevitas.config.IGNORE_MISSING_KEYS = False
 MAX_WAV_VALUE = 32768.0
-import random
 import os
+import random
 
 SEED = 123456
 
 parser = argparse.ArgumentParser()
 parser.add_argument('--input-folder', help='path to folder containing the val folder')
 parser.add_argument('--workers', default=32, type=int, help='number of data loading workers')
 parser.add_argument('--batch-size', default=16, type=int, help='Minibatch size')
 parser.add_argument('--gpu', default=None, type=int, help='GPU id to use.')
-parser.add_argument('--pretrained', action='store_true', default=True, help='Load pretrained checkpoint')
+parser.add_argument(
+    '--pretrained', action='store_true', default=True, help='Load pretrained checkpoint')
 parser.add_argument('--model', type=str, default='quant_melgan_8b', help='Name of the model')
 
 
 def main():
     args = parser.parse_args()
     random.seed(SEED)
     torch.manual_seed(SEED)
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/text_to_speech/preprocess_dataset.py` & `brevitas-0.9.0/src/brevitas_examples/text_to_speech/preprocess_dataset.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,67 +1,70 @@
 # Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 
-
-import os
-import glob
-import tqdm
-import torch
 import argparse
-import numpy as np
 from configparser import ConfigParser
+import glob
+import os
+
+import numpy as np
+import torch
+import tqdm
 
-from .utilities.stft import TacotronSTFT
 from .utilities.audio_processing import read_wav_np
+from .utilities.stft import TacotronSTFT
 
 
 def preprocess(cfg, args):
     filter_length = cfg.getint('AUDIO', 'filter_length')
     hop_length = cfg.getint('AUDIO', 'hop_length')
     win_length = cfg.getint('AUDIO', 'win_length')
     n_mel_channels = cfg.getint('AUDIO', 'n_mel_channels')
     sampling_rate = cfg.getint('AUDIO', 'sampling_rate')
     mel_fmin = cfg.getfloat('AUDIO', 'mel_fmin')
     mel_fmax = cfg.getfloat('AUDIO', 'mel_fmax')
 
     segment_length = cfg.getint('AUDIO', 'segment_length')
     pad_short = cfg.getint('AUDIO', 'pad_short')
 
-    stft = TacotronSTFT(filter_length=filter_length,
-                        hop_length=hop_length,
-                        win_length=win_length,
-                        n_mel_channels=n_mel_channels,
-                        sampling_rate=sampling_rate,
-                        mel_fmin=mel_fmin,
-                        mel_fmax=mel_fmax)
+    stft = TacotronSTFT(
+        filter_length=filter_length,
+        hop_length=hop_length,
+        win_length=win_length,
+        n_mel_channels=n_mel_channels,
+        sampling_rate=sampling_rate,
+        mel_fmin=mel_fmin,
+        mel_fmax=mel_fmax)
 
     wav_files = glob.glob(os.path.join(args.data_path, '**', '*.wav'), recursive=True)
 
     for wavpath in tqdm.tqdm(wav_files, desc='preprocess wav to mel'):
         sr, wav = read_wav_np(wavpath)
         assert sr == sampling_rate, \
             "sample rate mismatch. expected %d, got %d at %s" % \
             (sampling_rate, sr, wavpath)
 
         if len(wav) < segment_length + pad_short:
-            wav = np.pad(wav, (0, segment_length + pad_short - len(wav)), mode='constant', constant_values=0.0)
+            wav = np.pad(
+                wav, (0, segment_length + pad_short - len(wav)),
+                mode='constant',
+                constant_values=0.0)
 
         wav = torch.from_numpy(wav).unsqueeze(0)
         mel = stft.mel_spectrogram(wav)
 
         melpath = wavpath.replace('.wav', '.mel')
         torch.save(mel, melpath)
 
 
 def main():
     parser = argparse.ArgumentParser()
-    parser.add_argument('-n', '--name', type=str, required=True,
-                        help="name of the model")
-    parser.add_argument('-d', '--data-path', type=str, required=True,
-                        help="root directory of wav files")
+    parser.add_argument('-n', '--name', type=str, required=True, help="name of the model")
+    parser.add_argument(
+        '-d', '--data-path', type=str, required=True, help="root directory of wav files")
     args = parser.parse_args()
     cfg = ConfigParser()
     current_dir = os.path.dirname(os.path.abspath(__file__))
     config_path = os.path.join(current_dir, 'cfg', args.name + '.ini')
     assert os.path.exists(config_path)
     cfg.read(config_path)
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/text_to_speech/utilities/audio_processing.py` & `brevitas-0.9.0/src/brevitas_examples/text_to_speech/utilities/audio_processing.py`

 * *Files 2% similar despite different names*

```diff
@@ -25,23 +25,23 @@
 DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
 FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."""
-import torch
-import numpy as np
-from scipy.signal import get_window
 import librosa.util as librosa_util
+import numpy as np
 from scipy.io.wavfile import read
+from scipy.signal import get_window
+import torch
 
 
-def window_sumsquare(window, n_frames, hop_length=200, win_length=800,
-                     n_fft=800, dtype=np.float32, norm=None):
+def window_sumsquare(
+        window, n_frames, hop_length=200, win_length=800, n_fft=800, dtype=np.float32, norm=None):
     """
     # from librosa 0.6
     Compute the sum-square envelope of a window function at a given hop length.
 
     This is used to estimate modulation effects induced by windowing
     observations in short-time fourier transforms.
 
@@ -74,15 +74,15 @@
         win_length = n_fft
 
     n = n_fft + hop_length * (n_frames - 1)
     x = np.zeros(n, dtype=dtype)
 
     # Compute the squared window at the desired length
     win_sq = get_window(window, win_length, fftbins=True)
-    win_sq = librosa_util.normalize(win_sq, norm=norm)**2
+    win_sq = librosa_util.normalize(win_sq, norm=norm) ** 2
     win_sq = librosa_util.pad_center(win_sq, n_fft)
 
     # Fill the envelope
     for i in range(n_frames):
         sample = i * hop_length
         x[sample:min(n, sample + n_fft)] += win_sq[:max(0, min(n_fft, n - sample))]
     return x
```

### Comparing `brevitas-0.8.0/src/brevitas_examples/text_to_speech/utilities/stft.py` & `brevitas-0.9.0/src/brevitas_examples/text_to_speech/utilities/stft.py`

 * *Files 6% similar despite different names*

```diff
@@ -28,47 +28,50 @@
 (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
 ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 """
 
-import torch
+from librosa.filters import mel as librosa_mel_fn
+from librosa.util import pad_center
+from librosa.util import tiny
 import numpy as np
-import torch.nn.functional as F
-from torch.autograd import Variable
 from scipy.signal import get_window
-from librosa.util import pad_center, tiny
-from .audio_processing import window_sumsquare, dynamic_range_compression, dynamic_range_decompression
-from librosa.filters import mel as librosa_mel_fn
+import torch
+from torch.autograd import Variable
+import torch.nn.functional as F
+
+from .audio_processing import dynamic_range_compression
+from .audio_processing import dynamic_range_decompression
+from .audio_processing import window_sumsquare
 
 
 class STFT(torch.nn.Module):
     """adapted from Prem Seetharaman's https://github.com/pseeth/pytorch-stft"""
-    def __init__(self, filter_length=800, hop_length=200, win_length=800,
-                 window='hann'):
+
+    def __init__(self, filter_length=800, hop_length=200, win_length=800, window='hann'):
         super(STFT, self).__init__()
         self.filter_length = filter_length
         self.hop_length = hop_length
         self.win_length = win_length
         self.window = window
         self.forward_transform = None
         scale = self.filter_length / self.hop_length
         fourier_basis = np.fft.fft(np.eye(self.filter_length))
 
         cutoff = int((self.filter_length / 2 + 1))
-        fourier_basis = np.vstack([np.real(fourier_basis[:cutoff, :]),
-                                   np.imag(fourier_basis[:cutoff, :])])
+        fourier_basis = np.vstack([
+            np.real(fourier_basis[:cutoff, :]), np.imag(fourier_basis[:cutoff, :])])
 
         forward_basis = torch.FloatTensor(fourier_basis[:, None, :])
-        inverse_basis = torch.FloatTensor(
-            np.linalg.pinv(scale * fourier_basis).T[:, None, :])
+        inverse_basis = torch.FloatTensor(np.linalg.pinv(scale * fourier_basis).T[:, None, :])
 
         if window is not None:
-            assert(filter_length >= win_length)
+            assert (filter_length >= win_length)
             # get window and zero center pad it to filter_length
             fft_window = get_window(window, win_length, fftbins=True)
             fft_window = pad_center(fft_window, filter_length)
             fft_window = torch.from_numpy(fft_window).float()
 
             # window the bases
             forward_basis *= fft_window
@@ -105,67 +108,73 @@
                 stride=self.hop_length,
                 padding=0).cpu()
 
         cutoff = int((self.filter_length / 2) + 1)
         real_part = forward_transform[:, :cutoff, :]
         imag_part = forward_transform[:, cutoff:, :]
 
-        magnitude = torch.sqrt(real_part**2 + imag_part**2)
-        phase = torch.autograd.Variable(
-            torch.atan2(imag_part.data, real_part.data))
+        magnitude = torch.sqrt(real_part ** 2 + imag_part ** 2)
+        phase = torch.autograd.Variable(torch.atan2(imag_part.data, real_part.data))
 
         return magnitude, phase
 
     def inverse(self, magnitude, phase):
         recombine_magnitude_phase = torch.cat(
-            [magnitude*torch.cos(phase), magnitude*torch.sin(phase)], dim=1)
+            [magnitude * torch.cos(phase), magnitude * torch.sin(phase)], dim=1)
 
         inverse_transform = F.conv_transpose1d(
             recombine_magnitude_phase,
             Variable(self.inverse_basis, requires_grad=False),
             stride=self.hop_length,
             padding=0)
 
         if self.window is not None:
             window_sum = window_sumsquare(
-                self.window, magnitude.size(-1), hop_length=self.hop_length,
-                win_length=self.win_length, n_fft=self.filter_length,
+                self.window,
+                magnitude.size(-1),
+                hop_length=self.hop_length,
+                win_length=self.win_length,
+                n_fft=self.filter_length,
                 dtype=np.float32)
             # remove modulation effects
-            approx_nonzero_indices = torch.from_numpy(
-                np.where(window_sum > tiny(window_sum))[0])
-            window_sum = torch.autograd.Variable(
-                torch.from_numpy(window_sum), requires_grad=False)
+            approx_nonzero_indices = torch.from_numpy(np.where(window_sum > tiny(window_sum))[0])
+            window_sum = torch.autograd.Variable(torch.from_numpy(window_sum), requires_grad=False)
             window_sum = window_sum.cuda() if magnitude.is_cuda else window_sum
             inverse_transform[:, :, approx_nonzero_indices] /= window_sum[approx_nonzero_indices]
 
             # scale by hop ratio
             inverse_transform *= float(self.filter_length) / self.hop_length
 
-        inverse_transform = inverse_transform[:, :, int(self.filter_length/2):]
-        inverse_transform = inverse_transform[:, :, :-int(self.filter_length/2):]
+        inverse_transform = inverse_transform[:, :, int(self.filter_length / 2):]
+        inverse_transform = inverse_transform[:, :, :-int(self.filter_length / 2):]
 
         return inverse_transform
 
     def forward(self, input_data):
         self.magnitude, self.phase = self.transform(input_data)
         reconstruction = self.inverse(self.magnitude, self.phase)
         return reconstruction
 
 
 class TacotronSTFT(torch.nn.Module):
-    def __init__(self, filter_length=1024, hop_length=256, win_length=1024,
-                 n_mel_channels=80, sampling_rate=22050, mel_fmin=0.0,
-                 mel_fmax=None):
+
+    def __init__(
+            self,
+            filter_length=1024,
+            hop_length=256,
+            win_length=1024,
+            n_mel_channels=80,
+            sampling_rate=22050,
+            mel_fmin=0.0,
+            mel_fmax=None):
         super(TacotronSTFT, self).__init__()
         self.n_mel_channels = n_mel_channels
         self.sampling_rate = sampling_rate
         self.stft_fn = STFT(filter_length, hop_length, win_length)
-        mel_basis = librosa_mel_fn(
-            sampling_rate, filter_length, n_mel_channels, mel_fmin, mel_fmax)
+        mel_basis = librosa_mel_fn(sampling_rate, filter_length, n_mel_channels, mel_fmin, mel_fmax)
         mel_basis = torch.from_numpy(mel_basis).float()
         self.register_buffer('mel_basis', mel_basis)
 
     def spectral_normalize(self, magnitudes):
         output = dynamic_range_compression(magnitudes)
         return output
 
@@ -179,15 +188,15 @@
         ------
         y: Variable(torch.FloatTensor) with shape (B, T) in range [-1, 1]
 
         RETURNS
         -------
         mel_output: torch.FloatTensor of shape (B, n_mel_channels, T)
         """
-        assert(torch.min(y.data) >= -1)
-        assert(torch.max(y.data) <= 1)
+        assert (torch.min(y.data) >= -1)
+        assert (torch.max(y.data) <= 1)
 
         magnitudes, phases = self.stft_fn.transform(y)
         magnitudes = magnitudes.data
         mel_output = torch.matmul(self.mel_basis, magnitudes)
         mel_output = self.spectral_normalize(mel_output)
         return mel_output
```

